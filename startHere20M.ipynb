{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":15901,"status":"ok","timestamp":1769186941491,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"},"user_tz":360},"id":"aftd1RezTYnF","outputId":"dc6f36a6-d38c-4daf-c11f-c4b478569058"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/addition\n","configuration_files\t       model_rope.py\n","configurator.py\t\t       model_t5bias.py\n","data\t\t\t       __pycache__\n","data_generate.py\t       README.md\n","data_generation_script\t       result_analysis.ipynb\n","eval_ckpt.py\t\t       result_analysis.py\n","evaluation_ckpt.py\t       results\n","evaluation_comparison.py       startHere100M.ipynb\n","evaluation.py\t\t       startHere1B.ipynb\n","extra_result_analysis_scripts  startHere20M.ipynb\n","legacy_code\t\t       startHere.ipynb\n","main_utilities.py\t       statistical_measurements.py\n","meta_all_ascii_chars.pkl       train.py\n","model.py\t\t       train_work_perfect_per_example_batch.py\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change directory to your code\n","%cd /content/drive/MyDrive/addition\n","%pwd   # verify you‚Äôre in the right place\n","!ls    # should show train.py, 4_operands_addition.txt, etc."]},{"cell_type":"markdown","metadata":{"id":"lbEXozJBCm_X"},"source":["# Quick Start Example\n","## Choose One Task and Start Training"]},{"cell_type":"markdown","metadata":{"id":"fUKKZHtTI1n3"},"source":["#### Generate Addition Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbKQwH__Bq4E"},"outputs":[],"source":["!python data_generate.py --task addition --num_operands 2 --experiment_name 2_operands_0_to_999_uniform --train_size 10000 --test_size 3000 --val_size 3000 --train_eval True --sample-size 3000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"DBmQXdsiI6YE"},"source":["#### Generate Multiplication Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hr3n0w_gIf7i"},"outputs":[],"source":["!python data_generate.py --task multiplication --num_operands 6 --experiment_name 0_to_999999_times_1_digit \\\n","        --train_size 10000 --test_size 3000 --val_size 3000 --train_eval True --sample-size 3000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"fB5aQJxiI8_h"},"source":["#### Generate Sorting Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Q7blcAsIVHV"},"outputs":[],"source":["!python data_generate.py --task sorting --experiment_name 4_operands_sorting_balanced_digit \\\n","        --train_size 1000000 --test_size 10000 --val_size 10000 --train_eval True --sample-size 10000"]},{"cell_type":"markdown","metadata":{"id":"8i_CDutAI_fC"},"source":["## Let's Start Training!"]},{"cell_type":"markdown","metadata":{"id":"Et7rULRph_Ne"},"source":["#### The .txt file is the configuration file"]},{"cell_type":"markdown","metadata":{"id":"dZJI2octhC3o"},"source":["## 2 Operands"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"qTJ1568DCuHp"},"outputs":[],"source":["!python train.py 2_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3C_q2HCzg9rf"},"outputs":[],"source":["!python train.py 2_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"JLXLrm_chIgo"},"source":["## 4 Operands"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8S_LDDfvhGLM"},"outputs":[],"source":["!python train.py 4_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0RjN0ldhMGz"},"outputs":[],"source":["!python train.py 4_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"QDxgYE9zhL02"},"source":["## Multiplication"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNcrd8bDha9j"},"outputs":[],"source":["!python train.py 2_operands_mul_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nn2GYDQhX7b"},"outputs":[],"source":["!python train.py 2_operands_mul_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"asYxDysPKOzI"},"source":["# Other Commands (May not be fully working)"]},{"cell_type":"markdown","metadata":{"id":"L0ttXoH9PGRV"},"source":["## 4 Operand Addition Scratchpad"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3F4qYRSCQZhE"},"outputs":[],"source":["%cat configuration_files/4_operands_addition_scratchpad.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ZEiUegj1UDm-"},"outputs":[],"source":["%cat evaluation.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"hGSNQOkyPFQF"},"outputs":[],"source":["!python train.py 4_operands_addition_scratchpad.txt"]},{"cell_type":"markdown","metadata":{"id":"dfLn10UAHrM2"},"source":["## 4 Operand Max"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"PFYzFx4vK6Px"},"outputs":[],"source":["%cat configuration_files/4_operands_max.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"CEGW4g1dLgnQ"},"outputs":[],"source":["%cat train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"SSG2giBcf0u9"},"outputs":[],"source":["%cat model.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"X2hV4DOEHtrh"},"outputs":[],"source":["!python train.py 4_operands_max.txt"]},{"cell_type":"markdown","metadata":{"id":"7UHO3zBc_jY3"},"source":["## 4 Operand Sorting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXiwk0EUyudt"},"outputs":[],"source":["%cat configuration_files/4_operands_sorting.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"elF2BaDD_izl"},"outputs":[],"source":["!python train.py 4_operands_sorting.txt"]},{"cell_type":"markdown","metadata":{"id":"hdumgWyAWlt6"},"source":["## Slicing -- Addition"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"b-jVt7X7Llry"},"outputs":[],"source":["!python train.py slicing_addition_2_operand.txt --batch slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"pdPiOKoWKw0q","outputId":"eaab6064-1d31-4de3-87e5-b56efbd740ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: slicing\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/slicing_addition_4_operand.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'slicing_4_operand_reversed_test'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","batch_size = 256\n","block_size = 512\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# whether need zero‚Äêpadding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000\n","lr_decay_iters = 800000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/slicing_reverse_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 97\n","Using vocabulary size: 97\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.82M\n","/content/drive/MyDrive/addition/train.py:510: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/slicing_reverse_out/wandb/run-20251226_175706-50j1rak3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mslicing_4_operand_reversed_test\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/50j1rak3\u001b[0m\n","WARNING: results directory results/4_operands_0_to_999_uniform/slicing_reverse_out/slicing_4_operand_reversed_test already exists, overwriting...\n","max_new_tokens: 5\n","iter 0: train loss 4.3047, val loss 4.3047\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:03<00:00, 26.24it/s]\n","/content/drive/MyDrive/addition/evaluation.py:408: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02<00:00, 33.80it/s]\n","iter 1000: train loss 1.5441, val loss 1.5426\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 32.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.68it/s]\n","iter 2000: train loss 1.4996, val loss 1.5018\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 37/10000  (0.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.44it/s]\n","iter 3000: train loss 1.4777, val loss 1.4784\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 48/10000  (0.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.00it/s]\n","iter 4000: train loss 1.4565, val loss 1.4589\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 73/10000  (0.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.70it/s]\n","iter 5000: train loss 1.4483, val loss 1.4494\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 69/10000  (0.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.40it/s]\n","iter 6000: train loss 1.4409, val loss 1.4424\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.90it/s]\n","iter 7000: train loss 1.4388, val loss 1.4407\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.39it/s]\n","iter 8000: train loss 1.4408, val loss 1.4400\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 105/10000  (1.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.89it/s]\n","iter 9000: train loss 1.4393, val loss 1.4408\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.95it/s]\n","iter 10000: train loss 1.4403, val loss 1.4400\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 90/10000  (0.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.64it/s]\n","iter 11000: train loss 1.4402, val loss 1.4399\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 106/10000  (1.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.88it/s]\n","iter 12000: train loss 1.4402, val loss 1.4386\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 109/10000  (1.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.28it/s]\n","iter 13000: train loss 1.4403, val loss 1.4390\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 89/10000  (0.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.04it/s]\n","iter 14000: train loss 1.4398, val loss 1.4412\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 101/10000  (1.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.35it/s]\n","iter 15000: train loss 1.4403, val loss 1.4399\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 91/10000  (0.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.28it/s]\n","iter 16000: train loss 1.4405, val loss 1.4397\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 100/10000  (1.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.28it/s]\n","iter 17000: train loss 1.4403, val loss 1.4413\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 82/10000  (0.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.00it/s]\n","iter 18000: train loss 1.3582, val loss 1.3571\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 753/10000  (7.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.74it/s]\n","iter 19000: train loss 1.3486, val loss 1.3509\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 901/10000  (9.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.27it/s]\n","iter 20000: train loss 1.3426, val loss 1.3440\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 965/10000  (9.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.11it/s]\n","iter 21000: train loss 1.3475, val loss 1.3474\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 903/10000  (9.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.70it/s]\n","iter 22000: train loss 1.3457, val loss 1.3452\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 911/10000  (9.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.64it/s]\n","iter 23000: train loss 1.3405, val loss 1.3408\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 985/10000  (9.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.48it/s]\n","iter 24000: train loss 1.3426, val loss 1.3435\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1008/10000  (10.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.03it/s]\n","iter 25000: train loss 1.3427, val loss 1.3431\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 964/10000  (9.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.79it/s]\n","iter 26000: train loss 1.3407, val loss 1.3417\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 977/10000  (9.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.04it/s]\n","iter 27000: train loss 1.3412, val loss 1.3427\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1012/10000  (10.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.35it/s]\n","iter 28000: train loss 1.3403, val loss 1.3406\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 993/10000  (9.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.44it/s]\n","iter 29000: train loss 1.3399, val loss 1.3411\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 975/10000  (9.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.29it/s]\n","iter 30000: train loss 1.3401, val loss 1.3429\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 980/10000  (9.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.56it/s]\n","iter 31000: train loss 1.3389, val loss 1.3408\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1024/10000  (10.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.52it/s]\n","iter 32000: train loss 1.3387, val loss 1.3410\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1002/10000  (10.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.23it/s]\n","iter 33000: train loss 1.3380, val loss 1.3420\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 999/10000  (9.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.28it/s]\n","iter 34000: train loss 1.3378, val loss 1.3425\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1030/10000  (10.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.12it/s]\n","iter 35000: train loss 1.3389, val loss 1.3412\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1007/10000  (10.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.17it/s]\n","iter 36000: train loss 1.3373, val loss 1.3426\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1019/10000  (10.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.71it/s]\n","iter 37000: train loss 1.3365, val loss 1.3439\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1005/10000  (10.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.04it/s]\n","iter 38000: train loss 1.2470, val loss 1.2504\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9125/10000  (91.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.01it/s]\n","iter 39000: train loss 1.2554, val loss 1.2584\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8058/10000  (80.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.83it/s]\n","iter 40000: train loss 1.2391, val loss 1.2430\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9980/10000  (99.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.98it/s]\n","iter 41000: train loss 1.2385, val loss 1.2427\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.27it/s]\n","iter 42000: train loss 1.2384, val loss 1.2432\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.95it/s]\n","iter 43000: train loss 1.2392, val loss 1.2407\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9955/10000  (99.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.89it/s]\n","iter 44000: train loss 1.2376, val loss 1.2430\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9965/10000  (99.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.76it/s]\n","iter 45000: train loss 1.2365, val loss 1.2432\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.23it/s]\n","iter 46000: train loss 1.2362, val loss 1.2442\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.22it/s]\n","iter 47000: train loss 1.2380, val loss 1.2437\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 32.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.92it/s]\n","iter 48000: train loss 1.2350, val loss 1.2448\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.91it/s]\n","iter 49000: train loss 1.2340, val loss 1.2459\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.07it/s]\n","iter 50000: train loss 1.2334, val loss 1.2454\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.71it/s]\n","iter 51000: train loss 1.2329, val loss 1.2470\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.36it/s]\n","iter 52000: train loss 1.2322, val loss 1.2479\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.98it/s]\n","iter 53000: train loss 1.2308, val loss 1.2479\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.21it/s]\n","iter 54000: train loss 1.2299, val loss 1.2492\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.45it/s]\n","iter 55000: train loss 1.2271, val loss 1.2506\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.79it/s]\n","iter 56000: train loss 1.2238, val loss 1.2557\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.48it/s]\n","iter 57000: train loss 1.2223, val loss 1.2561\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.12it/s]\n","iter 58000: train loss 1.2194, val loss 1.2587\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.93it/s]\n","iter 59000: train loss 1.2163, val loss 1.2574\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.72it/s]\n","iter 60000: train loss 1.2147, val loss 1.2614\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.97it/s]\n","iter 61000: train loss 1.2113, val loss 1.2644\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.78it/s]\n","iter 62000: train loss 1.2089, val loss 1.2648\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.89it/s]\n","iter 63000: train loss 1.2083, val loss 1.2619\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.68it/s]\n","iter 64000: train loss 1.2055, val loss 1.2656\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.65it/s]\n","iter 65000: train loss 1.1998, val loss 1.2697\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.33it/s]\n","iter 66000: train loss 1.1998, val loss 1.2692\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.02it/s]\n","iter 67000: train loss 1.1985, val loss 1.2678\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.25it/s]\n","iter 68000: train loss 1.1971, val loss 1.2714\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.31it/s]\n","iter 69000: train loss 1.1943, val loss 1.2756\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.03it/s]\n","iter 70000: train loss 1.1927, val loss 1.2747\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.73it/s]\n","iter 71000: train loss 1.1901, val loss 1.2750\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.87it/s]\n","iter 72000: train loss 1.1895, val loss 1.2787\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.54it/s]\n","iter 73000: train loss 1.1889, val loss 1.2784\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.79it/s]\n","iter 74000: train loss 1.1872, val loss 1.2781\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.24it/s]\n","iter 75000: train loss 1.1829, val loss 1.2819\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 32.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.54it/s]\n","iter 76000: train loss 1.1831, val loss 1.2802\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.17it/s]\n","iter 77000: train loss 1.1814, val loss 1.2807\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.71it/s]\n","iter 78000: train loss 1.1791, val loss 1.2817\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.27it/s]\n","iter 79000: train loss 1.1790, val loss 1.2834\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.83it/s]\n","iter 80000: train loss 1.1808, val loss 1.2861\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.54it/s]\n"]}],"source":["!python train.py slicing_addition_4_operand.txt --batch slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"eAMCHgiqoXXW","outputId":"86e0de8c-f8f1-4964-ccc3-fcff5ce3ad30"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: slicing\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/slicing_addition_4_operand.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'slicing_4_operand_reversed_test'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","batch_size = 256\n","block_size = 512\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# whether need zero‚Äêpadding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000\n","lr_decay_iters = 800000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/slicing_reverse_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","Creating meta file for all reasonable characters...\n","all the unique characters: \n"," !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n","vocab size: 96\n","data has 22,518,028 tokens\n","data has 225,140 tokens\n","Using vocabulary size: 96\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.82M\n","/content/drive/MyDrive/addition/train_test.py:396: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run fak2f44e (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run fak2f44e (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m setting up run fak2f44e (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/slicing_reverse_out/wandb/run-20251226_170814-fak2f44e\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mslicing_4_operand_reversed_test\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/fak2f44e\u001b[0m\n","WARNING: results directory results/4_operands_0_to_999_uniform/slicing_reverse_out/slicing_4_operand_reversed_test already exists, overwriting...\n","max_new_tokens: 5\n","iter 0: train loss 4.2711, val loss 4.2679\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:02<00:00, 30.34it/s]\n","/content/drive/MyDrive/addition/evaluation.py:408: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02<00:00, 34.32it/s]\n","iter 1000: train loss 1.5417, val loss 1.5410\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.72it/s]\n","iter 2000: train loss 1.4927, val loss 1.4913\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 26/10000  (0.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.70it/s]\n","iter 3000: train loss 1.4625, val loss 1.4616\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 75/10000  (0.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.62it/s]\n","iter 4000: train loss 1.4452, val loss 1.4435\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 103/10000  (1.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.66it/s]\n","iter 5000: train loss 1.4417, val loss 1.4402\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 96/10000  (0.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.75it/s]\n","iter 6000: train loss 1.4415, val loss 1.4403\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 116/10000  (1.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.63it/s]\n","iter 7000: train loss 1.4393, val loss 1.4400\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 88/10000  (0.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.79it/s]\n","iter 8000: train loss 1.4404, val loss 1.4402\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.91it/s]\n","iter 9000: train loss 1.4401, val loss 1.4404\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 122/10000  (1.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.03it/s]\n","iter 10000: train loss 1.4525, val loss 1.4537\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 85/10000  (0.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.84it/s]\n","iter 11000: train loss 1.4399, val loss 1.4417\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 118/10000  (1.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.57it/s]\n","iter 12000: train loss 1.4401, val loss 1.4405\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 108/10000  (1.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.44it/s]\n","iter 13000: train loss 1.4387, val loss 1.4396\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 96/10000  (0.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.74it/s]\n","iter 14000: train loss 1.4408, val loss 1.4420\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 118/10000  (1.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.41it/s]\n","iter 15000: train loss 1.3669, val loss 1.3684\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 582/10000  (5.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.53it/s]\n","iter 16000: train loss 1.3520, val loss 1.3522\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 879/10000  (8.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.97it/s]\n","iter 17000: train loss 1.3405, val loss 1.3415\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1040/10000  (10.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.23it/s]\n","iter 18000: train loss 1.3430, val loss 1.3431\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 887/10000  (8.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.59it/s]\n","iter 19000: train loss 1.3402, val loss 1.3415\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1027/10000  (10.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.18it/s]\n","iter 20000: train loss 1.3424, val loss 1.3413\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 963/10000  (9.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.55it/s]\n","iter 21000: train loss 1.3420, val loss 1.3402\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1048/10000  (10.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.23it/s]\n","iter 22000: train loss 1.3406, val loss 1.3408\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 990/10000  (9.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.97it/s]\n","iter 23000: train loss 1.3406, val loss 1.3404\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1005/10000  (10.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.75it/s]\n","iter 24000: train loss 1.3420, val loss 1.3417\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 967/10000  (9.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.39it/s]\n","iter 25000: train loss 1.3398, val loss 1.3401\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 989/10000  (9.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.01it/s]\n","iter 26000: train loss 1.3407, val loss 1.3413\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 993/10000  (9.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.95it/s]\n","iter 27000: train loss 1.3401, val loss 1.3410\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1021/10000  (10.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.11it/s]\n","iter 28000: train loss 1.3396, val loss 1.3401\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1024/10000  (10.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.63it/s]\n","iter 29000: train loss 1.3379, val loss 1.3402\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 941/10000  (9.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.69it/s]\n"]}],"source":["!python train_test.py slicing_addition_4_operand.txt --batch slicing"]},{"cell_type":"markdown","source":["## Scaling"],"metadata":{"id":"sUhzfB2OeOi-"}},{"cell_type":"code","source":["!python train.py 20M_4_operands_addition_reversed.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifio3FpGeSYG","executionInfo":{"status":"ok","timestamp":1766796510682,"user_tz":360,"elapsed":8194259,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"437964dd-d299-420c-f77b-bf1de541a8ef","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/20M_4_operands_addition_reversed.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 2000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '20M4_operands_0_to_999_uniform_reverse'\n","\n","# baby GPT model :)\n","n_layer = 12\n","n_head = 12\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 6e-4\n","\n","# to edit: the number of training iterations\n","max_iters = 800000 # for four operands addition, recommend >= 200000\n","lr_decay_iters = 800000 # make equal to max_iters\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 500\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/20M_reverse_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: Can be 'units', 'tens', 'hundreds', 'thousands', or None\n","randomize = 'None'\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 97\n","Using vocabulary size: 97\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","pad_id: 30 eos_id: 5\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 21.26M\n","/content/drive/MyDrive/addition/train.py:509: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run h97j6xam (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run h97j6xam (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run h97j6xam (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/20M_reverse_out/wandb/run-20251226_223208-h97j6xam\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m20M4_operands_0_to_999_uniform_reverse\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/h97j6xam\u001b[0m\n","max_new_tokens: 5\n","iter 0: train loss 4.5851, val loss 4.5892\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:04<00:00, 18.86it/s]\n","/content/drive/MyDrive/addition/evaluation.py:401: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:03<00:00, 21.08it/s]\n","iter 2000: train loss 1.6587, val loss 1.6609\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.85it/s]\n","iter 4000: train loss 1.5490, val loss 1.5523\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 846/10000  (8.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.40it/s]\n","iter 6000: train loss 1.5383, val loss 1.5448\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 984/10000  (9.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.69it/s]\n","iter 8000: train loss 1.5400, val loss 1.5434\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1033/10000  (10.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.80it/s]\n","iter 10000: train loss 1.5408, val loss 1.5457\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 990/10000  (9.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04<00:00, 20.01it/s]\n","iter 12000: train loss 1.5408, val loss 1.5431\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 972/10000  (9.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.72it/s]\n","iter 14000: train loss 1.5377, val loss 1.5427\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1011/10000  (10.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.79it/s]\n","iter 16000: train loss 1.5391, val loss 1.5432\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 964/10000  (9.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.85it/s]\n","iter 18000: train loss 1.5399, val loss 1.5428\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1017/10000  (10.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.92it/s]\n","iter 20000: train loss 1.5434, val loss 1.5428\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 995/10000  (9.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.71it/s]\n","iter 22000: train loss 1.5374, val loss 1.5429\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 946/10000  (9.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.94it/s]\n","iter 24000: train loss 1.5388, val loss 1.5427\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 924/10000  (9.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.13it/s]\n","iter 26000: train loss 1.5362, val loss 1.5417\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 969/10000  (9.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.90it/s]\n","iter 28000: train loss 1.5404, val loss 1.5421\n","Using precomputed batches\n","100% 80/80 [00:04<00:00, 19.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 980/10000  (9.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.44it/s]\n","iter 30000: train loss 1.5398, val loss 1.5423\n","Using precomputed batches\n","100% 80/80 [00:04<00:00, 19.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1042/10000  (10.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.69it/s]\n","iter 32000: train loss 1.5406, val loss 1.5420\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1023/10000  (10.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.69it/s]\n","iter 34000: train loss 1.5442, val loss 1.5437\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1031/10000  (10.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.96it/s]\n","iter 36000: train loss 1.5334, val loss 1.5402\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1105/10000  (11.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.45it/s]\n","iter 38000: train loss 1.4307, val loss 1.4360\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6301/10000  (63.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.94it/s]\n","iter 40000: train loss 1.4219, val loss 1.4251\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6966/10000  (69.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.72it/s]\n","iter 42000: train loss 1.4202, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7320/10000  (73.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.83it/s]\n","iter 44000: train loss 1.4249, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7295/10000  (72.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.73it/s]\n","iter 46000: train loss 1.4236, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7459/10000  (74.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.00it/s]\n","iter 48000: train loss 1.4235, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7263/10000  (72.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.72it/s]\n","iter 50000: train loss 1.4234, val loss 1.4245\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7469/10000  (74.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.89it/s]\n","iter 52000: train loss 1.4241, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7392/10000  (73.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.78it/s]\n","iter 54000: train loss 1.4224, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7414/10000  (74.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.98it/s]\n","iter 56000: train loss 1.4183, val loss 1.4235\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7253/10000  (72.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.59it/s]\n","iter 58000: train loss 1.4192, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7401/10000  (74.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.92it/s]\n","iter 60000: train loss 1.4271, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7369/10000  (73.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.73it/s]\n","iter 62000: train loss 1.4217, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7296/10000  (72.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.65it/s]\n","iter 64000: train loss 1.4187, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7386/10000  (73.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.00it/s]\n","iter 66000: train loss 1.4206, val loss 1.4248\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7554/10000  (75.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.02it/s]\n","iter 68000: train loss 1.4204, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7406/10000  (74.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.71it/s]\n","iter 70000: train loss 1.4228, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7321/10000  (73.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.77it/s]\n","iter 72000: train loss 1.4190, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7394/10000  (73.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.82it/s]\n","iter 74000: train loss 1.4237, val loss 1.4251\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7090/10000  (70.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.94it/s]\n","iter 76000: train loss 1.4195, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7267/10000  (72.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.96it/s]\n","iter 78000: train loss 1.4183, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7363/10000  (73.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.39it/s]\n","iter 80000: train loss 1.4247, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7201/10000  (72.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.94it/s]\n","iter 82000: train loss 1.4166, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7274/10000  (72.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.00it/s]\n","iter 84000: train loss 1.4186, val loss 1.4251\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7186/10000  (71.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.76it/s]\n","iter 86000: train loss 1.4229, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7575/10000  (75.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.94it/s]\n","iter 88000: train loss 1.4204, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7224/10000  (72.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.20it/s]\n","iter 90000: train loss 1.4227, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7349/10000  (73.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.04it/s]\n","iter 92000: train loss 1.4236, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7463/10000  (74.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.84it/s]\n","iter 94000: train loss 1.4237, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7359/10000  (73.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.11it/s]\n","iter 96000: train loss 1.4210, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7396/10000  (73.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.93it/s]\n","iter 98000: train loss 1.4234, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7362/10000  (73.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.55it/s]\n","iter 100000: train loss 1.4222, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7147/10000  (71.47%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.37it/s]\n","iter 102000: train loss 1.4157, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7241/10000  (72.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.49it/s]\n","iter 104000: train loss 1.4232, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7259/10000  (72.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.47it/s]\n","iter 106000: train loss 1.4218, val loss 1.4235\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7344/10000  (73.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.68it/s]\n","iter 108000: train loss 1.4205, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7271/10000  (72.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.78it/s]\n","iter 110000: train loss 1.4243, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7336/10000  (73.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04<00:00, 20.03it/s]\n","iter 112000: train loss 1.4187, val loss 1.4234\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7472/10000  (74.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.63it/s]\n","iter 114000: train loss 1.4235, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7123/10000  (71.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.52it/s]\n","iter 116000: train loss 1.4229, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7438/10000  (74.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.26it/s]\n","iter 118000: train loss 1.4206, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7291/10000  (72.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04<00:00, 19.89it/s]\n","iter 120000: train loss 1.4202, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7420/10000  (74.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.50it/s]\n","iter 122000: train loss 1.4183, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.71it/s]\n","iter 124000: train loss 1.4286, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7491/10000  (74.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.45it/s]\n","iter 126000: train loss 1.4282, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.89it/s]\n","iter 128000: train loss 1.4213, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7185/10000  (71.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.48it/s]\n","iter 130000: train loss 1.4229, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7429/10000  (74.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.50it/s]\n","iter 132000: train loss 1.4165, val loss 1.4234\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7287/10000  (72.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.33it/s]\n","iter 134000: train loss 1.4223, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7218/10000  (72.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.32it/s]\n","iter 136000: train loss 1.3200, val loss 1.3228\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9814/10000  (98.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.37it/s]\n","iter 138000: train loss 1.3078, val loss 1.3088\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9975/10000  (99.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.41it/s]\n","iter 140000: train loss 1.3001, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.38it/s]\n","iter 142000: train loss 1.3007, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.65it/s]\n","iter 144000: train loss 1.3075, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.74it/s]\n","iter 146000: train loss 1.3039, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.34it/s]\n","iter 148000: train loss 1.3029, val loss 1.3077\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.50it/s]\n","iter 150000: train loss 1.3042, val loss 1.3065\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.76it/s]\n","iter 152000: train loss 1.3041, val loss 1.3081\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.72it/s]\n","iter 154000: train loss 1.3021, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9983/10000  (99.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.65it/s]\n","iter 156000: train loss 1.3037, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04<00:00, 20.08it/s]\n","iter 158000: train loss 1.3018, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.50it/s]\n","iter 160000: train loss 1.3088, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.65it/s]\n","iter 162000: train loss 1.3045, val loss 1.3066\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.81it/s]\n","iter 164000: train loss 1.3024, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04<00:00, 20.16it/s]\n","iter 166000: train loss 1.3042, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.33it/s]\n","iter 168000: train loss 1.2989, val loss 1.3053\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04<00:00, 20.25it/s]\n","iter 170000: train loss 1.3047, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.46it/s]\n","iter 172000: train loss 1.3036, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.57it/s]\n","iter 174000: train loss 1.3068, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.68it/s]\n","iter 176000: train loss 1.3055, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.86it/s]\n","iter 178000: train loss 1.3021, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.71it/s]\n","iter 180000: train loss 1.3052, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.73it/s]\n","iter 182000: train loss 1.3057, val loss 1.3063\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9993/10000  (99.93%)\n","\n","Using precomputed batches\n"," 75% 61/81 [00:03<00:00, 20.16it/s]\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/addition/evaluation.py\", line 186, in evaluate_addition_precomputed\n","    y = model.generate(\n","        ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n","    return func(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/addition/model.py\", line 230, in generate\n","    logits, _ = self(idx_cond)\n","                ^^^^^^^^^^^^^^\n","^C\n"]}]},{"cell_type":"markdown","source":["## Long Multiplication"],"metadata":{"id":"PCaYEmqawRRI"}},{"cell_type":"code","source":["! python train.py 50_1_digits_mul_reversed.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZvjpu5VwVuo","outputId":"92f63cb5-8cd0-4803-af70-fb42789d7f47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/50_1_digits_mul_reversed.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 100\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'multiplication'\n","\n","# to edit: wandb run name\n","wandb_run_name = '50_digit_times_1_digit_reversed'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'algo_reasoning'\n","data_format='reverse'\n","operator='*'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 110 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_plain.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 50\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 54\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 10000\n","lr_decay_iters = 10000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/50_digit_times_1_digit/reverse_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/50_digit_times_1_digit/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# If it's a directory, \"main_test_name\" is the name of the test file (without extension) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: whether do mutual information tacking (addition supported only)\n","mi_measurement = False\n","more_early_eval1 = False \n","early_eval_interval1 = 5\n","early_eval_border1 = 100\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","all the unique characters: \\n, $, *, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, <pad>, =\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/50_digit_times_1_digit/test_reverse.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.66M\n","/content/drive/MyDrive/addition/train.py:556: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.24.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/50_digit_times_1_digit/reverse_out/wandb/run-20260123_164957-dc8nrzqj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m50_digit_times_1_digit_reversed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/multiplication\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/multiplication/runs/dc8nrzqj\u001b[0m\n","max_new_tokens: 54\n","iter 0: train loss 2.6747, val loss 2.6736\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/50_digit_times_1_digit/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/50_digit_times_1_digit/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 105 batches\n","100% 105/105 [00:37<00:00,  2.81it/s]\n","/content/drive/MyDrive/addition/evaluation.py:551: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 3/10000  (0.03%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/50_digit_times_1_digit/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/50_digit_times_1_digit/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 103 batches\n","100% 103/103 [00:35<00:00,  2.86it/s]\n","iter 100: train loss 2.2928, val loss 2.2911\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.85it/s]\n","iter 200: train loss 2.2577, val loss 2.2588\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 826/10000  (8.26%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.82it/s]\n","iter 300: train loss 2.2398, val loss 2.2407\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1010/10000  (10.10%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.85it/s]\n","iter 400: train loss 2.2320, val loss 2.2318\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1011/10000  (10.11%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.83it/s]\n","iter 500: train loss 2.2246, val loss 2.2243\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1007/10000  (10.07%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.86it/s]\n","iter 600: train loss 2.2142, val loss 2.2151\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1014/10000  (10.14%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.86it/s]\n","iter 700: train loss 2.2127, val loss 2.2122\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1015/10000  (10.15%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.80it/s]\n","iter 800: train loss 2.2012, val loss 2.2025\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1016/10000  (10.16%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.85it/s]\n","iter 900: train loss 2.1815, val loss 2.1828\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1015/10000  (10.15%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.82it/s]\n","iter 1000: train loss 2.1046, val loss 2.1008\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1012/10000  (10.12%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.86it/s]\n","iter 1100: train loss 1.9415, val loss 1.9705\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1017/10000  (10.17%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.82it/s]\n","iter 1200: train loss 1.8284, val loss 1.8488\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1031/10000  (10.31%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.81it/s]\n","iter 1300: train loss 1.6485, val loss 1.6680\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1127/10000  (11.27%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.86it/s]\n","iter 1400: train loss 1.6590, val loss 1.6746\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1045/10000  (10.45%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.85it/s]\n","iter 1500: train loss 1.4874, val loss 1.4917\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1155/10000  (11.55%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.85it/s]\n","iter 1600: train loss 1.3940, val loss 1.3841\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1445/10000  (14.45%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.84it/s]\n","iter 1700: train loss 1.2702, val loss 1.2756\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 2086/10000  (20.86%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.86it/s]\n","iter 1800: train loss 1.2580, val loss 1.2548\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 2763/10000  (27.63%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.86it/s]\n","iter 1900: train loss 1.3085, val loss 1.3196\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1738/10000  (17.38%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.88it/s]\n","iter 2000: train loss 1.2474, val loss 1.2428\n","Using precomputed batches\n","100% 105/105 [00:35<00:00,  2.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 3290/10000  (32.90%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.86it/s]\n","iter 2100: train loss 1.2421, val loss 1.2374\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 3807/10000  (38.07%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.88it/s]\n","iter 2200: train loss 1.2372, val loss 1.2259\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 4467/10000  (44.67%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.86it/s]\n","iter 2300: train loss 1.2175, val loss 1.2117\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6098/10000  (60.98%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.86it/s]\n","iter 2400: train loss 1.2529, val loss 1.2263\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 4946/10000  (49.46%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.87it/s]\n","iter 2500: train loss 1.2402, val loss 1.2203\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 5129/10000  (51.29%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.87it/s]\n","iter 2600: train loss 1.2128, val loss 1.2047\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7082/10000  (70.82%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.85it/s]\n","iter 2700: train loss 1.2357, val loss 1.2128\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6400/10000  (64.00%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.85it/s]\n","iter 2800: train loss 1.2131, val loss 1.2056\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7061/10000  (70.61%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.86it/s]\n","iter 2900: train loss 1.2151, val loss 1.2148\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6343/10000  (63.43%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.87it/s]\n","iter 3000: train loss 1.2207, val loss 1.2034\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7533/10000  (75.33%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.86it/s]\n","iter 3100: train loss 1.2168, val loss 1.1965\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8821/10000  (88.21%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.87it/s]\n","iter 3200: train loss 1.1994, val loss 1.1982\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8691/10000  (86.91%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.87it/s]\n","iter 3300: train loss 1.2102, val loss 1.1984\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8587/10000  (85.87%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.87it/s]\n","iter 3400: train loss 1.2056, val loss 1.1935\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9834/10000  (98.34%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.84it/s]\n","iter 3500: train loss 1.1945, val loss 1.1942\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9761/10000  (97.61%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.88it/s]\n","iter 3600: train loss 1.1983, val loss 1.1929\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.87it/s]\n","iter 3700: train loss 1.2002, val loss 1.1933\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.87it/s]\n","iter 3800: train loss 1.1938, val loss 1.1925\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9946/10000  (99.46%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.86it/s]\n","iter 3900: train loss 1.1993, val loss 1.1932\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.87it/s]\n","iter 4000: train loss 1.1966, val loss 1.1927\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 103/103 [00:35<00:00,  2.86it/s]\n","iter 4100: train loss 1.2099, val loss 1.1926\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.85it/s]\n","iter 4200: train loss 1.1897, val loss 1.1927\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.82it/s]\n","iter 4300: train loss 1.1902, val loss 1.1935\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 103/103 [00:36<00:00,  2.85it/s]\n","iter 4400: train loss 1.2001, val loss 1.1930\n","Using precomputed batches\n","100% 105/105 [00:36<00:00,  2.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9991/10000  (99.91%)\n","\n","Using precomputed batches\n","  7% 7/103 [00:02<00:34,  2.76it/s]"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
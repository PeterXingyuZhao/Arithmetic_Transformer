{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":12892,"status":"ok","timestamp":1767412698841,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":360},"id":"aftd1RezTYnF","outputId":"5930f7fa-6e8e-4b64-f660-6b812d801a6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/addition\n","configuration_files\t       model_rope.py\n","configurator.py\t\t       model_t5bias.py\n","data\t\t\t       __pycache__\n","data_generate.py\t       README.md\n","data_generation_script\t       result_analysis.ipynb\n","eval_ckpt.py\t\t       result_analysis.py\n","evaluation_ckpt.py\t       results\n","evaluation.py\t\t       startHere100M.ipynb\n","extra_result_analysis_scripts  startHere1B.ipynb\n","legacy_code\t\t       startHere20M.ipynb\n","main_utilities.py\t       startHere.ipynb\n","meta_all_ascii_chars.pkl       statistical_measurements.py\n","model.py\t\t       train.py\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change directory to your code\n","%cd /content/drive/MyDrive/addition\n","%pwd   # verify you‚Äôre in the right place\n","!ls    # should show train.py, 4_operands_addition.txt, etc."]},{"cell_type":"markdown","metadata":{"id":"lbEXozJBCm_X"},"source":["# Quick Start Example\n","## Choose One Task and Start Training"]},{"cell_type":"markdown","metadata":{"id":"fUKKZHtTI1n3"},"source":["#### Generate Addition Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbKQwH__Bq4E"},"outputs":[],"source":["!python data_generate.py --task addition --num_operands 2 --experiment_name 2_operands_0_to_999_uniform --train_size 10000 --test_size 3000 --val_size 3000 --train_eval True --sample-size 3000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"DBmQXdsiI6YE"},"source":["#### Generate Multiplication Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hr3n0w_gIf7i"},"outputs":[],"source":["!python data_generate.py --task multiplication --num_operands 6 --experiment_name 0_to_999999_times_1_digit \\\n","        --train_size 10000 --test_size 3000 --val_size 3000 --train_eval True --sample-size 3000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"fB5aQJxiI8_h"},"source":["#### Generate Sorting Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Q7blcAsIVHV"},"outputs":[],"source":["!python data_generate.py --task sorting --experiment_name 4_operands_sorting_balanced_digit \\\n","        --train_size 1000000 --test_size 10000 --val_size 10000 --train_eval True --sample-size 10000"]},{"cell_type":"markdown","metadata":{"id":"8i_CDutAI_fC"},"source":["## Let's Start Training!"]},{"cell_type":"markdown","metadata":{"id":"Et7rULRph_Ne"},"source":["#### The .txt file is the configuration file"]},{"cell_type":"markdown","metadata":{"id":"dZJI2octhC3o"},"source":["## 2 Operands"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"qTJ1568DCuHp"},"outputs":[],"source":["!python train.py 2_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3C_q2HCzg9rf"},"outputs":[],"source":["!python train.py 2_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"JLXLrm_chIgo"},"source":["## 4 Operands"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8S_LDDfvhGLM"},"outputs":[],"source":["!python train.py 4_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0RjN0ldhMGz"},"outputs":[],"source":["!python train.py 4_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"QDxgYE9zhL02"},"source":["## Multiplication"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNcrd8bDha9j"},"outputs":[],"source":["!python train.py 2_operands_mul_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nn2GYDQhX7b"},"outputs":[],"source":["!python train.py 2_operands_mul_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"asYxDysPKOzI"},"source":["# Other Commands (May not be fully working)"]},{"cell_type":"markdown","metadata":{"id":"L0ttXoH9PGRV"},"source":["## 4 Operand Addition Scratchpad"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3F4qYRSCQZhE"},"outputs":[],"source":["%cat configuration_files/4_operands_addition_scratchpad.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ZEiUegj1UDm-"},"outputs":[],"source":["%cat evaluation.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"hGSNQOkyPFQF"},"outputs":[],"source":["!python train.py 4_operands_addition_scratchpad.txt"]},{"cell_type":"markdown","metadata":{"id":"dfLn10UAHrM2"},"source":["## 4 Operand Max"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"PFYzFx4vK6Px"},"outputs":[],"source":["%cat configuration_files/4_operands_max.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"CEGW4g1dLgnQ"},"outputs":[],"source":["%cat train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"SSG2giBcf0u9"},"outputs":[],"source":["%cat model.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"X2hV4DOEHtrh"},"outputs":[],"source":["!python train.py 4_operands_max.txt"]},{"cell_type":"markdown","metadata":{"id":"7UHO3zBc_jY3"},"source":["## 4 Operand Sorting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXiwk0EUyudt"},"outputs":[],"source":["%cat configuration_files/4_operands_sorting.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"elF2BaDD_izl"},"outputs":[],"source":["!python train.py 4_operands_sorting.txt"]},{"cell_type":"markdown","metadata":{"id":"hdumgWyAWlt6"},"source":["## Slicing -- Addition"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"b-jVt7X7Llry"},"outputs":[],"source":["!python train.py slicing_addition_2_operand.txt --batch slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"pdPiOKoWKw0q","outputId":"eaab6064-1d31-4de3-87e5-b56efbd740ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: slicing\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/slicing_addition_4_operand.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'slicing_4_operand_reversed_test'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","batch_size = 256\n","block_size = 512\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# whether need zero‚Äêpadding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000\n","lr_decay_iters = 800000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/slicing_reverse_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 97\n","Using vocabulary size: 97\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.82M\n","/content/drive/MyDrive/addition/train.py:510: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W\u0026B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W\u0026B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/slicing_reverse_out/wandb/run-20251226_175706-50j1rak3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mslicing_4_operand_reversed_test\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/50j1rak3\u001b[0m\n","WARNING: results directory results/4_operands_0_to_999_uniform/slicing_reverse_out/slicing_4_operand_reversed_test already exists, overwriting...\n","max_new_tokens: 5\n","iter 0: train loss 4.3047, val loss 4.3047\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:03\u003c00:00, 26.24it/s]\n","/content/drive/MyDrive/addition/evaluation.py:408: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02\u003c00:00, 33.80it/s]\n","iter 1000: train loss 1.5441, val loss 1.5426\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 32.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.68it/s]\n","iter 2000: train loss 1.4996, val loss 1.5018\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 37/10000  (0.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.44it/s]\n","iter 3000: train loss 1.4777, val loss 1.4784\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 48/10000  (0.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.00it/s]\n","iter 4000: train loss 1.4565, val loss 1.4589\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 73/10000  (0.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.70it/s]\n","iter 5000: train loss 1.4483, val loss 1.4494\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 69/10000  (0.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.40it/s]\n","iter 6000: train loss 1.4409, val loss 1.4424\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.90it/s]\n","iter 7000: train loss 1.4388, val loss 1.4407\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.39it/s]\n","iter 8000: train loss 1.4408, val loss 1.4400\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 105/10000  (1.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.89it/s]\n","iter 9000: train loss 1.4393, val loss 1.4408\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.95it/s]\n","iter 10000: train loss 1.4403, val loss 1.4400\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 90/10000  (0.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.64it/s]\n","iter 11000: train loss 1.4402, val loss 1.4399\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 106/10000  (1.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 32.88it/s]\n","iter 12000: train loss 1.4402, val loss 1.4386\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 109/10000  (1.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.28it/s]\n","iter 13000: train loss 1.4403, val loss 1.4390\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 89/10000  (0.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.04it/s]\n","iter 14000: train loss 1.4398, val loss 1.4412\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 101/10000  (1.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.35it/s]\n","iter 15000: train loss 1.4403, val loss 1.4399\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 91/10000  (0.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.28it/s]\n","iter 16000: train loss 1.4405, val loss 1.4397\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 100/10000  (1.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.28it/s]\n","iter 17000: train loss 1.4403, val loss 1.4413\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 82/10000  (0.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.00it/s]\n","iter 18000: train loss 1.3582, val loss 1.3571\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 753/10000  (7.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.74it/s]\n","iter 19000: train loss 1.3486, val loss 1.3509\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 901/10000  (9.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.27it/s]\n","iter 20000: train loss 1.3426, val loss 1.3440\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 965/10000  (9.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.11it/s]\n","iter 21000: train loss 1.3475, val loss 1.3474\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 903/10000  (9.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.70it/s]\n","iter 22000: train loss 1.3457, val loss 1.3452\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 911/10000  (9.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.64it/s]\n","iter 23000: train loss 1.3405, val loss 1.3408\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 985/10000  (9.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.48it/s]\n","iter 24000: train loss 1.3426, val loss 1.3435\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1008/10000  (10.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.03it/s]\n","iter 25000: train loss 1.3427, val loss 1.3431\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 964/10000  (9.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.79it/s]\n","iter 26000: train loss 1.3407, val loss 1.3417\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 977/10000  (9.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.04it/s]\n","iter 27000: train loss 1.3412, val loss 1.3427\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1012/10000  (10.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.35it/s]\n","iter 28000: train loss 1.3403, val loss 1.3406\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 993/10000  (9.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.44it/s]\n","iter 29000: train loss 1.3399, val loss 1.3411\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 975/10000  (9.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.29it/s]\n","iter 30000: train loss 1.3401, val loss 1.3429\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 980/10000  (9.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.56it/s]\n","iter 31000: train loss 1.3389, val loss 1.3408\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1024/10000  (10.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.52it/s]\n","iter 32000: train loss 1.3387, val loss 1.3410\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1002/10000  (10.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.23it/s]\n","iter 33000: train loss 1.3380, val loss 1.3420\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 999/10000  (9.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.28it/s]\n","iter 34000: train loss 1.3378, val loss 1.3425\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1030/10000  (10.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.12it/s]\n","iter 35000: train loss 1.3389, val loss 1.3412\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1007/10000  (10.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.17it/s]\n","iter 36000: train loss 1.3373, val loss 1.3426\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1019/10000  (10.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.71it/s]\n","iter 37000: train loss 1.3365, val loss 1.3439\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1005/10000  (10.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.04it/s]\n","iter 38000: train loss 1.2470, val loss 1.2504\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9125/10000  (91.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.01it/s]\n","iter 39000: train loss 1.2554, val loss 1.2584\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8058/10000  (80.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.83it/s]\n","iter 40000: train loss 1.2391, val loss 1.2430\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9980/10000  (99.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.98it/s]\n","iter 41000: train loss 1.2385, val loss 1.2427\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.27it/s]\n","iter 42000: train loss 1.2384, val loss 1.2432\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.95it/s]\n","iter 43000: train loss 1.2392, val loss 1.2407\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9955/10000  (99.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.89it/s]\n","iter 44000: train loss 1.2376, val loss 1.2430\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9965/10000  (99.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.76it/s]\n","iter 45000: train loss 1.2365, val loss 1.2432\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.23it/s]\n","iter 46000: train loss 1.2362, val loss 1.2442\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.22it/s]\n","iter 47000: train loss 1.2380, val loss 1.2437\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 32.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 32.92it/s]\n","iter 48000: train loss 1.2350, val loss 1.2448\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 32.91it/s]\n","iter 49000: train loss 1.2340, val loss 1.2459\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.07it/s]\n","iter 50000: train loss 1.2334, val loss 1.2454\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.71it/s]\n","iter 51000: train loss 1.2329, val loss 1.2470\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.36it/s]\n","iter 52000: train loss 1.2322, val loss 1.2479\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.98it/s]\n","iter 53000: train loss 1.2308, val loss 1.2479\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.21it/s]\n","iter 54000: train loss 1.2299, val loss 1.2492\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.45it/s]\n","iter 55000: train loss 1.2271, val loss 1.2506\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.79it/s]\n","iter 56000: train loss 1.2238, val loss 1.2557\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.48it/s]\n","iter 57000: train loss 1.2223, val loss 1.2561\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.12it/s]\n","iter 58000: train loss 1.2194, val loss 1.2587\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.93it/s]\n","iter 59000: train loss 1.2163, val loss 1.2574\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.72it/s]\n","iter 60000: train loss 1.2147, val loss 1.2614\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 32.97it/s]\n","iter 61000: train loss 1.2113, val loss 1.2644\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.78it/s]\n","iter 62000: train loss 1.2089, val loss 1.2648\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.89it/s]\n","iter 63000: train loss 1.2083, val loss 1.2619\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.68it/s]\n","iter 64000: train loss 1.2055, val loss 1.2656\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 32.65it/s]\n","iter 65000: train loss 1.1998, val loss 1.2697\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.33it/s]\n","iter 66000: train loss 1.1998, val loss 1.2692\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.02it/s]\n","iter 67000: train loss 1.1985, val loss 1.2678\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.25it/s]\n","iter 68000: train loss 1.1971, val loss 1.2714\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 32.31it/s]\n","iter 69000: train loss 1.1943, val loss 1.2756\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.03it/s]\n","iter 70000: train loss 1.1927, val loss 1.2747\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.73it/s]\n","iter 71000: train loss 1.1901, val loss 1.2750\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.87it/s]\n","iter 72000: train loss 1.1895, val loss 1.2787\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.54it/s]\n","iter 73000: train loss 1.1889, val loss 1.2784\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.79it/s]\n","iter 74000: train loss 1.1872, val loss 1.2781\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.24it/s]\n","iter 75000: train loss 1.1829, val loss 1.2819\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 32.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.54it/s]\n","iter 76000: train loss 1.1831, val loss 1.2802\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.17it/s]\n","iter 77000: train loss 1.1814, val loss 1.2807\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.71it/s]\n","iter 78000: train loss 1.1791, val loss 1.2817\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.27it/s]\n","iter 79000: train loss 1.1790, val loss 1.2834\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.83it/s]\n","iter 80000: train loss 1.1808, val loss 1.2861\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.54it/s]\n"]}],"source":["!python train.py slicing_addition_4_operand.txt --batch slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"eAMCHgiqoXXW","outputId":"86e0de8c-f8f1-4964-ccc3-fcff5ce3ad30"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: slicing\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/slicing_addition_4_operand.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'slicing_4_operand_reversed_test'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","batch_size = 256\n","block_size = 512\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# whether need zero‚Äêpadding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000\n","lr_decay_iters = 800000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/slicing_reverse_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","Creating meta file for all reasonable characters...\n","all the unique characters: \n"," !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n","vocab size: 96\n","data has 22,518,028 tokens\n","data has 225,140 tokens\n","Using vocabulary size: 96\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.82M\n","/content/drive/MyDrive/addition/train_test.py:396: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run fak2f44e (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run fak2f44e (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m setting up run fak2f44e (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/slicing_reverse_out/wandb/run-20251226_170814-fak2f44e\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mslicing_4_operand_reversed_test\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/fak2f44e\u001b[0m\n","WARNING: results directory results/4_operands_0_to_999_uniform/slicing_reverse_out/slicing_4_operand_reversed_test already exists, overwriting...\n","max_new_tokens: 5\n","iter 0: train loss 4.2711, val loss 4.2679\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:02\u003c00:00, 30.34it/s]\n","/content/drive/MyDrive/addition/evaluation.py:408: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02\u003c00:00, 34.32it/s]\n","iter 1000: train loss 1.5417, val loss 1.5410\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.72it/s]\n","iter 2000: train loss 1.4927, val loss 1.4913\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 26/10000  (0.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.70it/s]\n","iter 3000: train loss 1.4625, val loss 1.4616\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 75/10000  (0.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.62it/s]\n","iter 4000: train loss 1.4452, val loss 1.4435\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 103/10000  (1.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.66it/s]\n","iter 5000: train loss 1.4417, val loss 1.4402\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 96/10000  (0.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.75it/s]\n","iter 6000: train loss 1.4415, val loss 1.4403\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 116/10000  (1.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.63it/s]\n","iter 7000: train loss 1.4393, val loss 1.4400\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 88/10000  (0.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.79it/s]\n","iter 8000: train loss 1.4404, val loss 1.4402\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.91it/s]\n","iter 9000: train loss 1.4401, val loss 1.4404\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 122/10000  (1.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.03it/s]\n","iter 10000: train loss 1.4525, val loss 1.4537\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 85/10000  (0.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.84it/s]\n","iter 11000: train loss 1.4399, val loss 1.4417\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 118/10000  (1.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.57it/s]\n","iter 12000: train loss 1.4401, val loss 1.4405\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 108/10000  (1.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.44it/s]\n","iter 13000: train loss 1.4387, val loss 1.4396\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 96/10000  (0.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.74it/s]\n","iter 14000: train loss 1.4408, val loss 1.4420\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 118/10000  (1.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.41it/s]\n","iter 15000: train loss 1.3669, val loss 1.3684\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 582/10000  (5.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.53it/s]\n","iter 16000: train loss 1.3520, val loss 1.3522\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 879/10000  (8.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.97it/s]\n","iter 17000: train loss 1.3405, val loss 1.3415\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1040/10000  (10.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.23it/s]\n","iter 18000: train loss 1.3430, val loss 1.3431\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 887/10000  (8.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.59it/s]\n","iter 19000: train loss 1.3402, val loss 1.3415\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1027/10000  (10.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.18it/s]\n","iter 20000: train loss 1.3424, val loss 1.3413\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 963/10000  (9.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.55it/s]\n","iter 21000: train loss 1.3420, val loss 1.3402\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1048/10000  (10.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.23it/s]\n","iter 22000: train loss 1.3406, val loss 1.3408\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 990/10000  (9.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.97it/s]\n","iter 23000: train loss 1.3406, val loss 1.3404\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1005/10000  (10.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.75it/s]\n","iter 24000: train loss 1.3420, val loss 1.3417\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 967/10000  (9.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.39it/s]\n","iter 25000: train loss 1.3398, val loss 1.3401\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 989/10000  (9.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.01it/s]\n","iter 26000: train loss 1.3407, val loss 1.3413\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 993/10000  (9.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.95it/s]\n","iter 27000: train loss 1.3401, val loss 1.3410\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1021/10000  (10.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.11it/s]\n","iter 28000: train loss 1.3396, val loss 1.3401\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1024/10000  (10.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 32.63it/s]\n","iter 29000: train loss 1.3379, val loss 1.3402\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 33.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 941/10000  (9.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 33.69it/s]\n"]}],"source":["!python train_test.py slicing_addition_4_operand.txt --batch slicing"]},{"cell_type":"markdown","metadata":{"id":"sUhzfB2OeOi-"},"source":["## Scaling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifio3FpGeSYG"},"outputs":[],"source":["!python train.py 100M_4_operands_addition_reversed.txt"]},{"cell_type":"markdown","metadata":{"id":"XuVVoKtiOknn"},"source":["## Positional Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"DFB2D6reOnSk","outputId":"e678cd8b-5407-44d1-f5cb-a13ce517a1d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","test_reverse.txt, 10000 examples: 7542/10000  (75.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.93it/s]\n","iter 928000: train loss 1.4244, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7333/10000  (73.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.38it/s]\n","iter 930000: train loss 1.4233, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.03it/s]\n","iter 932000: train loss 1.4258, val loss 1.4285\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7326/10000  (73.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.12it/s]\n","iter 934000: train loss 1.4219, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7520/10000  (75.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.36it/s]\n","iter 936000: train loss 1.4237, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7507/10000  (75.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.10it/s]\n","iter 938000: train loss 1.4217, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7364/10000  (73.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.21it/s]\n","iter 940000: train loss 1.4254, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7434/10000  (74.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.88it/s]\n","iter 942000: train loss 1.4293, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7476/10000  (74.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.92it/s]\n","iter 944000: train loss 1.4261, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7396/10000  (73.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.12it/s]\n","iter 946000: train loss 1.4230, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.02it/s]\n","iter 948000: train loss 1.4246, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7487/10000  (74.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.95it/s]\n","iter 950000: train loss 1.4259, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7457/10000  (74.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.83it/s]\n","iter 952000: train loss 1.4216, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7205/10000  (72.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.13it/s]\n","iter 954000: train loss 1.4236, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7587/10000  (75.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.11it/s]\n","iter 956000: train loss 1.4229, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.54it/s]\n","iter 958000: train loss 1.4257, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7531/10000  (75.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.25it/s]\n","iter 960000: train loss 1.4275, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7403/10000  (74.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.79it/s]\n","iter 962000: train loss 1.4195, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7579/10000  (75.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.97it/s]\n","iter 964000: train loss 1.4236, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7399/10000  (73.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.05it/s]\n","iter 966000: train loss 1.4232, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7403/10000  (74.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.71it/s]\n","iter 968000: train loss 1.4259, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7316/10000  (73.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.63it/s]\n","iter 970000: train loss 1.4250, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.28it/s]\n","iter 972000: train loss 1.4246, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7498/10000  (74.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.00it/s]\n","iter 974000: train loss 1.4233, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7428/10000  (74.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.15it/s]\n","iter 976000: train loss 1.4210, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7317/10000  (73.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.13it/s]\n","iter 978000: train loss 1.4266, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7445/10000  (74.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.77it/s]\n","iter 980000: train loss 1.4225, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7391/10000  (73.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.67it/s]\n","iter 982000: train loss 1.4239, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7368/10000  (73.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.16it/s]\n","iter 984000: train loss 1.4233, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7508/10000  (75.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.11it/s]\n","iter 986000: train loss 1.4266, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7317/10000  (73.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.24it/s]\n","iter 988000: train loss 1.4175, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7246/10000  (72.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.90it/s]\n","iter 990000: train loss 1.4215, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7358/10000  (73.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.31it/s]\n","iter 992000: train loss 1.4214, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7463/10000  (74.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.07it/s]\n","iter 994000: train loss 1.4285, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7468/10000  (74.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.13it/s]\n","iter 996000: train loss 1.4227, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7546/10000  (75.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.26it/s]\n","iter 998000: train loss 1.4239, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7664/10000  (76.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.81it/s]\n","iter 1000000: train loss 1.4253, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7237/10000  (72.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.10it/s]\n","iter 1002000: train loss 1.4262, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7441/10000  (74.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.04it/s]\n","iter 1004000: train loss 1.4248, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7400/10000  (74.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.20it/s]\n","iter 1006000: train loss 1.4210, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7325/10000  (73.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.93it/s]\n","iter 1008000: train loss 1.4199, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7396/10000  (73.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.08it/s]\n","iter 1010000: train loss 1.4300, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7375/10000  (73.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.86it/s]\n","iter 1012000: train loss 1.4237, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7253/10000  (72.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.94it/s]\n","iter 1014000: train loss 1.4236, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7501/10000  (75.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.07it/s]\n","iter 1016000: train loss 1.4233, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7484/10000  (74.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.99it/s]\n","iter 1018000: train loss 1.4274, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7411/10000  (74.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.54it/s]\n","iter 1020000: train loss 1.4227, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7399/10000  (73.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.13it/s]\n","iter 1022000: train loss 1.4260, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7362/10000  (73.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.82it/s]\n","iter 1024000: train loss 1.4215, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7376/10000  (73.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.67it/s]\n","iter 1026000: train loss 1.4225, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7609/10000  (76.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.99it/s]\n","iter 1028000: train loss 1.4242, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7504/10000  (75.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.94it/s]\n","iter 1030000: train loss 1.4211, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7401/10000  (74.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.91it/s]\n","iter 1032000: train loss 1.4218, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7423/10000  (74.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.98it/s]\n","iter 1034000: train loss 1.4276, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7404/10000  (74.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.12it/s]\n","iter 1036000: train loss 1.4312, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7298/10000  (72.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.35it/s]\n","iter 1038000: train loss 1.4279, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7456/10000  (74.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.77it/s]\n","iter 1040000: train loss 1.4255, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7347/10000  (73.47%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.10it/s]\n","iter 1042000: train loss 1.4271, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7461/10000  (74.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.75it/s]\n","iter 1044000: train loss 1.4225, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7193/10000  (71.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.96it/s]\n","iter 1046000: train loss 1.4260, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7369/10000  (73.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.81it/s]\n","iter 1048000: train loss 1.4231, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7209/10000  (72.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.18it/s]\n","iter 1050000: train loss 1.4220, val loss 1.4287\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.34it/s]\n","iter 1052000: train loss 1.4245, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7361/10000  (73.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.09it/s]\n","iter 1054000: train loss 1.4269, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7418/10000  (74.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.49it/s]\n","iter 1056000: train loss 1.4260, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7373/10000  (73.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.18it/s]\n","iter 1058000: train loss 1.4172, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7334/10000  (73.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.24it/s]\n","iter 1060000: train loss 1.4248, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7367/10000  (73.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.20it/s]\n","iter 1062000: train loss 1.4235, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7372/10000  (73.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.01it/s]\n","iter 1064000: train loss 1.4214, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7527/10000  (75.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.73it/s]\n","iter 1066000: train loss 1.4229, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7402/10000  (74.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.09it/s]\n","iter 1068000: train loss 1.4227, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7458/10000  (74.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.67it/s]\n","iter 1070000: train loss 1.4224, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7349/10000  (73.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.02it/s]\n","iter 1072000: train loss 1.4174, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7434/10000  (74.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.64it/s]\n","iter 1074000: train loss 1.4214, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7550/10000  (75.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.59it/s]\n","iter 1076000: train loss 1.4305, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7384/10000  (73.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.37it/s]\n","iter 1078000: train loss 1.4218, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7417/10000  (74.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.97it/s]\n","iter 1080000: train loss 1.4237, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7368/10000  (73.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.27it/s]\n","iter 1082000: train loss 1.4265, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7376/10000  (73.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.81it/s]\n","iter 1084000: train loss 1.4276, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7166/10000  (71.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.82it/s]\n","iter 1086000: train loss 1.4262, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7428/10000  (74.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.46it/s]\n","iter 1088000: train loss 1.4289, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7366/10000  (73.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.74it/s]\n","iter 1090000: train loss 1.4223, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.24it/s]\n","iter 1092000: train loss 1.4228, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.96it/s]\n","iter 1094000: train loss 1.4273, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7360/10000  (73.60%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.97it/s]\n","iter 1096000: train loss 1.4244, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7307/10000  (73.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.11it/s]\n","iter 1098000: train loss 1.4242, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7418/10000  (74.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.12it/s]\n","iter 1100000: train loss 1.4234, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7496/10000  (74.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.06it/s]\n","iter 1102000: train loss 1.4280, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7549/10000  (75.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.26it/s]\n","iter 1104000: train loss 1.4278, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7543/10000  (75.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.27it/s]\n","iter 1106000: train loss 1.4238, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7348/10000  (73.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.81it/s]\n","iter 1108000: train loss 1.4281, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7482/10000  (74.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.06it/s]\n","iter 1110000: train loss 1.4251, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7364/10000  (73.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.47it/s]\n","iter 1112000: train loss 1.4289, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7395/10000  (73.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.30it/s]\n","iter 1114000: train loss 1.4208, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7437/10000  (74.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.83it/s]\n","iter 1116000: train loss 1.4217, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7384/10000  (73.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.92it/s]\n","iter 1118000: train loss 1.4237, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7318/10000  (73.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.42it/s]\n","iter 1120000: train loss 1.4264, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7567/10000  (75.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.17it/s]\n","iter 1122000: train loss 1.4175, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7314/10000  (73.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.25it/s]\n","iter 1124000: train loss 1.4231, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7519/10000  (75.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.56it/s]\n","iter 1126000: train loss 1.4229, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7333/10000  (73.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.51it/s]\n","iter 1128000: train loss 1.4213, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7258/10000  (72.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.86it/s]\n","iter 1130000: train loss 1.4298, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7356/10000  (73.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.98it/s]\n","iter 1132000: train loss 1.4285, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7539/10000  (75.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.07it/s]\n","iter 1134000: train loss 1.4200, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7540/10000  (75.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.97it/s]\n","iter 1136000: train loss 1.4229, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7165/10000  (71.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.35it/s]\n","iter 1138000: train loss 1.4242, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7632/10000  (76.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.96it/s]\n","iter 1140000: train loss 1.4253, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7372/10000  (73.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.24it/s]\n","iter 1142000: train loss 1.4200, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7359/10000  (73.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.01it/s]\n","iter 1144000: train loss 1.4203, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7472/10000  (74.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.03it/s]\n","iter 1146000: train loss 1.4243, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7451/10000  (74.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.11it/s]\n","iter 1148000: train loss 1.4233, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7602/10000  (76.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.89it/s]\n","iter 1150000: train loss 1.4269, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7439/10000  (74.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.93it/s]\n","iter 1152000: train loss 1.4244, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7495/10000  (74.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.29it/s]\n","iter 1154000: train loss 1.4231, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7398/10000  (73.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.16it/s]\n","iter 1156000: train loss 1.4221, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7550/10000  (75.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.15it/s]\n","iter 1158000: train loss 1.4252, val loss 1.4296\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7382/10000  (73.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.21it/s]\n","iter 1160000: train loss 1.4291, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7425/10000  (74.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.56it/s]\n","iter 1162000: train loss 1.4224, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7496/10000  (74.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.76it/s]\n","iter 1164000: train loss 1.4236, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.51it/s]\n","iter 1166000: train loss 1.4234, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7436/10000  (74.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.04it/s]\n","iter 1168000: train loss 1.4217, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7470/10000  (74.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.27it/s]\n","iter 1170000: train loss 1.4236, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7497/10000  (74.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.11it/s]\n","iter 1172000: train loss 1.4226, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7390/10000  (73.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.05it/s]\n","iter 1174000: train loss 1.4223, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7536/10000  (75.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.66it/s]\n","iter 1176000: train loss 1.4238, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7211/10000  (72.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.24it/s]\n","iter 1178000: train loss 1.4214, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7506/10000  (75.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.61it/s]\n","iter 1180000: train loss 1.4277, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7486/10000  (74.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.71it/s]\n","iter 1182000: train loss 1.4204, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7396/10000  (73.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.92it/s]\n","iter 1184000: train loss 1.4245, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7550/10000  (75.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.89it/s]\n","iter 1186000: train loss 1.4258, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7490/10000  (74.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.03it/s]\n","iter 1188000: train loss 1.4236, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7567/10000  (75.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.05it/s]\n","iter 1190000: train loss 1.4282, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7462/10000  (74.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.07it/s]\n","iter 1192000: train loss 1.4246, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7472/10000  (74.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.02it/s]\n","iter 1194000: train loss 1.4236, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7532/10000  (75.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.08it/s]\n","iter 1196000: train loss 1.4240, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7375/10000  (73.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.75it/s]\n","iter 1198000: train loss 1.4228, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7340/10000  (73.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.00it/s]\n","iter 1200000: train loss 1.4238, val loss 1.4266\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7493/10000  (74.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.11it/s]\n","iter 1202000: train loss 1.4172, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7430/10000  (74.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.61it/s]\n","iter 1204000: train loss 1.4255, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7512/10000  (75.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.04it/s]\n","iter 1206000: train loss 1.4263, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7442/10000  (74.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.84it/s]\n","iter 1208000: train loss 1.4190, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7425/10000  (74.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.88it/s]\n","iter 1210000: train loss 1.4286, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7344/10000  (73.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.62it/s]\n","iter 1212000: train loss 1.4215, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7443/10000  (74.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.04it/s]\n","iter 1214000: train loss 1.4247, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7533/10000  (75.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.18it/s]\n","iter 1216000: train loss 1.4184, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7463/10000  (74.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.81it/s]\n","iter 1218000: train loss 1.4291, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7477/10000  (74.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.64it/s]\n","iter 1220000: train loss 1.4299, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7373/10000  (73.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.79it/s]\n","iter 1222000: train loss 1.4207, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7314/10000  (73.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.89it/s]\n","iter 1224000: train loss 1.4253, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7357/10000  (73.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.06it/s]\n","iter 1226000: train loss 1.4274, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7321/10000  (73.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.81it/s]\n","iter 1228000: train loss 1.4222, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7445/10000  (74.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.50it/s]\n","iter 1230000: train loss 1.4201, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7424/10000  (74.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.96it/s]\n","iter 1232000: train loss 1.4242, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7344/10000  (73.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.08it/s]\n","iter 1234000: train loss 1.4253, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7373/10000  (73.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.79it/s]\n","iter 1236000: train loss 1.4250, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7418/10000  (74.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.93it/s]\n","iter 1238000: train loss 1.4211, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7468/10000  (74.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.08it/s]\n","iter 1240000: train loss 1.4252, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7492/10000  (74.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.81it/s]\n","iter 1242000: train loss 1.4233, val loss 1.4285\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7395/10000  (73.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.78it/s]\n","iter 1244000: train loss 1.4197, val loss 1.4282\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7603/10000  (76.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.67it/s]\n","iter 1246000: train loss 1.4242, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7633/10000  (76.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.00it/s]\n","iter 1248000: train loss 1.4217, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7181/10000  (71.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.30it/s]\n","iter 1250000: train loss 1.4235, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.39it/s]\n","iter 1252000: train loss 1.4211, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7232/10000  (72.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.87it/s]\n","iter 1254000: train loss 1.4262, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7446/10000  (74.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.90it/s]\n","iter 1256000: train loss 1.4256, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7356/10000  (73.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.28it/s]\n","iter 1258000: train loss 1.4287, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7343/10000  (73.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.05it/s]\n","iter 1260000: train loss 1.4250, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7420/10000  (74.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.03it/s]\n","iter 1262000: train loss 1.4240, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7301/10000  (73.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.03it/s]\n","iter 1264000: train loss 1.4277, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7439/10000  (74.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.15it/s]\n","iter 1266000: train loss 1.4284, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7449/10000  (74.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.77it/s]\n","iter 1268000: train loss 1.4261, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7407/10000  (74.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.84it/s]\n","iter 1270000: train loss 1.4249, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7430/10000  (74.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.60it/s]\n","iter 1272000: train loss 1.4248, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7449/10000  (74.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.78it/s]\n","iter 1274000: train loss 1.4181, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7269/10000  (72.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.77it/s]\n","iter 1276000: train loss 1.4234, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7415/10000  (74.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.96it/s]\n","iter 1278000: train loss 1.4278, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7399/10000  (73.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.67it/s]\n","iter 1280000: train loss 1.4247, val loss 1.4282\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7417/10000  (74.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.66it/s]\n","iter 1282000: train loss 1.4270, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7255/10000  (72.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.64it/s]\n","iter 1284000: train loss 1.4247, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7430/10000  (74.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.50it/s]\n","iter 1286000: train loss 1.4228, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7463/10000  (74.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.49it/s]\n","iter 1288000: train loss 1.4285, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7329/10000  (73.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.04it/s]\n","iter 1290000: train loss 1.4264, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7348/10000  (73.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.06it/s]\n","iter 1292000: train loss 1.4200, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.06it/s]\n","iter 1294000: train loss 1.4293, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7400/10000  (74.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.93it/s]\n","iter 1296000: train loss 1.4208, val loss 1.4265\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7321/10000  (73.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.03it/s]\n","iter 1298000: train loss 1.4231, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7349/10000  (73.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.47it/s]\n","iter 1300000: train loss 1.4184, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7616/10000  (76.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.74it/s]\n","iter 1302000: train loss 1.4239, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.02it/s]\n","iter 1304000: train loss 1.4257, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7504/10000  (75.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.57it/s]\n","iter 1306000: train loss 1.4220, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7407/10000  (74.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.77it/s]\n","iter 1308000: train loss 1.4297, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7502/10000  (75.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.07it/s]\n","iter 1310000: train loss 1.4269, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7330/10000  (73.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.04it/s]\n","iter 1312000: train loss 1.4205, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7442/10000  (74.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.73it/s]\n","iter 1314000: train loss 1.4227, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7276/10000  (72.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.60it/s]\n","iter 1316000: train loss 1.4197, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7440/10000  (74.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.68it/s]\n","iter 1318000: train loss 1.4224, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7501/10000  (75.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.95it/s]\n","iter 1320000: train loss 1.4249, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7372/10000  (73.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.31it/s]\n","iter 1322000: train loss 1.4214, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7275/10000  (72.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.69it/s]\n","iter 1324000: train loss 1.4221, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7230/10000  (72.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.83it/s]\n","iter 1326000: train loss 1.4272, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7400/10000  (74.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.50it/s]\n","iter 1328000: train loss 1.4248, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7408/10000  (74.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.70it/s]\n","iter 1330000: train loss 1.4272, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7475/10000  (74.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.98it/s]\n","iter 1332000: train loss 1.4246, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7456/10000  (74.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.93it/s]\n","iter 1334000: train loss 1.4253, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7534/10000  (75.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.01it/s]\n","iter 1336000: train loss 1.4230, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7296/10000  (72.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.85it/s]\n","iter 1338000: train loss 1.4206, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7566/10000  (75.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.80it/s]\n","iter 1340000: train loss 1.4244, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7520/10000  (75.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.84it/s]\n","iter 1342000: train loss 1.4226, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7389/10000  (73.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.10it/s]\n","iter 1344000: train loss 1.4247, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7406/10000  (74.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.19it/s]\n","iter 1346000: train loss 1.4217, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7537/10000  (75.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.04it/s]\n","iter 1348000: train loss 1.4224, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7396/10000  (73.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.04it/s]\n","iter 1350000: train loss 1.4213, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7383/10000  (73.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.76it/s]\n","iter 1352000: train loss 1.4240, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7286/10000  (72.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.71it/s]\n","iter 1354000: train loss 1.4302, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7216/10000  (72.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.30it/s]\n","iter 1356000: train loss 1.4244, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7459/10000  (74.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.59it/s]\n","iter 1358000: train loss 1.4260, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7487/10000  (74.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.90it/s]\n","iter 1360000: train loss 1.4232, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7420/10000  (74.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.78it/s]\n","iter 1362000: train loss 1.4248, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7457/10000  (74.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.79it/s]\n","iter 1364000: train loss 1.4283, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7441/10000  (74.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.90it/s]\n","iter 1366000: train loss 1.4290, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7179/10000  (71.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.54it/s]\n","iter 1368000: train loss 1.4219, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7593/10000  (75.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.08it/s]\n","iter 1370000: train loss 1.4218, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7232/10000  (72.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.94it/s]\n","iter 1372000: train loss 1.4258, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7304/10000  (73.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.64it/s]\n","iter 1374000: train loss 1.4253, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7488/10000  (74.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.14it/s]\n","iter 1376000: train loss 1.4315, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7259/10000  (72.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.11it/s]\n","iter 1378000: train loss 1.4310, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7556/10000  (75.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.19it/s]\n","iter 1380000: train loss 1.4221, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7333/10000  (73.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.82it/s]\n","iter 1382000: train loss 1.4260, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7402/10000  (74.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.05it/s]\n","iter 1384000: train loss 1.4232, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7403/10000  (74.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.96it/s]\n","iter 1386000: train loss 1.4210, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7399/10000  (73.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.91it/s]\n","iter 1388000: train loss 1.4241, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.57it/s]\n","iter 1390000: train loss 1.4256, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7459/10000  (74.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.00it/s]\n","iter 1392000: train loss 1.4228, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7381/10000  (73.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.58it/s]\n","iter 1394000: train loss 1.4263, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7363/10000  (73.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.54it/s]\n","iter 1396000: train loss 1.4254, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7376/10000  (73.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.76it/s]\n","iter 1398000: train loss 1.4208, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7529/10000  (75.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.82it/s]\n","iter 1400000: train loss 1.4213, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7414/10000  (74.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.78it/s]\n","iter 1402000: train loss 1.4286, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7343/10000  (73.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.53it/s]\n","iter 1404000: train loss 1.4252, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7296/10000  (72.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.49it/s]\n","iter 1406000: train loss 1.4195, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7397/10000  (73.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.69it/s]\n","iter 1408000: train loss 1.4267, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7287/10000  (72.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.91it/s]\n","iter 1410000: train loss 1.4229, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7413/10000  (74.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.72it/s]\n","iter 1412000: train loss 1.4209, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7426/10000  (74.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.48it/s]\n","iter 1414000: train loss 1.4256, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7519/10000  (75.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.75it/s]\n","iter 1416000: train loss 1.4190, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7325/10000  (73.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.69it/s]\n","iter 1418000: train loss 1.4272, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7411/10000  (74.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.16it/s]\n","iter 1420000: train loss 1.4237, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7439/10000  (74.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.63it/s]\n","iter 1422000: train loss 1.4248, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7427/10000  (74.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.91it/s]\n","iter 1424000: train loss 1.4213, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7370/10000  (73.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.00it/s]\n","iter 1426000: train loss 1.4233, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7607/10000  (76.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.49it/s]\n","iter 1428000: train loss 1.4246, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7394/10000  (73.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.67it/s]\n","iter 1430000: train loss 1.4195, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7513/10000  (75.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.41it/s]\n","iter 1432000: train loss 1.4253, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7320/10000  (73.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.90it/s]\n","iter 1434000: train loss 1.4256, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7416/10000  (74.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.64it/s]\n","iter 1436000: train loss 1.4262, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7493/10000  (74.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.73it/s]\n","iter 1438000: train loss 1.4216, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7344/10000  (73.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.08it/s]\n","iter 1440000: train loss 1.4234, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7371/10000  (73.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.92it/s]\n","iter 1442000: train loss 1.4172, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.18it/s]\n","iter 1444000: train loss 1.4249, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7552/10000  (75.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.03it/s]\n","iter 1446000: train loss 1.4222, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7446/10000  (74.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.70it/s]\n","iter 1448000: train loss 1.4235, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7546/10000  (75.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.03it/s]\n","iter 1450000: train loss 1.4224, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7404/10000  (74.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.56it/s]\n","iter 1452000: train loss 1.4213, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7581/10000  (75.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.58it/s]\n","iter 1454000: train loss 1.4252, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7350/10000  (73.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.87it/s]\n","iter 1456000: train loss 1.4222, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7481/10000  (74.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.65it/s]\n","iter 1458000: train loss 1.4226, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7351/10000  (73.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.42it/s]\n","iter 1460000: train loss 1.4250, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7493/10000  (74.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.68it/s]\n","iter 1462000: train loss 1.4230, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7488/10000  (74.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.83it/s]\n","iter 1464000: train loss 1.4209, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7394/10000  (73.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.16it/s]\n","iter 1466000: train loss 1.4263, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7480/10000  (74.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.87it/s]\n","iter 1468000: train loss 1.4211, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7435/10000  (74.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.41it/s]\n","iter 1470000: train loss 1.4294, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7426/10000  (74.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.80it/s]\n","iter 1472000: train loss 1.4241, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7373/10000  (73.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.79it/s]\n","iter 1474000: train loss 1.4230, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7455/10000  (74.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.49it/s]\n","iter 1476000: train loss 1.4216, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7406/10000  (74.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.87it/s]\n","iter 1478000: train loss 1.4210, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7289/10000  (72.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.55it/s]\n","iter 1480000: train loss 1.4223, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7284/10000  (72.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.77it/s]\n","iter 1482000: train loss 1.4266, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7331/10000  (73.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.40it/s]\n","iter 1484000: train loss 1.4241, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7574/10000  (75.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.04it/s]\n","iter 1486000: train loss 1.4244, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7359/10000  (73.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.49it/s]\n","iter 1488000: train loss 1.4250, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7273/10000  (72.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.98it/s]\n","iter 1490000: train loss 1.4225, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7508/10000  (75.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.07it/s]\n","iter 1492000: train loss 1.4235, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7511/10000  (75.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.10it/s]\n","iter 1494000: train loss 1.4246, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7386/10000  (73.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.89it/s]\n","iter 1496000: train loss 1.4268, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7572/10000  (75.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.95it/s]\n","iter 1498000: train loss 1.4281, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7366/10000  (73.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.84it/s]\n","iter 1500000: train loss 1.4243, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7558/10000  (75.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.89it/s]\n","iter 1502000: train loss 1.4230, val loss 1.4266\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7548/10000  (75.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.65it/s]\n","iter 1504000: train loss 1.4223, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7288/10000  (72.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.76it/s]\n","iter 1506000: train loss 1.4204, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7429/10000  (74.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.04it/s]\n","iter 1508000: train loss 1.4259, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7228/10000  (72.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.84it/s]\n","iter 1510000: train loss 1.4275, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7440/10000  (74.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.59it/s]\n","iter 1512000: train loss 1.4262, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7411/10000  (74.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.95it/s]\n","iter 1514000: train loss 1.4210, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7558/10000  (75.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.14it/s]\n","iter 1516000: train loss 1.4222, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7381/10000  (73.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.25it/s]\n","iter 1518000: train loss 1.4235, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7416/10000  (74.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.03it/s]\n","iter 1520000: train loss 1.4253, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7347/10000  (73.47%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.29it/s]\n","iter 1522000: train loss 1.4268, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7465/10000  (74.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.21it/s]\n","iter 1524000: train loss 1.4266, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7311/10000  (73.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.95it/s]\n","iter 1526000: train loss 1.4234, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7510/10000  (75.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.37it/s]\n","iter 1528000: train loss 1.4286, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.95it/s]\n","iter 1530000: train loss 1.4221, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.77it/s]\n","iter 1532000: train loss 1.4260, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7364/10000  (73.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.48it/s]\n","iter 1534000: train loss 1.4215, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7430/10000  (74.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.92it/s]\n","iter 1536000: train loss 1.4230, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7242/10000  (72.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.17it/s]\n","iter 1538000: train loss 1.4206, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7657/10000  (76.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.75it/s]\n","iter 1540000: train loss 1.4226, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7472/10000  (74.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.24it/s]\n","iter 1542000: train loss 1.4233, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7436/10000  (74.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.01it/s]\n","iter 1544000: train loss 1.4244, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7277/10000  (72.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.43it/s]\n","iter 1546000: train loss 1.4262, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7617/10000  (76.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.07it/s]\n","iter 1548000: train loss 1.4244, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7368/10000  (73.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.91it/s]\n","iter 1550000: train loss 1.4214, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7330/10000  (73.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.88it/s]\n","iter 1552000: train loss 1.4256, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.15it/s]\n","iter 1554000: train loss 1.4219, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7171/10000  (71.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.76it/s]\n","iter 1556000: train loss 1.4235, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7407/10000  (74.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.72it/s]\n","iter 1558000: train loss 1.4232, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7474/10000  (74.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.97it/s]\n","iter 1560000: train loss 1.4325, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7418/10000  (74.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.63it/s]\n","iter 1562000: train loss 1.4284, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7487/10000  (74.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.68it/s]\n","iter 1564000: train loss 1.4221, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7384/10000  (73.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.61it/s]\n","iter 1566000: train loss 1.4220, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7502/10000  (75.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.91it/s]\n","iter 1568000: train loss 1.4275, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7440/10000  (74.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.83it/s]\n","iter 1570000: train loss 1.4216, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7467/10000  (74.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.89it/s]\n","iter 1572000: train loss 1.4191, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7571/10000  (75.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.90it/s]\n","iter 1574000: train loss 1.4240, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7544/10000  (75.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.12it/s]\n","iter 1576000: train loss 1.4268, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7357/10000  (73.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.09it/s]\n","iter 1578000: train loss 1.4216, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7419/10000  (74.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.79it/s]\n","iter 1580000: train loss 1.4307, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7438/10000  (74.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.36it/s]\n","iter 1582000: train loss 1.4241, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7191/10000  (71.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.45it/s]\n","iter 1584000: train loss 1.4271, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7408/10000  (74.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.98it/s]\n","iter 1586000: train loss 1.4226, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7429/10000  (74.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.89it/s]\n","iter 1588000: train loss 1.4232, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7472/10000  (74.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.85it/s]\n","iter 1590000: train loss 1.4242, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7489/10000  (74.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.50it/s]\n","iter 1592000: train loss 1.4207, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7551/10000  (75.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.36it/s]\n","iter 1594000: train loss 1.4243, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7427/10000  (74.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.57it/s]\n","iter 1596000: train loss 1.4261, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7578/10000  (75.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.87it/s]\n","iter 1598000: train loss 1.4221, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7584/10000  (75.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.82it/s]\n","iter 1600000: train loss 1.4214, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7453/10000  (74.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.02it/s]\n","iter 1602000: train loss 1.4280, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7375/10000  (73.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.76it/s]\n","iter 1604000: train loss 1.4210, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7434/10000  (74.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.14it/s]\n","iter 1606000: train loss 1.4202, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7470/10000  (74.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.12it/s]\n","iter 1608000: train loss 1.4199, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7396/10000  (73.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.04it/s]\n","iter 1610000: train loss 1.4210, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.77it/s]\n","iter 1612000: train loss 1.4258, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7416/10000  (74.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.14it/s]\n","iter 1614000: train loss 1.4263, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7428/10000  (74.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.63it/s]\n","iter 1616000: train loss 1.4232, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7417/10000  (74.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.98it/s]\n","iter 1618000: train loss 1.4189, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7388/10000  (73.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.99it/s]\n","iter 1620000: train loss 1.4272, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 19.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7513/10000  (75.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.75it/s]\n","iter 1622000: train loss 1.4187, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7315/10000  (73.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.69it/s]\n","iter 1624000: train loss 1.4193, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7665/10000  (76.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.42it/s]\n","iter 1626000: train loss 1.4285, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7480/10000  (74.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.47it/s]\n","iter 1628000: train loss 1.4267, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7289/10000  (72.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.28it/s]\n","iter 1630000: train loss 1.4275, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7293/10000  (72.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.82it/s]\n","iter 1632000: train loss 1.4215, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.28it/s]\n","iter 1634000: train loss 1.4252, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7319/10000  (73.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 19.12it/s]\n","iter 1636000: train loss 1.4236, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7358/10000  (73.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.93it/s]\n","iter 1638000: train loss 1.4189, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7482/10000  (74.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.38it/s]\n","iter 1640000: train loss 1.4287, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7289/10000  (72.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.51it/s]\n","iter 1642000: train loss 1.4247, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7367/10000  (73.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.60it/s]\n","iter 1644000: train loss 1.4322, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7507/10000  (75.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.53it/s]\n","iter 1646000: train loss 1.4238, val loss 1.4266\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7522/10000  (75.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.78it/s]\n","iter 1648000: train loss 1.4263, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7483/10000  (74.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.36it/s]\n","iter 1650000: train loss 1.4293, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7166/10000  (71.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.20it/s]\n","iter 1652000: train loss 1.4264, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7510/10000  (75.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.95it/s]\n","iter 1654000: train loss 1.4246, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7336/10000  (73.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.36it/s]\n","iter 1656000: train loss 1.4201, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7313/10000  (73.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.27it/s]\n","iter 1658000: train loss 1.4233, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7443/10000  (74.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.31it/s]\n","iter 1660000: train loss 1.4246, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7424/10000  (74.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.18it/s]\n","iter 1662000: train loss 1.4239, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7251/10000  (72.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.96it/s]\n","iter 1664000: train loss 1.4196, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7503/10000  (75.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.33it/s]\n","iter 1666000: train loss 1.4241, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7242/10000  (72.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.43it/s]\n","iter 1668000: train loss 1.4218, val loss 1.4284\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7380/10000  (73.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.28it/s]\n","iter 1670000: train loss 1.4258, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7443/10000  (74.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.29it/s]\n","iter 1672000: train loss 1.4175, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7515/10000  (75.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.14it/s]\n","iter 1674000: train loss 1.4166, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7426/10000  (74.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.55it/s]\n","iter 1676000: train loss 1.4254, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7537/10000  (75.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.85it/s]\n","iter 1678000: train loss 1.4242, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7474/10000  (74.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.54it/s]\n","iter 1680000: train loss 1.4234, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7301/10000  (73.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.50it/s]\n","iter 1682000: train loss 1.4277, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7234/10000  (72.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.51it/s]\n","iter 1684000: train loss 1.4201, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7220/10000  (72.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.85it/s]\n","iter 1686000: train loss 1.4257, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7382/10000  (73.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.37it/s]\n","iter 1688000: train loss 1.4222, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7286/10000  (72.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.89it/s]\n","iter 1690000: train loss 1.4250, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7487/10000  (74.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.21it/s]\n","iter 1692000: train loss 1.4238, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7633/10000  (76.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.06it/s]\n","iter 1694000: train loss 1.4237, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7329/10000  (73.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.21it/s]\n","iter 1696000: train loss 1.4245, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7422/10000  (74.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.38it/s]\n","iter 1698000: train loss 1.4249, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7375/10000  (73.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.33it/s]\n","iter 1700000: train loss 1.4281, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7379/10000  (73.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.33it/s]\n","iter 1702000: train loss 1.4268, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7482/10000  (74.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.24it/s]\n","iter 1704000: train loss 1.4258, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7355/10000  (73.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.93it/s]\n","iter 1706000: train loss 1.4246, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7518/10000  (75.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.51it/s]\n","iter 1708000: train loss 1.4219, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.78it/s]\n","iter 1710000: train loss 1.4171, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7531/10000  (75.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.42it/s]\n","iter 1712000: train loss 1.4231, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.71it/s]\n","iter 1714000: train loss 1.4255, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7264/10000  (72.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.45it/s]\n","iter 1716000: train loss 1.4202, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7131/10000  (71.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.24it/s]\n","iter 1718000: train loss 1.4219, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7388/10000  (73.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.53it/s]\n","iter 1720000: train loss 1.4173, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7281/10000  (72.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.53it/s]\n","iter 1722000: train loss 1.4283, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7333/10000  (73.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.08it/s]\n","iter 1724000: train loss 1.4233, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.21it/s]\n","iter 1726000: train loss 1.4246, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7210/10000  (72.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.37it/s]\n","iter 1728000: train loss 1.4219, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7426/10000  (74.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.67it/s]\n","iter 1730000: train loss 1.4282, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7592/10000  (75.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.48it/s]\n","iter 1732000: train loss 1.4207, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7530/10000  (75.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.65it/s]\n","iter 1734000: train loss 1.4200, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 16.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7361/10000  (73.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.62it/s]\n","iter 1736000: train loss 1.4315, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7530/10000  (75.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.41it/s]\n","iter 1738000: train loss 1.4256, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7492/10000  (74.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.35it/s]\n","iter 1740000: train loss 1.4200, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7374/10000  (73.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.26it/s]\n","iter 1742000: train loss 1.4185, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7270/10000  (72.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.02it/s]\n","iter 1744000: train loss 1.4209, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7342/10000  (73.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.47it/s]\n","iter 1746000: train loss 1.4226, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7308/10000  (73.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.88it/s]\n","iter 1748000: train loss 1.4279, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7460/10000  (74.60%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.28it/s]\n","iter 1750000: train loss 1.4254, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7678/10000  (76.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.14it/s]\n","iter 1752000: train loss 1.4242, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7357/10000  (73.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.20it/s]\n","iter 1754000: train loss 1.4242, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7581/10000  (75.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.16it/s]\n","iter 1756000: train loss 1.4229, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7456/10000  (74.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.50it/s]\n","iter 1758000: train loss 1.4284, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7435/10000  (74.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.37it/s]\n","iter 1760000: train loss 1.4258, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7349/10000  (73.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.31it/s]\n","iter 1762000: train loss 1.4245, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7501/10000  (75.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.97it/s]\n","iter 1764000: train loss 1.4234, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7478/10000  (74.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.51it/s]\n","iter 1766000: train loss 1.4284, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7423/10000  (74.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.35it/s]\n","iter 1768000: train loss 1.4254, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7483/10000  (74.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.59it/s]\n","iter 1770000: train loss 1.4219, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7418/10000  (74.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.98it/s]\n","iter 1772000: train loss 1.4251, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.24it/s]\n","iter 1774000: train loss 1.4270, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7462/10000  (74.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.98it/s]\n","iter 1776000: train loss 1.4250, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7202/10000  (72.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.70it/s]\n","iter 1778000: train loss 1.4249, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7536/10000  (75.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.56it/s]\n","iter 1780000: train loss 1.4300, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7313/10000  (73.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.14it/s]\n","iter 1782000: train loss 1.4194, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7400/10000  (74.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.51it/s]\n","iter 1784000: train loss 1.4244, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7481/10000  (74.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.52it/s]\n","iter 1786000: train loss 1.4234, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7220/10000  (72.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.29it/s]\n","iter 1788000: train loss 1.4249, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7487/10000  (74.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.28it/s]\n","iter 1790000: train loss 1.4230, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7541/10000  (75.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.28it/s]\n","iter 1792000: train loss 1.4251, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7469/10000  (74.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.34it/s]\n","iter 1794000: train loss 1.4297, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7431/10000  (74.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.52it/s]\n","iter 1796000: train loss 1.4258, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7293/10000  (72.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.54it/s]\n","iter 1798000: train loss 1.4241, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7577/10000  (75.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.62it/s]\n","iter 1800000: train loss 1.4227, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7299/10000  (72.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.22it/s]\n","iter 1802000: train loss 1.4246, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7518/10000  (75.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.64it/s]\n","iter 1804000: train loss 1.4267, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7374/10000  (73.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.96it/s]\n","iter 1806000: train loss 1.4257, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7517/10000  (75.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.07it/s]\n","iter 1808000: train loss 1.4248, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7331/10000  (73.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.43it/s]\n","iter 1810000: train loss 1.4230, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7287/10000  (72.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.28it/s]\n","iter 1812000: train loss 1.4242, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7507/10000  (75.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.52it/s]\n","iter 1814000: train loss 1.4251, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7403/10000  (74.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.20it/s]\n","iter 1816000: train loss 1.4187, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.45it/s]\n","iter 1818000: train loss 1.4223, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7367/10000  (73.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.52it/s]\n","iter 1820000: train loss 1.4320, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7481/10000  (74.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.77it/s]\n","iter 1822000: train loss 1.4243, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7389/10000  (73.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.75it/s]\n","iter 1824000: train loss 1.4224, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7387/10000  (73.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.05it/s]\n","iter 1826000: train loss 1.4195, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7499/10000  (74.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.85it/s]\n","iter 1828000: train loss 1.4255, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7475/10000  (74.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.97it/s]\n","iter 1830000: train loss 1.4232, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7394/10000  (73.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.12it/s]\n","iter 1832000: train loss 1.4221, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7533/10000  (75.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.92it/s]\n","iter 1834000: train loss 1.4235, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.33it/s]\n","iter 1836000: train loss 1.4201, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7467/10000  (74.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.15it/s]\n","iter 1838000: train loss 1.4204, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7478/10000  (74.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.96it/s]\n","iter 1840000: train loss 1.4206, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7437/10000  (74.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.29it/s]\n","iter 1842000: train loss 1.4275, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7453/10000  (74.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.56it/s]\n","iter 1844000: train loss 1.4335, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7336/10000  (73.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.45it/s]\n","iter 1846000: train loss 1.4272, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7495/10000  (74.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.02it/s]\n","iter 1848000: train loss 1.4224, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7359/10000  (73.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.22it/s]\n","iter 1850000: train loss 1.4283, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7422/10000  (74.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.27it/s]\n","iter 1852000: train loss 1.4185, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7452/10000  (74.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.59it/s]\n","iter 1854000: train loss 1.4295, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.73it/s]\n","iter 1856000: train loss 1.4245, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7467/10000  (74.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.37it/s]\n","iter 1858000: train loss 1.4265, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7301/10000  (73.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.03it/s]\n","iter 1860000: train loss 1.4305, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7221/10000  (72.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.84it/s]\n","iter 1862000: train loss 1.4246, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7397/10000  (73.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.19it/s]\n","iter 1864000: train loss 1.4213, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7538/10000  (75.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.38it/s]\n","iter 1866000: train loss 1.4262, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7419/10000  (74.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.80it/s]\n","iter 1868000: train loss 1.4221, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7291/10000  (72.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.02it/s]\n","iter 1870000: train loss 1.4234, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7372/10000  (73.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.02it/s]\n","iter 1872000: train loss 1.4242, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7391/10000  (73.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.23it/s]\n","iter 1874000: train loss 1.4224, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7284/10000  (72.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.31it/s]\n","iter 1876000: train loss 1.4262, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7233/10000  (72.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.11it/s]\n","iter 1878000: train loss 1.4229, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7480/10000  (74.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.21it/s]\n","iter 1880000: train loss 1.4213, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7450/10000  (74.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.72it/s]\n","iter 1882000: train loss 1.4231, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7445/10000  (74.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.05it/s]\n","iter 1884000: train loss 1.4219, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7515/10000  (75.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.29it/s]\n","iter 1886000: train loss 1.4195, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7517/10000  (75.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.55it/s]\n","iter 1888000: train loss 1.4273, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7392/10000  (73.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.03it/s]\n","iter 1890000: train loss 1.4233, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7269/10000  (72.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.81it/s]\n","iter 1892000: train loss 1.4242, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.46it/s]\n","iter 1894000: train loss 1.4289, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7428/10000  (74.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.18it/s]\n","iter 1896000: train loss 1.4201, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7266/10000  (72.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.57it/s]\n","iter 1898000: train loss 1.4222, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7321/10000  (73.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.92it/s]\n","iter 1900000: train loss 1.4205, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7290/10000  (72.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.54it/s]\n","iter 1902000: train loss 1.4278, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7366/10000  (73.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.88it/s]\n","iter 1904000: train loss 1.4226, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7459/10000  (74.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.79it/s]\n","iter 1906000: train loss 1.4202, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7443/10000  (74.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.97it/s]\n","iter 1908000: train loss 1.4231, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7348/10000  (73.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.57it/s]\n","iter 1910000: train loss 1.4267, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7524/10000  (75.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.64it/s]\n","iter 1912000: train loss 1.4244, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7465/10000  (74.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.18it/s]\n","iter 1914000: train loss 1.4241, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7440/10000  (74.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.45it/s]\n","iter 1916000: train loss 1.4244, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7427/10000  (74.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.59it/s]\n","iter 1918000: train loss 1.4250, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7569/10000  (75.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.93it/s]\n","iter 1920000: train loss 1.4262, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7487/10000  (74.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.49it/s]\n","iter 1922000: train loss 1.4198, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 16.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7434/10000  (74.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.46it/s]\n","iter 1924000: train loss 1.4217, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7528/10000  (75.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.69it/s]\n","iter 1926000: train loss 1.4277, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7541/10000  (75.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.91it/s]\n","iter 1928000: train loss 1.4247, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7479/10000  (74.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.42it/s]\n","iter 1930000: train loss 1.4238, val loss 1.4264\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7289/10000  (72.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.78it/s]\n","iter 1932000: train loss 1.4242, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.58it/s]\n","iter 1934000: train loss 1.4238, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7277/10000  (72.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.70it/s]\n","iter 1936000: train loss 1.4253, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7314/10000  (73.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.48it/s]\n","iter 1938000: train loss 1.4238, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7423/10000  (74.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.93it/s]\n","iter 1940000: train loss 1.4253, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7386/10000  (73.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.56it/s]\n","iter 1942000: train loss 1.4272, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7357/10000  (73.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.51it/s]\n","iter 1944000: train loss 1.4243, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7297/10000  (72.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.79it/s]\n","iter 1946000: train loss 1.4233, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7565/10000  (75.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.47it/s]\n","iter 1948000: train loss 1.4262, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7523/10000  (75.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.82it/s]\n","iter 1950000: train loss 1.4226, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7360/10000  (73.60%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.65it/s]\n","iter 1952000: train loss 1.4204, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7542/10000  (75.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.82it/s]\n","iter 1954000: train loss 1.4253, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7303/10000  (73.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.70it/s]\n","iter 1956000: train loss 1.4229, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7436/10000  (74.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.68it/s]\n","iter 1958000: train loss 1.4217, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7490/10000  (74.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.61it/s]\n","iter 1960000: train loss 1.4235, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7425/10000  (74.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 16.97it/s]\n","iter 1962000: train loss 1.4249, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7458/10000  (74.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.15it/s]\n","iter 1964000: train loss 1.4220, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7357/10000  (73.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.64it/s]\n","iter 1966000: train loss 1.4223, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7375/10000  (73.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.47it/s]\n","iter 1968000: train loss 1.4202, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7633/10000  (76.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.50it/s]\n","iter 1970000: train loss 1.4228, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.70it/s]\n","iter 1972000: train loss 1.4257, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7454/10000  (74.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.80it/s]\n","iter 1974000: train loss 1.4200, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7329/10000  (73.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.42it/s]\n","iter 1976000: train loss 1.4257, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7364/10000  (73.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.77it/s]\n","iter 1978000: train loss 1.4199, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7336/10000  (73.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.56it/s]\n","iter 1980000: train loss 1.4255, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7453/10000  (74.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.22it/s]\n","iter 1982000: train loss 1.4262, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7473/10000  (74.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.25it/s]\n","iter 1984000: train loss 1.4230, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7410/10000  (74.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.50it/s]\n","iter 1986000: train loss 1.4275, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7542/10000  (75.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.56it/s]\n","iter 1988000: train loss 1.4257, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7333/10000  (73.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.57it/s]\n","iter 1990000: train loss 1.4287, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7303/10000  (73.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.93it/s]\n","iter 1992000: train loss 1.4207, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7439/10000  (74.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.71it/s]\n","iter 1994000: train loss 1.4244, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7531/10000  (75.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.93it/s]\n","iter 1996000: train loss 1.4251, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7555/10000  (75.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.74it/s]\n","iter 1998000: train loss 1.4257, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7479/10000  (74.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.76it/s]\n","iter 2000000: train loss 1.4217, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7454/10000  (74.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.70it/s]\n","iter 2002000: train loss 1.4261, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7436/10000  (74.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.48it/s]\n","iter 2004000: train loss 1.4227, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7456/10000  (74.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.11it/s]\n","iter 2006000: train loss 1.4232, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.61it/s]\n","iter 2008000: train loss 1.4265, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7434/10000  (74.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.04it/s]\n","iter 2010000: train loss 1.4263, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7242/10000  (72.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.09it/s]\n","iter 2012000: train loss 1.4176, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7223/10000  (72.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.37it/s]\n","iter 2014000: train loss 1.4217, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7480/10000  (74.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.21it/s]\n","iter 2016000: train loss 1.4277, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7361/10000  (73.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.98it/s]\n","iter 2018000: train loss 1.4290, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7356/10000  (73.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.21it/s]\n","iter 2020000: train loss 1.4245, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.10it/s]\n","iter 2022000: train loss 1.4199, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7379/10000  (73.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 17.96it/s]\n","iter 2024000: train loss 1.4202, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7528/10000  (75.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.00it/s]\n","iter 2026000: train loss 1.4247, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7468/10000  (74.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.22it/s]\n","iter 2028000: train loss 1.4291, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 17.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7414/10000  (74.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.29it/s]\n","iter 2030000: train loss 1.4220, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7525/10000  (75.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.05it/s]\n","iter 2032000: train loss 1.4213, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7615/10000  (76.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.19it/s]\n","iter 2034000: train loss 1.4237, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7532/10000  (75.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.50it/s]\n","iter 2036000: train loss 1.4263, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:04\u003c00:00, 18.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7513/10000  (75.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:04\u003c00:00, 18.64it/s]\n"]}],"source":["!python train.py 4_operands_addition_reversed.txt --PE t5"]},{"cell_type":"markdown","metadata":{"id":"HBZphd6jo-sG"},"source":["## Long Multiplication\n","### Plain"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2315,"status":"ok","timestamp":1767392893474,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":360},"id":"Mcyc9Kbqo6jV","outputId":"3ff98a40-b79c-415e-ece9-5a15b9f01e8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33m40_digit_times_1_digit\u001b[0m at: \u001b[34m\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mresults/40_digit_times_1_digit/plain_out/wandb/run-20260102_213807-o0hhtvrt/logs\u001b[0m\n"]}],"source":["! python train.py 40_1_digits_mul_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"nB-lquM70298"},"source":["### Greedy Decoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3fepC7J04x-"},"outputs":[],"source":["!python train.py 2_operands_addition_plain.txt --greedy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"0DkL00TlAe0P"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/4_operands_addition_reversed.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 2000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_0_to_999_uniform_reverse_greedy'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000 # for four operands addition, recommend \u003e= 200000\n","lr_decay_iters = 800000 # make equal to max_iters\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/reverse_out_greedy'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: Can be 'units', 'tens', 'hundreds', 'thousands', or None\n","randomize = None\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","pad_id: 13 eos_id: 1\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.63M\n","/content/drive/MyDrive/addition/train.py:590: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: a4aba93ba2c1ed22804b69ecf18fa5a072b81c47\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Invalid choice\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W\u0026B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W\u0026B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/reverse_out_greedy/wandb/run-20260103_035938-iivy29ki\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m4_operands_0_to_999_uniform_reverse_greedy\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/iivy29ki\u001b[0m\n","max_new_tokens: 5\n","W0103 04:00:06.988000 3659 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","iter 0: train loss 2.7106, val loss 2.7093\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:02\u003c00:00, 35.69it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02\u003c00:00, 39.24it/s]\n","iter 2000: train loss 1.5844, val loss 1.5956\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 65/10000  (0.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.33it/s]\n","iter 4000: train loss 1.5595, val loss 1.5582\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 89/10000  (0.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.85it/s]\n","iter 6000: train loss 1.5461, val loss 1.5493\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 98/10000  (0.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.59it/s]\n","iter 8000: train loss 1.5424, val loss 1.5460\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 104/10000  (1.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.29it/s]\n","iter 10000: train loss 1.5381, val loss 1.5444\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 113/10000  (1.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.40it/s]\n","iter 12000: train loss 1.5464, val loss 1.5460\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.83it/s]\n","iter 14000: train loss 1.5396, val loss 1.5437\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 98/10000  (0.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.70it/s]\n","iter 16000: train loss 1.5400, val loss 1.5441\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 106/10000  (1.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.65it/s]\n","iter 18000: train loss 1.5394, val loss 1.5441\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 113/10000  (1.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.94it/s]\n","iter 20000: train loss 1.5389, val loss 1.5441\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 109/10000  (1.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.97it/s]\n","iter 22000: train loss 1.5344, val loss 1.5430\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 98/10000  (0.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.04it/s]\n","iter 24000: train loss 1.5390, val loss 1.5435\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 81/10000  (0.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.86it/s]\n","iter 26000: train loss 1.5386, val loss 1.5426\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 83/10000  (0.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.42it/s]\n","iter 28000: train loss 1.5419, val loss 1.5435\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 119/10000  (1.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.18it/s]\n","iter 30000: train loss 1.4322, val loss 1.4381\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 913/10000  (9.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.40it/s]\n","iter 32000: train loss 1.4312, val loss 1.4351\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1000/10000  (10.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.23it/s]\n","iter 34000: train loss 1.4274, val loss 1.4303\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 972/10000  (9.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.39it/s]\n","iter 36000: train loss 1.4169, val loss 1.4255\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1021/10000  (10.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.84it/s]\n","iter 38000: train loss 1.4193, val loss 1.4252\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1029/10000  (10.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.88it/s]\n","iter 40000: train loss 1.4188, val loss 1.4248\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 989/10000  (9.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.81it/s]\n","iter 42000: train loss 1.4277, val loss 1.4245\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1008/10000  (10.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.51it/s]\n","iter 44000: train loss 1.4287, val loss 1.4284\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 947/10000  (9.47%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.28it/s]\n","iter 46000: train loss 1.4501, val loss 1.4496\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 820/10000  (8.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.16it/s]\n","iter 48000: train loss 1.4231, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1018/10000  (10.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.56it/s]\n","iter 50000: train loss 1.4324, val loss 1.4308\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 975/10000  (9.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.08it/s]\n","iter 52000: train loss 1.4193, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1006/10000  (10.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.23it/s]\n","iter 54000: train loss 1.4208, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 973/10000  (9.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.73it/s]\n","iter 56000: train loss 1.4215, val loss 1.4253\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 985/10000  (9.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.19it/s]\n","iter 58000: train loss 1.4180, val loss 1.4246\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1020/10000  (10.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.94it/s]\n","iter 60000: train loss 1.4215, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 975/10000  (9.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.85it/s]\n","iter 62000: train loss 1.4221, val loss 1.4248\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 950/10000  (9.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.45it/s]\n","iter 64000: train loss 1.4191, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1015/10000  (10.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.54it/s]\n","iter 66000: train loss 1.4200, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1048/10000  (10.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.62it/s]\n","iter 68000: train loss 1.4191, val loss 1.4245\n","Using precomputed batches\n","100% 80/80 [00:01\u003c00:00, 40.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 991/10000  (9.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.88it/s]\n","iter 70000: train loss 1.4192, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1034/10000  (10.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.57it/s]\n","iter 72000: train loss 1.4187, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1005/10000  (10.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.00it/s]\n","iter 74000: train loss 1.4170, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 986/10000  (9.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.17it/s]\n","iter 76000: train loss 1.4222, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1044/10000  (10.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.44it/s]\n","iter 78000: train loss 1.4263, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1059/10000  (10.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.34it/s]\n","iter 80000: train loss 1.4194, val loss 1.4235\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1001/10000  (10.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.23it/s]\n","iter 82000: train loss 1.4208, val loss 1.4245\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1028/10000  (10.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.39it/s]\n","iter 84000: train loss 1.4190, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1012/10000  (10.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.50it/s]\n","iter 86000: train loss 1.4217, val loss 1.4249\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1023/10000  (10.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.98it/s]\n","iter 88000: train loss 1.4210, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1018/10000  (10.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.29it/s]\n","iter 90000: train loss 1.4159, val loss 1.4251\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 987/10000  (9.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.56it/s]\n","iter 92000: train loss 1.4229, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 998/10000  (9.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.17it/s]\n","iter 94000: train loss 1.4204, val loss 1.4251\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 972/10000  (9.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.77it/s]\n","iter 96000: train loss 1.4233, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1003/10000  (10.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.55it/s]\n","iter 98000: train loss 1.4203, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 989/10000  (9.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.06it/s]\n","iter 100000: train loss 1.4175, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:01\u003c00:00, 40.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 987/10000  (9.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.51it/s]\n","iter 102000: train loss 1.4235, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1024/10000  (10.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.03it/s]\n","iter 104000: train loss 1.4244, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 966/10000  (9.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 40.04it/s]\n","iter 106000: train loss 1.4206, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 986/10000  (9.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.62it/s]\n","iter 108000: train loss 1.4249, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1039/10000  (10.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.66it/s]\n","iter 110000: train loss 1.4202, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 989/10000  (9.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.52it/s]\n","iter 112000: train loss 1.4190, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1046/10000  (10.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.88it/s]\n","iter 114000: train loss 1.4223, val loss 1.4233\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1008/10000  (10.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.08it/s]\n","iter 116000: train loss 1.4180, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1063/10000  (10.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.11it/s]\n","iter 118000: train loss 1.4189, val loss 1.4234\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1017/10000  (10.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.92it/s]\n","iter 120000: train loss 1.4231, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 997/10000  (9.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.87it/s]\n","iter 122000: train loss 1.4241, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1000/10000  (10.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.56it/s]\n","iter 124000: train loss 1.4187, val loss 1.4234\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 949/10000  (9.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.57it/s]\n","iter 126000: train loss 1.4205, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1027/10000  (10.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.84it/s]\n","iter 128000: train loss 1.4179, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 988/10000  (9.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.01it/s]\n","iter 130000: train loss 1.4232, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 958/10000  (9.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 40.04it/s]\n","iter 132000: train loss 1.4215, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 948/10000  (9.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.75it/s]\n","iter 134000: train loss 1.4192, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1015/10000  (10.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.65it/s]\n","iter 136000: train loss 1.4239, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1024/10000  (10.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.29it/s]\n","iter 138000: train loss 1.4237, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1037/10000  (10.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.06it/s]\n","iter 140000: train loss 1.4203, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 986/10000  (9.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.84it/s]\n","iter 142000: train loss 1.4205, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1008/10000  (10.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.21it/s]\n","iter 144000: train loss 1.4270, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 956/10000  (9.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.34it/s]\n","iter 146000: train loss 1.4208, val loss 1.4233\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 963/10000  (9.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.29it/s]\n","iter 148000: train loss 1.4189, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 962/10000  (9.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.25it/s]\n","iter 150000: train loss 1.4151, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1006/10000  (10.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.46it/s]\n","iter 152000: train loss 1.4188, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1030/10000  (10.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.64it/s]\n","iter 154000: train loss 1.4198, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1001/10000  (10.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 40.09it/s]\n","iter 156000: train loss 1.4204, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 982/10000  (9.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.13it/s]\n","iter 158000: train loss 1.4191, val loss 1.4247\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1023/10000  (10.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.32it/s]\n","iter 160000: train loss 1.4242, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 981/10000  (9.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.10it/s]\n","iter 162000: train loss 1.4188, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1018/10000  (10.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.57it/s]\n","iter 164000: train loss 1.4207, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 954/10000  (9.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.73it/s]\n","iter 166000: train loss 1.4208, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1068/10000  (10.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.46it/s]\n","iter 168000: train loss 1.4217, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 971/10000  (9.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.00it/s]\n","iter 170000: train loss 1.4215, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1022/10000  (10.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.53it/s]\n","iter 172000: train loss 1.4161, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 992/10000  (9.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.27it/s]\n","iter 174000: train loss 1.4209, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1062/10000  (10.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.55it/s]\n","iter 176000: train loss 1.4199, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:01\u003c00:00, 40.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1032/10000  (10.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.57it/s]\n","iter 178000: train loss 1.4198, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 943/10000  (9.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.52it/s]\n","iter 180000: train loss 1.4188, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1027/10000  (10.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.87it/s]\n","iter 182000: train loss 1.4150, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1030/10000  (10.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.52it/s]\n","iter 184000: train loss 1.4200, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1042/10000  (10.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.08it/s]\n","iter 186000: train loss 1.4190, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1040/10000  (10.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.75it/s]\n","iter 188000: train loss 1.4211, val loss 1.4251\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1002/10000  (10.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.98it/s]\n","iter 190000: train loss 1.4152, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1033/10000  (10.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.67it/s]\n","iter 192000: train loss 1.4228, val loss 1.4251\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 958/10000  (9.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.95it/s]\n","iter 194000: train loss 1.4218, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1017/10000  (10.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.67it/s]\n","iter 196000: train loss 1.4219, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 986/10000  (9.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.86it/s]\n","iter 198000: train loss 1.4228, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:01\u003c00:00, 40.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 969/10000  (9.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 40.24it/s]\n","iter 200000: train loss 1.4230, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 984/10000  (9.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.10it/s]\n","iter 202000: train loss 1.4199, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 969/10000  (9.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.51it/s]\n","iter 204000: train loss 1.4168, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1050/10000  (10.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.24it/s]\n","iter 206000: train loss 1.4253, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 988/10000  (9.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.23it/s]\n","iter 208000: train loss 1.4191, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 988/10000  (9.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.22it/s]\n","iter 210000: train loss 1.4177, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1061/10000  (10.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.79it/s]\n","iter 212000: train loss 1.4210, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 993/10000  (9.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.70it/s]\n","iter 214000: train loss 1.4181, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 971/10000  (9.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.95it/s]\n","iter 216000: train loss 1.4182, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 992/10000  (9.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.62it/s]\n","iter 218000: train loss 1.4192, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1045/10000  (10.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.84it/s]\n","iter 220000: train loss 1.4205, val loss 1.4235\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 973/10000  (9.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.21it/s]\n","iter 222000: train loss 1.4214, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1033/10000  (10.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.07it/s]\n","iter 224000: train loss 1.4196, val loss 1.4252\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1045/10000  (10.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.14it/s]\n","iter 226000: train loss 1.4187, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1034/10000  (10.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.63it/s]\n","iter 228000: train loss 1.4208, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1023/10000  (10.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.31it/s]\n","iter 230000: train loss 1.4186, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1004/10000  (10.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.71it/s]\n","iter 232000: train loss 1.4197, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1013/10000  (10.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.73it/s]\n","iter 234000: train loss 1.4196, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 983/10000  (9.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.87it/s]\n","iter 236000: train loss 1.4204, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1008/10000  (10.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.11it/s]\n","iter 238000: train loss 1.4222, val loss 1.4245\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1023/10000  (10.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.90it/s]\n","iter 240000: train loss 1.4269, val loss 1.4235\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1051/10000  (10.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.72it/s]\n","iter 242000: train loss 1.4212, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1013/10000  (10.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.70it/s]\n","iter 244000: train loss 1.4226, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1043/10000  (10.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.56it/s]\n","iter 246000: train loss 1.4180, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1019/10000  (10.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.90it/s]\n","iter 248000: train loss 1.4233, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 959/10000  (9.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.86it/s]\n","iter 250000: train loss 1.4225, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1020/10000  (10.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.07it/s]\n","iter 252000: train loss 1.4202, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1003/10000  (10.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.35it/s]\n","iter 254000: train loss 1.4221, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1006/10000  (10.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 40.00it/s]\n","iter 256000: train loss 1.4218, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1021/10000  (10.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.82it/s]\n","iter 258000: train loss 1.4234, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 977/10000  (9.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.39it/s]\n","iter 260000: train loss 1.4142, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1054/10000  (10.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.14it/s]\n","iter 262000: train loss 1.4182, val loss 1.4234\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 935/10000  (9.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.86it/s]\n","iter 264000: train loss 1.4212, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1021/10000  (10.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.85it/s]\n","iter 266000: train loss 1.4228, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1004/10000  (10.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.20it/s]\n","iter 268000: train loss 1.4296, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 937/10000  (9.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.63it/s]\n","iter 270000: train loss 1.4180, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1043/10000  (10.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.49it/s]\n","iter 272000: train loss 1.4265, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1044/10000  (10.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.07it/s]\n","iter 274000: train loss 1.4215, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1027/10000  (10.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.70it/s]\n","iter 276000: train loss 1.4235, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 940/10000  (9.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.81it/s]\n","iter 278000: train loss 1.4232, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 995/10000  (9.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.69it/s]\n","iter 280000: train loss 1.4193, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1005/10000  (10.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.26it/s]\n","iter 282000: train loss 1.4188, val loss 1.4235\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1033/10000  (10.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.82it/s]\n","iter 284000: train loss 1.4206, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 955/10000  (9.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.08it/s]\n","iter 286000: train loss 1.4200, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 955/10000  (9.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.46it/s]\n","iter 288000: train loss 1.4223, val loss 1.4245\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 951/10000  (9.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.51it/s]\n","iter 290000: train loss 1.4165, val loss 1.4234\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 959/10000  (9.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.01it/s]\n","iter 292000: train loss 1.4225, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1018/10000  (10.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.96it/s]\n","iter 294000: train loss 1.4196, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 988/10000  (9.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 40.13it/s]\n","iter 296000: train loss 1.4171, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1071/10000  (10.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.75it/s]\n","iter 298000: train loss 1.4221, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1041/10000  (10.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.02it/s]\n","iter 300000: train loss 1.4178, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1019/10000  (10.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.94it/s]\n","iter 302000: train loss 1.4216, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1012/10000  (10.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.27it/s]\n","iter 304000: train loss 1.4229, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1009/10000  (10.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.18it/s]\n","iter 306000: train loss 1.4203, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1015/10000  (10.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.84it/s]\n","iter 308000: train loss 1.4189, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 988/10000  (9.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.06it/s]\n","iter 310000: train loss 1.4274, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1018/10000  (10.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.51it/s]\n","iter 312000: train loss 1.4222, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1051/10000  (10.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.73it/s]\n","iter 314000: train loss 1.4226, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1062/10000  (10.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.76it/s]\n","iter 316000: train loss 1.4180, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 982/10000  (9.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.51it/s]\n","iter 318000: train loss 1.4192, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1040/10000  (10.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.96it/s]\n","iter 320000: train loss 1.4224, val loss 1.4235\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 986/10000  (9.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.45it/s]\n","iter 322000: train loss 1.4203, val loss 1.4245\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1015/10000  (10.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.60it/s]\n","iter 324000: train loss 1.4196, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 956/10000  (9.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.26it/s]\n","iter 326000: train loss 1.4207, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1021/10000  (10.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 40.04it/s]\n","iter 328000: train loss 1.4223, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1013/10000  (10.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.13it/s]\n","iter 330000: train loss 1.4217, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 995/10000  (9.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.68it/s]\n","iter 332000: train loss 1.4222, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 988/10000  (9.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.68it/s]\n","iter 334000: train loss 1.4164, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1078/10000  (10.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.48it/s]\n","iter 336000: train loss 1.4230, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 984/10000  (9.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.47it/s]\n","iter 338000: train loss 1.4191, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 960/10000  (9.60%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.89it/s]\n","iter 340000: train loss 1.4204, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1014/10000  (10.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.89it/s]\n","iter 342000: train loss 1.4241, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1029/10000  (10.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.53it/s]\n","iter 344000: train loss 1.4209, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 983/10000  (9.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.16it/s]\n","iter 346000: train loss 1.4218, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1037/10000  (10.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.85it/s]\n","iter 348000: train loss 1.4203, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 948/10000  (9.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.54it/s]\n","iter 350000: train loss 1.4187, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1060/10000  (10.60%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.35it/s]\n","iter 352000: train loss 1.4213, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1033/10000  (10.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.73it/s]\n","iter 354000: train loss 1.4194, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1037/10000  (10.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.72it/s]\n","iter 356000: train loss 1.4145, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1038/10000  (10.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.58it/s]\n","iter 358000: train loss 1.4217, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 996/10000  (9.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.91it/s]\n","iter 360000: train loss 1.4206, val loss 1.4234\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1037/10000  (10.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.01it/s]\n","iter 362000: train loss 1.4197, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1028/10000  (10.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.45it/s]\n","iter 364000: train loss 1.4225, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 973/10000  (9.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.71it/s]\n","iter 366000: train loss 1.4203, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:01\u003c00:00, 40.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 998/10000  (9.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.46it/s]\n","iter 368000: train loss 1.4201, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1006/10000  (10.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.38it/s]\n","iter 370000: train loss 1.4211, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1029/10000  (10.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.16it/s]\n","iter 372000: train loss 1.4237, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 940/10000  (9.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.07it/s]\n","iter 374000: train loss 1.4214, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 984/10000  (9.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.71it/s]\n","iter 376000: train loss 1.4234, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 998/10000  (9.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.90it/s]\n","iter 378000: train loss 1.4240, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 949/10000  (9.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.25it/s]\n","iter 380000: train loss 1.4190, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1006/10000  (10.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.23it/s]\n","iter 382000: train loss 1.4224, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1038/10000  (10.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.33it/s]\n","iter 384000: train loss 1.4162, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 954/10000  (9.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.67it/s]\n","iter 386000: train loss 1.4212, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 959/10000  (9.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.67it/s]\n","iter 388000: train loss 1.4271, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1012/10000  (10.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.47it/s]\n","iter 390000: train loss 1.4211, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1035/10000  (10.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.12it/s]\n","iter 392000: train loss 1.4246, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1001/10000  (10.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.84it/s]\n","iter 394000: train loss 1.4186, val loss 1.4242\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1006/10000  (10.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.22it/s]\n","iter 396000: train loss 1.4174, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1035/10000  (10.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.79it/s]\n","iter 398000: train loss 1.4255, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1008/10000  (10.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.84it/s]\n","iter 400000: train loss 1.4207, val loss 1.4236\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1022/10000  (10.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.91it/s]\n","iter 402000: train loss 1.4250, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 999/10000  (9.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.12it/s]\n","iter 404000: train loss 1.4226, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 997/10000  (9.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.16it/s]\n","iter 406000: train loss 1.4253, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1006/10000  (10.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.01it/s]\n","iter 408000: train loss 1.4161, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1057/10000  (10.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.49it/s]\n","iter 410000: train loss 1.4167, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1013/10000  (10.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.75it/s]\n","iter 412000: train loss 1.3838, val loss 1.3894\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 3950/10000  (39.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.03it/s]\n","iter 414000: train loss 1.3285, val loss 1.3352\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7747/10000  (77.47%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.68it/s]\n","iter 416000: train loss 1.3141, val loss 1.3168\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9349/10000  (93.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.46it/s]\n","iter 418000: train loss 1.3113, val loss 1.3149\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9205/10000  (92.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.14it/s]\n","iter 420000: train loss 1.3126, val loss 1.3100\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9720/10000  (97.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.84it/s]\n","iter 422000: train loss 1.3104, val loss 1.3092\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9819/10000  (98.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.82it/s]\n","iter 424000: train loss 1.3075, val loss 1.3089\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9792/10000  (97.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.38it/s]\n","iter 426000: train loss 1.3050, val loss 1.3070\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9952/10000  (99.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.71it/s]\n","iter 428000: train loss 1.3028, val loss 1.3070\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9925/10000  (99.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.94it/s]\n","iter 430000: train loss 1.3033, val loss 1.3068\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9967/10000  (99.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.17it/s]\n","iter 432000: train loss 1.3104, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9961/10000  (99.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.07it/s]\n","iter 434000: train loss 1.3070, val loss 1.3076\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9807/10000  (98.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.89it/s]\n","iter 436000: train loss 1.3030, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9991/10000  (99.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.88it/s]\n","iter 438000: train loss 1.2999, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9982/10000  (99.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.27it/s]\n","iter 440000: train loss 1.3038, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9968/10000  (99.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.10it/s]\n","iter 442000: train loss 1.3062, val loss 1.3069\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9921/10000  (99.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.84it/s]\n","iter 444000: train loss 1.3041, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.43it/s]\n","iter 446000: train loss 1.3086, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.21it/s]\n","iter 448000: train loss 1.3044, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.30it/s]\n","iter 450000: train loss 1.3019, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.85it/s]\n","iter 452000: train loss 1.2998, val loss 1.3062\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.34it/s]\n","iter 454000: train loss 1.3028, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.84it/s]\n","iter 456000: train loss 1.3057, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.72it/s]\n","iter 458000: train loss 1.3075, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9989/10000  (99.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.50it/s]\n","iter 460000: train loss 1.3035, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9993/10000  (99.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.24it/s]\n","iter 462000: train loss 1.3059, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.79it/s]\n","iter 464000: train loss 1.3017, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.91it/s]\n","iter 466000: train loss 1.3024, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.17it/s]\n","iter 468000: train loss 1.3000, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.13it/s]\n","iter 470000: train loss 1.2988, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.46it/s]\n","iter 472000: train loss 1.3052, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.86it/s]\n","iter 474000: train loss 1.3062, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.00it/s]\n","iter 476000: train loss 1.3026, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.96it/s]\n","iter 478000: train loss 1.3063, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.30it/s]\n","iter 480000: train loss 1.3032, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.22it/s]\n","iter 482000: train loss 1.3027, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9985/10000  (99.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.47it/s]\n","iter 484000: train loss 1.3048, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.08it/s]\n","iter 486000: train loss 1.3012, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.12it/s]\n","iter 488000: train loss 1.3042, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9986/10000  (99.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.31it/s]\n","iter 490000: train loss 1.3045, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.53it/s]\n","iter 492000: train loss 1.3075, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.10it/s]\n","iter 494000: train loss 1.3065, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.37it/s]\n","iter 496000: train loss 1.3060, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.84it/s]\n","iter 498000: train loss 1.2993, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.42it/s]\n","iter 500000: train loss 1.3011, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.73it/s]\n","iter 502000: train loss 1.3055, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9991/10000  (99.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.37it/s]\n","iter 504000: train loss 1.2991, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.97it/s]\n","iter 506000: train loss 1.3040, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.76it/s]\n","iter 508000: train loss 1.3041, val loss 1.3054\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.28it/s]\n","iter 510000: train loss 1.3038, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.63it/s]\n","iter 512000: train loss 1.3037, val loss 1.3054\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.63it/s]\n","iter 514000: train loss 1.3057, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9988/10000  (99.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.72it/s]\n","iter 516000: train loss 1.3037, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.69it/s]\n","iter 518000: train loss 1.3079, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.40it/s]\n","iter 520000: train loss 1.3032, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.37it/s]\n","iter 522000: train loss 1.3036, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.61it/s]\n","iter 524000: train loss 1.3073, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.22it/s]\n","iter 526000: train loss 1.3023, val loss 1.3054\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.50it/s]\n","iter 528000: train loss 1.3068, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.02it/s]\n","iter 530000: train loss 1.3039, val loss 1.3066\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.27it/s]\n","iter 532000: train loss 1.3023, val loss 1.3054\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.24it/s]\n","iter 534000: train loss 1.3031, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.22it/s]\n","iter 536000: train loss 1.3039, val loss 1.3054\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.11it/s]\n","iter 538000: train loss 1.3041, val loss 1.3063\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.62it/s]\n","iter 540000: train loss 1.3043, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.47it/s]\n","iter 542000: train loss 1.3070, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.63it/s]\n","iter 544000: train loss 1.3042, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.76it/s]\n","iter 546000: train loss 1.3028, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.95it/s]\n","iter 548000: train loss 1.3008, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.15it/s]\n","iter 550000: train loss 1.3040, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.53it/s]\n","iter 552000: train loss 1.3036, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.61it/s]\n","iter 554000: train loss 1.3028, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.71it/s]\n","iter 556000: train loss 1.2994, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.75it/s]\n","iter 558000: train loss 1.3041, val loss 1.3054\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.76it/s]\n","iter 560000: train loss 1.3040, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.98it/s]\n","iter 562000: train loss 1.3022, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.33it/s]\n","iter 564000: train loss 1.2996, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9993/10000  (99.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.44it/s]\n","iter 566000: train loss 1.3051, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.12it/s]\n","iter 568000: train loss 1.3050, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.52it/s]\n","iter 570000: train loss 1.3016, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.10it/s]\n","iter 572000: train loss 1.3021, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.58it/s]\n","iter 574000: train loss 1.3045, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.26it/s]\n","iter 576000: train loss 1.3018, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.90it/s]\n","iter 578000: train loss 1.3021, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.37it/s]\n","iter 580000: train loss 1.3036, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.37it/s]\n","iter 582000: train loss 1.3006, val loss 1.3053\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.59it/s]\n","iter 584000: train loss 1.2999, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.07it/s]\n","iter 586000: train loss 1.2988, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.97it/s]\n","iter 588000: train loss 1.3005, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.39it/s]\n","iter 590000: train loss 1.3045, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.01it/s]\n","iter 592000: train loss 1.3009, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.19it/s]\n","iter 594000: train loss 1.3045, val loss 1.3063\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.95it/s]\n","iter 596000: train loss 1.3026, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.25it/s]\n","iter 598000: train loss 1.3009, val loss 1.3054\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.17it/s]\n","iter 600000: train loss 1.3009, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.52it/s]\n","iter 602000: train loss 1.2993, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.47it/s]\n","iter 604000: train loss 1.3059, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.58it/s]\n","iter 606000: train loss 1.3053, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.28it/s]\n","iter 608000: train loss 1.2981, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.14it/s]\n","iter 610000: train loss 1.3059, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.41it/s]\n","iter 612000: train loss 1.3066, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.40it/s]\n","iter 614000: train loss 1.3040, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.50it/s]\n","iter 616000: train loss 1.3044, val loss 1.3053\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.08it/s]\n","iter 618000: train loss 1.3010, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.47it/s]\n","iter 620000: train loss 1.3050, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.99it/s]\n","iter 622000: train loss 1.3025, val loss 1.3068\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9953/10000  (99.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.95it/s]\n","iter 624000: train loss 1.2989, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.45it/s]\n","iter 626000: train loss 1.3018, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.70it/s]\n","iter 628000: train loss 1.3045, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.94it/s]\n","iter 630000: train loss 1.3017, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.02it/s]\n","iter 632000: train loss 1.3026, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.57it/s]\n","iter 634000: train loss 1.3038, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.58it/s]\n","iter 636000: train loss 1.3019, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.33it/s]\n","iter 638000: train loss 1.3032, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.05it/s]\n","iter 640000: train loss 1.3047, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.10it/s]\n","iter 642000: train loss 1.3050, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.06it/s]\n","iter 644000: train loss 1.3013, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.72it/s]\n","iter 646000: train loss 1.3004, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.55it/s]\n","iter 648000: train loss 1.3018, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.57it/s]\n","iter 650000: train loss 1.3039, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.73it/s]\n","iter 652000: train loss 1.3028, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.57it/s]\n","iter 654000: train loss 1.3047, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.17it/s]\n","iter 656000: train loss 1.3010, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.65it/s]\n","iter 658000: train loss 1.3053, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.30it/s]\n","iter 660000: train loss 1.3037, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.36it/s]\n","iter 662000: train loss 1.3027, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.24it/s]\n","iter 664000: train loss 1.3017, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.06it/s]\n","iter 666000: train loss 1.3062, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.89it/s]\n","iter 668000: train loss 1.3034, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.82it/s]\n","iter 670000: train loss 1.3004, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.04it/s]\n","iter 672000: train loss 1.3023, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.85it/s]\n","iter 674000: train loss 1.3061, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.05it/s]\n","iter 676000: train loss 1.3055, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.07it/s]\n","iter 678000: train loss 1.3047, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.42it/s]\n","iter 680000: train loss 1.3023, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.74it/s]\n","iter 682000: train loss 1.3022, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.70it/s]\n","iter 684000: train loss 1.3050, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.50it/s]\n","iter 686000: train loss 1.3036, val loss 1.3063\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 40.27it/s]\n","iter 688000: train loss 1.3041, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.38it/s]\n","iter 690000: train loss 1.3040, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.68it/s]\n","iter 692000: train loss 1.3066, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.68it/s]\n","iter 694000: train loss 1.3013, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.31it/s]\n","iter 696000: train loss 1.3034, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.87it/s]\n","iter 698000: train loss 1.3074, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.23it/s]\n","iter 700000: train loss 1.3009, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.43it/s]\n","iter 702000: train loss 1.3081, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.93it/s]\n","iter 704000: train loss 1.3024, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.39it/s]\n","iter 706000: train loss 1.3046, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 39.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.91it/s]\n","iter 708000: train loss 1.3013, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.11it/s]\n","iter 710000: train loss 1.3027, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 39.16it/s]\n","iter 712000: train loss 1.3052, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.48it/s]\n","iter 714000: train loss 1.2996, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.94it/s]\n","iter 716000: train loss 1.3053, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.39it/s]\n","iter 718000: train loss 1.2995, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.03it/s]\n","iter 720000: train loss 1.3045, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.96it/s]\n","iter 722000: train loss 1.3101, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.45it/s]\n","iter 724000: train loss 1.3009, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.83it/s]\n","iter 726000: train loss 1.2998, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.75it/s]\n","iter 728000: train loss 1.3015, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.85it/s]\n","iter 730000: train loss 1.3021, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 38.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.47it/s]\n","iter 732000: train loss 1.3005, val loss 1.3062\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 38.44it/s]\n","iter 734000: train loss 1.3029, val loss 1.3062\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.60it/s]\n"]}],"source":["!python train.py 4_operands_addition_reversed.txt --greedy"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
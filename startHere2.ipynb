{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":669,"status":"ok","timestamp":1769626545724,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"},"user_tz":360},"id":"aftd1RezTYnF","outputId":"9feea33a-bbe8-451e-ece6-0a50fb16e599"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/addition\n","configuration_files\t       legacy_code\t   result_analysis.ipynb\n","configurator.py\t\t       llama_adapter.py    result_analysis.py\n","data\t\t\t       llama_tokenizer.py  result_analysis_script\n","data_generate.py\t       main_utilities.py   results\n","data_generation_script\t       model.py\t\t   startHere2.ipynb\n","error_examples\t\t       model_rope.py\t   startHere.ipynb\n","evaluation.py\t\t       model_t5bias.py\t   statistical_measurements.py\n","extra_result_analysis_scripts  __pycache__\t   train.py\n","gsm_test\t\t       README.md\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change directory to your code\n","%cd /content/drive/MyDrive/addition\n","%pwd   # verify you‚Äôre in the right place\n","!ls    # should show train.py, 4_operands_addition.txt, etc."]},{"cell_type":"markdown","metadata":{"id":"lbEXozJBCm_X"},"source":["# I. Generate Data (choose one synthetic task)"]},{"cell_type":"markdown","metadata":{"id":"fUKKZHtTI1n3"},"source":["## Addition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbKQwH__Bq4E"},"outputs":[],"source":["!python data_generate.py --task addition --num_operands 4 --experiment_name 4_operands_0_to_999_uniform_test_garbage --train_size 1000000 --test_size 10000 --val_size 10000 --train_eval True --sample-size 10000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"h95zEWGChhca"},"source":["#### Ablation in Addition (e.g. randomize thousands-place of the output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JKK2-u_hhAC"},"outputs":[],"source":["!python data_generate.py --task addition --randomize thousands --num_operands 4 --experiment_name 4_operands_0_to_999_output_randomize_thousands --train_size 1000000 --test_size 10000 --val_size 10000 --train_eval True --sample-size 10000 --generate_reverse True\n"]},{"cell_type":"markdown","metadata":{"id":"D0LlhemPvB9K"},"source":["#### Addition with scratchpad (form 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJTghfzsvMrw"},"outputs":[],"source":["!python data_generate.py --task addition --reasoning_mode 1 --num_operands 4 --experiment_name 4_operands_0_to_999_uniform_scratchpad1 --train_size 1000000 --test_size 10000 --val_size 10000 --train_eval True --sample-size 10000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"Ny8G_OmQvakc"},"source":["#### Addition with scratchpad (form 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fH5xyTWpvcOb"},"outputs":[],"source":["!python data_generate.py --task addition --reasoning_mode 2 --num_operands 4 --experiment_name 4_operands_0_to_999_uniform_scratchpad2 --train_size 1000000 --test_size 10000 --val_size 10000 --train_eval True --sample-size 10000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"DBmQXdsiI6YE"},"source":["## Multiplication"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hr3n0w_gIf7i"},"outputs":[],"source":["!python data_generate.py --task multiplication --experiment_name 40_digit_times_1_digit --train_size 1000000 --test_size 10000 --val_size 10000 \\\n","--a_max_digits 40 --b_max_digits 1 --train_eval True --sample-size 10000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"H8G1HEBCl161"},"source":["## Comparison (Balanced data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eY6q7tsEl--q"},"outputs":[],"source":["!python data_generate.py --task comparison --experiment_name comparison_bal --train_eval True --sample-size 5000"]},{"cell_type":"markdown","metadata":{"id":"fB5aQJxiI8_h"},"source":["## Sorting (Doubly balanced data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Q7blcAsIVHV"},"outputs":[],"source":["!python data_generate.py --task sorting --experiment_name 4_operands_sorting_doubly_balanced --train_eval True --sample-size 5000"]},{"cell_type":"markdown","metadata":{"id":"8i_CDutAI_fC"},"source":["# II. Let's Start Training!"]},{"cell_type":"markdown","metadata":{"id":"Et7rULRph_Ne"},"source":["#### The .txt file is the configuration file"]},{"cell_type":"markdown","metadata":{"id":"JLXLrm_chIgo"},"source":["## 4 Operands Addition"]},{"cell_type":"markdown","metadata":{"id":"3ItqnAmcm0mN"},"source":["#### Reverse Output format"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"8S_LDDfvhGLM","outputId":"92031f7f-ee52-4853-9e9e-47759c73bc2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/4_operands_addition_reversed.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 2000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_0_to_999_uniform_reverse'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000 # for four operands addition, recommend \u003e= 200000\n","lr_decay_iters = 800000 # make equal to max_iters\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/reverse_out_test_garbage'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform_test_garbage/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: Can be 'units', 'tens', 'hundreds', 'thousands', or None\n","randomize = None\n","\n","# to edit: whether do mutual information tacking (addition supported only)\n","mi_measurement = True\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","all the unique characters: \\n, $, +, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \u003cpad\u003e, =\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_test_garbage/test_reverse.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.63M\n","/content/drive/MyDrive/addition/train.py:712: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W\u0026B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W\u0026B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m setting up run sorywn0c (0.5s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.24.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/reverse_out_test_garbage/wandb/run-20260128_152613-sorywn0c\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m4_operands_0_to_999_uniform_reverse\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/sorywn0c\u001b[0m\n","max_new_tokens: 5\n","iter 0: train loss 2.7106, val loss 2.7093\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_test_garbage/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_test_garbage/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:02\u003c00:00, 29.02it/s]\n","/content/drive/MyDrive/addition/evaluation.py:689: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_test_garbage/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_test_garbage/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 82 batches\n","100% 82/82 [00:02\u003c00:00, 36.40it/s]\n","iter 2000: train loss 1.6455, val loss 1.6569\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 13/10000  (0.13%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.74it/s]\n","iter 4000: train loss 1.5562, val loss 1.5564\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 75/10000  (0.75%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.46it/s]\n","iter 6000: train loss 1.5465, val loss 1.5503\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 85/10000  (0.85%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.41it/s]\n","iter 8000: train loss 1.5433, val loss 1.5465\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 100/10000  (1.00%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.15it/s]\n","iter 10000: train loss 1.5394, val loss 1.5456\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 101/10000  (1.01%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.23it/s]\n","iter 12000: train loss 1.5441, val loss 1.5437\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 96/10000  (0.96%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.29it/s]\n","iter 14000: train loss 1.5393, val loss 1.5438\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 106/10000  (1.06%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.48it/s]\n","iter 16000: train loss 1.5383, val loss 1.5433\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 113/10000  (1.13%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.88it/s]\n","iter 18000: train loss 1.4488, val loss 1.4540\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 686/10000  (6.86%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.92it/s]\n","iter 20000: train loss 1.4427, val loss 1.4490\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 769/10000  (7.69%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.43it/s]\n","iter 22000: train loss 1.4239, val loss 1.4372\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 903/10000  (9.03%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.45it/s]\n","iter 24000: train loss 1.4270, val loss 1.4327\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 965/10000  (9.65%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.66it/s]\n","iter 26000: train loss 1.4249, val loss 1.4291\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 992/10000  (9.92%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.55it/s]\n","iter 28000: train loss 1.4256, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1025/10000  (10.25%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.23it/s]\n","iter 30000: train loss 1.4200, val loss 1.4259\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 964/10000  (9.64%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.79it/s]\n","iter 32000: train loss 1.4229, val loss 1.4257\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1008/10000  (10.08%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.48it/s]\n","iter 34000: train loss 1.4229, val loss 1.4249\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1035/10000  (10.35%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.93it/s]\n","iter 36000: train loss 1.4162, val loss 1.4253\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 979/10000  (9.79%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.40it/s]\n","iter 38000: train loss 1.4198, val loss 1.4253\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 942/10000  (9.42%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.50it/s]\n","iter 40000: train loss 1.4191, val loss 1.4246\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1006/10000  (10.06%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.68it/s]\n","iter 42000: train loss 1.4284, val loss 1.4247\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1025/10000  (10.25%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.17it/s]\n","iter 44000: train loss 1.4244, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1007/10000  (10.07%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.81it/s]\n","iter 46000: train loss 1.4224, val loss 1.4248\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 962/10000  (9.62%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 33.98it/s]\n","iter 48000: train loss 1.4238, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 999/10000  (9.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.70it/s]\n","iter 50000: train loss 1.4250, val loss 1.4247\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1047/10000  (10.47%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.89it/s]\n","iter 52000: train loss 1.4201, val loss 1.4252\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 986/10000  (9.86%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.66it/s]\n","iter 54000: train loss 1.4203, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1027/10000  (10.27%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.11it/s]\n","iter 56000: train loss 1.4212, val loss 1.4256\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1012/10000  (10.12%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.76it/s]\n","iter 58000: train loss 1.4177, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 955/10000  (9.55%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.32it/s]\n","iter 60000: train loss 1.4217, val loss 1.4250\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 997/10000  (9.97%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.28it/s]\n","iter 62000: train loss 1.4217, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 999/10000  (9.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 35.95it/s]\n","iter 64000: train loss 1.4189, val loss 1.4249\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1019/10000  (10.19%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.45it/s]\n","iter 66000: train loss 1.4198, val loss 1.4253\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 968/10000  (9.68%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.11it/s]\n","iter 68000: train loss 1.4194, val loss 1.4250\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1020/10000  (10.20%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02\u003c00:00, 36.06it/s]\n"]}],"source":["!python train.py 4_operands_addition_reversed.txt"]},{"cell_type":"markdown","metadata":{"id":"7ugltcNHm5QO"},"source":["#### Plain output format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0RjN0ldhMGz"},"outputs":[],"source":["!python train.py 4_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"Bos8hxP4nFaL"},"source":["#### Scratchpad Form 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuDQXox3nMgj"},"outputs":[],"source":["!python train.py 4_operands_addition_plain_scratchpad1.txt"]},{"cell_type":"markdown","metadata":{"id":"1kSdQeSdnJaG"},"source":["#### Scratchpad Form 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"EWxsTkXVnQ5m"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/4_operands_addition_plain_scratchpad2.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 500\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","more_early_eval1 = True  # Enable frequent early evaluation\n","early_eval_interval1 = 25  # Evaluate every 25 iterations early on\n","early_eval_border1 = 1000  # Until iteration 1000\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition_scratchpad'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_0_to_999_uniform_plain_scratchpad2'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='plain'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 128 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 90\n","\n","drop_leading_digit = False\n","\n","# whether need zero‚Äêpadding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 50000 # for four operands addition, recommend \u003e= 200000\n","lr_decay_iters = 50000 # make equal to max_iters\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform_scratchpad2/plain_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform_scratchpad2/'\n","\n","# to edit: training data\n","train_data_name = 'train_scratchpad2.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_scratchpad2.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_scratchpad2.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_scratchpad2.txt'\n","main_test_name = \"test_scratchpad2\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","reasoning_chain = True\n","reasoning = True\n","mi_measurement = False\n","\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 17\n","all the unique characters: \\n, $, (, ), +, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \u003cpad\u003e, =\n","Using vocabulary size: 17\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_scratchpad2/test_scratchpad2.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.67M\n","/content/drive/MyDrive/addition/train.py:712: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W\u0026B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W\u0026B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.24.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform_scratchpad2/plain_out/wandb/run-20260128_185626-o2v4twsy\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m4_operands_0_to_999_uniform_plain_scratchpad2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition_scratchpad\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition_scratchpad/runs/o2v4twsy\u001b[0m\n","WARNING: results directory results/4_operands_0_to_999_uniform_scratchpad2/plain_out/4_operands_0_to_999_uniform_plain_scratchpad2 already exists, overwriting...\n","max_new_tokens: 90\n","iter 0: train loss 2.6754, val loss 2.6775\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_scratchpad2/test_scratchpad2.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_scratchpad2/test_scratchpad2.txt\n","Created 80 batches\n","100% 80/80 [00:50\u003c00:00,  1.59it/s]\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_scratchpad2/test_scratchpad2.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_scratchpad2/test_scratchpad2.txt\n","Created 82 batches\n","100% 82/82 [00:03\u003c00:00, 23.27it/s]\n","/content/drive/MyDrive/addition/evaluation.py:612: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df_reasoning = pd.concat([acc_df_reasoning, new_row_reasoning], ignore_index=True)\n","/content/drive/MyDrive/addition/evaluation.py:689: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_scratchpad2/train_eval_scratchpad2.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_scratchpad2/train_eval_scratchpad2.txt\n","Created 81 batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 25: train loss 1.0415, val loss 1.0511\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.22it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 50: train loss 0.9249, val loss 0.9301\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.20it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 75: train loss 0.7434, val loss 0.7469\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.29it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 100: train loss 0.5875, val loss 0.5886\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.20it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 125: train loss 0.5270, val loss 0.5310\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.17it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 1/10000  (0.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 150: train loss 0.4775, val loss 0.4807\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.23it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 3/10000  (0.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 175: train loss 0.4526, val loss 0.4582\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.23it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 13/10000  (0.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 200: train loss 0.4305, val loss 0.4328\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.59it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 22.94it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 51/10000  (0.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.59it/s]\n","iter 225: train loss 0.4193, val loss 0.4205\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.59it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.08it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 71/10000  (0.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:51\u003c00:00,  1.59it/s]\n","iter 250: train loss 0.4040, val loss 0.4037\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.59it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.01it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 170/10000  (1.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.59it/s]\n","iter 275: train loss 0.4009, val loss 0.3998\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.59it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.06it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 222/10000  (2.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 300: train loss 0.3906, val loss 0.3934\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.26it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 343/10000  (3.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 325: train loss 0.4060, val loss 0.4083\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.35it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 97/10000  (0.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 350: train loss 0.4610, val loss 0.4610\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.21it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 375: train loss 0.3814, val loss 0.3835\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.25it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 604/10000  (6.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 400: train loss 0.3929, val loss 0.3939\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.38it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 269/10000  (2.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 425: train loss 0.3823, val loss 0.3811\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.31it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 634/10000  (6.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 450: train loss 0.3978, val loss 0.3992\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.28it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 166/10000  (1.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 475: train loss 0.3761, val loss 0.3764\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.32it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 838/10000  (8.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 500: train loss 0.3765, val loss 0.3765\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.09it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 921/10000  (9.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 525: train loss 0.3825, val loss 0.3828\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.59it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.15it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 552/10000  (5.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 550: train loss 0.3851, val loss 0.3868\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.16it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 397/10000  (3.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 575: train loss 0.3749, val loss 0.3740\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.26it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 850/10000  (8.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 600: train loss 0.3874, val loss 0.3875\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.59it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.13it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 346/10000  (3.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 625: train loss 0.3810, val loss 0.3830\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.22it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 464/10000  (4.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 650: train loss 0.3700, val loss 0.3698\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.23it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 1194/10000  (11.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 675: train loss 0.3678, val loss 0.3686\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.10it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 1446/10000  (14.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 700: train loss 0.3639, val loss 0.3652\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.32it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 1727/10000  (17.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 725: train loss 0.3635, val loss 0.3628\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.24it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 1816/10000  (18.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 750: train loss 0.3591, val loss 0.3609\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.28it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 2483/10000  (24.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 775: train loss 0.3540, val loss 0.3538\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.61it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.27it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 3786/10000  (37.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 800: train loss 0.3532, val loss 0.3545\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.28it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 3523/10000  (35.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 825: train loss 0.3494, val loss 0.3493\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.26it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 4400/10000  (44.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 850: train loss 0.3475, val loss 0.3473\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.35it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 5742/10000  (57.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 875: train loss 0.3567, val loss 0.3571\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.10it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 3018/10000  (30.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 900: train loss 0.3432, val loss 0.3434\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.29it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 6983/10000  (69.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 925: train loss 0.3424, val loss 0.3424\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.23it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 6920/10000  (69.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 950: train loss 0.3396, val loss 0.3409\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.21it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 7893/10000  (78.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 975: train loss 0.3410, val loss 0.3409\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.28it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 7652/10000  (76.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 1000: train loss 0.3389, val loss 0.3387\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.11it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 8598/10000  (85.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 1500: train loss 0.3344, val loss 0.3349\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.27it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 9821/10000  (98.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 2000: train loss 0.3347, val loss 0.3347\n","Using precomputed batches\n","100% 80/80 [00:50\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.32it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 9890/10000  (98.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 2500: train loss 0.3337, val loss 0.3343\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.26it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 9889/10000  (98.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.60it/s]\n","iter 3000: train loss 0.3347, val loss 0.3339\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.28it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 9953/10000  (99.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 3500: train loss 0.3340, val loss 0.3338\n","Using precomputed batches\n","100% 80/80 [00:49\u003c00:00,  1.60it/s]\n","Using precomputed batches\n","100% 82/82 [00:03\u003c00:00, 23.25it/s]\n","\n","Test Results:\n","test_scratchpad2.txt, 10000 examples: 9916/10000  (99.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:50\u003c00:00,  1.61it/s]\n","iter 4000: train loss 0.3339, val loss 0.3339\n","Using precomputed batches\n"," 70% 56/80 [00:35\u003c00:15,  1.58it/s]"]}],"source":["!python train.py 4_operands_addition_plain_scratchpad2.txt"]},{"cell_type":"markdown","metadata":{"id":"QDxgYE9zhL02"},"source":["## Simpel Multiplication"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNcrd8bDha9j"},"outputs":[],"source":["!python train.py 40_1_digits_mul_reversed.txt"]},{"cell_type":"markdown","metadata":{"id":"z0dZhitvF1au"},"source":["## Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hpg2znaxF48M"},"outputs":[],"source":["!python train.py comparison_bal.txt"]},{"cell_type":"markdown","metadata":{"id":"7UHO3zBc_jY3"},"source":["## Sorting"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"elF2BaDD_izl"},"outputs":[],"source":["!python train.py 4_operands_sorting_doubly_bal.txt"]},{"cell_type":"markdown","metadata":{"id":"hdumgWyAWlt6"},"source":["## Slicing -- Addition"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"pdPiOKoWKw0q"},"outputs":[],"source":["!python train.py slicing_addition_4_operand_plain.txt --batch slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"zqlR08suC-Na"},"outputs":[],"source":["!python train.py slicing_addition_4_operand_reverse.txt --batch slicing"]},{"cell_type":"markdown","metadata":{"id":"30Zi4RZFvtkw"},"source":["## Positional Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"F9dDD_DmvtPz"},"outputs":[],"source":["!python train.py 4_operands_addition_reversed.txt --PE RoPE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOqmOrsHZshs"},"outputs":[],"source":["!python train.py 4_operands_addition_reversed.txt --PE t5"]},{"cell_type":"markdown","metadata":{"id":"PxLUxyr_u8m-"},"source":["### Greedy Decoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UE5ttFeNvBfb"},"outputs":[],"source":["!python train.py 4_operands_addition_reversed.txt --greedy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4yS-i2zvMLT"},"outputs":[],"source":["!python train.py 4_operands_addition_plain.txt --greedy"]},{"cell_type":"markdown","metadata":{"id":"S_xUM7XznZfi"},"source":["# III. Result Analysis"]},{"cell_type":"markdown","metadata":{"id":"wpid4Duzum1v"},"source":["## Addition Task"]},{"cell_type":"markdown","metadata":{"id":"ZLm0jWwznbvH"},"source":["#### Digitwise Error Rates (4 operands addition)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuRo6Qlfneh7"},"outputs":[],"source":["!python result_analysis_script/digitwise_error.py results/4_operands_0_to_999_uniform/reverse_out/4_operands_0_to_999_uniform_reverse/test_reverse_results.csv"]},{"cell_type":"markdown","metadata":{"id":"KKGV9OLkxorj"},"source":["#### Fit Normal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64BwhPVFxsUv"},"outputs":[],"source":["!python result_analysis_script/fit_normal.py \\\n","  --input results/4_operands_0_to_999_uniform/reverse_out_early_dense_eval/early_dense_eval_for_normal_distr_4_operands_0_to_999_uniform_reverse/test_reverse_results.csv \\\n","  --iter-start 1000 --iter-end 1800 --iter-step 200 \\\n","  --diff-min -800 --diff-max 800\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAo9jUv32OZb"},"outputs":[],"source":["!python result_analysis_script/fit_normal.py \\\n","  --input results/4_operands_0_to_999_uniform/reverse_out/4_operands_0_to_999_uniform_reverse/test_reverse_results.csv \\\n","  --iter-start 8000 --iter-end 12000 --iter-step 2000 \\\n","  --diff-min -100 --diff-max 100\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDuXMuie2lX6"},"outputs":[],"source":["!python result_analysis_script/fit_normal.py \\\n","  --input results/4_operands_0_to_999_uniform/reverse_out/4_operands_0_to_999_uniform_reverse/test_reverse_results.csv \\\n","  --iter-start 60000 --iter-end 64000 --iter-step 2000 \\\n","  --diff-min -20 --diff-max 20\n"]},{"cell_type":"markdown","metadata":{"id":"KeRml6m6u6D9"},"source":["#### Mutual Information Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6uRsOwxFu9ix"},"outputs":[],"source":["!python result_analysis_script/plot_mi_metrics.py \\\n","  results/4_operands_0_to_999_uniform/reverse_out_complete_MI_1M_lines/4_operands_0_to_999_uniform_reverse/mi_metrics.csv"]},{"cell_type":"markdown","metadata":{"id":"xO41d50Fuuhb"},"source":["## Simple Multiplication Task"]},{"cell_type":"markdown","metadata":{"id":"_MSBBSKMpivg"},"source":["#### Digitwise Error (Simple multiplication, Colormap)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ubs8HAlept0H"},"outputs":[],"source":["!python result_analysis_script/mul_digitwise_error_colormap.py results/40_digit_times_1_digit/reverse_out/40_digit_times_1_digit/test_reverse_results.csv --max_steps 3000"]},{"cell_type":"markdown","metadata":{"id":"BFe2zjmUXmBG"},"source":["## Comparison Task\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fTNrvoFH1Gns"},"source":["#### Comparison Error Rate (Contrast Pairs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10936,"status":"ok","timestamp":1769390017898,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"},"user_tz":360},"id":"lvPu4BRg1hzv","outputId":"c9bb6e08-9a6b-4bda-ce24-d767709cba20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saved plot to results/comparison_bal/comparison_bal_1/contrast_pair_error_rate\n"]}],"source":["!python result_analysis_script/comparison_error_rate.py \\\n","  results/comparison_bal/comparison_bal_1/thousands_diff_only_results.csv \\\n","  results/comparison_bal/comparison_bal_1/hundreds_diff_only_results.csv \\\n","  results/comparison_bal/comparison_bal_1/tens_diff_only_results.csv \\\n","  results/comparison_bal/comparison_bal_1/units_diff_only_results.csv \\\n","  --output_file_name contrast_pair_error_rate"]},{"cell_type":"markdown","metadata":{"id":"zRHI0Sm3uyhG"},"source":["## Sorting Task"]},{"cell_type":"markdown","metadata":{"id":"nvWbmBTe_eo2"},"source":["#### Sorting Subskill from 10% to 90% Range"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4i7Dx_c2_mC2"},"outputs":[],"source":["!python result_analysis_script/sorting_acc_10_90_range.py \\\n","  --csv \\\n","    results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction/4_operands_sorting_doubly_balanced_conflicting_same_correction/test_results.csv \\\n","    results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction/4_operands_sorting_doubly_balanced_conflicting_same_correction/digitwise_random_results.csv \\\n","    results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction/4_operands_sorting_doubly_balanced_conflicting_same_correction/digitwise_thousand_results.csv \\\n","    results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction/4_operands_sorting_doubly_balanced_conflicting_same_correction/digitwise_hundred_results.csv \\\n","    results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction/4_operands_sorting_doubly_balanced_conflicting_same_correction/digitwise_ten_results.csv \\\n","  --positions 1,2,3,4 \\\n","  --mode length first second third fourth\n"]},{"cell_type":"markdown","metadata":{"id":"7Pn0bJmxsooN"},"source":["#### Sorting Mixing Error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSM9s55usiis"},"outputs":[],"source":["!python result_analysis_script/mixing_error.py results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction_v2/4_operands_sorting_doubly_balanced_conflicting_same_correction_v2/1_3_same_2_4_agreeing_v2_results.csv\n"]},{"cell_type":"markdown","metadata":{"id":"PNjTQtpaVazE"},"source":["# IV. NanoGPT Scaling"]},{"cell_type":"markdown","metadata":{"id":"q0eO2DJ3VgL-"},"source":["#### 20M Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XTfVndPoVacJ"},"outputs":[],"source":["!python train.py 20M_4_operands_addition_reversed.txt"]},{"cell_type":"markdown","metadata":{"id":"NC8I2HzzWDO2"},"source":["#### 100M Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzOQSKlWWFdC"},"outputs":[],"source":["!python train.py 100M_4_operands_addition_reversed.txt"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
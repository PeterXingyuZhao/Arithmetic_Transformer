{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":25381,"status":"ok","timestamp":1767462310231,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":360},"id":"aftd1RezTYnF","outputId":"18d77f6a-e9b3-4fab-d9c6-b8da0353d4c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/addition\n","configuration_files\t       model_rope.py\n","configurator.py\t\t       model_t5bias.py\n","data\t\t\t       __pycache__\n","data_generate.py\t       README.md\n","data_generation_script\t       result_analysis.ipynb\n","eval_ckpt.py\t\t       result_analysis.py\n","evaluation_ckpt.py\t       results\n","evaluation.py\t\t       startHere100M.ipynb\n","extra_result_analysis_scripts  startHere1B.ipynb\n","legacy_code\t\t       startHere20M.ipynb\n","main_utilities.py\t       startHere.ipynb\n","meta_all_ascii_chars.pkl       statistical_measurements.py\n","model.py\t\t       train.py\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change directory to your code\n","%cd /content/drive/MyDrive/addition\n","%pwd   # verify you‚Äôre in the right place\n","!ls    # should show train.py, 4_operands_addition.txt, etc."]},{"cell_type":"markdown","metadata":{"id":"lbEXozJBCm_X"},"source":["# Quick Start Example\n","## Choose One Task and Start Training"]},{"cell_type":"markdown","metadata":{"id":"fUKKZHtTI1n3"},"source":["#### Generate Addition Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbKQwH__Bq4E"},"outputs":[],"source":["!python data_generate.py --task addition --num_operands 2 --experiment_name 2_operands_0_to_999_uniform --train_size 10000 --test_size 3000 --val_size 3000 --train_eval True --sample-size 3000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"DBmQXdsiI6YE"},"source":["#### Generate Multiplication Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hr3n0w_gIf7i"},"outputs":[],"source":["!python data_generate.py --task multiplication --num_operands 6 --experiment_name 0_to_999999_times_1_digit \\\n","        --train_size 10000 --test_size 3000 --val_size 3000 --train_eval True --sample-size 3000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"fB5aQJxiI8_h"},"source":["#### Generate Sorting Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Q7blcAsIVHV"},"outputs":[],"source":["!python data_generate.py --task sorting --experiment_name 4_operands_sorting_balanced_digit \\\n","        --train_size 1000000 --test_size 10000 --val_size 10000 --train_eval True --sample-size 10000"]},{"cell_type":"markdown","metadata":{"id":"8i_CDutAI_fC"},"source":["## Let's Start Training!"]},{"cell_type":"markdown","metadata":{"id":"Et7rULRph_Ne"},"source":["#### The .txt file is the configuration file"]},{"cell_type":"markdown","metadata":{"id":"dZJI2octhC3o"},"source":["## 2 Operands"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"qTJ1568DCuHp"},"outputs":[],"source":["!python train.py 2_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3C_q2HCzg9rf"},"outputs":[],"source":["!python train.py 2_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"JLXLrm_chIgo"},"source":["## 4 Operands"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8S_LDDfvhGLM"},"outputs":[],"source":["!python train.py 4_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0RjN0ldhMGz"},"outputs":[],"source":["!python train.py 4_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"QDxgYE9zhL02"},"source":["## Multiplication"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNcrd8bDha9j"},"outputs":[],"source":["!python train.py 2_operands_mul_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nn2GYDQhX7b"},"outputs":[],"source":["!python train.py 2_operands_mul_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"asYxDysPKOzI"},"source":["# Other Commands (May not be fully working)"]},{"cell_type":"markdown","metadata":{"id":"L0ttXoH9PGRV"},"source":["## 4 Operand Addition Scratchpad"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3F4qYRSCQZhE"},"outputs":[],"source":["%cat configuration_files/4_operands_addition_scratchpad.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ZEiUegj1UDm-"},"outputs":[],"source":["%cat evaluation.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"hGSNQOkyPFQF"},"outputs":[],"source":["!python train.py 4_operands_addition_scratchpad.txt"]},{"cell_type":"markdown","metadata":{"id":"dfLn10UAHrM2"},"source":["## 4 Operand Max"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"PFYzFx4vK6Px"},"outputs":[],"source":["%cat configuration_files/4_operands_max.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"CEGW4g1dLgnQ"},"outputs":[],"source":["%cat train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"SSG2giBcf0u9"},"outputs":[],"source":["%cat model.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"X2hV4DOEHtrh"},"outputs":[],"source":["!python train.py 4_operands_max.txt"]},{"cell_type":"markdown","metadata":{"id":"7UHO3zBc_jY3"},"source":["## 4 Operand Sorting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXiwk0EUyudt"},"outputs":[],"source":["%cat configuration_files/4_operands_sorting.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"elF2BaDD_izl"},"outputs":[],"source":["!python train.py 4_operands_sorting_doubly_bal.txt"]},{"cell_type":"code","source":["!python train.py 4_operands_sorting_uniform.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LvAt9sXNNuFw","outputId":"1e3b7b26-db73-49e8-c159-879295170d80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/4_operands_sorting_uniform.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 100\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_sorting_uniform'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='sorting'\n","operator=''\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 64 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: num of digits in each operand\n","num_digit = 4\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 32\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 10000\n","lr_decay_iters = 10000 # make equal to max_iters\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_sorting_uniform/intend_poor_performance'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_sorting_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# If it's a directory, \"main_test_name\" is the name of the test file (without extension) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test'\n","main_test_name = \"test\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_hundred.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_random.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_ten.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_thousand.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/distinct_length_test.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/doubly_bal_test.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/test.txt\n","pad_id: 13 eos_id: 1\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.65M\n","/content/drive/MyDrive/addition/train.py:590: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_sorting_uniform/intend_poor_performance/wandb/run-20260103_045829-i17tyekj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m4_operands_sorting_uniform\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/i17tyekj\u001b[0m\n","max_new_tokens: 32\n","W0103 04:58:57.991000 4477 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","iter 0: train loss 2.7030, val loss 2.7052\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_hundred.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_hundred.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:10<00:00,  2.38it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_random.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_random.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:09<00:00,  2.50it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_ten.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_ten.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:09<00:00,  2.50it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_thousand.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/digitwise_thousand.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:09<00:00,  2.50it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/distinct_length_test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/distinct_length_test.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:07<00:00,  3.23it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/doubly_bal_test.txt\n","Preparing batches for 5000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/doubly_bal_test.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 41 batches\n","100% 41/41 [00:14<00:00,  2.80it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/test.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 82 batches\n","100% 82/82 [00:26<00:00,  3.05it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/train_eval.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_uniform/train_eval.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 82 batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 100: train loss 1.2136, val loss 1.2207\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:26<00:00,  3.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 1/10000  (0.01%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 200: train loss 0.8854, val loss 0.8929\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 5938/10000  (59.38%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 300: train loss 0.8663, val loss 0.8725\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:26<00:00,  3.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 7738/10000  (77.38%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 400: train loss 0.8587, val loss 0.8637\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 8209/10000  (82.09%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 500: train loss 0.8612, val loss 0.8593\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 8873/10000  (88.73%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 600: train loss 0.8492, val loss 0.8531\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9209/10000  (92.09%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 700: train loss 0.8528, val loss 0.8521\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9364/10000  (93.64%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 800: train loss 0.8489, val loss 0.8509\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9525/10000  (95.25%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 900: train loss 0.8497, val loss 0.8500\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9606/10000  (96.06%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 1000: train loss 0.8476, val loss 0.8487\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9684/10000  (96.84%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 1100: train loss 0.8463, val loss 0.8487\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.17it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9747/10000  (97.47%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 1200: train loss 0.8501, val loss 0.8478\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9756/10000  (97.56%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 1300: train loss 0.8475, val loss 0.8472\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9831/10000  (98.31%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 1400: train loss 0.8490, val loss 0.8482\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9690/10000  (96.90%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 1500: train loss 0.8477, val loss 0.8475\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:26<00:00,  3.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9886/10000  (98.86%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 1600: train loss 0.8518, val loss 0.8477\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.22it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:26<00:00,  3.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9877/10000  (98.77%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 1700: train loss 0.8490, val loss 0.8488\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9877/10000  (98.77%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 1800: train loss 0.8465, val loss 0.8475\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.22it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9907/10000  (99.07%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 1900: train loss 0.8494, val loss 0.8466\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9909/10000  (99.09%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 2000: train loss 0.8461, val loss 0.8474\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.18it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:26<00:00,  3.05it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9916/10000  (99.16%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 2100: train loss 0.8464, val loss 0.8465\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.22it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9945/10000  (99.45%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 2200: train loss 0.8464, val loss 0.8464\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9947/10000  (99.47%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 2300: train loss 0.8434, val loss 0.8471\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.22it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9948/10000  (99.48%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 2400: train loss 0.8468, val loss 0.8473\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9949/10000  (99.49%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 2500: train loss 0.8461, val loss 0.8461\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.18it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9953/10000  (99.53%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 2600: train loss 0.8446, val loss 0.8465\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9966/10000  (99.66%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 2700: train loss 0.8438, val loss 0.8462\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9967/10000  (99.67%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 2800: train loss 0.8408, val loss 0.8468\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.22it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9963/10000  (99.63%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 2900: train loss 0.8461, val loss 0.8459\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.22it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:26<00:00,  3.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9958/10000  (99.58%)\n","\n","Using precomputed batches\n","100% 82/82 [00:26<00:00,  3.04it/s]\n","iter 3000: train loss 0.8441, val loss 0.8466\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9967/10000  (99.67%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 3100: train loss 0.8517, val loss 0.8464\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9973/10000  (99.73%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 3200: train loss 0.8472, val loss 0.8466\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:26<00:00,  3.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9986/10000  (99.86%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 3300: train loss 0.8437, val loss 0.8461\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9969/10000  (99.69%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 3400: train loss 0.8483, val loss 0.8468\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9980/10000  (99.80%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 3500: train loss 0.8442, val loss 0.8466\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.22it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9967/10000  (99.67%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 3600: train loss 0.8430, val loss 0.8458\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.22it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9972/10000  (99.72%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 3700: train loss 0.8441, val loss 0.8459\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9976/10000  (99.76%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 3800: train loss 0.8422, val loss 0.8463\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9983/10000  (99.83%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 3900: train loss 0.8471, val loss 0.8462\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9978/10000  (99.78%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 4000: train loss 0.8456, val loss 0.8461\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9987/10000  (99.87%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 4100: train loss 0.8433, val loss 0.8456\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9986/10000  (99.86%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 4200: train loss 0.8480, val loss 0.8465\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9976/10000  (99.76%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 4300: train loss 0.8460, val loss 0.8459\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9984/10000  (99.84%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 4400: train loss 0.8450, val loss 0.8463\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9987/10000  (99.87%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 4500: train loss 0.8432, val loss 0.8459\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 4600: train loss 0.8455, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 4700: train loss 0.8449, val loss 0.8458\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9988/10000  (99.88%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 4800: train loss 0.8421, val loss 0.8459\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9989/10000  (99.89%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.00it/s]\n","iter 4900: train loss 0.8447, val loss 0.8453\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 5000: train loss 0.8458, val loss 0.8457\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.18it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 5100: train loss 0.8437, val loss 0.8459\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9988/10000  (99.88%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 5200: train loss 0.8451, val loss 0.8457\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.00it/s]\n","iter 5300: train loss 0.8455, val loss 0.8461\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.18it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9989/10000  (99.89%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 5400: train loss 0.8442, val loss 0.8460\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9987/10000  (99.87%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 5500: train loss 0.8432, val loss 0.8463\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.17it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 5600: train loss 0.8427, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.18it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 5700: train loss 0.8426, val loss 0.8459\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 5800: train loss 0.8449, val loss 0.8457\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.18it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9993/10000  (99.93%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 5900: train loss 0.8456, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 6000: train loss 0.8432, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 6100: train loss 0.8439, val loss 0.8456\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","iter 6200: train loss 0.8451, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 6300: train loss 0.8459, val loss 0.8454\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 6400: train loss 0.8447, val loss 0.8461\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.18it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 6500: train loss 0.8428, val loss 0.8458\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 6600: train loss 0.8423, val loss 0.8454\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 6700: train loss 0.8434, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 6800: train loss 0.8446, val loss 0.8454\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 6900: train loss 0.8435, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.77it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 7000: train loss 0.8443, val loss 0.8456\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.16it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 7100: train loss 0.8447, val loss 0.8458\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.18it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 7200: train loss 0.8438, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.17it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.03it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 7300: train loss 0.8452, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 7400: train loss 0.8451, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 7500: train loss 0.8430, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 7600: train loss 0.8443, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 7700: train loss 0.8472, val loss 0.8456\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 7800: train loss 0.8431, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.18it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 7900: train loss 0.8411, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.17it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 8000: train loss 0.8439, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.21it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 8100: train loss 0.8449, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 8200: train loss 0.8445, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.18it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","iter 8300: train loss 0.8447, val loss 0.8454\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.00it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.00it/s]\n","iter 8400: train loss 0.8429, val loss 0.8453\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.00it/s]\n","iter 8500: train loss 0.8464, val loss 0.8452\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.19it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.00it/s]\n","iter 8600: train loss 0.8462, val loss 0.8455\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:07<00:00,  3.20it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:27<00:00,  3.02it/s]\n","iter 8700: train loss 0.8455, val loss 0.8453\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n"," 83% 20/24 [00:06<00:01,  3.12it/s]"]}]},{"cell_type":"markdown","metadata":{"id":"hdumgWyAWlt6"},"source":["## Slicing -- Addition"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"b-jVt7X7Llry"},"outputs":[],"source":["!python train.py slicing_addition_2_operand.txt --batch slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"pdPiOKoWKw0q","outputId":"eaab6064-1d31-4de3-87e5-b56efbd740ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: slicing\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/slicing_addition_4_operand.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'slicing_4_operand_reversed_test'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","batch_size = 256\n","block_size = 512\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# whether need zero‚Äêpadding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000\n","lr_decay_iters = 800000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/slicing_reverse_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 97\n","Using vocabulary size: 97\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.82M\n","/content/drive/MyDrive/addition/train.py:510: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/slicing_reverse_out/wandb/run-20251226_175706-50j1rak3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mslicing_4_operand_reversed_test\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/50j1rak3\u001b[0m\n","WARNING: results directory results/4_operands_0_to_999_uniform/slicing_reverse_out/slicing_4_operand_reversed_test already exists, overwriting...\n","max_new_tokens: 5\n","iter 0: train loss 4.3047, val loss 4.3047\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:03<00:00, 26.24it/s]\n","/content/drive/MyDrive/addition/evaluation.py:408: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02<00:00, 33.80it/s]\n","iter 1000: train loss 1.5441, val loss 1.5426\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 32.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.68it/s]\n","iter 2000: train loss 1.4996, val loss 1.5018\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 37/10000  (0.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.44it/s]\n","iter 3000: train loss 1.4777, val loss 1.4784\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 48/10000  (0.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.00it/s]\n","iter 4000: train loss 1.4565, val loss 1.4589\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 73/10000  (0.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.70it/s]\n","iter 5000: train loss 1.4483, val loss 1.4494\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 69/10000  (0.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.40it/s]\n","iter 6000: train loss 1.4409, val loss 1.4424\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.90it/s]\n","iter 7000: train loss 1.4388, val loss 1.4407\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.39it/s]\n","iter 8000: train loss 1.4408, val loss 1.4400\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 105/10000  (1.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.89it/s]\n","iter 9000: train loss 1.4393, val loss 1.4408\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.95it/s]\n","iter 10000: train loss 1.4403, val loss 1.4400\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 90/10000  (0.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.64it/s]\n","iter 11000: train loss 1.4402, val loss 1.4399\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 106/10000  (1.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.88it/s]\n","iter 12000: train loss 1.4402, val loss 1.4386\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 109/10000  (1.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.28it/s]\n","iter 13000: train loss 1.4403, val loss 1.4390\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 89/10000  (0.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.04it/s]\n","iter 14000: train loss 1.4398, val loss 1.4412\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 101/10000  (1.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.35it/s]\n","iter 15000: train loss 1.4403, val loss 1.4399\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 91/10000  (0.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.28it/s]\n","iter 16000: train loss 1.4405, val loss 1.4397\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 100/10000  (1.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.28it/s]\n","iter 17000: train loss 1.4403, val loss 1.4413\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 82/10000  (0.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.00it/s]\n","iter 18000: train loss 1.3582, val loss 1.3571\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 753/10000  (7.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.74it/s]\n","iter 19000: train loss 1.3486, val loss 1.3509\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 901/10000  (9.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.27it/s]\n","iter 20000: train loss 1.3426, val loss 1.3440\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 965/10000  (9.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.11it/s]\n","iter 21000: train loss 1.3475, val loss 1.3474\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 903/10000  (9.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.70it/s]\n","iter 22000: train loss 1.3457, val loss 1.3452\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 911/10000  (9.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.64it/s]\n","iter 23000: train loss 1.3405, val loss 1.3408\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 985/10000  (9.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.48it/s]\n","iter 24000: train loss 1.3426, val loss 1.3435\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1008/10000  (10.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.03it/s]\n","iter 25000: train loss 1.3427, val loss 1.3431\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 964/10000  (9.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.79it/s]\n","iter 26000: train loss 1.3407, val loss 1.3417\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 977/10000  (9.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.04it/s]\n","iter 27000: train loss 1.3412, val loss 1.3427\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1012/10000  (10.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.35it/s]\n","iter 28000: train loss 1.3403, val loss 1.3406\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 993/10000  (9.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.44it/s]\n","iter 29000: train loss 1.3399, val loss 1.3411\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 975/10000  (9.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.29it/s]\n","iter 30000: train loss 1.3401, val loss 1.3429\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 980/10000  (9.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.56it/s]\n","iter 31000: train loss 1.3389, val loss 1.3408\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1024/10000  (10.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.52it/s]\n","iter 32000: train loss 1.3387, val loss 1.3410\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1002/10000  (10.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.23it/s]\n","iter 33000: train loss 1.3380, val loss 1.3420\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 999/10000  (9.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.28it/s]\n","iter 34000: train loss 1.3378, val loss 1.3425\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1030/10000  (10.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.12it/s]\n","iter 35000: train loss 1.3389, val loss 1.3412\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1007/10000  (10.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.17it/s]\n","iter 36000: train loss 1.3373, val loss 1.3426\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1019/10000  (10.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.71it/s]\n","iter 37000: train loss 1.3365, val loss 1.3439\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1005/10000  (10.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.04it/s]\n","iter 38000: train loss 1.2470, val loss 1.2504\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9125/10000  (91.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.01it/s]\n","iter 39000: train loss 1.2554, val loss 1.2584\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8058/10000  (80.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.83it/s]\n","iter 40000: train loss 1.2391, val loss 1.2430\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9980/10000  (99.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.98it/s]\n","iter 41000: train loss 1.2385, val loss 1.2427\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.27it/s]\n","iter 42000: train loss 1.2384, val loss 1.2432\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.95it/s]\n","iter 43000: train loss 1.2392, val loss 1.2407\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9955/10000  (99.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.89it/s]\n","iter 44000: train loss 1.2376, val loss 1.2430\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9965/10000  (99.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.76it/s]\n","iter 45000: train loss 1.2365, val loss 1.2432\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.23it/s]\n","iter 46000: train loss 1.2362, val loss 1.2442\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.22it/s]\n","iter 47000: train loss 1.2380, val loss 1.2437\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 32.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.92it/s]\n","iter 48000: train loss 1.2350, val loss 1.2448\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.91it/s]\n","iter 49000: train loss 1.2340, val loss 1.2459\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.07it/s]\n","iter 50000: train loss 1.2334, val loss 1.2454\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.71it/s]\n","iter 51000: train loss 1.2329, val loss 1.2470\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.36it/s]\n","iter 52000: train loss 1.2322, val loss 1.2479\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.98it/s]\n","iter 53000: train loss 1.2308, val loss 1.2479\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.21it/s]\n","iter 54000: train loss 1.2299, val loss 1.2492\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.45it/s]\n","iter 55000: train loss 1.2271, val loss 1.2506\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.79it/s]\n","iter 56000: train loss 1.2238, val loss 1.2557\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.48it/s]\n","iter 57000: train loss 1.2223, val loss 1.2561\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.12it/s]\n","iter 58000: train loss 1.2194, val loss 1.2587\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.93it/s]\n","iter 59000: train loss 1.2163, val loss 1.2574\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.72it/s]\n","iter 60000: train loss 1.2147, val loss 1.2614\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.97it/s]\n","iter 61000: train loss 1.2113, val loss 1.2644\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.78it/s]\n","iter 62000: train loss 1.2089, val loss 1.2648\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.89it/s]\n","iter 63000: train loss 1.2083, val loss 1.2619\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.68it/s]\n","iter 64000: train loss 1.2055, val loss 1.2656\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.65it/s]\n","iter 65000: train loss 1.1998, val loss 1.2697\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.33it/s]\n","iter 66000: train loss 1.1998, val loss 1.2692\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.02it/s]\n","iter 67000: train loss 1.1985, val loss 1.2678\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.25it/s]\n","iter 68000: train loss 1.1971, val loss 1.2714\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.31it/s]\n","iter 69000: train loss 1.1943, val loss 1.2756\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.03it/s]\n","iter 70000: train loss 1.1927, val loss 1.2747\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.73it/s]\n","iter 71000: train loss 1.1901, val loss 1.2750\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.87it/s]\n","iter 72000: train loss 1.1895, val loss 1.2787\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.54it/s]\n","iter 73000: train loss 1.1889, val loss 1.2784\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.79it/s]\n","iter 74000: train loss 1.1872, val loss 1.2781\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.24it/s]\n","iter 75000: train loss 1.1829, val loss 1.2819\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 32.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.54it/s]\n","iter 76000: train loss 1.1831, val loss 1.2802\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.17it/s]\n","iter 77000: train loss 1.1814, val loss 1.2807\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.71it/s]\n","iter 78000: train loss 1.1791, val loss 1.2817\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10000/10000  (100.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.27it/s]\n","iter 79000: train loss 1.1790, val loss 1.2834\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.83it/s]\n","iter 80000: train loss 1.1808, val loss 1.2861\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9999/10000  (99.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.54it/s]\n"]}],"source":["!python train.py slicing_addition_4_operand.txt --batch slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"eAMCHgiqoXXW","outputId":"86e0de8c-f8f1-4964-ccc3-fcff5ce3ad30"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: slicing\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/slicing_addition_4_operand.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'slicing_4_operand_reversed_test'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","batch_size = 256\n","block_size = 512\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# whether need zero‚Äêpadding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000\n","lr_decay_iters = 800000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/slicing_reverse_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","Creating meta file for all reasonable characters...\n","all the unique characters: \n"," !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n","vocab size: 96\n","data has 22,518,028 tokens\n","data has 225,140 tokens\n","Using vocabulary size: 96\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.82M\n","/content/drive/MyDrive/addition/train_test.py:396: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run fak2f44e (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run fak2f44e (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m setting up run fak2f44e (0.2s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/slicing_reverse_out/wandb/run-20251226_170814-fak2f44e\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mslicing_4_operand_reversed_test\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/fak2f44e\u001b[0m\n","WARNING: results directory results/4_operands_0_to_999_uniform/slicing_reverse_out/slicing_4_operand_reversed_test already exists, overwriting...\n","max_new_tokens: 5\n","iter 0: train loss 4.2711, val loss 4.2679\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:02<00:00, 30.34it/s]\n","/content/drive/MyDrive/addition/evaluation.py:408: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02<00:00, 34.32it/s]\n","iter 1000: train loss 1.5417, val loss 1.5410\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.72it/s]\n","iter 2000: train loss 1.4927, val loss 1.4913\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 26/10000  (0.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.70it/s]\n","iter 3000: train loss 1.4625, val loss 1.4616\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 75/10000  (0.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.62it/s]\n","iter 4000: train loss 1.4452, val loss 1.4435\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 103/10000  (1.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.66it/s]\n","iter 5000: train loss 1.4417, val loss 1.4402\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 96/10000  (0.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.75it/s]\n","iter 6000: train loss 1.4415, val loss 1.4403\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 116/10000  (1.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.63it/s]\n","iter 7000: train loss 1.4393, val loss 1.4400\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 88/10000  (0.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.79it/s]\n","iter 8000: train loss 1.4404, val loss 1.4402\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.91it/s]\n","iter 9000: train loss 1.4401, val loss 1.4404\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 122/10000  (1.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.03it/s]\n","iter 10000: train loss 1.4525, val loss 1.4537\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 85/10000  (0.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.84it/s]\n","iter 11000: train loss 1.4399, val loss 1.4417\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 118/10000  (1.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.57it/s]\n","iter 12000: train loss 1.4401, val loss 1.4405\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 108/10000  (1.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.44it/s]\n","iter 13000: train loss 1.4387, val loss 1.4396\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 96/10000  (0.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.74it/s]\n","iter 14000: train loss 1.4408, val loss 1.4420\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 118/10000  (1.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.41it/s]\n","iter 15000: train loss 1.3669, val loss 1.3684\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 582/10000  (5.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.53it/s]\n","iter 16000: train loss 1.3520, val loss 1.3522\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 879/10000  (8.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.97it/s]\n","iter 17000: train loss 1.3405, val loss 1.3415\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1040/10000  (10.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.23it/s]\n","iter 18000: train loss 1.3430, val loss 1.3431\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 887/10000  (8.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.59it/s]\n","iter 19000: train loss 1.3402, val loss 1.3415\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1027/10000  (10.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.18it/s]\n","iter 20000: train loss 1.3424, val loss 1.3413\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 963/10000  (9.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.55it/s]\n","iter 21000: train loss 1.3420, val loss 1.3402\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1048/10000  (10.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.23it/s]\n","iter 22000: train loss 1.3406, val loss 1.3408\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 990/10000  (9.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.97it/s]\n","iter 23000: train loss 1.3406, val loss 1.3404\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1005/10000  (10.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.75it/s]\n","iter 24000: train loss 1.3420, val loss 1.3417\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 967/10000  (9.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.39it/s]\n","iter 25000: train loss 1.3398, val loss 1.3401\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 989/10000  (9.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.01it/s]\n","iter 26000: train loss 1.3407, val loss 1.3413\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 993/10000  (9.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.95it/s]\n","iter 27000: train loss 1.3401, val loss 1.3410\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1021/10000  (10.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.11it/s]\n","iter 28000: train loss 1.3396, val loss 1.3401\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1024/10000  (10.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 32.63it/s]\n","iter 29000: train loss 1.3379, val loss 1.3402\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 33.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 941/10000  (9.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.69it/s]\n"]}],"source":["!python train_test.py slicing_addition_4_operand.txt --batch slicing"]},{"cell_type":"markdown","metadata":{"id":"sUhzfB2OeOi-"},"source":["## Scaling"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ifio3FpGeSYG"},"outputs":[],"source":["!python train.py 1B_4_operands_addition_reversed.txt"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
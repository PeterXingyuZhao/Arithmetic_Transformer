{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":18131,"status":"ok","timestamp":1757945756333,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"aftd1RezTYnF","outputId":"255abb7a-b87d-44da-e8b3-5ef64453813f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/addition\n","configuration_files  mae_by_pred_iter.png  result_analysis.ipynb\n","configurator.py      main_utilities.py\t   results\n","data\t\t     model.py\t\t   startHere.ipynb\n","evaluation.py\t     __pycache__\t   statistical_measurements.py\n","legacy_code\t     README.md\t\t   train.py\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change directory to your code\n","%cd /content/drive/MyDrive/addition\n","%pwd   # verify you’re in the right place\n","!ls    # should show train.py, 4_operands_addition.txt, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":70531,"status":"ok","timestamp":1750438599342,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"rA7h8hQBUoev","outputId":"e6118237-bc39-4278-9560-29dec70fd21f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (24.2)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.30.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}],"source":["!pip install torch tqdm pyyaml wandb pandas requests\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":3951,"status":"ok","timestamp":1750438606144,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"Y_HQPy0Fpgje","outputId":"e399b748-7749-451b-b5aa-f0d6ce754406"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (24.2)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.30.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n","Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"]}],"source":["!pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":3512,"status":"ok","timestamp":1750438612116,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"CHz-J7yVpl_n","outputId":"907ab510-e86f-473d-c0c3-b98c93ce604d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]}],"source":["# Paste your 40‑character WandB API key between the quotes:\n","!wandb login a4aba93ba2c1ed22804b69ecf18fa5a072b81c47"]},{"cell_type":"markdown","metadata":{"id":"-DdllYgURa_-"},"source":["## Reproduce 2 Operands 0-999 Addition (Their Code, Their Data)"]},{"cell_type":"markdown","metadata":{"id":"KcS1hYyLRhZF"},"source":["### Plain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":910,"status":"ok","timestamp":1750737207145,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"l8-KppIBUWBg","outputId":"89228d5c-c06e-437e-8e1f-a16a6d48ee15"},"outputs":[{"name":"stdout","output_type":"stream","text":["# train plain addition on NanoGPT\n","dtype = 'float16'\n","init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n","resume_iter = False # if True, resume from saved iter_num, otherwise resume from iter_num 0\n","ckpt_path_name = 'ckpt_teaching_addition_plain_10000.pt'\n","# ===== Evaluation and Checkpointing ===== #\n","out_dir = 'out2/addition_plain_more_early_eval'\n","eval_interval = 250 \n","eval_iters = 200\n","log_interval = 10 \n","always_save_checkpoint = False\n","\n","# ===== Wandb logging ===== #\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","wandb_run_name = 'teaching_addition_plain_more_early_eval'\n","\n","# ===== Dataset ===== #\n","data_type='text'\n","data_format='plain'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 256\n","block_size = 256 # context of up to 256 previous characters\n","train_data_path = 'train_3digit_10000.txt'\n","eval_addition = True\n","start = \"FILE:data/bal/test_10000.txt\"\n","eval_addition_train = True\n","\n","# ===== NanoGPT model configuration ===== #\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually\n","beta2 = 0.99\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval = True\n","early_eval_interval = 25\n","early_eval_iters = 1000"]}],"source":["%cat config2/addition/plain/train_addition_bal.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":3403690,"status":"ok","timestamp":1750740622202,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"FfgKRsFGRia9","outputId":"ae6096ca-b684-403e-c0ed-60e29fbf4915"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","outputs(x):  322+648=960\n","904\n","wrong  : 322+648=960\n","correct: 322+648=970\n","outputs(x):  571+826=1497\n","69\n","wrong  : 571+826=1497\n","correct: 571+826=1397\n","outputs(x):  429+372=701\n","362\n","wrong  : 429+372=701\n","correct: 429+372=801\n"," 26% 21/80 [00:00<00:02, 25.74it/s]outputs(x):  817+383=1100\n","54\n","wrong  : 817+383=1100\n","correct: 817+383=1200\n","outputs(x):  382+418=700\n","0+9\n","wrong  : 382+418=700\n","correct: 382+418=800\n","outputs(x):  981+318=1399\n","27\n","wrong  : 981+318=1399\n","correct: 981+318=1299\n","outputs(x):  623+875=1598\n","21\n","wrong  : 623+875=1598\n","correct: 623+875=1498\n","outputs(x):  632+318=940\n","369\n","wrong  : 632+318=940\n","correct: 632+318=950\n","outputs(x):  533+163=796\n","389\n","wrong  : 533+163=796\n","correct: 533+163=696\n","outputs(x):  804+945=1759\n","87\n","wrong  : 804+945=1759\n","correct: 804+945=1749\n","outputs(x):  525+276=701\n","694\n","wrong  : 525+276=701\n","correct: 525+276=801\n","outputs(x):  195+112=207\n","297\n","wrong  : 195+112=207\n","correct: 195+112=307\n","outputs(x):  142+664=706\n","969\n","wrong  : 142+664=706\n","correct: 142+664=806\n","outputs(x):  346+494=830\n","599\n","wrong  : 346+494=830\n","correct: 346+494=840\n","outputs(x):  112+586=798\n","812\n","wrong  : 112+586=798\n","correct: 112+586=698\n","outputs(x):  479+529=908\n","89+\n","wrong  : 479+529=908\n","correct: 479+529=1008\n"," 30% 24/80 [00:00<00:02, 25.34it/s]outputs(x):  128+272=300\n","210\n","wrong  : 128+272=300\n","correct: 128+272=400\n","outputs(x):  457+213=660\n","962\n","wrong  : 457+213=660\n","correct: 457+213=670\n","outputs(x):  488+421=919\n","71+\n","wrong  : 488+421=919\n","correct: 488+421=909\n","outputs(x):  509+293=702\n","6+4\n","wrong  : 509+293=702\n","correct: 509+293=802\n","outputs(x):  637+165=702\n","825\n","wrong  : 637+165=702\n","correct: 637+165=802\n","outputs(x):  594+165=769\n","397\n","wrong  : 594+165=769\n","correct: 594+165=759\n","outputs(x):  598+207=705\n","121\n","wrong  : 598+207=705\n","correct: 598+207=805\n","outputs(x):  469+311=770\n","38+\n","wrong  : 469+311=770\n","correct: 469+311=780\n","outputs(x):  521+168=699\n","828\n","wrong  : 521+168=699\n","correct: 521+168=689\n","outputs(x):  704+996=1600\n","46\n","wrong  : 704+996=1600\n","correct: 704+996=1700\n","outputs(x):  799+104=803\n","220\n","wrong  : 799+104=803\n","correct: 799+104=903\n","outputs(x):  263+717=970\n","944\n","wrong  : 263+717=970\n","correct: 263+717=980\n","outputs(x):  931+569=1400\n","39\n","wrong  : 931+569=1400\n","correct: 931+569=1500\n","outputs(x):  719+983=1602\n","40\n","wrong  : 719+983=1602\n","correct: 719+983=1702\n","outputs(x):  683+606=1389\n","64\n","wrong  : 683+606=1389\n","correct: 683+606=1289\n","outputs(x):  471+358=839\n","244\n","wrong  : 471+358=839\n","correct: 471+358=829\n"," 34% 27/80 [00:01<00:02, 24.73it/s]outputs(x):  517+477=1994\n","11\n","wrong  : 517+477=1994\n","correct: 517+477=994\n","outputs(x):  183+235=428\n","647\n","wrong  : 183+235=428\n","correct: 183+235=418\n","outputs(x):  114+645=769\n","271\n","wrong  : 114+645=769\n","correct: 114+645=759\n","outputs(x):  663+737=1300\n","51\n","wrong  : 663+737=1300\n","correct: 663+737=1400\n","outputs(x):  572+927=1599\n","69\n","wrong  : 572+927=1599\n","correct: 572+927=1499\n","outputs(x):  862+139=901\n","101\n","wrong  : 862+139=901\n","correct: 862+139=1001\n","outputs(x):  836+134=960\n","121\n","wrong  : 836+134=960\n","correct: 836+134=970\n","outputs(x):  479+301=770\n","969\n","wrong  : 479+301=770\n","correct: 479+301=780\n"," 38% 30/80 [00:01<00:02, 24.83it/s]outputs(x):  331+365=796\n","84+\n","wrong  : 331+365=796\n","correct: 331+365=696\n","outputs(x):  839+157=1096\n","39\n","wrong  : 839+157=1096\n","correct: 839+157=996\n","outputs(x):  433+862=1395\n","79\n","wrong  : 433+862=1395\n","correct: 433+862=1295\n","outputs(x):  504+515=1029\n","23\n","wrong  : 504+515=1029\n","correct: 504+515=1019\n","outputs(x):  815+385=1100\n","80\n","wrong  : 815+385=1100\n","correct: 815+385=1200\n","outputs(x):  135+105=230\n","538\n","wrong  : 135+105=230\n","correct: 135+105=240\n","outputs(x):  871+926=1897\n","93\n","wrong  : 871+926=1897\n","correct: 871+926=1797\n","outputs(x):  220+489=719\n","170\n","wrong  : 220+489=719\n","correct: 220+489=709\n","outputs(x):  411+398=819\n","17+\n","wrong  : 411+398=819\n","correct: 411+398=809\n","outputs(x):  349+248=697\n","13+\n","wrong  : 349+248=697\n","correct: 349+248=597\n","outputs(x):  703+795=1598\n","67\n","wrong  : 703+795=1598\n","correct: 703+795=1498\n"," 41% 33/80 [00:01<00:01, 24.49it/s]outputs(x):  618+288=806\n","396\n","wrong  : 618+288=806\n","correct: 618+288=906\n","outputs(x):  322+677=1099\n","79\n","wrong  : 322+677=1099\n","correct: 322+677=999\n","outputs(x):  314+685=1099\n","70\n","wrong  : 314+685=1099\n","correct: 314+685=999\n","outputs(x):  501+408=919\n","73+\n","wrong  : 501+408=919\n","correct: 501+408=909\n","outputs(x):  106+390=596\n","103\n","wrong  : 106+390=596\n","correct: 106+390=496\n","outputs(x):  117+283=300\n","531\n","wrong  : 117+283=300\n","correct: 117+283=400\n"," 45% 36/80 [00:01<00:01, 24.63it/s]outputs(x):  197+305=402\n","813\n","wrong  : 197+305=402\n","correct: 197+305=502\n","outputs(x):  415+465=870\n","135\n","wrong  : 415+465=870\n","correct: 415+465=880\n","outputs(x):  959+841=1700\n","87\n","wrong  : 959+841=1700\n","correct: 959+841=1800\n","outputs(x):  507+313=810\n","346\n","wrong  : 507+313=810\n","correct: 507+313=820\n","outputs(x):  558+222=770\n","869\n","wrong  : 558+222=770\n","correct: 558+222=780\n","outputs(x):  331+398=739\n","279\n","wrong  : 331+398=739\n","correct: 331+398=729\n","outputs(x):  405+993=1498\n","26\n","wrong  : 405+993=1498\n","correct: 405+993=1398\n","outputs(x):  581+358=949\n","466\n","wrong  : 581+358=949\n","correct: 581+358=939\n","outputs(x):  242+507=759\n","63+\n","wrong  : 242+507=759\n","correct: 242+507=749\n","outputs(x):  893+107=900\n","44+\n","wrong  : 893+107=900\n","correct: 893+107=1000\n","outputs(x):  337+163=400\n","242\n","wrong  : 337+163=400\n","correct: 337+163=500\n","outputs(x):  104+591=795\n","60+\n","wrong  : 104+591=795\n","correct: 104+591=695\n","outputs(x):  134+856=1090\n","92\n","wrong  : 134+856=1090\n","correct: 134+856=990\n","outputs(x):  928+772=1600\n","47\n","wrong  : 928+772=1600\n","correct: 928+772=1700\n","outputs(x):  812+192=904\n","995\n","wrong  : 812+192=904\n","correct: 812+192=1004\n","outputs(x):  201+495=796\n","895\n","wrong  : 201+495=796\n","correct: 201+495=696\n","outputs(x):  257+413=660\n","237\n","wrong  : 257+413=660\n","correct: 257+413=670\n","outputs(x):  384+466=840\n","748\n","wrong  : 384+466=840\n","correct: 384+466=850\n","outputs(x):  499+404=803\n","987\n","wrong  : 499+404=803\n","correct: 499+404=903\n","outputs(x):  402+692=1194\n","65\n","wrong  : 402+692=1194\n","correct: 402+692=1094\n","outputs(x):  642+452=1194\n","64\n","wrong  : 642+452=1194\n","correct: 642+452=1094\n","outputs(x):  814+386=1100\n","80\n","wrong  : 814+386=1100\n","correct: 814+386=1200\n"," 49% 39/80 [00:01<00:01, 24.70it/s]outputs(x):  192+110=202\n","31+\n","wrong  : 192+110=202\n","correct: 192+110=302\n","outputs(x):  134+973=1007\n","74\n","wrong  : 134+973=1007\n","correct: 134+973=1107\n","outputs(x):  234+196=420\n","986\n","wrong  : 234+196=420\n","correct: 234+196=430\n","outputs(x):  471+539=910\n","366\n","wrong  : 471+539=910\n","correct: 471+539=1010\n","outputs(x):  602+494=1196\n","64\n","wrong  : 602+494=1196\n","correct: 602+494=1096\n","outputs(x):  892+112=904\n","120\n","wrong  : 892+112=904\n","correct: 892+112=1004\n","outputs(x):  374+625=1009\n","71\n","wrong  : 374+625=1009\n","correct: 374+625=999\n","outputs(x):  354+245=699\n","994\n","wrong  : 354+245=699\n","correct: 354+245=599\n","outputs(x):  933+866=1899\n","91\n","wrong  : 933+866=1899\n","correct: 933+866=1799\n","outputs(x):  622+107=739\n","832\n","wrong  : 622+107=739\n","correct: 622+107=729\n","outputs(x):  254+254=518\n","12+\n","wrong  : 254+254=518\n","correct: 254+254=508\n","outputs(x):  810+215=1015\n","46\n","wrong  : 810+215=1015\n","correct: 810+215=1025\n"," 52% 42/80 [00:01<00:01, 24.91it/s]outputs(x):  257+383=630\n","542\n","wrong  : 257+383=630\n","correct: 257+383=640\n","outputs(x):  357+262=629\n","680\n","wrong  : 357+262=629\n","correct: 357+262=619\n","outputs(x):  819+111=920\n","882\n","wrong  : 819+111=920\n","correct: 819+111=930\n","outputs(x):  258+349=507\n","742\n","wrong  : 258+349=507\n","correct: 258+349=607\n","outputs(x):  506+501=1017\n","49\n","wrong  : 506+501=1017\n","correct: 506+501=1007\n","outputs(x):  417+574=1091\n","78\n","wrong  : 417+574=1091\n","correct: 417+574=991\n","outputs(x):  719+171=880\n","848\n","wrong  : 719+171=880\n","correct: 719+171=890\n","outputs(x):  436+368=704\n","854\n","wrong  : 436+368=704\n","correct: 436+368=804\n"," 56% 45/80 [00:01<00:01, 25.17it/s]outputs(x):  158+622=770\n","356\n","wrong  : 158+622=770\n","correct: 158+622=780\n","outputs(x):  800+199=1099\n","32\n","wrong  : 800+199=1099\n","correct: 800+199=999\n","outputs(x):  423+976=1499\n","59\n","wrong  : 423+976=1499\n","correct: 423+976=1399\n","outputs(x):  432+965=1497\n","59\n","wrong  : 432+965=1497\n","correct: 432+965=1397\n","outputs(x):  289+491=770\n","971\n","wrong  : 289+491=770\n","correct: 289+491=780\n","outputs(x):  302+298=500\n","476\n","wrong  : 302+298=500\n","correct: 302+298=600\n","outputs(x):  154+726=870\n","35+\n","wrong  : 154+726=870\n","correct: 154+726=880\n","outputs(x):  155+705=850\n","598\n","wrong  : 155+705=850\n","correct: 155+705=860\n","outputs(x):  156+784=930\n","59+\n","wrong  : 156+784=930\n","correct: 156+784=940\n","outputs(x):  293+536=839\n","88+\n","wrong  : 293+536=839\n","correct: 293+536=829\n","outputs(x):  246+504=740\n","732\n","wrong  : 246+504=740\n","correct: 246+504=750\n","outputs(x):  164+637=701\n","242\n","wrong  : 164+637=701\n","correct: 164+637=801\n"," 60% 48/80 [00:01<00:01, 25.36it/s]outputs(x):  894+805=1799\n","34\n","wrong  : 894+805=1799\n","correct: 894+805=1699\n","outputs(x):  582+119=601\n","571\n","wrong  : 582+119=601\n","correct: 582+119=701\n","outputs(x):  276+374=640\n","541\n","wrong  : 276+374=640\n","correct: 276+374=650\n","outputs(x):  230+379=619\n","254\n","wrong  : 230+379=619\n","correct: 230+379=609\n","outputs(x):  148+222=360\n","136\n","wrong  : 148+222=360\n","correct: 148+222=370\n","outputs(x):  578+223=701\n","366\n","wrong  : 578+223=701\n","correct: 578+223=801\n","outputs(x):  623+572=1295\n","58\n","wrong  : 623+572=1295\n","correct: 623+572=1195\n","outputs(x):  669+301=960\n","952\n","wrong  : 669+301=960\n","correct: 669+301=970\n","outputs(x):  131+368=509\n","684\n","wrong  : 131+368=509\n","correct: 131+368=499\n"," 64% 51/80 [00:02<00:01, 25.49it/s]outputs(x):  170+279=459\n","740\n","wrong  : 170+279=459\n","correct: 170+279=449\n","outputs(x):  790+507=1397\n","10\n","wrong  : 790+507=1397\n","correct: 790+507=1297\n","outputs(x):  880+218=1198\n","95\n","wrong  : 880+218=1198\n","correct: 880+218=1098\n","outputs(x):  392+208=590\n","476\n","wrong  : 392+208=590\n","correct: 392+208=600\n","outputs(x):  734+665=1499\n","58\n","wrong  : 734+665=1499\n","correct: 734+665=1399\n","outputs(x):  498+308=706\n","941\n","wrong  : 498+308=706\n","correct: 498+308=806\n","outputs(x):  438+431=879\n","628\n","wrong  : 438+431=879\n","correct: 438+431=869\n","outputs(x):  236+268=404\n","504\n","wrong  : 236+268=404\n","correct: 236+268=504\n","outputs(x):  327+676=903\n","130\n","wrong  : 327+676=903\n","correct: 327+676=1003\n","outputs(x):  746+245=1091\n","32\n","wrong  : 746+245=1091\n","correct: 746+245=991\n","outputs(x):  915+914=1839\n","82\n","wrong  : 915+914=1839\n","correct: 915+914=1829\n","outputs(x):  711+290=901\n","709\n","wrong  : 711+290=901\n","correct: 711+290=1001\n"," 68% 54/80 [00:02<00:01, 25.43it/s]outputs(x):  680+819=1599\n","37\n","wrong  : 680+819=1599\n","correct: 680+819=1499\n","outputs(x):  557+943=1400\n","89\n","wrong  : 557+943=1400\n","correct: 557+943=1500\n","outputs(x):  916+384=1200\n","54\n","wrong  : 916+384=1200\n","correct: 916+384=1300\n","outputs(x):  101+595=796\n","969\n","wrong  : 101+595=796\n","correct: 101+595=696\n","outputs(x):  104+366=460\n","562\n","wrong  : 104+366=460\n","correct: 104+366=470\n","outputs(x):  561+240=701\n","248\n","wrong  : 561+240=701\n","correct: 561+240=801\n","outputs(x):  139+765=804\n","621\n","wrong  : 139+765=804\n","correct: 139+765=904\n","outputs(x):  629+172=701\n","41+\n","wrong  : 629+172=701\n","correct: 629+172=801\n","outputs(x):  631+367=1098\n","79\n","wrong  : 631+367=1098\n","correct: 631+367=998\n","outputs(x):  572+627=1299\n","44\n","wrong  : 572+627=1299\n","correct: 572+627=1199\n","outputs(x):  463+338=701\n","252\n","wrong  : 463+338=701\n","correct: 463+338=801\n","outputs(x):  880+618=1598\n","27\n","wrong  : 880+618=1598\n","correct: 880+618=1498\n","outputs(x):  972+927=1999\n","58\n","wrong  : 972+927=1999\n","correct: 972+927=1899\n","outputs(x):  241+560=701\n","248\n","wrong  : 241+560=701\n","correct: 241+560=801\n","outputs(x):  370+159=539\n","766\n","wrong  : 370+159=539\n","correct: 370+159=529\n"," 71% 57/80 [00:02<00:00, 24.88it/s]outputs(x):  527+443=960\n","501\n","wrong  : 527+443=960\n","correct: 527+443=970\n","outputs(x):  481+718=1299\n","63\n","wrong  : 481+718=1299\n","correct: 481+718=1199\n","outputs(x):  199+701=800\n","979\n","wrong  : 199+701=800\n","correct: 199+701=900\n","outputs(x):  392+209=501\n","46+\n","wrong  : 392+209=501\n","correct: 392+209=601\n","outputs(x):  182+710=992\n","588\n","wrong  : 182+710=992\n","correct: 182+710=892\n","outputs(x):  721+377=1198\n","11\n","wrong  : 721+377=1198\n","correct: 721+377=1098\n","outputs(x):  230+289=529\n","524\n","wrong  : 230+289=529\n","correct: 230+289=519\n","outputs(x):  209+151=350\n","682\n","wrong  : 209+151=350\n","correct: 209+151=360\n","outputs(x):  411+290=601\n","609\n","wrong  : 411+290=601\n","correct: 411+290=701\n","outputs(x):  711+278=999\n","538\n","wrong  : 711+278=999\n","correct: 711+278=989\n","outputs(x):  839+149=1088\n","39\n","wrong  : 839+149=1088\n","correct: 839+149=988\n","outputs(x):  757+103=850\n","492\n","wrong  : 757+103=850\n","correct: 757+103=860\n","outputs(x):  771+149=910\n","115\n","wrong  : 771+149=910\n","correct: 771+149=920\n","outputs(x):  333+866=1299\n","79\n","wrong  : 333+866=1299\n","correct: 333+866=1199\n","outputs(x):  198+204=302\n","214\n","wrong  : 198+204=302\n","correct: 198+204=402\n","outputs(x):  542+653=1295\n","89\n","wrong  : 542+653=1295\n","correct: 542+653=1195\n","outputs(x):  444+815=1269\n","98\n","wrong  : 444+815=1269\n","correct: 444+815=1259\n","outputs(x):  503+996=1599\n","96\n","wrong  : 503+996=1599\n","correct: 503+996=1499\n","outputs(x):  238+412=640\n","68+\n","wrong  : 238+412=640\n","correct: 238+412=650\n"," 75% 60/80 [00:02<00:00, 24.32it/s]outputs(x):  537+269=706\n","682\n","wrong  : 537+269=706\n","correct: 537+269=806\n","outputs(x):  435+171=506\n","295\n","wrong  : 435+171=506\n","correct: 435+171=606\n","outputs(x):  646+304=940\n","5+9\n","wrong  : 646+304=940\n","correct: 646+304=950\n","outputs(x):  444+752=1296\n","85\n","wrong  : 444+752=1296\n","correct: 444+752=1196\n","outputs(x):  557+151=608\n","753\n","wrong  : 557+151=608\n","correct: 557+151=708\n","outputs(x):  924+999=1823\n","51\n","wrong  : 924+999=1823\n","correct: 924+999=1923\n","outputs(x):  168+541=719\n","159\n","wrong  : 168+541=719\n","correct: 168+541=709\n","outputs(x):  729+241=960\n","181\n","wrong  : 729+241=960\n","correct: 729+241=970\n","outputs(x):  690+606=1396\n","10\n","wrong  : 690+606=1396\n","correct: 690+606=1296\n","outputs(x):  593+606=1299\n","79\n","wrong  : 593+606=1299\n","correct: 593+606=1199\n","outputs(x):  240+158=498\n","367\n","wrong  : 240+158=498\n","correct: 240+158=398\n","outputs(x):  114+677=891\n","944\n","wrong  : 114+677=891\n","correct: 114+677=791\n","outputs(x):  345+160=405\n","839\n","wrong  : 345+160=405\n","correct: 345+160=505\n","outputs(x):  381+349=720\n","938\n","wrong  : 381+349=720\n","correct: 381+349=730\n","outputs(x):  645+954=1699\n","97\n","wrong  : 645+954=1699\n","correct: 645+954=1599\n","outputs(x):  770+528=1398\n","12\n","wrong  : 770+528=1398\n","correct: 770+528=1298\n"," 79% 63/80 [00:02<00:00, 24.11it/s]outputs(x):  9+483=1452\n","75\n","wrong  : 9+483=1452\n","correct: 9+483=492\n","outputs(x):  1+116=447\n","650\n","wrong  : 1+116=447\n","correct: 1+116=117\n","outputs(x):  7+869=1716\n","50\n","wrong  : 7+869=1716\n","correct: 7+869=876\n","outputs(x):  191+5=106\n","299\n","wrong  : 191+5=106\n","correct: 191+5=196\n","outputs(x):  0+646=1136\n","21\n","wrong  : 0+646=1136\n","correct: 0+646=646\n","outputs(x):  35+33=468\n","765\n","wrong  : 35+33=468\n","correct: 35+33=68\n","outputs(x):  3+849=1602\n","28\n","wrong  : 3+849=1602\n","correct: 3+849=852\n","outputs(x):  29+49=768\n","60+\n","wrong  : 29+49=768\n","correct: 29+49=78\n","outputs(x):  3+229=1022\n","65\n","wrong  : 3+229=1022\n","correct: 3+229=232\n","outputs(x):  8+842=1830\n","51\n","wrong  : 8+842=1830\n","correct: 8+842=850\n","outputs(x):  7+499=1216\n","38\n","wrong  : 7+499=1216\n","correct: 7+499=506\n","outputs(x):  17+46=95\n","856+\n","wrong  : 17+46=95\n","correct: 17+46=63\n","outputs(x):  42+38=170\n","369\n","wrong  : 42+38=170\n","correct: 42+38=80\n","outputs(x):  5+172=537\n","437\n","wrong  : 5+172=537\n","correct: 5+172=177\n","outputs(x):  26+42=798\n","650\n","wrong  : 26+42=798\n","correct: 26+42=68\n","outputs(x):  2+115=427\n","970\n","wrong  : 2+115=427\n","correct: 2+115=117\n","outputs(x):  10+42=452\n","52+\n","wrong  : 10+42=452\n","correct: 10+42=52\n","outputs(x):  4+829=1523\n","37\n","wrong  : 4+829=1523\n","correct: 4+829=833\n","outputs(x):  9+507=1266\n","36\n","wrong  : 9+507=1266\n","correct: 9+507=516\n","outputs(x):  87+84=1071\n","51\n","wrong  : 87+84=1071\n","correct: 87+84=171\n","outputs(x):  4+787=1291\n","61\n","wrong  : 4+787=1291\n","correct: 4+787=791\n","outputs(x):  854+9=853\n","828\n","wrong  : 854+9=853\n","correct: 854+9=863\n","outputs(x):  9+750=1449\n","60\n","wrong  : 9+750=1449\n","correct: 9+750=759\n","outputs(x):  4+233=887\n","339\n","wrong  : 4+233=887\n","correct: 4+233=237\n","outputs(x):  38+13=451\n","269\n","wrong  : 38+13=451\n","correct: 38+13=51\n","outputs(x):  6+512=1058\n","94\n","wrong  : 6+512=1058\n","correct: 6+512=518\n","outputs(x):  2+650=1292\n","62\n","wrong  : 2+650=1292\n","correct: 2+650=652\n","outputs(x):  6+213=979\n","724\n","wrong  : 6+213=979\n","correct: 6+213=219\n","outputs(x):  65+41=786\n","721\n","wrong  : 65+41=786\n","correct: 65+41=106\n","outputs(x):  19+40=80\n","698+\n","wrong  : 19+40=80\n","correct: 19+40=59\n","outputs(x):  3+453=1256\n","42\n","wrong  : 3+453=1256\n","correct: 3+453=456\n","outputs(x):  2+878=1750\n","78\n","wrong  : 2+878=1750\n","correct: 2+878=880\n","outputs(x):  3+442=1135\n","58\n","wrong  : 3+442=1135\n","correct: 3+442=445\n","outputs(x):  3+457=1240\n","10\n","wrong  : 3+457=1240\n","correct: 3+457=460\n","outputs(x):  164+4=108\n","902\n","wrong  : 164+4=108\n","correct: 164+4=168\n","outputs(x):  8+118=426\n","139\n","wrong  : 8+118=426\n","correct: 8+118=126\n","outputs(x):  696+2=608\n","34+\n","wrong  : 696+2=608\n","correct: 696+2=698\n","outputs(x):  6+636=1412\n","64\n","wrong  : 6+636=1412\n","correct: 6+636=642\n","outputs(x):  0+741=1461\n","42\n","wrong  : 0+741=1461\n","correct: 0+741=741\n","outputs(x):  5+902=1207\n","39\n","wrong  : 5+902=1207\n","correct: 5+902=907\n","outputs(x):  456+4=450\n","951\n","wrong  : 456+4=450\n","correct: 456+4=460\n","outputs(x):  71+10=371\n","96+\n","wrong  : 71+10=371\n","correct: 71+10=81\n","outputs(x):  2+545=657\n","327\n","wrong  : 2+545=657\n","correct: 2+545=547\n","outputs(x):  16+84=970\n","122\n","wrong  : 16+84=970\n","correct: 16+84=100\n","outputs(x):  7+161=478\n","246\n","wrong  : 7+161=478\n","correct: 7+161=168\n","outputs(x):  5+120=825\n","12+\n","wrong  : 5+120=825\n","correct: 5+120=125\n","outputs(x):  52+31=753\n","544\n","wrong  : 52+31=753\n","correct: 52+31=83\n","outputs(x):  3+553=1056\n","87\n","wrong  : 3+553=1056\n","correct: 3+553=556\n","outputs(x):  292+8=290\n","485\n","wrong  : 292+8=290\n","correct: 292+8=300\n","outputs(x):  15+58=76\n","695+\n","wrong  : 15+58=76\n","correct: 15+58=73\n","outputs(x):  336+7=353\n","146\n","wrong  : 336+7=353\n","correct: 336+7=343\n","outputs(x):  31+44=84\n","96+6\n","wrong  : 31+44=84\n","correct: 31+44=75\n","outputs(x):  7+977=1544\n","54\n","wrong  : 7+977=1544\n","correct: 7+977=984\n","outputs(x):  7+999=1066\n","16\n","wrong  : 7+999=1066\n","correct: 7+999=1006\n","outputs(x):  8+488=1486\n","88\n","wrong  : 8+488=1486\n","correct: 8+488=496\n","outputs(x):  4+636=1250\n","17\n","wrong  : 4+636=1250\n","correct: 4+636=640\n","outputs(x):  6+598=1194\n","20\n","wrong  : 6+598=1194\n","correct: 6+598=604\n","outputs(x):  4+447=1301\n","58\n","wrong  : 4+447=1301\n","correct: 4+447=451\n","outputs(x):  3+576=1539\n","57\n","wrong  : 3+576=1539\n","correct: 3+576=579\n","outputs(x):  1+685=1526\n","27\n","wrong  : 1+685=1526\n","correct: 1+685=686\n","outputs(x):  38+81=129\n","458\n","wrong  : 38+81=129\n","correct: 38+81=119\n","outputs(x):  559+2=551\n","844\n","wrong  : 559+2=551\n","correct: 559+2=561\n","outputs(x):  2+120=752\n","995\n","wrong  : 2+120=752\n","correct: 2+120=122\n","outputs(x):  0+551=671\n","796\n","wrong  : 0+551=671\n","correct: 0+551=551\n","outputs(x):  7+996=1543\n","46\n","wrong  : 7+996=1543\n","correct: 7+996=1003\n","outputs(x):  7+106=623\n","84+\n","wrong  : 7+106=623\n","correct: 7+106=113\n","outputs(x):  6+791=1587\n","18\n","wrong  : 6+791=1587\n","correct: 6+791=797\n","outputs(x):  34+49=93\n","725+\n","wrong  : 34+49=93\n","correct: 34+49=83\n","outputs(x):  3+510=1473\n","15\n","wrong  : 3+510=1473\n","correct: 3+510=513\n","outputs(x):  9+987=1566\n","43\n","wrong  : 9+987=1566\n","correct: 9+987=996\n","outputs(x):  31+78=1009\n","86\n","wrong  : 31+78=1009\n","correct: 31+78=109\n","outputs(x):  4+511=605\n","443\n","wrong  : 4+511=605\n","correct: 4+511=515\n","outputs(x):  63+26=96\n","843+\n","wrong  : 63+26=96\n","correct: 63+26=89\n","outputs(x):  24+43=467\n","608\n","wrong  : 24+43=467\n","correct: 24+43=67\n","outputs(x):  3+635=1318\n","12\n","wrong  : 3+635=1318\n","correct: 3+635=638\n","outputs(x):  140+5=155\n","949\n","wrong  : 140+5=155\n","correct: 140+5=145\n","outputs(x):  3+723=1336\n","73\n","wrong  : 3+723=1336\n","correct: 3+723=726\n","outputs(x):  519+7=516\n","8+2\n","wrong  : 519+7=516\n","correct: 519+7=526\n","outputs(x):  45+38=93\n","104+\n","wrong  : 45+38=93\n","correct: 45+38=83\n","outputs(x):  0+360=870\n","616\n","wrong  : 0+360=870\n","correct: 0+360=360\n","outputs(x):  32+28=51\n","734+\n","wrong  : 32+28=51\n","correct: 32+28=60\n","outputs(x):  2+980=1862\n","34\n","wrong  : 2+980=1862\n","correct: 2+980=982\n","outputs(x):  4+353=897\n","984\n","wrong  : 4+353=897\n","correct: 4+353=357\n","outputs(x):  2+197=719\n","464\n","wrong  : 2+197=719\n","correct: 2+197=199\n","outputs(x):  3+228=1111\n","42\n","wrong  : 3+228=1111\n","correct: 3+228=231\n","outputs(x):  977+8=1085\n","48\n","wrong  : 977+8=1085\n","correct: 977+8=985\n","outputs(x):  151+6=167\n","699\n","wrong  : 151+6=167\n","correct: 151+6=157\n","outputs(x):  6+555=771\n","628\n","wrong  : 6+555=771\n","correct: 6+555=561\n","outputs(x):  66+25=90\n","110+\n","wrong  : 66+25=90\n","correct: 66+25=91\n","outputs(x):  7+975=1832\n","49\n","wrong  : 7+975=1832\n","correct: 7+975=982\n","outputs(x):  2+712=1284\n","83\n","wrong  : 2+712=1284\n","correct: 2+712=714\n","outputs(x):  567+8=565\n","114\n","wrong  : 567+8=565\n","correct: 567+8=575\n","outputs(x):  76+41=707\n","869\n","wrong  : 76+41=707\n","correct: 76+41=117\n","outputs(x):  21+85=1076\n","89\n","wrong  : 21+85=1076\n","correct: 21+85=106\n","outputs(x):  7+785=1412\n","65\n","wrong  : 7+785=1412\n","correct: 7+785=792\n","outputs(x):  9+210=689\n","89+\n","wrong  : 9+210=689\n","correct: 9+210=219\n","outputs(x):  4+386=480\n","57+\n","wrong  : 4+386=480\n","correct: 4+386=390\n","outputs(x):  1+985=1346\n","74\n","wrong  : 1+985=1346\n","correct: 1+985=986\n","outputs(x):  83+47=120\n","542\n","wrong  : 83+47=120\n","correct: 83+47=130\n","outputs(x):  80+39=129\n","969\n","wrong  : 80+39=129\n","correct: 80+39=119\n","outputs(x):  7+287=314\n","414\n","wrong  : 7+287=314\n","correct: 7+287=294\n","outputs(x):  407+5=413\n","638\n","wrong  : 407+5=413\n","correct: 407+5=412\n","outputs(x):  28+63=691\n","436\n","wrong  : 28+63=691\n","correct: 28+63=91\n","outputs(x):  112+9=11\n","74+9\n","wrong  : 112+9=11\n","correct: 112+9=121\n","outputs(x):  6+301=717\n","490\n","wrong  : 6+301=717\n","correct: 6+301=307\n","outputs(x):  8+572=1420\n","61\n","wrong  : 8+572=1420\n","correct: 8+572=580\n","outputs(x):  5+564=1209\n","94\n","wrong  : 5+564=1209\n","correct: 5+564=569\n","outputs(x):  346+2=358\n","103\n","wrong  : 346+2=358\n","correct: 346+2=348\n","outputs(x):  7+699=1486\n","74\n","wrong  : 7+699=1486\n","correct: 7+699=706\n","outputs(x):  166+7=163\n","300\n","wrong  : 166+7=163\n","correct: 166+7=173\n","outputs(x):  6+957=1643\n","15\n","wrong  : 6+957=1643\n","correct: 6+957=963\n","outputs(x):  9+617=1406\n","57\n","wrong  : 9+617=1406\n","correct: 9+617=626\n","outputs(x):  280+7=286\n","24+\n","wrong  : 280+7=286\n","correct: 280+7=287\n","outputs(x):  0+536=1526\n","76\n","wrong  : 0+536=1526\n","correct: 0+536=536\n","outputs(x):  275+5=200\n","963\n","wrong  : 275+5=200\n","correct: 275+5=280\n","outputs(x):  40+67=1007\n","96\n","wrong  : 40+67=1007\n","correct: 40+67=107\n","outputs(x):  883+9=992\n","720\n","wrong  : 883+9=992\n","correct: 883+9=892\n","outputs(x):  208+4=222\n","854\n","wrong  : 208+4=222\n","correct: 208+4=212\n","outputs(x):  9+219=618\n","464\n","wrong  : 9+219=618\n","correct: 9+219=228\n","outputs(x):  608+3=601\n","246\n","wrong  : 608+3=601\n","correct: 608+3=611\n","outputs(x):  44+24=688\n","898\n","wrong  : 44+24=688\n","correct: 44+24=68\n","outputs(x):  392+8=490\n","818\n","wrong  : 392+8=490\n","correct: 392+8=400\n","outputs(x):  8+758=1756\n","88\n","wrong  : 8+758=1756\n","correct: 8+758=766\n","outputs(x):  9+845=1744\n","80\n","wrong  : 9+845=1744\n","correct: 9+845=854\n","outputs(x):  49+98=1037\n","29\n","wrong  : 49+98=1037\n","correct: 49+98=147\n","outputs(x):  416+1=427\n","759\n","wrong  : 416+1=427\n","correct: 416+1=417\n","outputs(x):  196+5=1001\n","66\n","wrong  : 196+5=1001\n","correct: 196+5=201\n","outputs(x):  8+625=1113\n","67\n","wrong  : 8+625=1113\n","correct: 8+625=633\n","outputs(x):  196+1=107\n","869\n","wrong  : 196+1=107\n","correct: 196+1=197\n","outputs(x):  9+201=1100\n","42\n","wrong  : 9+201=1100\n","correct: 9+201=210\n","outputs(x):  1+517=668\n","637\n","wrong  : 1+517=668\n","correct: 1+517=518\n","outputs(x):  4+919=1143\n","87\n","wrong  : 4+919=1143\n","correct: 4+919=923\n","outputs(x):  4+456=990\n","259\n","wrong  : 4+456=990\n","correct: 4+456=460\n","outputs(x):  352+8=450\n","756\n","wrong  : 352+8=450\n","correct: 352+8=360\n","outputs(x):  7+520=1217\n","49\n","wrong  : 7+520=1217\n","correct: 7+520=527\n","outputs(x):  49+27=566\n","515\n","wrong  : 49+27=566\n","correct: 49+27=76\n","outputs(x):  45+52=867\n","801\n","wrong  : 45+52=867\n","correct: 45+52=97\n","outputs(x):  4+294=1168\n","80\n","wrong  : 4+294=1168\n","correct: 4+294=298\n","outputs(x):  2+817=1259\n","69\n","wrong  : 2+817=1259\n","correct: 2+817=819\n","outputs(x):  3+323=766\n","161\n","wrong  : 3+323=766\n","correct: 3+323=326\n","outputs(x):  642+9=641\n","824\n","wrong  : 642+9=641\n","correct: 642+9=651\n","outputs(x):  789+4=893\n","372\n","wrong  : 789+4=893\n","correct: 789+4=793\n","outputs(x):  9+550=739\n","794\n","wrong  : 9+550=739\n","correct: 9+550=559\n","outputs(x):  4+712=996\n","79+\n","wrong  : 4+712=996\n","correct: 4+712=716\n","outputs(x):  36+16=59\n","120+\n","wrong  : 36+16=59\n","correct: 36+16=52\n","outputs(x):  46+27=95\n","199+\n","wrong  : 46+27=95\n","correct: 46+27=73\n","outputs(x):  2+899=1081\n","76\n","wrong  : 2+899=1081\n","correct: 2+899=901\n","outputs(x):  6+989=1495\n","26\n","wrong  : 6+989=1495\n","correct: 6+989=995\n","outputs(x):  26+37=62\n","413+\n","wrong  : 26+37=62\n","correct: 26+37=63\n","outputs(x):  6+631=957\n","701\n","wrong  : 6+631=957\n","correct: 6+631=637\n","outputs(x):  4+644=1598\n","47\n","wrong  : 4+644=1598\n","correct: 4+644=648\n","outputs(x):  55+41=796\n","336\n","wrong  : 55+41=796\n","correct: 55+41=96\n","outputs(x):  9+162=1131\n","15\n","wrong  : 9+162=1131\n","correct: 9+162=171\n","outputs(x):  7+250=777\n","423\n","wrong  : 7+250=777\n","correct: 7+250=257\n"," 82% 66/80 [00:02<00:00, 21.52it/s]outputs(x):  412+89=401\n","627\n","wrong  : 412+89=401\n","correct: 412+89=501\n","outputs(x):  83+729=1512\n","36\n","wrong  : 83+729=1512\n","correct: 83+729=812\n","outputs(x):  50+739=1289\n","62\n","wrong  : 50+739=1289\n","correct: 50+739=789\n","outputs(x):  83+718=1601\n","19\n","wrong  : 83+718=1601\n","correct: 83+718=801\n","outputs(x):  51+116=967\n","552\n","wrong  : 51+116=967\n","correct: 51+116=167\n","outputs(x):  91+592=1083\n","20\n","wrong  : 91+592=1083\n","correct: 91+592=683\n","outputs(x):  21+176=1097\n","59\n","wrong  : 21+176=1097\n","correct: 21+176=197\n","outputs(x):  193+24=227\n","56+\n","wrong  : 193+24=227\n","correct: 193+24=217\n","outputs(x):  37+424=1261\n","82\n","wrong  : 37+424=1261\n","correct: 37+424=461\n","outputs(x):  54+219=1073\n","49\n","wrong  : 54+219=1073\n","correct: 54+219=273\n","outputs(x):  72+667=1539\n","21\n","wrong  : 72+667=1539\n","correct: 72+667=739\n","outputs(x):  79+563=1442\n","83\n","wrong  : 79+563=1442\n","correct: 79+563=642\n","outputs(x):  18+194=1112\n","45\n","wrong  : 18+194=1112\n","correct: 18+194=212\n","outputs(x):  605+28=623\n","438\n","wrong  : 605+28=623\n","correct: 605+28=633\n","outputs(x):  171+58=239\n","113\n","wrong  : 171+58=239\n","correct: 171+58=229\n","outputs(x):  502+38=530\n","794\n","wrong  : 502+38=530\n","correct: 502+38=540\n","outputs(x):  15+480=895\n","946\n","wrong  : 15+480=895\n","correct: 15+480=495\n","outputs(x):  64+228=392\n","768\n","wrong  : 64+228=392\n","correct: 64+228=292\n","outputs(x):  63+660=1323\n","24\n","wrong  : 63+660=1323\n","correct: 63+660=723\n","outputs(x):  99+453=1352\n","79\n","wrong  : 99+453=1352\n","correct: 99+453=552\n","outputs(x):  90+365=1255\n","2+\n","wrong  : 90+365=1255\n","correct: 90+365=455\n","outputs(x):  84+121=1005\n","11\n","wrong  : 84+121=1005\n","correct: 84+121=205\n","outputs(x):  19+193=312\n","664\n","wrong  : 19+193=312\n","correct: 19+193=212\n","outputs(x):  107+44=141\n","790\n","wrong  : 107+44=141\n","correct: 107+44=151\n","outputs(x):  212+58=260\n","327\n","wrong  : 212+58=260\n","correct: 212+58=270\n","outputs(x):  464+45=519\n","60+\n","wrong  : 464+45=519\n","correct: 464+45=509\n","outputs(x):  17+768=1185\n","61\n","wrong  : 17+768=1185\n","correct: 17+768=785\n","outputs(x):  20+934=1654\n","50\n","wrong  : 20+934=1654\n","correct: 20+934=954\n","outputs(x):  12+682=1294\n","65\n","wrong  : 12+682=1294\n","correct: 12+682=694\n","outputs(x):  53+548=1201\n","65\n","wrong  : 53+548=1201\n","correct: 53+548=601\n","outputs(x):  80+789=1269\n","58\n","wrong  : 80+789=1269\n","correct: 80+789=869\n","outputs(x):  92+285=1077\n","20\n","wrong  : 92+285=1077\n","correct: 92+285=377\n","outputs(x):  748+91=849\n","27+\n","wrong  : 748+91=849\n","correct: 748+91=839\n","outputs(x):  75+361=736\n","975\n","wrong  : 75+361=736\n","correct: 75+361=436\n","outputs(x):  518+27=535\n","251\n","wrong  : 518+27=535\n","correct: 518+27=545\n","outputs(x):  72+907=1879\n","50\n","wrong  : 72+907=1879\n","correct: 72+907=979\n","outputs(x):  40+994=1434\n","31\n","wrong  : 40+994=1434\n","correct: 40+994=1034\n","outputs(x):  89+817=1406\n","62\n","wrong  : 89+817=1406\n","correct: 89+817=906\n","outputs(x):  463+66=539\n","210\n","wrong  : 463+66=539\n","correct: 463+66=529\n","outputs(x):  82+336=618\n","194\n","wrong  : 82+336=618\n","correct: 82+336=418\n","outputs(x):  17+717=834\n","589\n","wrong  : 17+717=834\n","correct: 17+717=734\n","outputs(x):  14+495=1009\n","79\n","wrong  : 14+495=1009\n","correct: 14+495=509\n","outputs(x):  85+950=1735\n","79\n","wrong  : 85+950=1735\n","correct: 85+950=1035\n","outputs(x):  79+175=654\n","559\n","wrong  : 79+175=654\n","correct: 79+175=254\n","outputs(x):  604+97=691\n","517\n","wrong  : 604+97=691\n","correct: 604+97=701\n","outputs(x):  14+611=825\n","570\n","wrong  : 14+611=825\n","correct: 14+611=625\n","outputs(x):  20+661=981\n","895\n","wrong  : 20+661=981\n","correct: 20+661=681\n","outputs(x):  59+706=1565\n","21\n","wrong  : 59+706=1565\n","correct: 59+706=765\n","outputs(x):  895+72=977\n","171\n","wrong  : 895+72=977\n","correct: 895+72=967\n","outputs(x):  35+791=1426\n","28\n","wrong  : 35+791=1426\n","correct: 35+791=826\n","outputs(x):  437+18=45\n","425+\n","wrong  : 437+18=45\n","correct: 437+18=455\n","outputs(x):  649+61=700\n","605\n","wrong  : 649+61=700\n","correct: 649+61=710\n","outputs(x):  49+570=819\n","255\n","wrong  : 49+570=819\n","correct: 49+570=619\n","outputs(x):  74+226=500\n","537\n","wrong  : 74+226=500\n","correct: 74+226=300\n","outputs(x):  78+979=1657\n","17\n","wrong  : 78+979=1657\n","correct: 78+979=1057\n","outputs(x):  901+26=937\n","746\n","wrong  : 901+26=937\n","correct: 901+26=927\n","outputs(x):  14+379=1093\n","77\n","wrong  : 14+379=1093\n","correct: 14+379=393\n","outputs(x):  42+804=946\n","100\n","wrong  : 42+804=946\n","correct: 42+804=846\n","outputs(x):  320+89=419\n","176\n","wrong  : 320+89=419\n","correct: 320+89=409\n","outputs(x):  28+756=1284\n","20\n","wrong  : 28+756=1284\n","correct: 28+756=784\n","outputs(x):  308+12=310\n","533\n","wrong  : 308+12=310\n","correct: 308+12=320\n","outputs(x):  838+32=860\n","549\n","wrong  : 838+32=860\n","correct: 838+32=870\n","outputs(x):  64+809=1573\n","21\n","wrong  : 64+809=1573\n","correct: 64+809=873\n","outputs(x):  16+544=1260\n","78\n","wrong  : 16+544=1260\n","correct: 16+544=560\n","outputs(x):  34+951=1685\n","23\n","wrong  : 34+951=1685\n","correct: 34+951=985\n","outputs(x):  44+853=1797\n","10\n","wrong  : 44+853=1797\n","correct: 44+853=897\n","outputs(x):  31+750=1781\n","95\n","wrong  : 31+750=1781\n","correct: 31+750=781\n","outputs(x):  52+501=953\n","843\n","wrong  : 52+501=953\n","correct: 52+501=553\n","outputs(x):  947+21=978\n","633\n","wrong  : 947+21=978\n","correct: 947+21=968\n","outputs(x):  77+649=926\n","856\n","wrong  : 77+649=926\n","correct: 77+649=726\n","outputs(x):  72+650=1522\n","27\n","wrong  : 72+650=1522\n","correct: 72+650=722\n","outputs(x):  23+396=1119\n","90\n","wrong  : 23+396=1119\n","correct: 23+396=419\n","outputs(x):  804+49=843\n","237\n","wrong  : 804+49=843\n","correct: 804+49=853\n","outputs(x):  57+159=916\n","925\n","wrong  : 57+159=916\n","correct: 57+159=216\n","outputs(x):  63+496=1359\n","17\n","wrong  : 63+496=1359\n","correct: 63+496=559\n","outputs(x):  191+94=295\n","80+\n","wrong  : 191+94=295\n","correct: 191+94=285\n","outputs(x):  26+646=1572\n","53\n","wrong  : 26+646=1572\n","correct: 26+646=672\n","outputs(x):  56+262=518\n","755\n","wrong  : 56+262=518\n","correct: 56+262=318\n","outputs(x):  22+821=1143\n","68\n","wrong  : 22+821=1143\n","correct: 22+821=843\n","outputs(x):  65+996=1961\n","74\n","wrong  : 65+996=1961\n","correct: 65+996=1061\n","outputs(x):  97+445=1242\n","1+\n","wrong  : 97+445=1242\n","correct: 97+445=542\n","outputs(x):  90+534=1424\n","73\n","wrong  : 90+534=1424\n","correct: 90+534=624\n","outputs(x):  77+615=1592\n","34\n","wrong  : 77+615=1592\n","correct: 77+615=692\n","outputs(x):  40+184=1024\n","79\n","wrong  : 40+184=1024\n","correct: 40+184=224\n","outputs(x):  17+314=731\n","392\n","wrong  : 17+314=731\n","correct: 17+314=331\n","outputs(x):  179+12=291\n","379\n","wrong  : 179+12=291\n","correct: 179+12=191\n","outputs(x):  56+422=1278\n","57\n","wrong  : 56+422=1278\n","correct: 56+422=478\n","outputs(x):  49+906=1855\n","13\n","wrong  : 49+906=1855\n","correct: 49+906=955\n","outputs(x):  74+168=1042\n","22\n","wrong  : 74+168=1042\n","correct: 74+168=242\n","outputs(x):  39+797=1636\n","76\n","wrong  : 39+797=1636\n","correct: 39+797=836\n","outputs(x):  55+668=1723\n","56\n","wrong  : 55+668=1723\n","correct: 55+668=723\n","outputs(x):  42+895=1237\n","96\n","wrong  : 42+895=1237\n","correct: 42+895=937\n","outputs(x):  81+740=1521\n","84\n","wrong  : 81+740=1521\n","correct: 81+740=821\n","outputs(x):  48+685=1033\n","65\n","wrong  : 48+685=1033\n","correct: 48+685=733\n","outputs(x):  89+401=1090\n","15\n","wrong  : 89+401=1090\n","correct: 89+401=490\n","outputs(x):  28+904=1732\n","53\n","wrong  : 28+904=1732\n","correct: 28+904=932\n","outputs(x):  376+13=399\n","553\n","wrong  : 376+13=399\n","correct: 376+13=389\n","outputs(x):  87+436=823\n","40+\n","wrong  : 87+436=823\n","correct: 87+436=523\n","outputs(x):  87+639=1026\n","47\n","wrong  : 87+639=1026\n","correct: 87+639=726\n","outputs(x):  94+856=1150\n","96\n","wrong  : 94+856=1150\n","correct: 94+856=950\n","outputs(x):  18+544=1162\n","28\n","wrong  : 18+544=1162\n","correct: 18+544=562\n","outputs(x):  93+929=1822\n","51\n","wrong  : 93+929=1822\n","correct: 93+929=1022\n","outputs(x):  48+571=919\n","77+\n","wrong  : 48+571=919\n","correct: 48+571=619\n","outputs(x):  20+830=1750\n","54\n","wrong  : 20+830=1750\n","correct: 20+830=850\n","outputs(x):  11+355=866\n","25+\n","wrong  : 11+355=866\n","correct: 11+355=366\n","outputs(x):  96+262=1058\n","43\n","wrong  : 96+262=1058\n","correct: 96+262=358\n","outputs(x):  45+587=1232\n","71\n","wrong  : 45+587=1232\n","correct: 45+587=632\n","outputs(x):  38+968=1106\n","19\n","wrong  : 38+968=1106\n","correct: 38+968=1006\n","outputs(x):  273+20=393\n","40+\n","wrong  : 273+20=393\n","correct: 273+20=293\n","outputs(x):  15+146=761\n","544\n","wrong  : 15+146=761\n","correct: 15+146=161\n","outputs(x):  29+726=1455\n","36\n","wrong  : 29+726=1455\n","correct: 29+726=755\n","outputs(x):  36+674=1510\n","64\n","wrong  : 36+674=1510\n","correct: 36+674=710\n","outputs(x):  34+713=1247\n","62\n","wrong  : 34+713=1247\n","correct: 34+713=747\n","outputs(x):  84+483=1467\n","35\n","wrong  : 84+483=1467\n","correct: 84+483=567\n","outputs(x):  87+624=1511\n","51\n","wrong  : 87+624=1511\n","correct: 87+624=711\n","outputs(x):  52+675=1027\n","97\n","wrong  : 52+675=1027\n","correct: 52+675=727\n","outputs(x):  48+214=562\n","869\n","wrong  : 48+214=562\n","correct: 48+214=262\n","outputs(x):  36+921=1057\n","92\n","wrong  : 36+921=1057\n","correct: 36+921=957\n","outputs(x):  359+52=401\n","962\n","wrong  : 359+52=401\n","correct: 359+52=411\n","outputs(x):  84+397=1081\n","76\n","wrong  : 84+397=1081\n","correct: 84+397=481\n","outputs(x):  39+986=1325\n","21\n","wrong  : 39+986=1325\n","correct: 39+986=1025\n","outputs(x):  34+828=1362\n","50\n","wrong  : 34+828=1362\n","correct: 34+828=862\n","outputs(x):  16+542=1258\n","67\n","wrong  : 16+542=1258\n","correct: 16+542=558\n","outputs(x):  727+81=708\n","22+\n","wrong  : 727+81=708\n","correct: 727+81=808\n","outputs(x):  579+18=697\n","264\n","wrong  : 579+18=697\n","correct: 579+18=597\n","outputs(x):  208+72=270\n","226\n","wrong  : 208+72=270\n","correct: 208+72=280\n","outputs(x):  11+439=850\n","794\n","wrong  : 11+439=850\n","correct: 11+439=450\n","outputs(x):  80+558=1538\n","29\n","wrong  : 80+558=1538\n","correct: 80+558=638\n","outputs(x):  29+889=1318\n","15\n","wrong  : 29+889=1318\n","correct: 29+889=918\n","outputs(x):  68+123=491\n","757\n","wrong  : 68+123=491\n","correct: 68+123=191\n","outputs(x):  19+579=998\n","54+\n","wrong  : 19+579=998\n","correct: 19+579=598\n","outputs(x):  517+18=525\n","374\n","wrong  : 517+18=525\n","correct: 517+18=535\n","outputs(x):  88+438=1326\n","39\n","wrong  : 88+438=1326\n","correct: 88+438=526\n","outputs(x):  64+165=1129\n","48\n","wrong  : 64+165=1129\n","correct: 64+165=229\n","outputs(x):  21+562=1383\n","33\n","wrong  : 21+562=1383\n","correct: 21+562=583\n","outputs(x):  93+905=1898\n","94\n","wrong  : 93+905=1898\n","correct: 93+905=998\n","outputs(x):  81+212=893\n","101\n","wrong  : 81+212=893\n","correct: 81+212=293\n","outputs(x):  21+838=1059\n","86\n","wrong  : 21+838=1059\n","correct: 21+838=859\n","outputs(x):  63+322=885\n","964\n","wrong  : 63+322=885\n","correct: 63+322=385\n","outputs(x):  708+99=797\n","700\n","wrong  : 708+99=797\n","correct: 708+99=807\n","outputs(x):  94+701=1295\n","7+\n","wrong  : 94+701=1295\n","correct: 94+701=795\n","outputs(x):  61+487=1348\n","41\n","wrong  : 61+487=1348\n","correct: 61+487=548\n","outputs(x):  56+749=1705\n","28\n","wrong  : 56+749=1705\n","correct: 56+749=805\n","outputs(x):  83+676=1059\n","94\n","wrong  : 83+676=1059\n","correct: 83+676=759\n","outputs(x):  829+30=869\n","575\n","wrong  : 829+30=869\n","correct: 829+30=859\n","outputs(x):  258+31=299\n","58+\n","wrong  : 258+31=299\n","correct: 258+31=289\n","outputs(x):  58+729=1487\n","77\n","wrong  : 58+729=1487\n","correct: 58+729=787\n","outputs(x):  992+52=1054\n","18\n","wrong  : 992+52=1054\n","correct: 992+52=1044\n","outputs(x):  80+165=1145\n","26\n","wrong  : 80+165=1145\n","correct: 80+165=245\n","outputs(x):  654+26=670\n","209\n","wrong  : 654+26=670\n","correct: 654+26=680\n","outputs(x):  41+733=1074\n","45\n","wrong  : 41+733=1074\n","correct: 41+733=774\n","outputs(x):  80+125=805\n","966\n","wrong  : 80+125=805\n","correct: 80+125=205\n","outputs(x):  21+401=822\n","52+\n","wrong  : 21+401=822\n","correct: 21+401=422\n","outputs(x):  21+155=1076\n","72\n","wrong  : 21+155=1076\n","correct: 21+155=176\n","outputs(x):  17+647=1264\n","83\n","wrong  : 17+647=1264\n","correct: 17+647=664\n","outputs(x):  27+363=890\n","0+8\n","wrong  : 27+363=890\n","correct: 27+363=390\n","outputs(x):  76+672=1448\n","50\n","wrong  : 76+672=1448\n","correct: 76+672=748\n","outputs(x):  157+92=259\n","578\n","wrong  : 157+92=259\n","correct: 157+92=249\n","outputs(x):  16+271=887\n","524\n","wrong  : 16+271=887\n","correct: 16+271=287\n","outputs(x):  76+773=1049\n","13\n","wrong  : 76+773=1049\n","correct: 76+773=849\n","outputs(x):  99+705=1604\n","52\n","wrong  : 99+705=1604\n","correct: 99+705=804\n","outputs(x):  97+686=1383\n","68\n","wrong  : 97+686=1383\n","correct: 97+686=783\n","outputs(x):  70+582=1252\n","89\n","wrong  : 70+582=1252\n","correct: 70+582=652\n","outputs(x):  38+991=1829\n","74\n","wrong  : 38+991=1829\n","correct: 38+991=1029\n","outputs(x):  40+492=632\n","52+\n","wrong  : 40+492=632\n","correct: 40+492=532\n","outputs(x):  507+93=590\n","987\n","wrong  : 507+93=590\n","correct: 507+93=600\n","outputs(x):  97+269=1266\n","16\n","wrong  : 97+269=1266\n","correct: 97+269=366\n","outputs(x):  18+207=325\n","129\n","wrong  : 18+207=325\n","correct: 18+207=225\n","outputs(x):  62+250=412\n","41+\n","wrong  : 62+250=412\n","correct: 62+250=312\n","outputs(x):  75+625=1600\n","59\n","wrong  : 75+625=1600\n","correct: 75+625=700\n","outputs(x):  52+572=1324\n","78\n","wrong  : 52+572=1324\n","correct: 52+572=624\n","outputs(x):  34+446=1180\n","10\n","wrong  : 34+446=1180\n","correct: 34+446=480\n","outputs(x):  17+408=725\n","37+\n","wrong  : 17+408=725\n","correct: 17+408=425\n","outputs(x):  979+18=1007\n","92\n","wrong  : 979+18=1007\n","correct: 979+18=997\n","outputs(x):  55+458=1013\n","98\n","wrong  : 55+458=1013\n","correct: 55+458=513\n","outputs(x):  62+941=1603\n","44\n","wrong  : 62+941=1603\n","correct: 62+941=1003\n","outputs(x):  469+21=480\n","186\n","wrong  : 469+21=480\n","correct: 469+21=490\n","outputs(x):  941+38=989\n","362\n","wrong  : 941+38=989\n","correct: 941+38=979\n","outputs(x):  42+699=1141\n","49\n","wrong  : 42+699=1141\n","correct: 42+699=741\n","outputs(x):  11+982=1493\n","50\n","wrong  : 11+982=1493\n","correct: 11+982=993\n","outputs(x):  92+561=1453\n","85\n","wrong  : 92+561=1453\n","correct: 92+561=653\n","outputs(x):  53+674=1327\n","78\n","wrong  : 53+674=1327\n","correct: 53+674=727\n","outputs(x):  12+476=1288\n","84\n","wrong  : 12+476=1288\n","correct: 12+476=488\n","outputs(x):  110+14=125\n","241\n","wrong  : 110+14=125\n","correct: 110+14=124\n","outputs(x):  52+641=1393\n","96\n","wrong  : 52+641=1393\n","correct: 52+641=693\n","outputs(x):  19+243=762\n","259\n","wrong  : 19+243=762\n","correct: 19+243=262\n","outputs(x):  261+58=329\n","118\n","wrong  : 261+58=329\n","correct: 261+58=319\n","outputs(x):  72+399=1071\n","79\n","wrong  : 72+399=1071\n","correct: 72+399=471\n","outputs(x):  19+436=1255\n","55\n","wrong  : 19+436=1255\n","correct: 19+436=455\n","outputs(x):  53+366=829\n","915\n","wrong  : 53+366=829\n","correct: 53+366=419\n","outputs(x):  82+924=1206\n","26\n","wrong  : 82+924=1206\n","correct: 82+924=1006\n","outputs(x):  703+87=780\n","44+\n","wrong  : 703+87=780\n","correct: 703+87=790\n","outputs(x):  91+191=982\n","362\n","wrong  : 91+191=982\n","correct: 91+191=282\n","outputs(x):  38+605=843\n","4+6\n","wrong  : 38+605=843\n","correct: 38+605=643\n","outputs(x):  93+740=1533\n","27\n","wrong  : 93+740=1533\n","correct: 93+740=833\n","outputs(x):  50+309=1159\n","61\n","wrong  : 50+309=1159\n","correct: 50+309=359\n","outputs(x):  478+19=597\n","271\n","wrong  : 478+19=597\n","correct: 478+19=497\n","outputs(x):  409+98=497\n","470\n","wrong  : 409+98=497\n","correct: 409+98=507\n","outputs(x):  122+46=178\n","6+3\n","wrong  : 122+46=178\n","correct: 122+46=168\n","outputs(x):  27+327=1054\n","83\n","wrong  : 27+327=1054\n","correct: 27+327=354\n","outputs(x):  77+878=1855\n","38\n","wrong  : 77+878=1855\n","correct: 77+878=955\n","outputs(x):  344+70=514\n","400\n","wrong  : 344+70=514\n","correct: 344+70=414\n","outputs(x):  23+270=993\n","477\n","wrong  : 23+270=993\n","correct: 23+270=293\n","outputs(x):  85+753=938\n","283\n","wrong  : 85+753=938\n","correct: 85+753=838\n","outputs(x):  43+110=453\n","41+\n","wrong  : 43+110=453\n","correct: 43+110=153\n","outputs(x):  79+755=1734\n","47\n","wrong  : 79+755=1734\n","correct: 79+755=834\n","outputs(x):  17+268=385\n","585\n","wrong  : 17+268=385\n","correct: 17+268=285\n","outputs(x):  87+130=717\n","132\n","wrong  : 87+130=717\n","correct: 87+130=217\n","outputs(x):  31+367=598\n","852\n","wrong  : 31+367=598\n","correct: 31+367=398\n","outputs(x):  27+174=1101\n","67\n","wrong  : 27+174=1101\n","correct: 27+174=201\n","outputs(x):  59+115=1074\n","3+\n","wrong  : 59+115=1074\n","correct: 59+115=174\n","outputs(x):  74+127=301\n","286\n","wrong  : 74+127=301\n","correct: 74+127=201\n","outputs(x):  469+59=538\n","659\n","wrong  : 469+59=538\n","correct: 469+59=528\n","outputs(x):  86+754=1240\n","97\n","wrong  : 86+754=1240\n","correct: 86+754=840\n"," 86% 69/80 [00:02<00:00, 18.64it/s]outputs(x):  75+618=1193\n","53\n","wrong  : 75+618=1193\n","correct: 75+618=693\n","outputs(x):  84+563=1047\n","13\n","wrong  : 84+563=1047\n","correct: 84+563=647\n","outputs(x):  124+26=140\n","47+\n","wrong  : 124+26=140\n","correct: 124+26=150\n","outputs(x):  747+64=801\n","960\n","wrong  : 747+64=801\n","correct: 747+64=811\n","outputs(x):  546+14=550\n","153\n","wrong  : 546+14=550\n","correct: 546+14=560\n","outputs(x):  57+145=602\n","763\n","wrong  : 57+145=602\n","correct: 57+145=202\n","outputs(x):  50+599=1049\n","29\n","wrong  : 50+599=1049\n","correct: 50+599=649\n","outputs(x):  450+50=400\n","90+\n","wrong  : 450+50=400\n","correct: 450+50=500\n","outputs(x):  60+258=518\n","621\n","wrong  : 60+258=518\n","correct: 60+258=318\n","outputs(x):  881+75=966\n","800\n","wrong  : 881+75=966\n","correct: 881+75=956\n","outputs(x):  45+312=957\n","722\n","wrong  : 45+312=957\n","correct: 45+312=357\n","outputs(x):  88+695=1383\n","30\n","wrong  : 88+695=1383\n","correct: 88+695=783\n","outputs(x):  11+149=560\n","784\n","wrong  : 11+149=560\n","correct: 11+149=160\n","outputs(x):  51+951=1402\n","88\n","wrong  : 51+951=1402\n","correct: 51+951=1002\n","outputs(x):  28+515=1343\n","55\n","wrong  : 28+515=1343\n","correct: 28+515=543\n","outputs(x):  17+812=1229\n","27\n","wrong  : 17+812=1229\n","correct: 17+812=829\n","outputs(x):  21+462=783\n","606\n","wrong  : 21+462=783\n","correct: 21+462=483\n","outputs(x):  608+82=680\n","286\n","wrong  : 608+82=680\n","correct: 608+82=690\n","outputs(x):  15+109=1024\n","38\n","wrong  : 15+109=1024\n","correct: 15+109=124\n","outputs(x):  41+144=155\n","266\n","wrong  : 41+144=155\n","correct: 41+144=185\n","outputs(x):  64+422=686\n","279\n","wrong  : 64+422=686\n","correct: 64+422=486\n","outputs(x):  15+667=882\n","173\n","wrong  : 15+667=882\n","correct: 15+667=682\n","outputs(x):  898+22=910\n","52+\n","wrong  : 898+22=910\n","correct: 898+22=920\n","outputs(x):  45+695=1540\n","42\n","wrong  : 45+695=1540\n","correct: 45+695=740\n","outputs(x):  98+633=1031\n","99\n","wrong  : 98+633=1031\n","correct: 98+633=731\n","outputs(x):  95+843=1338\n","39\n","wrong  : 95+843=1338\n","correct: 95+843=938\n","outputs(x):  76+139=1015\n","17\n","wrong  : 76+139=1015\n","correct: 76+139=215\n","outputs(x):  40+733=1673\n","83\n","wrong  : 40+733=1673\n","correct: 40+733=773\n","outputs(x):  17+541=958\n","683\n","wrong  : 17+541=958\n","correct: 17+541=558\n","outputs(x):  71+760=1731\n","38\n","wrong  : 71+760=1731\n","correct: 71+760=831\n","outputs(x):  91+958=1949\n","74\n","wrong  : 91+958=1949\n","correct: 91+958=1049\n","outputs(x):  60+446=1206\n","57\n","wrong  : 60+446=1206\n","correct: 60+446=506\n","outputs(x):  77+301=478\n","82+\n","wrong  : 77+301=478\n","correct: 77+301=378\n","outputs(x):  81+525=1206\n","29\n","wrong  : 81+525=1206\n","correct: 81+525=606\n","outputs(x):  22+457=879\n","139\n","wrong  : 22+457=879\n","correct: 22+457=479\n","outputs(x):  60+992=1752\n","98\n","wrong  : 60+992=1752\n","correct: 60+992=1052\n","outputs(x):  38+702=1540\n","98\n","wrong  : 38+702=1540\n","correct: 38+702=740\n","outputs(x):  85+412=1397\n","79\n","wrong  : 85+412=1397\n","correct: 85+412=497\n","outputs(x):  79+233=1112\n","54\n","wrong  : 79+233=1112\n","correct: 79+233=312\n","outputs(x):  84+578=1262\n","75\n","wrong  : 84+578=1262\n","correct: 84+578=662\n","outputs(x):  18+461=1379\n","39\n","wrong  : 18+461=1379\n","correct: 18+461=479\n","outputs(x):  962+56=1028\n","46\n","wrong  : 962+56=1028\n","correct: 962+56=1018\n","outputs(x):  93+753=1046\n","58\n","wrong  : 93+753=1046\n","correct: 93+753=846\n","outputs(x):  88+830=1418\n","25\n","wrong  : 88+830=1418\n","correct: 88+830=918\n","outputs(x):  17+611=828\n","56+\n","wrong  : 17+611=828\n","correct: 17+611=628\n","outputs(x):  36+997=1533\n","51\n","wrong  : 36+997=1533\n","correct: 36+997=1033\n","outputs(x):  93+505=1098\n","74\n","wrong  : 93+505=1098\n","correct: 93+505=598\n","outputs(x):  87+593=1580\n","99\n","wrong  : 87+593=1580\n","correct: 87+593=680\n","outputs(x):  55+339=894\n","734\n","wrong  : 55+339=894\n","correct: 55+339=394\n","outputs(x):  84+800=1784\n","70\n","wrong  : 84+800=1784\n","correct: 84+800=884\n","outputs(x):  89+414=703\n","997\n","wrong  : 89+414=703\n","correct: 89+414=503\n","outputs(x):  15+422=737\n","25+\n","wrong  : 15+422=737\n","correct: 15+422=437\n","outputs(x):  89+970=1659\n","18\n","wrong  : 89+970=1659\n","correct: 89+970=1059\n","outputs(x):  44+156=600\n","204\n","wrong  : 44+156=600\n","correct: 44+156=200\n","outputs(x):  64+403=967\n","51+\n","wrong  : 64+403=967\n","correct: 64+403=467\n","outputs(x):  96+888=1584\n","13\n","wrong  : 96+888=1584\n","correct: 96+888=984\n","outputs(x):  15+886=1401\n","63\n","wrong  : 15+886=1401\n","correct: 15+886=901\n","outputs(x):  57+821=1478\n","67\n","wrong  : 57+821=1478\n","correct: 57+821=878\n","outputs(x):  41+343=884\n","318\n","wrong  : 41+343=884\n","correct: 41+343=384\n","outputs(x):  78+545=1523\n","34\n","wrong  : 78+545=1523\n","correct: 78+545=623\n","outputs(x):  32+549=781\n","956\n","wrong  : 32+549=781\n","correct: 32+549=581\n","outputs(x):  63+727=1690\n","61\n","wrong  : 63+727=1690\n","correct: 63+727=790\n","outputs(x):  48+303=451\n","269\n","wrong  : 48+303=451\n","correct: 48+303=351\n","outputs(x):  94+902=1096\n","97\n","wrong  : 94+902=1096\n","correct: 94+902=996\n","outputs(x):  28+755=1583\n","54\n","wrong  : 28+755=1583\n","correct: 28+755=783\n","outputs(x):  98+189=787\n","142\n","wrong  : 98+189=787\n","correct: 98+189=287\n","outputs(x):  31+904=1435\n","93\n","wrong  : 31+904=1435\n","correct: 31+904=935\n","outputs(x):  90+647=1237\n","27\n","wrong  : 90+647=1237\n","correct: 90+647=737\n","outputs(x):  619+26=635\n","284\n","wrong  : 619+26=635\n","correct: 619+26=645\n","outputs(x):  99+798=1497\n","37\n","wrong  : 99+798=1497\n","correct: 99+798=897\n","outputs(x):  88+832=1720\n","82\n","wrong  : 88+832=1720\n","correct: 88+832=920\n","outputs(x):  83+376=1159\n","16\n","wrong  : 83+376=1159\n","correct: 83+376=459\n","outputs(x):  74+590=1264\n","85\n","wrong  : 74+590=1264\n","correct: 74+590=664\n","outputs(x):  10+721=1531\n","44\n","wrong  : 10+721=1531\n","correct: 10+721=731\n","outputs(x):  73+527=1300\n","53\n","wrong  : 73+527=1300\n","correct: 73+527=600\n","outputs(x):  647+33=670\n","755\n","wrong  : 647+33=670\n","correct: 647+33=680\n","outputs(x):  45+597=742\n","79+\n","wrong  : 45+597=742\n","correct: 45+597=642\n","outputs(x):  592+28=610\n","259\n","wrong  : 592+28=610\n","correct: 592+28=620\n","outputs(x):  57+884=1641\n","32\n","wrong  : 57+884=1641\n","correct: 57+884=941\n","outputs(x):  69+722=1291\n","93\n","wrong  : 69+722=1291\n","correct: 69+722=791\n","outputs(x):  26+530=1356\n","50\n","wrong  : 26+530=1356\n","correct: 26+530=556\n","outputs(x):  55+819=1774\n","68\n","wrong  : 55+819=1774\n","correct: 55+819=874\n","outputs(x):  34+373=1207\n","40\n","wrong  : 34+373=1207\n","correct: 34+373=407\n","outputs(x):  44+981=1825\n","46\n","wrong  : 44+981=1825\n","correct: 44+981=1025\n","outputs(x):  992+68=1050\n","48\n","wrong  : 992+68=1050\n","correct: 992+68=1060\n","outputs(x):  52+908=1860\n","69\n","wrong  : 52+908=1860\n","correct: 52+908=960\n","outputs(x):  23+721=1344\n","57\n","wrong  : 23+721=1344\n","correct: 23+721=744\n","outputs(x):  11+292=1103\n","51\n","wrong  : 11+292=1103\n","correct: 11+292=303\n","outputs(x):  90+638=1328\n","95\n","wrong  : 90+638=1328\n","correct: 90+638=728\n","outputs(x):  78+338=1316\n","13\n","wrong  : 78+338=1316\n","correct: 78+338=416\n","outputs(x):  41+811=952\n","705\n","wrong  : 41+811=952\n","correct: 41+811=852\n","outputs(x):  70+921=1491\n","82\n","wrong  : 70+921=1491\n","correct: 70+921=991\n","outputs(x):  41+879=1320\n","42\n","wrong  : 41+879=1320\n","correct: 41+879=920\n","outputs(x):  18+597=1115\n","36\n","wrong  : 18+597=1115\n","correct: 18+597=615\n","outputs(x):  75+695=1170\n","72\n","wrong  : 75+695=1170\n","correct: 75+695=770\n","outputs(x):  15+895=1310\n","12\n","wrong  : 15+895=1310\n","correct: 15+895=910\n","outputs(x):  47+495=1242\n","80\n","wrong  : 47+495=1242\n","correct: 47+495=542\n","outputs(x):  529+83=602\n","52+\n","wrong  : 529+83=602\n","correct: 529+83=612\n","outputs(x):  96+364=1360\n","77\n","wrong  : 96+364=1360\n","correct: 96+364=460\n","outputs(x):  24+622=746\n","56+\n","wrong  : 24+622=746\n","correct: 24+622=646\n","outputs(x):  71+516=687\n","635\n","wrong  : 71+516=687\n","correct: 71+516=587\n","outputs(x):  93+863=1356\n","95\n","wrong  : 93+863=1356\n","correct: 93+863=956\n","outputs(x):  79+242=521\n","806\n","wrong  : 79+242=521\n","correct: 79+242=321\n","outputs(x):  76+953=1829\n","10\n","wrong  : 76+953=1829\n","correct: 76+953=1029\n","outputs(x):  19+945=1464\n","33\n","wrong  : 19+945=1464\n","correct: 19+945=964\n","outputs(x):  61+965=1226\n","56\n","wrong  : 61+965=1226\n","correct: 61+965=1026\n","outputs(x):  51+214=465\n","853\n","wrong  : 51+214=465\n","correct: 51+214=265\n","outputs(x):  98+884=1282\n","76\n","wrong  : 98+884=1282\n","correct: 98+884=982\n","outputs(x):  96+196=1192\n","99\n","wrong  : 96+196=1192\n","correct: 96+196=292\n","outputs(x):  89+549=1038\n","23\n","wrong  : 89+549=1038\n","correct: 89+549=638\n","outputs(x):  86+308=794\n","36+\n","wrong  : 86+308=794\n","correct: 86+308=394\n","outputs(x):  706+34=730\n","5+4\n","wrong  : 706+34=730\n","correct: 706+34=740\n","outputs(x):  27+458=1285\n","20\n","wrong  : 27+458=1285\n","correct: 27+458=485\n","outputs(x):  94+333=827\n","743\n","wrong  : 94+333=827\n","correct: 94+333=427\n","outputs(x):  73+145=528\n","431\n","wrong  : 73+145=528\n","correct: 73+145=218\n","outputs(x):  66+448=1414\n","33\n","wrong  : 66+448=1414\n","correct: 66+448=514\n","outputs(x):  56+883=1139\n","13\n","wrong  : 56+883=1139\n","correct: 56+883=939\n","outputs(x):  969+88=1067\n","88\n","wrong  : 969+88=1067\n","correct: 969+88=1057\n","outputs(x):  82+852=1634\n","42\n","wrong  : 82+852=1634\n","correct: 82+852=934\n","outputs(x):  67+378=1345\n","34\n","wrong  : 67+378=1345\n","correct: 67+378=445\n","outputs(x):  28+709=1537\n","57\n","wrong  : 28+709=1537\n","correct: 28+709=737\n","outputs(x):  89+521=1110\n","24\n","wrong  : 89+521=1110\n","correct: 89+521=610\n","outputs(x):  628+83=701\n","96+\n","wrong  : 628+83=701\n","correct: 628+83=711\n","outputs(x):  68+755=1423\n","44\n","wrong  : 68+755=1423\n","correct: 68+755=823\n","outputs(x):  66+576=1442\n","99\n","wrong  : 66+576=1442\n","correct: 66+576=642\n","outputs(x):  22+790=912\n","255\n","wrong  : 22+790=912\n","correct: 22+790=812\n","outputs(x):  32+990=1622\n","38\n","wrong  : 32+990=1622\n","correct: 32+990=1022\n","outputs(x):  46+509=955\n","712\n","wrong  : 46+509=955\n","correct: 46+509=555\n","outputs(x):  61+841=1202\n","88\n","wrong  : 61+841=1202\n","correct: 61+841=902\n","outputs(x):  32+848=1580\n","15\n","wrong  : 32+848=1580\n","correct: 32+848=880\n","outputs(x):  11+598=719\n","17+\n","wrong  : 11+598=719\n","correct: 11+598=609\n","outputs(x):  43+825=968\n","719\n","wrong  : 43+825=968\n","correct: 43+825=868\n","outputs(x):  12+743=1055\n","5+\n","wrong  : 12+743=1055\n","correct: 12+743=755\n","outputs(x):  82+551=1033\n","99\n","wrong  : 82+551=1033\n","correct: 82+551=633\n","outputs(x):  40+706=1246\n","58\n","wrong  : 40+706=1246\n","correct: 40+706=746\n","outputs(x):  49+820=969\n","340\n","wrong  : 49+820=969\n","correct: 49+820=869\n","outputs(x):  303+19=312\n","753\n","wrong  : 303+19=312\n","correct: 303+19=322\n","outputs(x):  41+246=787\n","318\n","wrong  : 41+246=787\n","correct: 41+246=287\n","outputs(x):  44+922=1666\n","42\n","wrong  : 44+922=1666\n","correct: 44+922=966\n","outputs(x):  809+47=846\n","617\n","wrong  : 809+47=846\n","correct: 809+47=856\n","outputs(x):  66+617=1083\n","88\n","wrong  : 66+617=1083\n","correct: 66+617=683\n","outputs(x):  56+969=1225\n","78\n","wrong  : 56+969=1225\n","correct: 56+969=1025\n","outputs(x):  66+370=1236\n","66\n","wrong  : 66+370=1236\n","correct: 66+370=436\n","outputs(x):  79+531=1010\n","34\n","wrong  : 79+531=1010\n","correct: 79+531=610\n","outputs(x):  996+15=1001\n","67\n","wrong  : 996+15=1001\n","correct: 996+15=1011\n","outputs(x):  55+203=658\n","353\n","wrong  : 55+203=658\n","correct: 55+203=258\n","outputs(x):  76+734=1610\n","22\n","wrong  : 76+734=1610\n","correct: 76+734=810\n","outputs(x):  32+672=1004\n","78\n","wrong  : 32+672=1004\n","correct: 32+672=704\n","outputs(x):  68+685=1253\n","82\n","wrong  : 68+685=1253\n","correct: 68+685=753\n"," 89% 71/80 [00:03<00:00, 17.44it/s]outputs(x):  536+54=580\n","641\n","wrong  : 536+54=580\n","correct: 536+54=590\n","outputs(x):  62+446=908\n","176\n","wrong  : 62+446=908\n","correct: 62+446=508\n","outputs(x):  45+261=1206\n","42\n","wrong  : 45+261=1206\n","correct: 45+261=306\n","outputs(x):  91+579=1270\n","62\n","wrong  : 91+579=1270\n","correct: 91+579=670\n","outputs(x):  90+965=1955\n","26\n","wrong  : 90+965=1955\n","correct: 90+965=1055\n","outputs(x):  65+326=491\n","356\n","wrong  : 65+326=491\n","correct: 65+326=391\n","outputs(x):  44+839=1183\n","54\n","wrong  : 44+839=1183\n","correct: 44+839=883\n","outputs(x):  97+499=1096\n","86\n","wrong  : 97+499=1096\n","correct: 97+499=596\n","outputs(x):  51+533=684\n","657\n","wrong  : 51+533=684\n","correct: 51+533=584\n","outputs(x):  803+25=838\n","341\n","wrong  : 803+25=838\n","correct: 803+25=828\n","outputs(x):  72+230=1102\n","37\n","wrong  : 72+230=1102\n","correct: 72+230=302\n","outputs(x):  77+125=1002\n","14\n","wrong  : 77+125=1002\n","correct: 77+125=202\n","outputs(x):  18+724=1442\n","66\n","wrong  : 18+724=1442\n","correct: 18+724=742\n","outputs(x):  61+978=1539\n","54\n","wrong  : 61+978=1539\n","correct: 61+978=1039\n","outputs(x):  89+874=1463\n","87\n","wrong  : 89+874=1463\n","correct: 89+874=963\n","outputs(x):  91+375=966\n","999\n","wrong  : 91+375=966\n","correct: 91+375=466\n","outputs(x):  13+213=726\n","579\n","wrong  : 13+213=726\n","correct: 13+213=226\n","outputs(x):  84+150=634\n","435\n","wrong  : 84+150=634\n","correct: 84+150=234\n","outputs(x):  386+33=429\n","9+1\n","wrong  : 386+33=429\n","correct: 386+33=419\n","outputs(x):  76+155=831\n","702\n","wrong  : 76+155=831\n","correct: 76+155=231\n","outputs(x):  13+929=1142\n","85\n","wrong  : 13+929=1142\n","correct: 13+929=942\n","outputs(x):  990+26=1026\n","39\n","wrong  : 990+26=1026\n","correct: 990+26=1016\n","outputs(x):  17+429=1246\n","98\n","wrong  : 17+429=1246\n","correct: 17+429=446\n","outputs(x):  84+799=1583\n","66\n","wrong  : 84+799=1583\n","correct: 84+799=883\n","outputs(x):  33+485=1118\n","38\n","wrong  : 33+485=1118\n","correct: 33+485=518\n","outputs(x):  273+34=207\n","511\n","wrong  : 273+34=207\n","correct: 273+34=307\n","outputs(x):  24+590=814\n","567\n","wrong  : 24+590=814\n","correct: 24+590=614\n","outputs(x):  47+577=1524\n","60\n","wrong  : 47+577=1524\n","correct: 47+577=624\n","outputs(x):  51+268=1019\n","22\n","wrong  : 51+268=1019\n","correct: 51+268=319\n","outputs(x):  92+840=1732\n","59\n","wrong  : 92+840=1732\n","correct: 92+840=932\n","outputs(x):  15+211=326\n","75+\n","wrong  : 15+211=326\n","correct: 15+211=226\n","outputs(x):  87+237=1124\n","34\n","wrong  : 87+237=1124\n","correct: 87+237=324\n","outputs(x):  68+705=1373\n","10\n","wrong  : 68+705=1373\n","correct: 68+705=773\n","outputs(x):  39+727=1666\n","82\n","wrong  : 39+727=1666\n","correct: 39+727=766\n","outputs(x):  89+115=404\n","659\n","wrong  : 89+115=404\n","correct: 89+115=204\n","outputs(x):  22+415=937\n","696\n","wrong  : 22+415=937\n","correct: 22+415=437\n","outputs(x):  50+481=1331\n","76\n","wrong  : 50+481=1331\n","correct: 50+481=531\n","outputs(x):  91+754=1545\n","61\n","wrong  : 91+754=1545\n","correct: 91+754=845\n","outputs(x):  80+533=913\n","354\n","wrong  : 80+533=913\n","correct: 80+533=613\n","outputs(x):  49+858=1707\n","33\n","wrong  : 49+858=1707\n","correct: 49+858=907\n","outputs(x):  28+802=1630\n","65\n","wrong  : 28+802=1630\n","correct: 28+802=830\n","outputs(x):  78+362=1340\n","70\n","wrong  : 78+362=1340\n","correct: 78+362=440\n","outputs(x):  77+140=317\n","122\n","wrong  : 77+140=317\n","correct: 77+140=217\n","outputs(x):  39+506=945\n","837\n","wrong  : 39+506=945\n","correct: 39+506=545\n","outputs(x):  58+472=830\n","549\n","wrong  : 58+472=830\n","correct: 58+472=530\n","outputs(x):  57+826=1283\n","58\n","wrong  : 57+826=1283\n","correct: 57+826=883\n","outputs(x):  48+115=263\n","922\n","wrong  : 48+115=263\n","correct: 48+115=163\n","outputs(x):  59+847=1206\n","84\n","wrong  : 59+847=1206\n","correct: 59+847=906\n","outputs(x):  75+513=988\n","62+\n","wrong  : 75+513=988\n","correct: 75+513=588\n","outputs(x):  50+123=673\n","148\n","wrong  : 50+123=673\n","correct: 50+123=173\n","outputs(x):  92+420=1112\n","84\n","wrong  : 92+420=1112\n","correct: 92+420=512\n","outputs(x):  39+387=1326\n","76\n","wrong  : 39+387=1326\n","correct: 39+387=426\n","outputs(x):  47+388=1135\n","76\n","wrong  : 47+388=1135\n","correct: 47+388=435\n","outputs(x):  22+749=1271\n","91\n","wrong  : 22+749=1271\n","correct: 22+749=771\n","outputs(x):  16+760=1576\n","11\n","wrong  : 16+760=1576\n","correct: 16+760=776\n","outputs(x):  25+901=1426\n","98\n","wrong  : 25+901=1426\n","correct: 25+901=926\n","outputs(x):  40+882=1222\n","89\n","wrong  : 40+882=1222\n","correct: 40+882=922\n","outputs(x):  918+69=977\n","43+\n","wrong  : 918+69=977\n","correct: 918+69=987\n","outputs(x):  48+360=508\n","784\n","wrong  : 48+360=508\n","correct: 48+360=408\n","outputs(x):  66+123=989\n","193\n","wrong  : 66+123=989\n","correct: 66+123=189\n","outputs(x):  35+120=1055\n","72\n","wrong  : 35+120=1055\n","correct: 35+120=155\n","outputs(x):  49+757=1306\n","69\n","wrong  : 49+757=1306\n","correct: 49+757=806\n","outputs(x):  16+587=1303\n","72\n","wrong  : 16+587=1303\n","correct: 16+587=603\n","outputs(x):  71+165=536\n","198\n","wrong  : 71+165=536\n","correct: 71+165=236\n","outputs(x):  17+127=744\n","876\n","wrong  : 17+127=744\n","correct: 17+127=144\n","outputs(x):  27+790=1017\n","17\n","wrong  : 27+790=1017\n","correct: 27+790=817\n","outputs(x):  913+69=972\n","4+1\n","wrong  : 913+69=972\n","correct: 913+69=982\n","outputs(x):  94+812=1506\n","12\n","wrong  : 94+812=1506\n","correct: 94+812=906\n","outputs(x):  48+277=1125\n","19\n","wrong  : 48+277=1125\n","correct: 48+277=325\n","outputs(x):  67+615=882\n","779\n","wrong  : 67+615=882\n","correct: 67+615=682\n","outputs(x):  30+554=684\n","957\n","wrong  : 30+554=684\n","correct: 30+554=584\n","outputs(x):  60+887=1547\n","58\n","wrong  : 60+887=1547\n","correct: 60+887=947\n","outputs(x):  53+930=1883\n","88\n","wrong  : 53+930=1883\n","correct: 53+930=983\n","outputs(x):  66+368=1334\n","87\n","wrong  : 66+368=1334\n","correct: 66+368=434\n","outputs(x):  30+327=957\n","4+2\n","wrong  : 30+327=957\n","correct: 30+327=357\n","outputs(x):  73+438=1111\n","97\n","wrong  : 73+438=1111\n","correct: 73+438=511\n","outputs(x):  100+16=126\n","786\n","wrong  : 100+16=126\n","correct: 100+16=116\n","outputs(x):  307+54=351\n","5+9\n","wrong  : 307+54=351\n","correct: 307+54=361\n","outputs(x):  19+505=1324\n","55\n","wrong  : 19+505=1324\n","correct: 19+505=524\n","outputs(x):  14+485=1199\n","81\n","wrong  : 14+485=1199\n","correct: 14+485=499\n","outputs(x):  916+28=934\n","448\n","wrong  : 916+28=934\n","correct: 916+28=944\n","outputs(x):  70+952=1322\n","48\n","wrong  : 70+952=1322\n","correct: 70+952=1022\n","outputs(x):  41+541=982\n","248\n","wrong  : 41+541=982\n","correct: 41+541=582\n","outputs(x):  12+738=1250\n","16\n","wrong  : 12+738=1250\n","correct: 12+738=750\n","outputs(x):  44+189=1033\n","67\n","wrong  : 44+189=1033\n","correct: 44+189=233\n","outputs(x):  66+745=1111\n","73\n","wrong  : 66+745=1111\n","correct: 66+745=811\n","outputs(x):  99+155=754\n","708\n","wrong  : 99+155=754\n","correct: 99+155=254\n","outputs(x):  454+26=470\n","15+\n","wrong  : 454+26=470\n","correct: 454+26=480\n","outputs(x):  863+27=880\n","64+\n","wrong  : 863+27=880\n","correct: 863+27=890\n","outputs(x):  29+282=611\n","505\n","wrong  : 29+282=611\n","correct: 29+282=311\n","outputs(x):  99+269=768\n","360\n","wrong  : 99+269=768\n","correct: 99+269=368\n","outputs(x):  681+58=749\n","65+\n","wrong  : 681+58=749\n","correct: 681+58=739\n","outputs(x):  48+153=1001\n","29\n","wrong  : 48+153=1001\n","correct: 48+153=201\n","outputs(x):  79+975=1354\n","85\n","wrong  : 79+975=1354\n","correct: 79+975=1054\n","outputs(x):  19+122=541\n","689\n","wrong  : 19+122=541\n","correct: 19+122=141\n","outputs(x):  17+726=943\n","46+\n","wrong  : 17+726=943\n","correct: 17+726=743\n","outputs(x):  671+28=799\n","91+\n","wrong  : 671+28=799\n","correct: 671+28=699\n","outputs(x):  709+43=742\n","47+\n","wrong  : 709+43=742\n","correct: 709+43=752\n","outputs(x):  56+772=1228\n","59\n","wrong  : 56+772=1228\n","correct: 56+772=828\n","outputs(x):  549+21=560\n","532\n","wrong  : 549+21=560\n","correct: 549+21=570\n","outputs(x):  55+553=1408\n","87\n","wrong  : 55+553=1408\n","correct: 55+553=608\n","outputs(x):  58+400=858\n","381\n","wrong  : 58+400=858\n","correct: 58+400=458\n","outputs(x):  22+160=782\n","386\n","wrong  : 22+160=782\n","correct: 22+160=182\n","outputs(x):  32+991=1323\n","57\n","wrong  : 32+991=1323\n","correct: 32+991=1023\n","outputs(x):  549+71=610\n","279\n","wrong  : 549+71=610\n","correct: 549+71=620\n","outputs(x):  70+741=1511\n","41\n","wrong  : 70+741=1511\n","correct: 70+741=811\n","outputs(x):  70+526=1096\n","84\n","wrong  : 70+526=1096\n","correct: 70+526=596\n","outputs(x):  10+286=1196\n","42\n","wrong  : 10+286=1196\n","correct: 10+286=296\n","outputs(x):  43+441=584\n","274\n","wrong  : 43+441=584\n","correct: 43+441=484\n","outputs(x):  42+725=1667\n","97\n","wrong  : 42+725=1667\n","correct: 42+725=767\n","outputs(x):  57+693=1650\n","38\n","wrong  : 57+693=1650\n","correct: 57+693=750\n","outputs(x):  32+818=950\n","434\n","wrong  : 32+818=950\n","correct: 32+818=850\n","outputs(x):  82+719=1501\n","54\n","wrong  : 82+719=1501\n","correct: 82+719=801\n","outputs(x):  115+57=162\n","334\n","wrong  : 115+57=162\n","correct: 115+57=172\n","outputs(x):  93+552=1445\n","62\n","wrong  : 93+552=1445\n","correct: 93+552=645\n","outputs(x):  716+19=725\n","271\n","wrong  : 716+19=725\n","correct: 716+19=735\n","outputs(x):  36+418=754\n","37+\n","wrong  : 36+418=754\n","correct: 36+418=454\n","outputs(x):  42+678=1320\n","43\n","wrong  : 42+678=1320\n","correct: 42+678=720\n","outputs(x):  57+758=1215\n","91\n","wrong  : 57+758=1215\n","correct: 57+758=815\n","outputs(x):  33+306=739\n","650\n","wrong  : 33+306=739\n","correct: 33+306=339\n","outputs(x):  18+904=1522\n","79\n","wrong  : 18+904=1522\n","correct: 18+904=922\n","outputs(x):  24+599=1223\n","8+\n","wrong  : 24+599=1223\n","correct: 24+599=623\n","outputs(x):  12+194=306\n","669\n","wrong  : 12+194=306\n","correct: 12+194=206\n","outputs(x):  91+804=1295\n","10\n","wrong  : 91+804=1295\n","correct: 91+804=895\n","outputs(x):  96+106=502\n","803\n","wrong  : 96+106=502\n","correct: 96+106=202\n","outputs(x):  66+615=1181\n","92\n","wrong  : 66+615=1181\n","correct: 66+615=681\n","outputs(x):  582+88=660\n","216\n","wrong  : 582+88=660\n","correct: 582+88=670\n","outputs(x):  59+315=974\n","2+7\n","wrong  : 59+315=974\n","correct: 59+315=374\n","outputs(x):  803+17=810\n","459\n","wrong  : 803+17=810\n","correct: 803+17=820\n","outputs(x):  87+763=1250\n","82\n","wrong  : 87+763=1250\n","correct: 87+763=850\n","outputs(x):  73+253=1126\n","44\n","wrong  : 73+253=1126\n","correct: 73+253=326\n","outputs(x):  406+53=469\n","759\n","wrong  : 406+53=469\n","correct: 406+53=459\n","outputs(x):  50+977=1827\n","52\n","wrong  : 50+977=1827\n","correct: 50+977=1027\n","outputs(x):  77+717=1294\n","11\n","wrong  : 77+717=1294\n","correct: 77+717=794\n","outputs(x):  20+390=1010\n","24\n","wrong  : 20+390=1010\n","correct: 20+390=410\n","outputs(x):  29+624=1453\n","73\n","wrong  : 29+624=1453\n","correct: 29+624=653\n","outputs(x):  75+882=1757\n","30\n","wrong  : 75+882=1757\n","correct: 75+882=957\n","outputs(x):  48+752=1700\n","85\n","wrong  : 48+752=1700\n","correct: 48+752=800\n","outputs(x):  95+726=1221\n","47\n","wrong  : 95+726=1221\n","correct: 95+726=821\n","outputs(x):  250+39=299\n","877\n","wrong  : 250+39=299\n","correct: 250+39=289\n","outputs(x):  94+558=1152\n","20\n","wrong  : 94+558=1152\n","correct: 94+558=652\n","outputs(x):  19+283=802\n","693\n","wrong  : 19+283=802\n","correct: 19+283=302\n","outputs(x):  15+830=945\n","989\n","wrong  : 15+830=945\n","correct: 15+830=845\n","outputs(x):  65+501=766\n","712\n","wrong  : 65+501=766\n","correct: 65+501=566\n","outputs(x):  169+11=170\n","78+\n","wrong  : 169+11=170\n","correct: 169+11=180\n","outputs(x):  72+965=1337\n","96\n","wrong  : 72+965=1337\n","correct: 72+965=1037\n","outputs(x):  86+977=1563\n","55\n","wrong  : 86+977=1563\n","correct: 86+977=1063\n","outputs(x):  61+249=510\n","846\n","wrong  : 61+249=510\n","correct: 61+249=310\n","outputs(x):  131+59=180\n","696\n","wrong  : 131+59=180\n","correct: 131+59=190\n","outputs(x):  55+734=1589\n","21\n","wrong  : 55+734=1589\n","correct: 55+734=789\n","outputs(x):  35+417=852\n","38+\n","wrong  : 35+417=852\n","correct: 35+417=452\n"," 91% 73/80 [00:03<00:00, 16.69it/s]outputs(x):  33+350=983\n","846\n","wrong  : 33+350=983\n","correct: 33+350=383\n","outputs(x):  198+17=205\n","253\n","wrong  : 198+17=205\n","correct: 198+17=215\n","outputs(x):  58+518=1476\n","77\n","wrong  : 58+518=1476\n","correct: 58+518=576\n","outputs(x):  56+497=1453\n","82\n","wrong  : 56+497=1453\n","correct: 56+497=553\n","outputs(x):  13+287=1100\n","30\n","wrong  : 13+287=1100\n","correct: 13+287=300\n","outputs(x):  22+755=1277\n","69\n","wrong  : 22+755=1277\n","correct: 22+755=777\n","outputs(x):  91+260=1251\n","84\n","wrong  : 91+260=1251\n","correct: 91+260=351\n","outputs(x):  64+712=1576\n","61\n","wrong  : 64+712=1576\n","correct: 64+712=776\n","outputs(x):  95+592=1087\n","63\n","wrong  : 95+592=1087\n","correct: 95+592=687\n","outputs(x):  71+387=1158\n","16\n","wrong  : 71+387=1158\n","correct: 71+387=458\n","outputs(x):  79+908=1687\n","34\n","wrong  : 79+908=1687\n","correct: 79+908=987\n","outputs(x):  11+873=984\n","408\n","wrong  : 11+873=984\n","correct: 11+873=884\n","outputs(x):  35+277=612\n","252\n","wrong  : 35+277=612\n","correct: 35+277=312\n","outputs(x):  46+944=1190\n","12\n","wrong  : 46+944=1190\n","correct: 46+944=990\n","outputs(x):  19+563=682\n","620\n","wrong  : 19+563=682\n","correct: 19+563=582\n","outputs(x):  92+195=887\n","450\n","wrong  : 92+195=887\n","correct: 92+195=287\n","outputs(x):  12+595=1507\n","66\n","wrong  : 12+595=1507\n","correct: 12+595=607\n","outputs(x):  92+587=1179\n","89\n","wrong  : 92+587=1179\n","correct: 92+587=679\n","outputs(x):  44+166=1110\n","48\n","wrong  : 44+166=1110\n","correct: 44+166=210\n","outputs(x):  92+712=1604\n","56\n","wrong  : 92+712=1604\n","correct: 92+712=804\n","outputs(x):  25+285=1110\n","14\n","wrong  : 25+285=1110\n","correct: 25+285=310\n","outputs(x):  17+661=1178\n","76\n","wrong  : 17+661=1178\n","correct: 17+661=678\n","outputs(x):  972+45=1027\n","64\n","wrong  : 972+45=1027\n","correct: 972+45=1017\n","outputs(x):  89+208=797\n","109\n","wrong  : 89+208=797\n","correct: 89+208=297\n","outputs(x):  99+808=1807\n","92\n","wrong  : 99+808=1807\n","correct: 99+808=907\n","outputs(x):  79+671=1450\n","72\n","wrong  : 79+671=1450\n","correct: 79+671=750\n","outputs(x):  18+315=1233\n","10\n","wrong  : 18+315=1233\n","correct: 18+315=333\n","outputs(x):  64+986=1250\n","37\n","wrong  : 64+986=1250\n","correct: 64+986=1050\n","outputs(x):  60+904=1764\n","50\n","wrong  : 60+904=1764\n","correct: 60+904=964\n","outputs(x):  69+734=1203\n","40\n","wrong  : 69+734=1203\n","correct: 69+734=803\n","outputs(x):  91+968=1559\n","18\n","wrong  : 91+968=1559\n","correct: 91+968=1059\n","outputs(x):  22+178=900\n","754\n","wrong  : 22+178=900\n","correct: 22+178=200\n","outputs(x):  28+722=1150\n","15\n","wrong  : 28+722=1150\n","correct: 28+722=750\n","outputs(x):  34+531=865\n","423\n","wrong  : 34+531=865\n","correct: 34+531=565\n","outputs(x):  24+423=647\n","15+\n","wrong  : 24+423=647\n","correct: 24+423=447\n","outputs(x):  87+972=1459\n","52\n","wrong  : 87+972=1459\n","correct: 87+972=1059\n","outputs(x):  74+244=818\n","602\n","wrong  : 74+244=818\n","correct: 74+244=318\n","outputs(x):  45+105=240\n","658\n","wrong  : 45+105=240\n","correct: 45+105=150\n","outputs(x):  59+444=1303\n","43\n","wrong  : 59+444=1303\n","correct: 59+444=503\n","outputs(x):  37+298=1135\n","76\n","wrong  : 37+298=1135\n","correct: 37+298=335\n","outputs(x):  23+365=588\n","142\n","wrong  : 23+365=588\n","correct: 23+365=388\n","outputs(x):  711+94=705\n","910\n","wrong  : 711+94=705\n","correct: 711+94=805\n","outputs(x):  49+721=1470\n","86\n","wrong  : 49+721=1470\n","correct: 49+721=770\n","outputs(x):  58+112=1070\n","12\n","wrong  : 58+112=1070\n","correct: 58+112=170\n","outputs(x):  46+521=667\n","880\n","wrong  : 46+521=667\n","correct: 46+521=567\n","outputs(x):  59+915=1374\n","10\n","wrong  : 59+915=1374\n","correct: 59+915=974\n","outputs(x):  29+570=699\n","564\n","wrong  : 29+570=699\n","correct: 29+570=599\n","outputs(x):  13+993=1606\n","22\n","wrong  : 13+993=1606\n","correct: 13+993=1006\n","outputs(x):  61+534=1395\n","89\n","wrong  : 61+534=1395\n","correct: 61+534=595\n","outputs(x):  41+918=1459\n","76\n","wrong  : 41+918=1459\n","correct: 41+918=959\n","outputs(x):  32+102=634\n","596\n","wrong  : 32+102=634\n","correct: 32+102=134\n","outputs(x):  54+528=882\n","867\n","wrong  : 54+528=882\n","correct: 54+528=582\n","outputs(x):  62+267=439\n","622\n","wrong  : 62+267=439\n","correct: 62+267=329\n","outputs(x):  715+16=721\n","370\n","wrong  : 715+16=721\n","correct: 715+16=731\n","outputs(x):  143+64=197\n","83+\n","wrong  : 143+64=197\n","correct: 143+64=207\n","outputs(x):  93+664=1357\n","13\n","wrong  : 93+664=1357\n","correct: 93+664=757\n","outputs(x):  55+977=1932\n","43\n","wrong  : 55+977=1932\n","correct: 55+977=1032\n","outputs(x):  57+439=696\n","919\n","wrong  : 57+439=696\n","correct: 57+439=496\n","outputs(x):  13+579=1492\n","47\n","wrong  : 13+579=1492\n","correct: 13+579=592\n","outputs(x):  82+695=1077\n","11\n","wrong  : 82+695=1077\n","correct: 82+695=777\n","outputs(x):  44+267=1211\n","48\n","wrong  : 44+267=1211\n","correct: 44+267=311\n","outputs(x):  92+436=1328\n","78\n","wrong  : 92+436=1328\n","correct: 92+436=528\n","outputs(x):  90+949=1639\n","88\n","wrong  : 90+949=1639\n","correct: 90+949=1039\n","outputs(x):  98+123=521\n","444\n","wrong  : 98+123=521\n","correct: 98+123=221\n","outputs(x):  90+789=1579\n","51\n","wrong  : 90+789=1579\n","correct: 90+789=879\n","outputs(x):  81+265=546\n","775\n","wrong  : 81+265=546\n","correct: 81+265=346\n","outputs(x):  508+72=570\n","216\n","wrong  : 508+72=570\n","correct: 508+72=580\n","outputs(x):  36+939=1175\n","95\n","wrong  : 36+939=1175\n","correct: 36+939=975\n","outputs(x):  98+856=1254\n","52\n","wrong  : 98+856=1254\n","correct: 98+856=954\n","outputs(x):  73+372=1245\n","43\n","wrong  : 73+372=1245\n","correct: 73+372=445\n","outputs(x):  56+300=556\n","590\n","wrong  : 56+300=556\n","correct: 56+300=356\n","outputs(x):  98+984=1182\n","77\n","wrong  : 98+984=1182\n","correct: 98+984=1082\n","outputs(x):  15+518=633\n","38+\n","wrong  : 15+518=633\n","correct: 15+518=533\n","outputs(x):  15+642=857\n","344\n","wrong  : 15+642=857\n","correct: 15+642=657\n","outputs(x):  334+59=383\n","645\n","wrong  : 334+59=383\n","correct: 334+59=393\n","outputs(x):  85+243=528\n","344\n","wrong  : 85+243=528\n","correct: 85+243=328\n","outputs(x):  14+448=1162\n","20\n","wrong  : 14+448=1162\n","correct: 14+448=462\n","outputs(x):  92+288=1080\n","87\n","wrong  : 92+288=1080\n","correct: 92+288=380\n","outputs(x):  51+511=662\n","284\n","wrong  : 51+511=662\n","correct: 51+511=562\n","outputs(x):  12+375=887\n","403\n","wrong  : 12+375=887\n","correct: 12+375=387\n","outputs(x):  99+560=959\n","798\n","wrong  : 99+560=959\n","correct: 99+560=659\n","outputs(x):  50+745=1395\n","85\n","wrong  : 50+745=1395\n","correct: 50+745=795\n","outputs(x):  51+340=591\n","158\n","wrong  : 51+340=591\n","correct: 51+340=391\n","outputs(x):  79+406=685\n","988\n","wrong  : 79+406=685\n","correct: 79+406=485\n","outputs(x):  34+583=1417\n","79\n","wrong  : 34+583=1417\n","correct: 34+583=617\n","outputs(x):  57+225=382\n","640\n","wrong  : 57+225=382\n","correct: 57+225=282\n","outputs(x):  61+644=1005\n","97\n","wrong  : 61+644=1005\n","correct: 61+644=705\n","outputs(x):  43+922=1765\n","88\n","wrong  : 43+922=1765\n","correct: 43+922=965\n","outputs(x):  961+29=980\n","390\n","wrong  : 961+29=980\n","correct: 961+29=990\n","outputs(x):  13+951=1464\n","99\n","wrong  : 13+951=1464\n","correct: 13+951=964\n","outputs(x):  83+587=1270\n","14\n","wrong  : 83+587=1270\n","correct: 83+587=670\n","outputs(x):  493+33=536\n","946\n","wrong  : 493+33=536\n","correct: 493+33=526\n","outputs(x):  95+173=368\n","454\n","wrong  : 95+173=368\n","correct: 95+173=268\n","outputs(x):  58+739=1397\n","84\n","wrong  : 58+739=1397\n","correct: 58+739=797\n","outputs(x):  17+880=997\n","115\n","wrong  : 17+880=997\n","correct: 17+880=897\n","outputs(x):  64+615=879\n","51+\n","wrong  : 64+615=879\n","correct: 64+615=679\n","outputs(x):  66+990=1556\n","68\n","wrong  : 66+990=1556\n","correct: 66+990=1056\n","outputs(x):  73+299=1172\n","92\n","wrong  : 73+299=1172\n","correct: 73+299=372\n","outputs(x):  65+211=376\n","722\n","wrong  : 65+211=376\n","correct: 65+211=276\n","outputs(x):  34+471=1405\n","88\n","wrong  : 34+471=1405\n","correct: 34+471=505\n","outputs(x):  17+158=675\n","543\n","wrong  : 17+158=675\n","correct: 17+158=175\n","outputs(x):  19+329=1248\n","84\n","wrong  : 19+329=1248\n","correct: 19+329=348\n","outputs(x):  43+639=1482\n","98\n","wrong  : 43+639=1482\n","correct: 43+639=682\n","outputs(x):  34+643=1577\n","63\n","wrong  : 34+643=1577\n","correct: 34+643=677\n","outputs(x):  39+755=1294\n","95\n","wrong  : 39+755=1294\n","correct: 39+755=794\n","outputs(x):  87+795=1482\n","1+\n","wrong  : 87+795=1482\n","correct: 87+795=882\n","outputs(x):  17+915=1432\n","61\n","wrong  : 17+915=1432\n","correct: 17+915=932\n","outputs(x):  471+42=413\n","256\n","wrong  : 471+42=413\n","correct: 471+42=513\n","outputs(x):  59+769=1028\n","71\n","wrong  : 59+769=1028\n","correct: 59+769=828\n","outputs(x):  57+308=1165\n","78\n","wrong  : 57+308=1165\n","correct: 57+308=365\n","outputs(x):  98+705=1103\n","50\n","wrong  : 98+705=1103\n","correct: 98+705=803\n","outputs(x):  32+847=1179\n","12\n","wrong  : 32+847=1179\n","correct: 32+847=879\n","outputs(x):  329+98=417\n","109\n","wrong  : 329+98=417\n","correct: 329+98=427\n","outputs(x):  52+972=1724\n","40\n","wrong  : 52+972=1724\n","correct: 52+972=1024\n","outputs(x):  82+198=1180\n","77\n","wrong  : 82+198=1180\n","correct: 82+198=280\n","outputs(x):  53+295=1148\n","90\n","wrong  : 53+295=1148\n","correct: 53+295=348\n","outputs(x):  71+565=1236\n","56\n","wrong  : 71+565=1236\n","correct: 71+565=636\n","outputs(x):  729+62=781\n","262\n","wrong  : 729+62=781\n","correct: 729+62=791\n","outputs(x):  75+268=1143\n","49\n","wrong  : 75+268=1143\n","correct: 75+268=343\n","outputs(x):  76+645=1321\n","23\n","wrong  : 76+645=1321\n","correct: 76+645=721\n","outputs(x):  983+73=1066\n","87\n","wrong  : 983+73=1066\n","correct: 983+73=1056\n","outputs(x):  21+150=471\n","875\n","wrong  : 21+150=471\n","correct: 21+150=171\n","outputs(x):  34+481=1415\n","7+\n","wrong  : 34+481=1415\n","correct: 34+481=515\n","outputs(x):  57+452=809\n","988\n","wrong  : 57+452=809\n","correct: 57+452=509\n","outputs(x):  136+13=159\n","411\n","wrong  : 136+13=159\n","correct: 136+13=149\n","outputs(x):  80+323=1003\n","88\n","wrong  : 80+323=1003\n","correct: 80+323=403\n","outputs(x):  76+433=1309\n","71\n","wrong  : 76+433=1309\n","correct: 76+433=509\n","outputs(x):  472+47=529\n","78+\n","wrong  : 472+47=529\n","correct: 472+47=519\n","outputs(x):  63+274=1237\n","43\n","wrong  : 63+274=1237\n","correct: 63+274=337\n","outputs(x):  519+62=571\n","533\n","wrong  : 519+62=571\n","correct: 519+62=581\n","outputs(x):  60+437=1097\n","93\n","wrong  : 60+437=1097\n","correct: 60+437=497\n","outputs(x):  50+656=1306\n","68\n","wrong  : 50+656=1306\n","correct: 50+656=706\n","outputs(x):  154+57=201\n","33+\n","wrong  : 154+57=201\n","correct: 154+57=211\n","outputs(x):  19+488=1307\n","78\n","wrong  : 19+488=1307\n","correct: 19+488=507\n","outputs(x):  65+483=1148\n","35\n","wrong  : 65+483=1148\n","correct: 65+483=548\n","outputs(x):  67+563=1430\n","61\n","wrong  : 67+563=1430\n","correct: 67+563=630\n","outputs(x):  76+794=1170\n","62\n","wrong  : 76+794=1170\n","correct: 76+794=870\n","outputs(x):  516+34=540\n","841\n","wrong  : 516+34=540\n","correct: 516+34=550\n","outputs(x):  62+366=1028\n","86\n","wrong  : 62+366=1028\n","correct: 62+366=428\n","outputs(x):  21+299=1120\n","35\n","wrong  : 21+299=1120\n","correct: 21+299=320\n"," 94% 75/80 [00:03<00:00, 15.74it/s]outputs(x):  99+865=1164\n","97\n","wrong  : 99+865=1164\n","correct: 99+865=964\n","outputs(x):  64+849=1113\n","26\n","wrong  : 64+849=1113\n","correct: 64+849=913\n","outputs(x):  32+879=1611\n","36\n","wrong  : 32+879=1611\n","correct: 32+879=911\n","outputs(x):  404+97=491\n","33+\n","wrong  : 404+97=491\n","correct: 404+97=501\n","outputs(x):  72+990=1662\n","99\n","wrong  : 72+990=1662\n","correct: 72+990=1062\n","outputs(x):  19+689=1208\n","94\n","wrong  : 19+689=1208\n","correct: 19+689=708\n","outputs(x):  15+645=1560\n","39\n","wrong  : 15+645=1560\n","correct: 15+645=660\n","outputs(x):  43+325=868\n","865\n","wrong  : 43+325=868\n","correct: 43+325=368\n","outputs(x):  977+12=999\n","773\n","wrong  : 977+12=999\n","correct: 977+12=989\n","outputs(x):  51+853=1704\n","97\n","wrong  : 51+853=1704\n","correct: 51+853=904\n","outputs(x):  828+92=910\n","569\n","wrong  : 828+92=910\n","correct: 828+92=920\n","outputs(x):  63+996=1259\n","21\n","wrong  : 63+996=1259\n","correct: 63+996=1059\n","outputs(x):  58+574=1532\n","58\n","wrong  : 58+574=1532\n","correct: 58+574=632\n","outputs(x):  39+894=1633\n","95\n","wrong  : 39+894=1633\n","correct: 39+894=933\n","outputs(x):  58+195=1153\n","75\n","wrong  : 58+195=1153\n","correct: 58+195=253\n","outputs(x):  622+99=711\n","278\n","wrong  : 622+99=711\n","correct: 622+99=721\n","outputs(x):  62+331=993\n","989\n","wrong  : 62+331=993\n","correct: 62+331=393\n","outputs(x):  69+164=1133\n","43\n","wrong  : 69+164=1133\n","correct: 69+164=233\n","outputs(x):  36+144=780\n","935\n","wrong  : 36+144=780\n","correct: 36+144=180\n","outputs(x):  95+131=526\n","60+\n","wrong  : 95+131=526\n","correct: 95+131=226\n","outputs(x):  95+305=1200\n","54\n","wrong  : 95+305=1200\n","correct: 95+305=400\n","outputs(x):  70+751=1321\n","79\n","wrong  : 70+751=1321\n","correct: 70+751=821\n","outputs(x):  86+503=989\n","266\n","wrong  : 86+503=989\n","correct: 86+503=589\n","outputs(x):  12+638=750\n","928\n","wrong  : 12+638=750\n","correct: 12+638=650\n","outputs(x):  13+783=1796\n","10\n","wrong  : 13+783=1796\n","correct: 13+783=796\n","outputs(x):  854+69=913\n","53+\n","wrong  : 854+69=913\n","correct: 854+69=923\n","outputs(x):  28+122=1050\n","13\n","wrong  : 28+122=1050\n","correct: 28+122=150\n","outputs(x):  57+599=1456\n","84\n","wrong  : 57+599=1456\n","correct: 57+599=656\n","outputs(x):  71+172=1143\n","16\n","wrong  : 71+172=1143\n","correct: 71+172=243\n","outputs(x):  26+642=1068\n","41\n","wrong  : 26+642=1068\n","correct: 26+642=668\n","outputs(x):  71+646=1217\n","64\n","wrong  : 71+646=1217\n","correct: 71+646=717\n","outputs(x):  42+590=1232\n","52\n","wrong  : 42+590=1232\n","correct: 42+590=632\n","outputs(x):  53+514=1267\n","98\n","wrong  : 53+514=1267\n","correct: 53+514=567\n","outputs(x):  53+861=1814\n","61\n","wrong  : 53+861=1814\n","correct: 53+861=914\n","outputs(x):  13+101=614\n","400\n","wrong  : 13+101=614\n","correct: 13+101=114\n","outputs(x):  38+663=1201\n","12\n","wrong  : 38+663=1201\n","correct: 38+663=701\n","outputs(x):  22+589=1011\n","72\n","wrong  : 22+589=1011\n","correct: 22+589=611\n","outputs(x):  39+257=1196\n","93\n","wrong  : 39+257=1196\n","correct: 39+257=296\n","outputs(x):  63+360=523\n","981\n","wrong  : 63+360=523\n","correct: 63+360=423\n","outputs(x):  77+351=1328\n","68\n","wrong  : 77+351=1328\n","correct: 77+351=428\n","outputs(x):  26+423=849\n","801\n","wrong  : 26+423=849\n","correct: 26+423=449\n","outputs(x):  97+448=1245\n","87\n","wrong  : 97+448=1245\n","correct: 97+448=545\n","outputs(x):  68+837=1505\n","99\n","wrong  : 68+837=1505\n","correct: 68+837=905\n","outputs(x):  42+268=1110\n","12\n","wrong  : 42+268=1110\n","correct: 42+268=310\n","outputs(x):  79+376=1355\n","74\n","wrong  : 79+376=1355\n","correct: 79+376=455\n","outputs(x):  17+568=785\n","276\n","wrong  : 17+568=785\n","correct: 17+568=585\n","outputs(x):  45+748=1693\n","63\n","wrong  : 45+748=1693\n","correct: 45+748=793\n","outputs(x):  31+318=1149\n","90\n","wrong  : 31+318=1149\n","correct: 31+318=349\n","outputs(x):  65+136=301\n","2+7\n","wrong  : 65+136=301\n","correct: 65+136=201\n","outputs(x):  958+46=904\n","1+4\n","wrong  : 958+46=904\n","correct: 958+46=1004\n","outputs(x):  55+106=661\n","892\n","wrong  : 55+106=661\n","correct: 55+106=161\n","outputs(x):  68+556=924\n","137\n","wrong  : 68+556=924\n","correct: 68+556=624\n","outputs(x):  293+17=300\n","517\n","wrong  : 293+17=300\n","correct: 293+17=310\n","outputs(x):  31+791=1222\n","57\n","wrong  : 31+791=1222\n","correct: 31+791=822\n","outputs(x):  88+236=1124\n","64\n","wrong  : 88+236=1124\n","correct: 88+236=324\n","outputs(x):  29+208=837\n","858\n","wrong  : 29+208=837\n","correct: 29+208=237\n","outputs(x):  88+194=1182\n","76\n","wrong  : 88+194=1182\n","correct: 88+194=282\n","outputs(x):  744+76=810\n","559\n","wrong  : 744+76=810\n","correct: 744+76=820\n","outputs(x):  74+816=1290\n","35\n","wrong  : 74+816=1290\n","correct: 74+816=890\n","outputs(x):  43+754=997\n","79+\n","wrong  : 43+754=997\n","correct: 43+754=797\n","outputs(x):  35+359=1094\n","3+\n","wrong  : 35+359=1094\n","correct: 35+359=394\n","outputs(x):  41+132=873\n","148\n","wrong  : 41+132=873\n","correct: 41+132=173\n","outputs(x):  84+380=864\n","328\n","wrong  : 84+380=864\n","correct: 84+380=464\n","outputs(x):  20+852=1672\n","90\n","wrong  : 20+852=1672\n","correct: 20+852=872\n","outputs(x):  99+109=908\n","119\n","wrong  : 99+109=908\n","correct: 99+109=208\n","outputs(x):  709+41=740\n","283\n","wrong  : 709+41=740\n","correct: 709+41=750\n","outputs(x):  19+544=1463\n","52\n","wrong  : 19+544=1463\n","correct: 19+544=563\n","outputs(x):  58+487=1445\n","53\n","wrong  : 58+487=1445\n","correct: 58+487=545\n","outputs(x):  59+178=937\n","374\n","wrong  : 59+178=937\n","correct: 59+178=237\n","outputs(x):  59+296=655\n","215\n","wrong  : 59+296=655\n","correct: 59+296=355\n","outputs(x):  55+441=1396\n","38\n","wrong  : 55+441=1396\n","correct: 55+441=496\n","outputs(x):  86+744=1330\n","43\n","wrong  : 86+744=1330\n","correct: 86+744=830\n","outputs(x):  932+69=991\n","552\n","wrong  : 932+69=991\n","correct: 932+69=1001\n","outputs(x):  57+410=1267\n","65\n","wrong  : 57+410=1267\n","correct: 57+410=467\n","outputs(x):  70+573=1043\n","58\n","wrong  : 70+573=1043\n","correct: 70+573=643\n","outputs(x):  59+756=1215\n","90\n","wrong  : 59+756=1215\n","correct: 59+756=815\n","outputs(x):  28+810=1538\n","55\n","wrong  : 28+810=1538\n","correct: 28+810=838\n","outputs(x):  90+744=1534\n","0+\n","wrong  : 90+744=1534\n","correct: 90+744=834\n","outputs(x):  69+392=861\n","897\n","wrong  : 69+392=861\n","correct: 69+392=461\n","outputs(x):  86+313=999\n","266\n","wrong  : 86+313=999\n","correct: 86+313=399\n","outputs(x):  93+922=1415\n","51\n","wrong  : 93+922=1415\n","correct: 93+922=1015\n","outputs(x):  14+624=1038\n","44\n","wrong  : 14+624=1038\n","correct: 14+624=638\n","outputs(x):  95+168=1163\n","75\n","wrong  : 95+168=1163\n","correct: 95+168=263\n","outputs(x):  58+608=1266\n","81\n","wrong  : 58+608=1266\n","correct: 58+608=666\n","outputs(x):  60+646=806\n","239\n","wrong  : 60+646=806\n","correct: 60+646=706\n","outputs(x):  18+538=1356\n","49\n","wrong  : 18+538=1356\n","correct: 18+538=556\n","outputs(x):  31+182=913\n","71+\n","wrong  : 31+182=913\n","correct: 31+182=213\n","outputs(x):  44+836=1680\n","40\n","wrong  : 44+836=1680\n","correct: 44+836=880\n","outputs(x):  19+880=1799\n","31\n","wrong  : 19+880=1799\n","correct: 19+880=899\n","outputs(x):  16+874=1290\n","36\n","wrong  : 16+874=1290\n","correct: 16+874=890\n","outputs(x):  88+369=1357\n","82\n","wrong  : 88+369=1357\n","correct: 88+369=457\n","outputs(x):  113+78=181\n","407\n","wrong  : 113+78=181\n","correct: 113+78=191\n","outputs(x):  56+594=1050\n","94\n","wrong  : 56+594=1050\n","correct: 56+594=650\n","outputs(x):  73+963=1436\n","45\n","wrong  : 73+963=1436\n","correct: 73+963=1036\n","outputs(x):  63+495=1158\n","97\n","wrong  : 63+495=1158\n","correct: 63+495=558\n","outputs(x):  43+792=935\n","58+\n","wrong  : 43+792=935\n","correct: 43+792=835\n","outputs(x):  679+72=741\n","535\n","wrong  : 679+72=741\n","correct: 679+72=751\n","outputs(x):  407+28=425\n","374\n","wrong  : 407+28=425\n","correct: 407+28=435\n","outputs(x):  64+106=270\n","261\n","wrong  : 64+106=270\n","correct: 64+106=170\n","outputs(x):  47+212=459\n","104\n","wrong  : 47+212=459\n","correct: 47+212=259\n","outputs(x):  12+982=1494\n","54\n","wrong  : 12+982=1494\n","correct: 12+982=994\n","outputs(x):  22+287=419\n","769\n","wrong  : 22+287=419\n","correct: 22+287=309\n","outputs(x):  73+178=851\n","681\n","wrong  : 73+178=851\n","correct: 73+178=251\n","outputs(x):  58+113=661\n","76+\n","wrong  : 58+113=661\n","correct: 58+113=171\n","outputs(x):  85+485=1370\n","29\n","wrong  : 85+485=1370\n","correct: 85+485=570\n","outputs(x):  53+179=1032\n","67\n","wrong  : 53+179=1032\n","correct: 53+179=232\n","outputs(x):  31+407=1238\n","87\n","wrong  : 31+407=1238\n","correct: 31+407=438\n","outputs(x):  67+636=1603\n","87\n","wrong  : 67+636=1603\n","correct: 67+636=703\n","outputs(x):  29+461=890\n","769\n","wrong  : 29+461=890\n","correct: 29+461=490\n","outputs(x):  509+91=590\n","370\n","wrong  : 509+91=590\n","correct: 509+91=600\n","outputs(x):  17+317=834\n","38+\n","wrong  : 17+317=834\n","correct: 17+317=334\n","outputs(x):  357+40=307\n","961\n","wrong  : 357+40=307\n","correct: 357+40=397\n","outputs(x):  62+879=1541\n","57\n","wrong  : 62+879=1541\n","correct: 62+879=941\n","outputs(x):  61+282=843\n","778\n","wrong  : 61+282=843\n","correct: 61+282=343\n","outputs(x):  86+624=1210\n","12\n","wrong  : 86+624=1210\n","correct: 86+624=710\n","outputs(x):  26+619=1445\n","24\n","wrong  : 26+619=1445\n","correct: 26+619=645\n","outputs(x):  60+141=1001\n","67\n","wrong  : 60+141=1001\n","correct: 60+141=201\n","outputs(x):  15+362=977\n","937\n","wrong  : 15+362=977\n","correct: 15+362=377\n","outputs(x):  95+953=1748\n","13\n","wrong  : 95+953=1748\n","correct: 95+953=1048\n","outputs(x):  89+300=889\n","35+\n","wrong  : 89+300=889\n","correct: 89+300=389\n","outputs(x):  41+409=1050\n","54\n","wrong  : 41+409=1050\n","correct: 41+409=450\n","outputs(x):  999+75=1064\n","26\n","wrong  : 999+75=1064\n","correct: 999+75=1074\n","outputs(x):  64+100=564\n","849\n","wrong  : 64+100=564\n","correct: 64+100=164\n","outputs(x):  57+643=1000\n","83\n","wrong  : 57+643=1000\n","correct: 57+643=700\n","outputs(x):  47+971=1618\n","10\n","wrong  : 47+971=1618\n","correct: 47+971=1018\n","outputs(x):  68+789=1557\n","27\n","wrong  : 68+789=1557\n","correct: 68+789=857\n","outputs(x):  54+872=1126\n","46\n","wrong  : 54+872=1126\n","correct: 54+872=926\n","outputs(x):  31+806=937\n","477\n","wrong  : 31+806=937\n","correct: 31+806=837\n","outputs(x):  90+541=1031\n","89\n","wrong  : 90+541=1031\n","correct: 90+541=631\n","outputs(x):  80+135=315\n","897\n","wrong  : 80+135=315\n","correct: 80+135=215\n","outputs(x):  39+948=1287\n","87\n","wrong  : 39+948=1287\n","correct: 39+948=987\n","outputs(x):  89+528=1117\n","35\n","wrong  : 89+528=1117\n","correct: 89+528=617\n","outputs(x):  83+663=1246\n","96\n","wrong  : 83+663=1246\n","correct: 83+663=746\n","outputs(x):  86+558=1344\n","87\n","wrong  : 86+558=1344\n","correct: 86+558=644\n","outputs(x):  28+954=1382\n","40\n","wrong  : 28+954=1382\n","correct: 28+954=982\n","outputs(x):  88+994=1682\n","51\n","wrong  : 88+994=1682\n","correct: 88+994=1082\n","outputs(x):  81+504=1485\n","52\n","wrong  : 81+504=1485\n","correct: 81+504=585\n","outputs(x):  23+686=1509\n","17\n","wrong  : 23+686=1509\n","correct: 23+686=709\n","outputs(x):  84+216=1200\n","29\n","wrong  : 84+216=1200\n","correct: 84+216=300\n","outputs(x):  16+613=829\n","935\n","wrong  : 16+613=829\n","correct: 16+613=629\n","outputs(x):  89+666=1355\n","74\n","wrong  : 89+666=1355\n","correct: 89+666=755\n","outputs(x):  83+862=1445\n","93\n","wrong  : 83+862=1445\n","correct: 83+862=945\n","outputs(x):  59+692=1151\n","77\n","wrong  : 59+692=1151\n","correct: 59+692=751\n","outputs(x):  68+804=1272\n","16\n","wrong  : 68+804=1272\n","correct: 68+804=872\n","outputs(x):  26+922=1848\n","68\n","wrong  : 26+922=1848\n","correct: 26+922=948\n","outputs(x):  85+161=1146\n","16\n","wrong  : 85+161=1146\n","correct: 85+161=246\n","outputs(x):  60+929=1589\n","81\n","wrong  : 60+929=1589\n","correct: 60+929=989\n","outputs(x):  54+722=1676\n","88\n","wrong  : 54+722=1676\n","correct: 54+722=776\n","outputs(x):  87+519=1306\n","13\n","wrong  : 87+519=1306\n","correct: 87+519=606\n","outputs(x):  78+524=1002\n","12\n","wrong  : 78+524=1002\n","correct: 78+524=602\n","outputs(x):  14+783=1297\n","6+\n","wrong  : 14+783=1297\n","correct: 14+783=797\n","outputs(x):  91+935=1726\n","62\n","wrong  : 91+935=1726\n","correct: 91+935=1026\n","outputs(x):  47+697=1244\n","13\n","wrong  : 47+697=1244\n","correct: 47+697=744\n"," 96% 77/80 [00:03<00:00, 14.84it/s]outputs(x):  85+697=1582\n","48\n","wrong  : 85+697=1582\n","correct: 85+697=782\n","outputs(x):  29+923=1452\n","73\n","wrong  : 29+923=1452\n","correct: 29+923=952\n","outputs(x):  34+888=1022\n","0+\n","wrong  : 34+888=1022\n","correct: 34+888=922\n","outputs(x):  50+442=892\n","318\n","wrong  : 50+442=892\n","correct: 50+442=492\n","outputs(x):  26+206=1032\n","18\n","wrong  : 26+206=1032\n","correct: 26+206=232\n","outputs(x):  70+644=1414\n","60\n","wrong  : 70+644=1414\n","correct: 70+644=714\n","outputs(x):  24+764=1588\n","57\n","wrong  : 24+764=1588\n","correct: 24+764=788\n","outputs(x):  57+796=1053\n","62\n","wrong  : 57+796=1053\n","correct: 57+796=853\n","outputs(x):  15+733=1248\n","62\n","wrong  : 15+733=1248\n","correct: 15+733=748\n","outputs(x):  86+780=1066\n","14\n","wrong  : 86+780=1066\n","correct: 86+780=866\n","outputs(x):  33+159=792\n","386\n","wrong  : 33+159=792\n","correct: 33+159=192\n","outputs(x):  39+609=1448\n","41\n","wrong  : 39+609=1448\n","correct: 39+609=648\n","outputs(x):  80+909=1889\n","51\n","wrong  : 80+909=1889\n","correct: 80+909=989\n","outputs(x):  46+260=706\n","688\n","wrong  : 46+260=706\n","correct: 46+260=306\n","outputs(x):  42+192=734\n","98+\n","wrong  : 42+192=734\n","correct: 42+192=234\n","outputs(x):  51+809=1860\n","10\n","wrong  : 51+809=1860\n","correct: 51+809=860\n","outputs(x):  98+975=1873\n","50\n","wrong  : 98+975=1873\n","correct: 98+975=1073\n","outputs(x):  96+377=1273\n","74\n","wrong  : 96+377=1273\n","correct: 96+377=473\n","outputs(x):  13+440=653\n","565\n","wrong  : 13+440=653\n","correct: 13+440=453\n","outputs(x):  81+965=1446\n","12\n","wrong  : 81+965=1446\n","correct: 81+965=1046\n","outputs(x):  87+901=1788\n","62\n","wrong  : 87+901=1788\n","correct: 87+901=988\n","outputs(x):  73+900=1573\n","99\n","wrong  : 73+900=1573\n","correct: 73+900=973\n","outputs(x):  45+315=460\n","851\n","wrong  : 45+315=460\n","correct: 45+315=360\n","outputs(x):  26+335=861\n","747\n","wrong  : 26+335=861\n","correct: 26+335=361\n","outputs(x):  27+731=1458\n","86\n","wrong  : 27+731=1458\n","correct: 27+731=758\n","outputs(x):  81+282=463\n","859\n","wrong  : 81+282=463\n","correct: 81+282=363\n","outputs(x):  19+669=1188\n","31\n","wrong  : 19+669=1188\n","correct: 19+669=688\n","outputs(x):  98+805=1303\n","17\n","wrong  : 98+805=1303\n","correct: 98+805=903\n","outputs(x):  73+584=1257\n","45\n","wrong  : 73+584=1257\n","correct: 73+584=657\n","outputs(x):  59+597=1256\n","74\n","wrong  : 59+597=1256\n","correct: 59+597=656\n","outputs(x):  57+538=1095\n","88\n","wrong  : 57+538=1095\n","correct: 57+538=595\n","outputs(x):  75+761=1536\n","45\n","wrong  : 75+761=1536\n","correct: 75+761=836\n","outputs(x):  339+54=383\n","649\n","wrong  : 339+54=383\n","correct: 339+54=393\n","outputs(x):  86+391=1277\n","63\n","wrong  : 86+391=1277\n","correct: 86+391=477\n","outputs(x):  45+578=1023\n","93\n","wrong  : 45+578=1023\n","correct: 45+578=623\n","outputs(x):  60+478=1438\n","57\n","wrong  : 60+478=1438\n","correct: 60+478=538\n","outputs(x):  95+577=1572\n","17\n","wrong  : 95+577=1572\n","correct: 95+577=672\n","outputs(x):  992+27=1029\n","35\n","wrong  : 992+27=1029\n","correct: 992+27=1019\n","outputs(x):  813+37=840\n","515\n","wrong  : 813+37=840\n","correct: 813+37=850\n","outputs(x):  62+747=1109\n","69\n","wrong  : 62+747=1109\n","correct: 62+747=809\n","outputs(x):  70+268=538\n","762\n","wrong  : 70+268=538\n","correct: 70+268=338\n","outputs(x):  25+399=1324\n","84\n","wrong  : 25+399=1324\n","correct: 25+399=424\n","outputs(x):  37+647=984\n","57+\n","wrong  : 37+647=984\n","correct: 37+647=684\n","outputs(x):  42+301=643\n","133\n","wrong  : 42+301=643\n","correct: 42+301=343\n","outputs(x):  82+447=1329\n","27\n","wrong  : 82+447=1329\n","correct: 82+447=529\n","outputs(x):  44+824=1668\n","65\n","wrong  : 44+824=1668\n","correct: 44+824=868\n","outputs(x):  35+146=481\n","354\n","wrong  : 35+146=481\n","correct: 35+146=181\n","outputs(x):  82+506=1088\n","46\n","wrong  : 82+506=1088\n","correct: 82+506=588\n","outputs(x):  57+982=1539\n","50\n","wrong  : 57+982=1539\n","correct: 57+982=1039\n","outputs(x):  49+302=851\n","84+\n","wrong  : 49+302=851\n","correct: 49+302=351\n","outputs(x):  29+827=1456\n","17\n","wrong  : 29+827=1456\n","correct: 29+827=856\n","outputs(x):  481+25=406\n","969\n","wrong  : 481+25=406\n","correct: 481+25=506\n","outputs(x):  10+967=1577\n","17\n","wrong  : 10+967=1577\n","correct: 10+967=977\n","outputs(x):  46+235=881\n","550\n","wrong  : 46+235=881\n","correct: 46+235=281\n","outputs(x):  71+932=1303\n","86\n","wrong  : 71+932=1303\n","correct: 71+932=1003\n","outputs(x):  385+64=459\n","597\n","wrong  : 385+64=459\n","correct: 385+64=449\n","outputs(x):  74+414=1088\n","42\n","wrong  : 74+414=1088\n","correct: 74+414=488\n","outputs(x):  787+72=869\n","535\n","wrong  : 787+72=869\n","correct: 787+72=859\n","outputs(x):  646+15=651\n","619\n","wrong  : 646+15=651\n","correct: 646+15=661\n","outputs(x):  92+925=1617\n","4+\n","wrong  : 92+925=1617\n","correct: 92+925=1017\n","outputs(x):  51+223=1174\n","80\n","wrong  : 51+223=1174\n","correct: 51+223=274\n","outputs(x):  88+221=709\n","693\n","wrong  : 88+221=709\n","correct: 88+221=309\n","outputs(x):  344+29=473\n","645\n","wrong  : 344+29=473\n","correct: 344+29=373\n","outputs(x):  45+180=525\n","37+\n","wrong  : 45+180=525\n","correct: 45+180=225\n","outputs(x):  91+104=295\n","525\n","wrong  : 91+104=295\n","correct: 91+104=195\n","outputs(x):  32+350=1182\n","62\n","wrong  : 32+350=1182\n","correct: 32+350=382\n","outputs(x):  57+907=1364\n","13\n","wrong  : 57+907=1364\n","correct: 57+907=964\n","outputs(x):  63+157=420\n","394\n","wrong  : 63+157=420\n","correct: 63+157=220\n","outputs(x):  59+149=808\n","744\n","wrong  : 59+149=808\n","correct: 59+149=208\n","outputs(x):  97+621=1418\n","51\n","wrong  : 97+621=1418\n","correct: 97+621=718\n","outputs(x):  46+776=1222\n","48\n","wrong  : 46+776=1222\n","correct: 46+776=822\n","outputs(x):  909+34=933\n","629\n","wrong  : 909+34=933\n","correct: 909+34=943\n","outputs(x):  90+766=1556\n","20\n","wrong  : 90+766=1556\n","correct: 90+766=856\n","outputs(x):  17+320=537\n","967\n","wrong  : 17+320=537\n","correct: 17+320=337\n","outputs(x):  60+679=1139\n","91\n","wrong  : 60+679=1139\n","correct: 60+679=739\n","outputs(x):  69+942=1211\n","36\n","wrong  : 69+942=1211\n","correct: 69+942=1011\n","outputs(x):  63+415=1278\n","97\n","wrong  : 63+415=1278\n","correct: 63+415=478\n","outputs(x):  91+521=1112\n","25\n","wrong  : 91+521=1112\n","correct: 91+521=612\n","outputs(x):  39+429=1068\n","50\n","wrong  : 39+429=1068\n","correct: 39+429=468\n","outputs(x):  427+74=491\n","530\n","wrong  : 427+74=491\n","correct: 427+74=501\n","outputs(x):  65+162=827\n","679\n","wrong  : 65+162=827\n","correct: 65+162=227\n","outputs(x):  99+221=1120\n","56\n","wrong  : 99+221=1120\n","correct: 99+221=320\n","outputs(x):  29+882=1411\n","17\n","wrong  : 29+882=1411\n","correct: 29+882=911\n","outputs(x):  93+805=1198\n","76\n","wrong  : 93+805=1198\n","correct: 93+805=898\n","outputs(x):  313+37=340\n","79+\n","wrong  : 313+37=340\n","correct: 313+37=350\n","outputs(x):  78+777=1355\n","77\n","wrong  : 78+777=1355\n","correct: 78+777=855\n","outputs(x):  25+241=566\n","956\n","wrong  : 25+241=566\n","correct: 25+241=266\n"," 99% 79/80 [00:03<00:00, 15.17it/s]outputs(x):  3+15=97\n","404+\n","wrong  : 3+15=97\n","correct: 3+15=18\n","outputs(x):  0+10=41\n","50+7\n","wrong  : 0+10=41\n","correct: 0+10=10\n","outputs(x):  2+16=108\n","275\n","wrong  : 2+16=108\n","correct: 2+16=18\n","outputs(x):  8+16=584\n","938\n","wrong  : 8+16=584\n","correct: 8+16=24\n","outputs(x):  9+31=90\n","480+\n","wrong  : 9+31=90\n","correct: 9+31=40\n","outputs(x):  76+3=78\n","936+\n","wrong  : 76+3=78\n","correct: 76+3=79\n","outputs(x):  75+2=7\n","648+0\n","wrong  : 75+2=7\n","correct: 75+2=77\n","outputs(x):  40+2=242\n","280\n","wrong  : 40+2=242\n","correct: 40+2=42\n","outputs(x):  9+23=72\n","623+\n","wrong  : 9+23=72\n","correct: 9+23=32\n","outputs(x):  4+39=93\n","725+\n","wrong  : 4+39=93\n","correct: 4+39=43\n","outputs(x):  78+5=423\n","678\n","wrong  : 78+5=423\n","correct: 78+5=83\n","outputs(x):  41+2=44\n","509+\n","wrong  : 41+2=44\n","correct: 41+2=43\n","outputs(x):  9+21=60\n","598+\n","wrong  : 9+21=60\n","correct: 9+21=30\n","outputs(x):  7+66=153\n","195\n","wrong  : 7+66=153\n","correct: 7+66=73\n","outputs(x):  92+4=876\n","59+\n","wrong  : 92+4=876\n","correct: 92+4=96\n","outputs(x):  26+3=379\n","573\n","wrong  : 26+3=379\n","correct: 26+3=29\n","100% 80/80 [00:03<00:00, 21.83it/s]\n","accuracy of 9900 examples: 8584/9900 (86.70707070707071%)\n","{'carry0': 81.0126582278481, 'carry1': 82.67477203647417, 'carry2': 89.75421972164644, 'carry3': 96.75226586102718, 'carry4': nan, 'carry5': nan}\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 18941.50it/s]\n","100% 80/80 [00:02<00:00, 27.13it/s]\n","accuracy of 10000 examples: 8663/10000 (86.63%)\n","{'carry0': 75.03999999999999, 'carry1': 82.04, 'carry2': 91.0, 'carry3': 98.44000000000001, 'carry4': nan, 'carry5': nan}\n","step 5000: train loss 0.0497, val loss 4.7952\n","saving checkpoint to out2/addition_plain_more_early_eval/ckpt_teaching_addition_plain_10000.pt\n","iter 5000: loss 0.0585, time 37829.34ms, mfu 6.06%\n","saving final checkpoint to out2/addition_plain_more_early_eval\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mteaching_addition_plain_more_early_eval\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/bz746r9c\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250624_035410-bz746r9c/logs\u001b[0m\n"]}],"source":["!python train.py config2/addition/plain/train_addition_bal.py"]},{"cell_type":"markdown","metadata":{"id":"aoCTHpp4OHkb"},"source":["### Reverse"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":463,"status":"ok","timestamp":1750779846403,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"DO-9mFicOil-","outputId":"c72fc746-fc08-48da-9aa5-aa643a1d6b81"},"outputs":[{"name":"stdout","output_type":"stream","text":["# train a miniature character-level shakespeare model\n","dtype = 'float16'\n","init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n","resume_iter = False # if True, resume from saved iter_num, otherwise resume from iter_num 0\n","ckpt_path_name = 'ckpt_teaching_addition_reverse_10000.pt'\n","# good for debugging and playing on macbooks and such\n","out_dir = 'out2/addition_reverse_more_early_eval_v2'\n","eval_interval = 250 # keep frequent because we'll overfit\n","eval_iters = 200\n","log_interval = 10 # don't print too too often\n","\n","# we expect to overfit on this small dataset, so only save when val improves\n","always_save_checkpoint = False\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","wandb_run_name = 'teaching_addition_reverse_more_early_eval_v2'\n","\n","data_type='text'\n","data_format='reverse'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 256\n","block_size = 256 # context of up to 256 previous characters\n","train_data_path = 'train_3digit_10000.txt'\n","# val_data_path = 'val.bin'\n","reverse_c = True\n","eval_addition = True\n","start = \"FILE:data/bal/test_10000.txt\"\n","eval_addition_train = True\n","# start_train = \"FILE:data/one-sided-subtraction/plain/add_examples_10000_trainprompt.txt\"\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually\n","beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n","\n","warmup_iters = 100 # not super necessary potentially\n","\n","device='cuda:0'\n","\n","# on macbook also add\n","# device = 'cpu'  # run on cpu only\n","# compile = False # do not torch compile the model\n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval1 = True\n","early_eval_interval1 = 25\n","early_eval_iters1 = 1200\n","\n","more_early_eval2 = True\n","early_eval_interval2 = 5\n","early_eval_iters2 = 1000"]}],"source":["%cat config2/addition/reverse/train_addition_bal.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1964311,"status":"ok","timestamp":1750790018413,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"_YGWQYIAOKZA","outputId":"5488484b-35a5-441c-ef2b-7ce3300a488d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","wrong  : 704+386=1089\n","correct: 704+386=1090\n","outputs(x):  $902+606=7051$\n","$\n","wrong  : 902+606=1507\n","correct: 902+606=1508\n","outputs(x):  $826+877=3971$\n","$\n","wrong  : 826+877=1793\n","correct: 826+877=1703\n","outputs(x):  $350+759=9001$\n","$\n","wrong  : 350+759=1009\n","correct: 350+759=1109\n","outputs(x):  $101+506=606$\n","$9\n","wrong  : 101+506=606\n","correct: 101+506=607\n","outputs(x):  $282+729=119$\n","$4\n","wrong  : 282+729=911\n","correct: 282+729=1011\n","outputs(x):  $191+838=829$\n","$8\n","wrong  : 191+838=928\n","correct: 191+838=1029\n","outputs(x):  $323+600=328$\n","$7\n","wrong  : 323+600=823\n","correct: 323+600=923\n","outputs(x):  $262+729=198$\n","$1\n","wrong  : 262+729=891\n","correct: 262+729=991\n","outputs(x):  $353+756=8011$\n","$\n","wrong  : 353+756=1108\n","correct: 353+756=1109\n","outputs(x):  $134+963=7801$\n","$\n","wrong  : 134+963=1087\n","correct: 134+963=1097\n","outputs(x):  $217+805=229$\n","$7\n","wrong  : 217+805=922\n","correct: 217+805=1022\n","outputs(x):  $373+477=948$\n","$5\n","wrong  : 373+477=849\n","correct: 373+477=850\n","outputs(x):  $981+866=6481$\n","$\n","wrong  : 981+866=1846\n","correct: 981+866=1847\n","outputs(x):  $162+839=109$\n","$7\n","wrong  : 162+839=901\n","correct: 162+839=1001\n","outputs(x):  $257+427=485$\n","$3\n","wrong  : 257+427=584\n","correct: 257+427=684\n","outputs(x):  $274+267=144$\n","$8\n","wrong  : 274+267=441\n","correct: 274+267=541\n","outputs(x):  $171+274=543$\n","$5\n","wrong  : 171+274=345\n","correct: 171+274=445\n","outputs(x):  $234+769=309$\n","$3\n","wrong  : 234+769=903\n","correct: 234+769=1003\n","outputs(x):  $958+856=4191$\n","$\n","wrong  : 958+856=1914\n","correct: 958+856=1814\n","outputs(x):  $338+715=359$\n","$9\n","wrong  : 338+715=953\n","correct: 338+715=1053\n","outputs(x):  $355+706=169$\n","$2\n","wrong  : 355+706=961\n","correct: 355+706=1061\n","outputs(x):  $237+113=943$\n","$2\n","wrong  : 237+113=349\n","correct: 237+113=350\n","outputs(x):  $522+190=207$\n","$6\n","wrong  : 522+190=702\n","correct: 522+190=712\n","outputs(x):  $283+798=189$\n","$6\n","wrong  : 283+798=981\n","correct: 283+798=1081\n","outputs(x):  $532+277=808$\n","$3\n","wrong  : 532+277=808\n","correct: 532+277=809\n","outputs(x):  $920+988=8981$\n","$\n","wrong  : 920+988=1898\n","correct: 920+988=1908\n","outputs(x):  $217+217=433$\n","$5\n","wrong  : 217+217=334\n","correct: 217+217=434\n","outputs(x):  $820+846=6561$\n","$\n","wrong  : 820+846=1656\n","correct: 820+846=1666\n","outputs(x):  $522+708=9221$\n","$\n","wrong  : 522+708=1229\n","correct: 522+708=1230\n","outputs(x):  $129+289=804$\n","$9\n","wrong  : 129+289=408\n","correct: 129+289=418\n","outputs(x):  $552+761=3031$\n","$\n","wrong  : 552+761=1303\n","correct: 552+761=1313\n","outputs(x):  $822+754=6661$\n","$\n","wrong  : 822+754=1666\n","correct: 822+754=1576\n","outputs(x):  $530+377=798$\n","$1\n","wrong  : 530+377=897\n","correct: 530+377=907\n","outputs(x):  $633+187=918$\n","$6\n","wrong  : 633+187=819\n","correct: 633+187=820\n","outputs(x):  $682+117=897$\n","$3\n","wrong  : 682+117=798\n","correct: 682+117=799\n"," 55% 44/80 [00:02<00:01, 21.46it/s]outputs(x):  $853+448=0031$\n","$\n","wrong  : 853+448=1300\n","correct: 853+448=1301\n","outputs(x):  $185+286=173$\n","$5\n","wrong  : 185+286=371\n","correct: 185+286=471\n","outputs(x):  $992+466=7541$\n","$\n","wrong  : 992+466=1457\n","correct: 992+466=1458\n","outputs(x):  $291+175=663$\n","$8\n","wrong  : 291+175=366\n","correct: 291+175=466\n","outputs(x):  $221+769=088$\n","$5\n","wrong  : 221+769=880\n","correct: 221+769=990\n","outputs(x):  $192+502=495$\n","$9\n","wrong  : 192+502=594\n","correct: 192+502=694\n","outputs(x):  $324+197=115$\n","$9\n","wrong  : 324+197=511\n","correct: 324+197=521\n","outputs(x):  $730+643=3631$\n","$\n","wrong  : 730+643=1363\n","correct: 730+643=1373\n","outputs(x):  $893+477=9631$\n","$\n","wrong  : 893+477=1369\n","correct: 893+477=1370\n","outputs(x):  $992+876=7681$\n","$\n","wrong  : 992+876=1867\n","correct: 992+876=1868\n","outputs(x):  $313+483=597$\n","$6\n","wrong  : 313+483=795\n","correct: 313+483=796\n","outputs(x):  $561+177=737$\n","$6\n","wrong  : 561+177=737\n","correct: 561+177=738\n","outputs(x):  $363+508=078$\n","$1\n","wrong  : 363+508=870\n","correct: 363+508=871\n","outputs(x):  $207+126=332$\n","$9\n","wrong  : 207+126=233\n","correct: 207+126=333\n","outputs(x):  $202+322=424$\n","$9\n","wrong  : 202+322=424\n","correct: 202+322=524\n","outputs(x):  $229+574=397$\n","$7\n","wrong  : 229+574=793\n","correct: 229+574=803\n","outputs(x):  $235+974=9911$\n","$\n","wrong  : 235+974=1199\n","correct: 235+974=1209\n","outputs(x):  $279+862=1401$\n","$\n","wrong  : 279+862=1041\n","correct: 279+862=1141\n","outputs(x):  $752+375=6211$\n","$\n","wrong  : 752+375=1126\n","correct: 752+375=1127\n","outputs(x):  $212+379=194$\n","$9\n","wrong  : 212+379=491\n","correct: 212+379=591\n","outputs(x):  $247+729=678$\n","$4\n","wrong  : 247+729=876\n","correct: 247+729=976\n","outputs(x):  $265+756=129$\n","$3\n","wrong  : 265+756=921\n","correct: 265+756=1021\n","outputs(x):  $733+744=6741$\n","$\n","wrong  : 733+744=1476\n","correct: 733+744=1477\n","outputs(x):  $632+567=8911$\n","$\n","wrong  : 632+567=1198\n","correct: 632+567=1199\n","outputs(x):  $893+996=8881$\n","$\n","wrong  : 893+996=1888\n","correct: 893+996=1889\n","outputs(x):  $920+876=6871$\n","$\n","wrong  : 920+876=1786\n","correct: 920+876=1796\n","outputs(x):  $120+516=635$\n","$1\n","wrong  : 120+516=536\n","correct: 120+516=636\n","outputs(x):  $291+763=459$\n","$9\n","wrong  : 291+763=954\n","correct: 291+763=1054\n","outputs(x):  $621+366=779$\n","$1\n","wrong  : 621+366=977\n","correct: 621+366=987\n","outputs(x):  $482+417=898$\n","$3\n","wrong  : 482+417=898\n","correct: 482+417=899\n","outputs(x):  $732+272=499$\n","$4\n","wrong  : 732+272=994\n","correct: 732+272=1004\n","outputs(x):  $351+846=7901$\n","$\n","wrong  : 351+846=1097\n","correct: 351+846=1197\n","outputs(x):  $117+364=183$\n","$5\n","wrong  : 117+364=381\n","correct: 117+364=481\n","outputs(x):  $383+865=8411$\n","$\n","wrong  : 383+865=1148\n","correct: 383+865=1248\n","outputs(x):  $377+939=6121$\n","$\n","wrong  : 377+939=1216\n","correct: 377+939=1316\n","outputs(x):  $440+310=056$\n","$6\n","wrong  : 440+310=650\n","correct: 440+310=750\n","outputs(x):  $219+743=268$\n","$9\n","wrong  : 219+743=862\n","correct: 219+743=962\n","outputs(x):  $850+700=0651$\n","$\n","wrong  : 850+700=1560\n","correct: 850+700=1550\n","outputs(x):  $161+240=103$\n","$4\n","wrong  : 161+240=301\n","correct: 161+240=401\n","outputs(x):  $132+580=207$\n","$6\n","wrong  : 132+580=702\n","correct: 132+580=712\n","outputs(x):  $254+948=2011$\n","$\n","wrong  : 254+948=1102\n","correct: 254+948=1202\n","outputs(x):  $221+475=685$\n","$5\n","wrong  : 221+475=586\n","correct: 221+475=696\n","outputs(x):  $822+381=3911$\n","$\n","wrong  : 822+381=1193\n","correct: 822+381=1203\n","outputs(x):  $256+838=499$\n","$7\n","wrong  : 256+838=994\n","correct: 256+838=1094\n","outputs(x):  $206+540=646$\n","$2\n","wrong  : 206+540=646\n","correct: 206+540=746\n","outputs(x):  $308+755=369$\n","$6\n","wrong  : 308+755=963\n","correct: 308+755=1063\n","outputs(x):  $424+344=857$\n","$9\n","wrong  : 424+344=758\n","correct: 424+344=768\n","outputs(x):  $211+184=583$\n","$1\n","wrong  : 211+184=385\n","correct: 211+184=395\n","outputs(x):  $503+996=8941$\n","$\n","wrong  : 503+996=1498\n","correct: 503+996=1499\n","outputs(x):  $983+567=9451$\n","$\n","wrong  : 983+567=1549\n","correct: 983+567=1550\n","outputs(x):  $423+946=8531$\n","$\n","wrong  : 423+946=1358\n","correct: 423+946=1369\n","outputs(x):  $392+786=7701$\n","$\n","wrong  : 392+786=1077\n","correct: 392+786=1178\n","outputs(x):  $399+718=7101$\n","$\n","wrong  : 399+718=1017\n","correct: 399+718=1117\n","outputs(x):  $324+389=307$\n","$9\n","wrong  : 324+389=703\n","correct: 324+389=713\n","outputs(x):  $269+885=4501$\n","$\n","wrong  : 269+885=1054\n","correct: 269+885=1154\n","outputs(x):  $662+727=8831$\n","$\n","wrong  : 662+727=1388\n","correct: 662+727=1389\n","outputs(x):  $226+285=104$\n","$8\n","wrong  : 226+285=401\n","correct: 226+285=511\n","outputs(x):  $222+203=523$\n","$2\n","wrong  : 222+203=325\n","correct: 222+203=425\n","outputs(x):  $203+345=744$\n","$5\n","wrong  : 203+345=447\n","correct: 203+345=548\n","outputs(x):  $837+478=5031$\n","$\n","wrong  : 837+478=1305\n","correct: 837+478=1315\n","outputs(x):  $365+624=988$\n","$3\n","wrong  : 365+624=889\n","correct: 365+624=989\n","outputs(x):  $229+777=698$\n","$1\n","wrong  : 229+777=896\n","correct: 229+777=1006\n","outputs(x):  $192+466=756$\n","$7\n","wrong  : 192+466=657\n","correct: 192+466=658\n","outputs(x):  $224+740=468$\n","$4\n","wrong  : 224+740=864\n","correct: 224+740=964\n","outputs(x):  $247+628=577$\n","$2\n","wrong  : 247+628=775\n","correct: 247+628=875\n","outputs(x):  $811+816=6261$\n","$\n","wrong  : 811+816=1626\n","correct: 811+816=1627\n","outputs(x):  $631+792=3251$\n","$\n","wrong  : 631+792=1523\n","correct: 631+792=1423\n","outputs(x):  $182+875=6501$\n","$\n","wrong  : 182+875=1056\n","correct: 182+875=1057\n","outputs(x):  $712+806=7151$\n","$\n","wrong  : 712+806=1517\n","correct: 712+806=1518\n","outputs(x):  $144+846=098$\n","$6\n","wrong  : 144+846=890\n","correct: 144+846=990\n","outputs(x):  $803+285=7801$\n","$\n","wrong  : 803+285=1087\n","correct: 803+285=1088\n","outputs(x):  $897+568=5741$\n","$\n","wrong  : 897+568=1475\n","correct: 897+568=1465\n","outputs(x):  $134+973=7901$\n","$\n","wrong  : 134+973=1097\n","correct: 134+973=1107\n","outputs(x):  $321+395=517$\n","$7\n","wrong  : 321+395=715\n","correct: 321+395=716\n","outputs(x):  $138+627=667$\n","$8\n","wrong  : 138+627=766\n","correct: 138+627=765\n","outputs(x):  $632+978=9951$\n","$\n","wrong  : 632+978=1599\n","correct: 632+978=1610\n"," 59% 47/80 [00:02<00:01, 21.43it/s]outputs(x):  $193+266=853$\n","$5\n","wrong  : 193+266=358\n","correct: 193+266=459\n","outputs(x):  $127+385=205$\n","$5\n","wrong  : 127+385=502\n","correct: 127+385=512\n","outputs(x):  $724+839=3551$\n","$\n","wrong  : 724+839=1553\n","correct: 724+839=1563\n","outputs(x):  $281+517=797$\n","$3\n","wrong  : 281+517=797\n","correct: 281+517=798\n","outputs(x):  $962+258=9121$\n","$\n","wrong  : 962+258=1219\n","correct: 962+258=1220\n","outputs(x):  $165+543=707$\n","$3\n","wrong  : 165+543=707\n","correct: 165+543=708\n","outputs(x):  $401+176=675$\n","$9\n","wrong  : 401+176=576\n","correct: 401+176=577\n","outputs(x):  $114+141=542$\n","$6\n","wrong  : 114+141=245\n","correct: 114+141=255\n","outputs(x):  $157+157=412$\n","$8\n","wrong  : 157+157=214\n","correct: 157+157=314\n","outputs(x):  $304+246=944$\n","$2\n","wrong  : 304+246=449\n","correct: 304+246=550\n","outputs(x):  $397+756=3501$\n","$\n","wrong  : 397+756=1053\n","correct: 397+756=1153\n","outputs(x):  $542+118=956$\n","$8\n","wrong  : 542+118=659\n","correct: 542+118=660\n","outputs(x):  $822+676=7841$\n","$\n","wrong  : 822+676=1487\n","correct: 822+676=1498\n","outputs(x):  $461+838=8921$\n","$\n","wrong  : 461+838=1298\n","correct: 461+838=1299\n","outputs(x):  $264+893=7501$\n","$\n","wrong  : 264+893=1057\n","correct: 264+893=1157\n","outputs(x):  $253+188=043$\n","$7\n","wrong  : 253+188=340\n","correct: 253+188=441\n","outputs(x):  $355+949=4021$\n","$\n","wrong  : 355+949=1204\n","correct: 355+949=1304\n","outputs(x):  $902+396=7921$\n","$\n","wrong  : 902+396=1297\n","correct: 902+396=1298\n","outputs(x):  $133+564=686$\n","$3\n","wrong  : 133+564=686\n","correct: 133+564=697\n","outputs(x):  $258+183=143$\n","$3\n","wrong  : 258+183=341\n","correct: 258+183=441\n","outputs(x):  $225+878=3001$\n","$\n","wrong  : 225+878=1003\n","correct: 225+878=1103\n","outputs(x):  $622+136=757$\n","$7\n","wrong  : 622+136=757\n","correct: 622+136=758\n","outputs(x):  $223+887=0001$\n","$\n","wrong  : 223+887=1000\n","correct: 223+887=1110\n","outputs(x):  $229+766=588$\n","$8\n","wrong  : 229+766=885\n","correct: 229+766=995\n","outputs(x):  $191+976=6611$\n","$\n","wrong  : 191+976=1166\n","correct: 191+976=1167\n","outputs(x):  $581+928=8051$\n","$\n","wrong  : 581+928=1508\n","correct: 581+928=1509\n","outputs(x):  $346+313=955$\n","$4\n","wrong  : 346+313=559\n","correct: 346+313=659\n","outputs(x):  $264+234=893$\n","$7\n","wrong  : 264+234=398\n","correct: 264+234=498\n","outputs(x):  $386+609=598$\n","$9\n","wrong  : 386+609=895\n","correct: 386+609=995\n","outputs(x):  $513+155=766$\n","$4\n","wrong  : 513+155=667\n","correct: 513+155=668\n","outputs(x):  $676+827=3061$\n","$\n","wrong  : 676+827=1603\n","correct: 676+827=1503\n","outputs(x):  $269+781=069$\n","$1\n","wrong  : 269+781=960\n","correct: 269+781=1050\n","outputs(x):  $238+315=354$\n","$8\n","wrong  : 238+315=453\n","correct: 238+315=553\n","outputs(x):  $733+707=9341$\n","$\n","wrong  : 733+707=1439\n","correct: 733+707=1440\n","outputs(x):  $210+450=065$\n","$6\n","wrong  : 210+450=560\n","correct: 210+450=660\n","outputs(x):  $591+528=8111$\n","$\n","wrong  : 591+528=1118\n","correct: 591+528=1119\n","outputs(x):  $627+171=887$\n","$4\n","wrong  : 627+171=788\n","correct: 627+171=798\n","outputs(x):  $361+796=7501$\n","$\n","wrong  : 361+796=1057\n","correct: 361+796=1157\n","outputs(x):  $694+663=7541$\n","$\n","wrong  : 694+663=1457\n","correct: 694+663=1357\n","outputs(x):  $753+187=939$\n","$2\n","wrong  : 753+187=939\n","correct: 753+187=940\n","outputs(x):  $202+755=758$\n","$3\n","wrong  : 202+755=857\n","correct: 202+755=957\n","outputs(x):  $361+750=1101$\n","$\n","wrong  : 361+750=1011\n","correct: 361+750=1111\n","outputs(x):  $993+688=1871$\n","$\n","wrong  : 993+688=1781\n","correct: 993+688=1681\n","outputs(x):  $681+477=7511$\n","$\n","wrong  : 681+477=1157\n","correct: 681+477=1158\n","outputs(x):  $732+726=7541$\n","$\n","wrong  : 732+726=1457\n","correct: 732+726=1458\n","outputs(x):  $257+178=533$\n","$9\n","wrong  : 257+178=335\n","correct: 257+178=435\n","outputs(x):  $120+383=394$\n","$9\n","wrong  : 120+383=493\n","correct: 120+383=503\n","outputs(x):  $571+887=7541$\n","$\n","wrong  : 571+887=1457\n","correct: 571+887=1458\n","outputs(x):  $852+316=7611$\n","$\n","wrong  : 852+316=1167\n","correct: 852+316=1168\n","outputs(x):  $201+108=803$\n","$1\n","wrong  : 201+108=308\n","correct: 201+108=309\n","outputs(x):  $434+817=0521$\n","$\n","wrong  : 434+817=1250\n","correct: 434+817=1251\n","outputs(x):  $701+186=688$\n","$1\n","wrong  : 701+186=886\n","correct: 701+186=887\n","outputs(x):  $572+927=8941$\n","$\n","wrong  : 572+927=1498\n","correct: 572+927=1499\n","outputs(x):  $192+578=967$\n","$3\n","wrong  : 192+578=769\n","correct: 192+578=770\n","outputs(x):  $117+283=003$\n","$2\n","wrong  : 117+283=300\n","correct: 117+283=400\n","outputs(x):  $191+877=7601$\n","$\n","wrong  : 191+877=1067\n","correct: 191+877=1068\n","outputs(x):  $206+829=539$\n","$1\n","wrong  : 206+829=935\n","correct: 206+829=1035\n","outputs(x):  $134+833=768$\n","$9\n","wrong  : 134+833=867\n","correct: 134+833=967\n","outputs(x):  $213+641=448$\n","$3\n","wrong  : 213+641=844\n","correct: 213+641=854\n","outputs(x):  $832+718=9451$\n","$\n","wrong  : 832+718=1549\n","correct: 832+718=1550\n","outputs(x):  $285+890=5701$\n","$\n","wrong  : 285+890=1075\n","correct: 285+890=1175\n","outputs(x):  $137+977=4011$\n","$\n","wrong  : 137+977=1104\n","correct: 137+977=1114\n","outputs(x):  $148+152=002$\n","$8\n","wrong  : 148+152=200\n","correct: 148+152=300\n","outputs(x):  $583+746=8231$\n","$\n","wrong  : 583+746=1328\n","correct: 583+746=1329\n","outputs(x):  $144+900=449$\n","$1\n","wrong  : 144+900=944\n","correct: 144+900=1044\n","outputs(x):  $144+297=143$\n","$7\n","wrong  : 144+297=341\n","correct: 144+297=441\n","outputs(x):  $243+230=373$\n","$7\n","wrong  : 243+230=373\n","correct: 243+230=473\n","outputs(x):  $373+905=7721$\n","$\n","wrong  : 373+905=1277\n","correct: 373+905=1278\n","outputs(x):  $306+407=316$\n","$6\n","wrong  : 306+407=613\n","correct: 306+407=713\n","outputs(x):  $311+660=178$\n","$9\n","wrong  : 311+660=871\n","correct: 311+660=971\n","outputs(x):  $852+857=8071$\n","$\n","wrong  : 852+857=1708\n","correct: 852+857=1709\n","outputs(x):  $133+726=858$\n","$5\n","wrong  : 133+726=858\n","correct: 133+726=859\n","outputs(x):  $723+540=3521$\n","$\n","wrong  : 723+540=1253\n","correct: 723+540=1263\n","outputs(x):  $602+437=8301$\n","$\n","wrong  : 602+437=1038\n","correct: 602+437=1039\n","outputs(x):  $438+470=898$\n","$7\n","wrong  : 438+470=898\n","correct: 438+470=908\n","outputs(x):  $113+967=9701$\n","$\n","wrong  : 113+967=1079\n","correct: 113+967=1080\n","outputs(x):  $751+898=8461$\n","$\n","wrong  : 751+898=1648\n","correct: 751+898=1649\n","outputs(x):  $101+843=448$\n","$9\n","wrong  : 101+843=844\n","correct: 101+843=944\n","outputs(x):  $764+166=929$\n","$6\n","wrong  : 764+166=929\n","correct: 764+166=930\n","outputs(x):  $852+438=9821$\n","$\n","wrong  : 852+438=1289\n","correct: 852+438=1290\n","outputs(x):  $232+178=903$\n","$8\n","wrong  : 232+178=309\n","correct: 232+178=410\n","outputs(x):  $784+766=0561$\n","$\n","wrong  : 784+766=1650\n","correct: 784+766=1550\n","outputs(x):  $288+969=7511$\n","$\n","wrong  : 288+969=1157\n","correct: 288+969=1257\n","outputs(x):  $115+848=368$\n","$1\n","wrong  : 115+848=863\n","correct: 115+848=963\n"," 62% 50/80 [00:02<00:01, 21.19it/s]outputs(x):  $180+147=722$\n","$1\n","wrong  : 180+147=227\n","correct: 180+147=327\n","outputs(x):  $204+220=423$\n","$6\n","wrong  : 204+220=324\n","correct: 204+220=424\n","outputs(x):  $113+822=538$\n","$3\n","wrong  : 113+822=835\n","correct: 113+822=935\n","outputs(x):  $832+851=3871$\n","$\n","wrong  : 832+851=1783\n","correct: 832+851=1683\n","outputs(x):  $344+993=7321$\n","$\n","wrong  : 344+993=1237\n","correct: 344+993=1337\n","outputs(x):  $731+580=1031$\n","$\n","wrong  : 731+580=1301\n","correct: 731+580=1311\n","outputs(x):  $662+422=3801$\n","$\n","wrong  : 662+422=1083\n","correct: 662+422=1084\n","outputs(x):  $322+451=276$\n","$5\n","wrong  : 322+451=672\n","correct: 322+451=773\n","outputs(x):  $202+138=932$\n","$2\n","wrong  : 202+138=239\n","correct: 202+138=340\n","outputs(x):  $152+346=794$\n","$3\n","wrong  : 152+346=497\n","correct: 152+346=498\n","outputs(x):  $134+499=326$\n","$6\n","wrong  : 134+499=623\n","correct: 134+499=633\n","outputs(x):  $142+216=753$\n","$9\n","wrong  : 142+216=357\n","correct: 142+216=358\n","outputs(x):  $362+214=574$\n","$6\n","wrong  : 362+214=475\n","correct: 362+214=576\n","outputs(x):  $311+824=5301$\n","$\n","wrong  : 311+824=1035\n","correct: 311+824=1135\n","outputs(x):  $912+872=4771$\n","$\n","wrong  : 912+872=1774\n","correct: 912+872=1784\n","outputs(x):  $200+758=858$\n","$5\n","wrong  : 200+758=858\n","correct: 200+758=958\n","outputs(x):  $383+236=816$\n","$6\n","wrong  : 383+236=618\n","correct: 383+236=619\n","outputs(x):  $268+833=1001$\n","$\n","wrong  : 268+833=1001\n","correct: 268+833=1101\n","outputs(x):  $245+216=163$\n","$9\n","wrong  : 245+216=361\n","correct: 245+216=461\n","outputs(x):  $635+449=4701$\n","$\n","wrong  : 635+449=1074\n","correct: 635+449=1084\n","outputs(x):  $126+841=759$\n","$3\n","wrong  : 126+841=957\n","correct: 126+841=967\n","outputs(x):  $251+245=693$\n","$8\n","wrong  : 251+245=396\n","correct: 251+245=496\n","outputs(x):  $502+653=4511$\n","$\n","wrong  : 502+653=1154\n","correct: 502+653=1155\n","outputs(x):  $204+840=449$\n","$4\n","wrong  : 204+840=944\n","correct: 204+840=1044\n","outputs(x):  $243+700=348$\n","$2\n","wrong  : 243+700=843\n","correct: 243+700=943\n","outputs(x):  $213+188=103$\n","$5\n","wrong  : 213+188=301\n","correct: 213+188=401\n","outputs(x):  $297+734=139$\n","$4\n","wrong  : 297+734=931\n","correct: 297+734=1031\n","outputs(x):  $623+557=9711$\n","$\n","wrong  : 623+557=1179\n","correct: 623+557=1180\n","outputs(x):  $228+177=503$\n","$1\n","wrong  : 228+177=305\n","correct: 228+177=405\n","outputs(x):  $241+118=853$\n","$4\n","wrong  : 241+118=358\n","correct: 241+118=359\n","outputs(x):  $322+535=748$\n","$8\n","wrong  : 322+535=847\n","correct: 322+535=857\n","outputs(x):  $183+116=892$\n","$6\n","wrong  : 183+116=298\n","correct: 183+116=299\n","outputs(x):  $431+296=627$\n","$6\n","wrong  : 431+296=726\n","correct: 431+296=727\n","outputs(x):  $923+180=3901$\n","$\n","wrong  : 923+180=1093\n","correct: 923+180=1103\n","outputs(x):  $820+763=3751$\n","$\n","wrong  : 820+763=1573\n","correct: 820+763=1583\n","outputs(x):  $535+198=327$\n","$3\n","wrong  : 535+198=723\n","correct: 535+198=733\n","outputs(x):  $753+667=9141$\n","$\n","wrong  : 753+667=1419\n","correct: 753+667=1420\n","outputs(x):  $329+342=175$\n","$9\n","wrong  : 329+342=571\n","correct: 329+342=671\n","outputs(x):  $205+900=5001$\n","$\n","wrong  : 205+900=1005\n","correct: 205+900=1105\n","outputs(x):  $128+272=093$\n","$3\n","wrong  : 128+272=390\n","correct: 128+272=400\n","outputs(x):  $611+317=729$\n","$2\n","wrong  : 611+317=927\n","correct: 611+317=928\n","outputs(x):  $710+982=2861$\n","$\n","wrong  : 710+982=1682\n","correct: 710+982=1692\n","outputs(x):  $830+674=4941$\n","$\n","wrong  : 830+674=1494\n","correct: 830+674=1504\n","outputs(x):  $271+415=685$\n","$5\n","wrong  : 271+415=586\n","correct: 271+415=686\n","outputs(x):  $924+772=6861$\n","$\n","wrong  : 924+772=1686\n","correct: 924+772=1696\n","outputs(x):  $822+466=7821$\n","$\n","wrong  : 822+466=1287\n","correct: 822+466=1288\n","outputs(x):  $771+206=679$\n","$6\n","wrong  : 771+206=976\n","correct: 771+206=977\n","outputs(x):  $340+806=6401$\n","$\n","wrong  : 340+806=1046\n","correct: 340+806=1146\n","outputs(x):  $324+274=885$\n","$9\n","wrong  : 324+274=588\n","correct: 324+274=598\n","outputs(x):  $442+637=8701$\n","$\n","wrong  : 442+637=1078\n","correct: 442+637=1079\n","outputs(x):  $201+781=288$\n","$7\n","wrong  : 201+781=882\n","correct: 201+781=982\n","outputs(x):  $234+334=864$\n","$6\n","wrong  : 234+334=468\n","correct: 234+334=568\n","outputs(x):  $818+996=3181$\n","$\n","wrong  : 818+996=1813\n","correct: 818+996=1814\n","outputs(x):  $926+853=8771$\n","$\n","wrong  : 926+853=1778\n","correct: 926+853=1779\n","outputs(x):  $215+885=0001$\n","$\n","wrong  : 215+885=1000\n","correct: 215+885=1100\n","outputs(x):  $218+324=244$\n","$3\n","wrong  : 218+324=442\n","correct: 218+324=542\n","outputs(x):  $992+596=7851$\n","$\n","wrong  : 992+596=1587\n","correct: 992+596=1588\n","outputs(x):  $224+753=778$\n","$8\n","wrong  : 224+753=877\n","correct: 224+753=977\n","outputs(x):  $337+335=275$\n","$2\n","wrong  : 337+335=572\n","correct: 337+335=672\n","outputs(x):  $851+196=6401$\n","$\n","wrong  : 851+196=1046\n","correct: 851+196=1047\n","outputs(x):  $152+288=933$\n","$2\n","wrong  : 152+288=339\n","correct: 152+288=440\n","outputs(x):  $286+894=0801$\n","$\n","wrong  : 286+894=1080\n","correct: 286+894=1180\n","outputs(x):  $782+276=7501$\n","$\n","wrong  : 782+276=1057\n","correct: 782+276=1058\n","outputs(x):  $102+837=839$\n","$9\n","wrong  : 102+837=938\n","correct: 102+837=939\n","outputs(x):  $101+905=5001$\n","$\n","wrong  : 101+905=1005\n","correct: 101+905=1006\n","outputs(x):  $208+705=318$\n","$8\n","wrong  : 208+705=813\n","correct: 208+705=913\n","outputs(x):  $221+576=786$\n","$2\n","wrong  : 221+576=687\n","correct: 221+576=797\n","outputs(x):  $514+772=6831$\n","$\n","wrong  : 514+772=1386\n","correct: 514+772=1286\n","outputs(x):  $163+165=723$\n","$3\n","wrong  : 163+165=327\n","correct: 163+165=328\n","outputs(x):  $342+816=7501$\n","$\n","wrong  : 342+816=1057\n","correct: 342+816=1158\n","outputs(x):  $221+513=436$\n","$8\n","wrong  : 221+513=634\n","correct: 221+513=734\n","outputs(x):  $190+866=6591$\n","$\n","wrong  : 190+866=1956\n","correct: 190+866=1056\n","outputs(x):  $785+856=0461$\n","$\n","wrong  : 785+856=1640\n","correct: 785+856=1641\n","outputs(x):  $129+889=8001$\n","$\n","wrong  : 129+889=1008\n","correct: 129+889=1018\n","outputs(x):  $218+254=273$\n","$6\n","wrong  : 218+254=372\n","correct: 218+254=472\n","outputs(x):  $922+218=9311$\n","$\n","wrong  : 922+218=1139\n","correct: 922+218=1140\n","outputs(x):  $288+136=423$\n","$4\n","wrong  : 288+136=324\n","correct: 288+136=424\n","outputs(x):  $813+976=8871$\n","$\n","wrong  : 813+976=1788\n","correct: 813+976=1789\n","outputs(x):  $342+692=3301$\n","$\n","wrong  : 342+692=1033\n","correct: 342+692=1034\n","outputs(x):  $372+613=588$\n","$5\n","wrong  : 372+613=885\n","correct: 372+613=985\n","outputs(x):  $276+975=1511$\n","$\n","wrong  : 276+975=1151\n","correct: 276+975=1251\n","outputs(x):  $733+981=4071$\n","$\n","wrong  : 733+981=1704\n","correct: 733+981=1714\n","outputs(x):  $247+231=873$\n","$5\n","wrong  : 247+231=378\n","correct: 247+231=478\n","outputs(x):  $218+265=383$\n","$5\n","wrong  : 218+265=383\n","correct: 218+265=483\n","outputs(x):  $183+897=9701$\n","$\n","wrong  : 183+897=1079\n","correct: 183+897=1080\n","outputs(x):  $412+298=907$\n","$4\n","wrong  : 412+298=709\n","correct: 412+298=710\n","outputs(x):  $331+457=787$\n","$8\n","wrong  : 331+457=787\n","correct: 331+457=788\n","outputs(x):  $131+896=6201$\n","$\n","wrong  : 131+896=1026\n","correct: 131+896=1027\n"," 66% 53/80 [00:02<00:01, 20.90it/s]outputs(x):  $672+878=0561$\n","$\n","wrong  : 672+878=1650\n","correct: 672+878=1550\n","outputs(x):  $810+640=0441$\n","$\n","wrong  : 810+640=1440\n","correct: 810+640=1450\n","outputs(x):  $591+728=8131$\n","$\n","wrong  : 591+728=1318\n","correct: 591+728=1319\n","outputs(x):  $242+568=908$\n","$7\n","wrong  : 242+568=809\n","correct: 242+568=810\n","outputs(x):  $533+543=5701$\n","$\n","wrong  : 533+543=1075\n","correct: 533+543=1076\n","outputs(x):  $114+316=033$\n","$2\n","wrong  : 114+316=330\n","correct: 114+316=430\n","outputs(x):  $213+325=834$\n","$5\n","wrong  : 213+325=438\n","correct: 213+325=538\n","outputs(x):  $414+241=546$\n","$6\n","wrong  : 414+241=645\n","correct: 414+241=655\n","outputs(x):  $603+372=479$\n","$7\n","wrong  : 603+372=974\n","correct: 603+372=975\n","outputs(x):  $284+122=603$\n","$6\n","wrong  : 284+122=306\n","correct: 284+122=406\n","outputs(x):  $833+947=9671$\n","$\n","wrong  : 833+947=1769\n","correct: 833+947=1780\n","outputs(x):  $941+477=8041$\n","$\n","wrong  : 941+477=1408\n","correct: 941+477=1418\n","outputs(x):  $516+951=7541$\n","$\n","wrong  : 516+951=1457\n","correct: 516+951=1467\n","outputs(x):  $227+584=108$\n","$7\n","wrong  : 227+584=801\n","correct: 227+584=811\n","outputs(x):  $673+115=787$\n","$2\n","wrong  : 673+115=787\n","correct: 673+115=788\n","outputs(x):  $812+887=8961$\n","$\n","wrong  : 812+887=1698\n","correct: 812+887=1699\n","outputs(x):  $312+333=446$\n","$2\n","wrong  : 312+333=644\n","correct: 312+333=645\n","outputs(x):  $555+553=7011$\n","$\n","wrong  : 555+553=1107\n","correct: 555+553=1108\n","outputs(x):  $821+428=8421$\n","$\n","wrong  : 821+428=1248\n","correct: 821+428=1249\n","outputs(x):  $481+158=836$\n","$5\n","wrong  : 481+158=638\n","correct: 481+158=639\n","outputs(x):  $286+883=9601$\n","$\n","wrong  : 286+883=1069\n","correct: 286+883=1169\n","outputs(x):  $553+847=9931$\n","$\n","wrong  : 553+847=1399\n","correct: 553+847=1400\n","outputs(x):  $622+685=6031$\n","$\n","wrong  : 622+685=1306\n","correct: 622+685=1307\n","outputs(x):  $243+418=165$\n","$4\n","wrong  : 243+418=561\n","correct: 243+418=661\n","outputs(x):  $903+887=9871$\n","$\n","wrong  : 903+887=1789\n","correct: 903+887=1790\n","outputs(x):  $272+813=589$\n","$8\n","wrong  : 272+813=985\n","correct: 272+813=1085\n","outputs(x):  $731+871=2961$\n","$\n","wrong  : 731+871=1692\n","correct: 731+871=1602\n","outputs(x):  $602+494=5901$\n","$\n","wrong  : 602+494=1095\n","correct: 602+494=1096\n","outputs(x):  $121+861=279$\n","$8\n","wrong  : 121+861=972\n","correct: 121+861=982\n","outputs(x):  $933+758=0961$\n","$\n","wrong  : 933+758=1690\n","correct: 933+758=1691\n","outputs(x):  $921+282=3911$\n","$\n","wrong  : 921+282=1193\n","correct: 921+282=1203\n","outputs(x):  $281+308=885$\n","$7\n","wrong  : 281+308=588\n","correct: 281+308=589\n","outputs(x):  $324+855=9601$\n","$\n","wrong  : 324+855=1069\n","correct: 324+855=1179\n","outputs(x):  $645+768=3041$\n","$\n","wrong  : 645+768=1403\n","correct: 645+768=1413\n","outputs(x):  $824+573=7831$\n","$\n","wrong  : 824+573=1387\n","correct: 824+573=1397\n","outputs(x):  $893+815=7071$\n","$\n","wrong  : 893+815=1707\n","correct: 893+815=1708\n","outputs(x):  $321+299=025$\n","$9\n","wrong  : 321+299=520\n","correct: 321+299=620\n","outputs(x):  $737+352=8801$\n","$\n","wrong  : 737+352=1088\n","correct: 737+352=1089\n","outputs(x):  $152+895=6401$\n","$\n","wrong  : 152+895=1046\n","correct: 152+895=1047\n","outputs(x):  $266+302=864$\n","$6\n","wrong  : 266+302=468\n","correct: 266+302=568\n","outputs(x):  $183+235=813$\n","$8\n","wrong  : 183+235=318\n","correct: 183+235=418\n","outputs(x):  $272+149=123$\n","$6\n","wrong  : 272+149=321\n","correct: 272+149=421\n","outputs(x):  $246+739=588$\n","$4\n","wrong  : 246+739=885\n","correct: 246+739=985\n","outputs(x):  $221+483=495$\n","$7\n","wrong  : 221+483=594\n","correct: 221+483=704\n","outputs(x):  $266+888=4501$\n","$\n","wrong  : 266+888=1054\n","correct: 266+888=1154\n","outputs(x):  $616+716=2231$\n","$\n","wrong  : 616+716=1322\n","correct: 616+716=1332\n","outputs(x):  $266+807=379$\n","$6\n","wrong  : 266+807=973\n","correct: 266+807=1073\n","outputs(x):  $301+845=5411$\n","$\n","wrong  : 301+845=1145\n","correct: 301+845=1146\n","outputs(x):  $217+481=895$\n","$2\n","wrong  : 217+481=598\n","correct: 217+481=698\n","outputs(x):  $216+942=8411$\n","$\n","wrong  : 216+942=1148\n","correct: 216+942=1158\n","outputs(x):  $571+367=739$\n","$6\n","wrong  : 571+367=937\n","correct: 571+367=938\n","outputs(x):  $932+883=5081$\n","$\n","wrong  : 932+883=1805\n","correct: 932+883=1815\n","outputs(x):  $614+796=0151$\n","$\n","wrong  : 614+796=1510\n","correct: 614+796=1410\n","outputs(x):  $382+418=997$\n","$6\n","wrong  : 382+418=799\n","correct: 382+418=800\n","outputs(x):  $463+977=9341$\n","$\n","wrong  : 463+977=1439\n","correct: 463+977=1440\n","outputs(x):  $235+240=573$\n","$4\n","wrong  : 235+240=375\n","correct: 235+240=475\n","outputs(x):  $343+502=448$\n","$6\n","wrong  : 343+502=844\n","correct: 343+502=845\n","outputs(x):  $134+246=082$\n","$1\n","wrong  : 134+246=280\n","correct: 134+246=380\n","outputs(x):  $521+307=728$\n","$2\n","wrong  : 521+307=827\n","correct: 521+307=828\n","outputs(x):  $534+344=868$\n","$4\n","wrong  : 534+344=868\n","correct: 534+344=878\n","outputs(x):  $248+136=482$\n","$8\n","wrong  : 248+136=284\n","correct: 248+136=384\n","outputs(x):  $368+413=186$\n","$5\n","wrong  : 368+413=681\n","correct: 368+413=781\n","outputs(x):  $693+854=6451$\n","$\n","wrong  : 693+854=1546\n","correct: 693+854=1547\n","outputs(x):  $818+171=988$\n","$8\n","wrong  : 818+171=889\n","correct: 818+171=989\n","outputs(x):  $822+718=9351$\n","$\n","wrong  : 822+718=1539\n","correct: 822+718=1540\n","outputs(x):  $731+705=5341$\n","$\n","wrong  : 731+705=1435\n","correct: 731+705=1436\n","outputs(x):  $320+770=0801$\n","$\n","wrong  : 320+770=1080\n","correct: 320+770=1090\n","outputs(x):  $463+203=566$\n","$7\n","wrong  : 463+203=665\n","correct: 463+203=666\n","outputs(x):  $881+916=6971$\n","$\n","wrong  : 881+916=1796\n","correct: 881+916=1797\n","outputs(x):  $623+475=8801$\n","$\n","wrong  : 623+475=1088\n","correct: 623+475=1098\n","outputs(x):  $753+118=078$\n","$1\n","wrong  : 753+118=870\n","correct: 753+118=871\n","outputs(x):  $231+198=824$\n","$7\n","wrong  : 231+198=428\n","correct: 231+198=429\n","outputs(x):  $301+781=289$\n","$7\n","wrong  : 301+781=982\n","correct: 301+781=1082\n","outputs(x):  $175+174=942$\n","$5\n","wrong  : 175+174=249\n","correct: 175+174=349\n","outputs(x):  $722+514=5321$\n","$\n","wrong  : 722+514=1235\n","correct: 722+514=1236\n","outputs(x):  $637+662=9821$\n","$\n","wrong  : 637+662=1289\n","correct: 637+662=1299\n","outputs(x):  $844+775=9061$\n","$\n","wrong  : 844+775=1609\n","correct: 844+775=1619\n","outputs(x):  $242+917=8501$\n","$\n","wrong  : 242+917=1058\n","correct: 242+917=1159\n","outputs(x):  $793+817=9061$\n","$\n","wrong  : 793+817=1609\n","correct: 793+817=1610\n","outputs(x):  $271+301=274$\n","$6\n","wrong  : 271+301=472\n","correct: 271+301=572\n","outputs(x):  $673+746=8141$\n","$\n","wrong  : 673+746=1418\n","correct: 673+746=1419\n","outputs(x):  $250+401=155$\n","$7\n","wrong  : 250+401=551\n","correct: 250+401=651\n","outputs(x):  $911+965=5781$\n","$\n","wrong  : 911+965=1875\n","correct: 911+965=1876\n","outputs(x):  $211+605=617$\n","$6\n","wrong  : 211+605=716\n","correct: 211+605=816\n","outputs(x):  $141+906=6401$\n","$\n","wrong  : 141+906=1046\n","correct: 141+906=1047\n","outputs(x):  $287+671=858$\n","$6\n","wrong  : 287+671=858\n","correct: 287+671=958\n","outputs(x):  $640+632=2621$\n","$\n","wrong  : 640+632=1262\n","correct: 640+632=1272\n","outputs(x):  $322+634=559$\n","$8\n","wrong  : 322+634=955\n","correct: 322+634=956\n","outputs(x):  $317+656=378$\n","$8\n","wrong  : 317+656=873\n","correct: 317+656=973\n","outputs(x):  $731+597=7231$\n","$\n","wrong  : 731+597=1327\n","correct: 731+597=1328\n","outputs(x):  $521+143=366$\n","$8\n","wrong  : 521+143=663\n","correct: 521+143=664\n","outputs(x):  $220+627=747$\n","$6\n","wrong  : 220+627=747\n","correct: 220+627=847\n"," 70% 56/80 [00:02<00:01, 20.47it/s]outputs(x):  $812+226=7301$\n","$\n","wrong  : 812+226=1037\n","correct: 812+226=1038\n","outputs(x):  $752+718=9641$\n","$\n","wrong  : 752+718=1469\n","correct: 752+718=1470\n","outputs(x):  $267+947=4111$\n","$\n","wrong  : 267+947=1114\n","correct: 267+947=1214\n","outputs(x):  $182+866=7401$\n","$\n","wrong  : 182+866=1047\n","correct: 182+866=1048\n","outputs(x):  $971+738=8071$\n","$\n","wrong  : 971+738=1708\n","correct: 971+738=1709\n","outputs(x):  $372+586=759$\n","$3\n","wrong  : 372+586=957\n","correct: 372+586=958\n","outputs(x):  $939+380=9031$\n","$\n","wrong  : 939+380=1309\n","correct: 939+380=1319\n","outputs(x):  $423+976=8831$\n","$\n","wrong  : 423+976=1388\n","correct: 423+976=1399\n","outputs(x):  $112+695=608$\n","$1\n","wrong  : 112+695=806\n","correct: 112+695=807\n","outputs(x):  $755+667=1241$\n","$\n","wrong  : 755+667=1421\n","correct: 755+667=1422\n","outputs(x):  $231+792=329$\n","$9\n","wrong  : 231+792=923\n","correct: 231+792=1023\n","outputs(x):  $258+679=738$\n","$7\n","wrong  : 258+679=837\n","correct: 258+679=937\n","outputs(x):  $271+764=539$\n","$5\n","wrong  : 271+764=935\n","correct: 271+764=1035\n","outputs(x):  $248+708=658$\n","$5\n","wrong  : 248+708=856\n","correct: 248+708=956\n","outputs(x):  $667+854=1261$\n","$\n","wrong  : 667+854=1621\n","correct: 667+854=1521\n","outputs(x):  $226+885=1001$\n","$\n","wrong  : 226+885=1001\n","correct: 226+885=1111\n","outputs(x):  $176+275=153$\n","$1\n","wrong  : 176+275=351\n","correct: 176+275=451\n","outputs(x):  $272+739=119$\n","$6\n","wrong  : 272+739=911\n","correct: 272+739=1011\n","outputs(x):  $151+808=858$\n","$3\n","wrong  : 151+808=858\n","correct: 151+808=959\n","outputs(x):  $953+418=0731$\n","$\n","wrong  : 953+418=1370\n","correct: 953+418=1371\n","outputs(x):  $625+950=5651$\n","$\n","wrong  : 625+950=1565\n","correct: 625+950=1575\n","outputs(x):  $131+917=7401$\n","$\n","wrong  : 131+917=1047\n","correct: 131+917=1048\n","outputs(x):  $773+967=9371$\n","$\n","wrong  : 773+967=1739\n","correct: 773+967=1740\n","outputs(x):  $170+279=943$\n","$6\n","wrong  : 170+279=349\n","correct: 170+279=449\n","outputs(x):  $613+805=7141$\n","$\n","wrong  : 613+805=1417\n","correct: 613+805=1418\n","outputs(x):  $991+638=8261$\n","$\n","wrong  : 991+638=1628\n","correct: 991+638=1629\n","outputs(x):  $720+674=4831$\n","$\n","wrong  : 720+674=1384\n","correct: 720+674=1394\n","outputs(x):  $614+741=5431$\n","$\n","wrong  : 614+741=1345\n","correct: 614+741=1355\n","outputs(x):  $342+444=587$\n","$2\n","wrong  : 342+444=785\n","correct: 342+444=786\n","outputs(x):  $328+167=584$\n","$4\n","wrong  : 328+167=485\n","correct: 328+167=495\n","outputs(x):  $348+701=949$\n","$4\n","wrong  : 348+701=949\n","correct: 348+701=1049\n","outputs(x):  $422+533=459$\n","$7\n","wrong  : 422+533=954\n","correct: 422+533=955\n","outputs(x):  $273+247=914$\n","$2\n","wrong  : 273+247=419\n","correct: 273+247=520\n","outputs(x):  $126+178=492$\n","$3\n","wrong  : 126+178=294\n","correct: 126+178=304\n","outputs(x):  $553+377=929$\n","$5\n","wrong  : 553+377=929\n","correct: 553+377=930\n","outputs(x):  $678+575=3621$\n","$\n","wrong  : 678+575=1263\n","correct: 678+575=1253\n","outputs(x):  $113+182=582$\n","$3\n","wrong  : 113+182=285\n","correct: 113+182=295\n","outputs(x):  $553+656=8021$\n","$\n","wrong  : 553+656=1208\n","correct: 553+656=1209\n","outputs(x):  $702+816=7151$\n","$\n","wrong  : 702+816=1517\n","correct: 702+816=1518\n","outputs(x):  $511+835=6331$\n","$\n","wrong  : 511+835=1336\n","correct: 511+835=1346\n","outputs(x):  $272+674=648$\n","$1\n","wrong  : 272+674=846\n","correct: 272+674=946\n","outputs(x):  $915+273=8711$\n","$\n","wrong  : 915+273=1178\n","correct: 915+273=1188\n","outputs(x):  $244+723=768$\n","$7\n","wrong  : 244+723=867\n","correct: 244+723=967\n","outputs(x):  $754+876=0371$\n","$\n","wrong  : 754+876=1730\n","correct: 754+876=1630\n","outputs(x):  $189+281=064$\n","$8\n","wrong  : 189+281=460\n","correct: 189+281=470\n","outputs(x):  $210+402=215$\n","$9\n","wrong  : 210+402=512\n","correct: 210+402=612\n","outputs(x):  $725+576=1921$\n","$\n","wrong  : 725+576=1291\n","correct: 725+576=1301\n","outputs(x):  $104+582=676$\n","$7\n","wrong  : 104+582=676\n","correct: 104+582=686\n","outputs(x):  $833+861=4861$\n","$\n","wrong  : 833+861=1684\n","correct: 833+861=1694\n","outputs(x):  $613+346=859$\n","$1\n","wrong  : 613+346=958\n","correct: 613+346=959\n","outputs(x):  $516+830=6331$\n","$\n","wrong  : 516+830=1336\n","correct: 516+830=1346\n","outputs(x):  $112+827=838$\n","$4\n","wrong  : 112+827=838\n","correct: 112+827=939\n","outputs(x):  $443+367=908$\n","$4\n","wrong  : 443+367=809\n","correct: 443+367=810\n","outputs(x):  $136+247=382$\n","$8\n","wrong  : 136+247=283\n","correct: 136+247=383\n","outputs(x):  $724+381=5901$\n","$\n","wrong  : 724+381=1095\n","correct: 724+381=1105\n","outputs(x):  $762+887=8461$\n","$\n","wrong  : 762+887=1648\n","correct: 762+887=1649\n","outputs(x):  $225+932=7411$\n","$\n","wrong  : 225+932=1147\n","correct: 225+932=1157\n","outputs(x):  $433+196=826$\n","$3\n","wrong  : 433+196=628\n","correct: 433+196=629\n","outputs(x):  $126+272=883$\n","$9\n","wrong  : 126+272=388\n","correct: 126+272=398\n","outputs(x):  $724+675=9831$\n","$\n","wrong  : 724+675=1389\n","correct: 724+675=1399\n","outputs(x):  $113+833=648$\n","$6\n","wrong  : 113+833=846\n","correct: 113+833=946\n","outputs(x):  $532+116=746$\n","$5\n","wrong  : 532+116=647\n","correct: 532+116=648\n","outputs(x):  $382+216=794$\n","$4\n","wrong  : 382+216=497\n","correct: 382+216=598\n","outputs(x):  $145+242=782$\n","$7\n","wrong  : 145+242=287\n","correct: 145+242=387\n","outputs(x):  $332+657=889$\n","$6\n","wrong  : 332+657=988\n","correct: 332+657=989\n","outputs(x):  $460+619=979$\n","$8\n","wrong  : 460+619=979\n","correct: 460+619=1079\n","outputs(x):  $325+347=275$\n","$1\n","wrong  : 325+347=572\n","correct: 325+347=672\n","outputs(x):  $833+556=8731$\n","$\n","wrong  : 833+556=1378\n","correct: 833+556=1389\n","outputs(x):  $712+785=6941$\n","$\n","wrong  : 712+785=1496\n","correct: 712+785=1497\n","outputs(x):  $365+964=9221$\n","$\n","wrong  : 365+964=1229\n","correct: 365+964=1329\n","outputs(x):  $232+484=517$\n","$8\n","wrong  : 232+484=715\n","correct: 232+484=716\n","outputs(x):  $502+268=967$\n","$4\n","wrong  : 502+268=769\n","correct: 502+268=770\n","outputs(x):  $127+868=589$\n","$5\n","wrong  : 127+868=985\n","correct: 127+868=995\n","outputs(x):  $214+263=773$\n","$2\n","wrong  : 214+263=377\n","correct: 214+263=477\n","outputs(x):  $162+987=8411$\n","$\n","wrong  : 162+987=1148\n","correct: 162+987=1149\n","outputs(x):  $646+563=9911$\n","$\n","wrong  : 646+563=1199\n","correct: 646+563=1209\n","outputs(x):  $583+257=938$\n","$1\n","wrong  : 583+257=839\n","correct: 583+257=840\n","outputs(x):  $753+607=9531$\n","$\n","wrong  : 753+607=1359\n","correct: 753+607=1360\n"," 74% 59/80 [00:02<00:01, 20.37it/s]outputs(x):  $225+312=734$\n","$8\n","wrong  : 225+312=437\n","correct: 225+312=537\n","outputs(x):  $922+427=8431$\n","$\n","wrong  : 922+427=1348\n","correct: 922+427=1349\n","outputs(x):  $236+636=277$\n","$5\n","wrong  : 236+636=772\n","correct: 236+636=872\n","outputs(x):  $206+330=634$\n","$5\n","wrong  : 206+330=436\n","correct: 206+330=536\n","outputs(x):  $685+718=3051$\n","$\n","wrong  : 685+718=1503\n","correct: 685+718=1403\n","outputs(x):  $910+341=1421$\n","$\n","wrong  : 910+341=1241\n","correct: 910+341=1251\n","outputs(x):  $186+196=282$\n","$9\n","wrong  : 186+196=282\n","correct: 186+196=382\n","outputs(x):  $242+507=847$\n","$4\n","wrong  : 242+507=748\n","correct: 242+507=749\n","outputs(x):  $143+525=865$\n","$3\n","wrong  : 143+525=568\n","correct: 143+525=668\n","outputs(x):  $243+912=5501$\n","$\n","wrong  : 243+912=1055\n","correct: 243+912=1155\n","outputs(x):  $729+377=6901$\n","$\n","wrong  : 729+377=1096\n","correct: 729+377=1106\n","outputs(x):  $413+430=338$\n","$4\n","wrong  : 413+430=833\n","correct: 413+430=843\n","outputs(x):  $316+917=3221$\n","$\n","wrong  : 316+917=1223\n","correct: 316+917=1233\n","outputs(x):  $245+349=494$\n","$2\n","wrong  : 245+349=494\n","correct: 245+349=594\n","outputs(x):  $324+454=876$\n","$3\n","wrong  : 324+454=678\n","correct: 324+454=778\n","outputs(x):  $221+916=6301$\n","$\n","wrong  : 221+916=1036\n","correct: 221+916=1137\n","outputs(x):  $258+805=369$\n","$7\n","wrong  : 258+805=963\n","correct: 258+805=1063\n","outputs(x):  $140+198=832$\n","$1\n","wrong  : 140+198=238\n","correct: 140+198=338\n","outputs(x):  $269+768=739$\n","$8\n","wrong  : 269+768=937\n","correct: 269+768=1037\n","outputs(x):  $773+827=9951$\n","$\n","wrong  : 773+827=1599\n","correct: 773+827=1600\n","outputs(x):  $731+528=8521$\n","$\n","wrong  : 731+528=1258\n","correct: 731+528=1259\n","outputs(x):  $129+466=585$\n","$7\n","wrong  : 129+466=585\n","correct: 129+466=595\n","outputs(x):  $312+937=8421$\n","$\n","wrong  : 312+937=1248\n","correct: 312+937=1249\n","outputs(x):  $826+761=7751$\n","$\n","wrong  : 826+761=1577\n","correct: 826+761=1587\n","outputs(x):  $287+899=6801$\n","$\n","wrong  : 287+899=1086\n","correct: 287+899=1186\n","outputs(x):  $294+174=863$\n","$7\n","wrong  : 294+174=368\n","correct: 294+174=468\n","outputs(x):  $323+617=939$\n","$5\n","wrong  : 323+617=939\n","correct: 323+617=940\n","outputs(x):  $381+709=099$\n","$1\n","wrong  : 381+709=990\n","correct: 381+709=1090\n","outputs(x):  $233+127=953$\n","$3\n","wrong  : 233+127=359\n","correct: 233+127=360\n","outputs(x):  $223+879=2001$\n","$\n","wrong  : 223+879=1002\n","correct: 223+879=1102\n","outputs(x):  $295+166=163$\n","$3\n","wrong  : 295+166=361\n","correct: 295+166=461\n","outputs(x):  $132+416=745$\n","$8\n","wrong  : 132+416=547\n","correct: 132+416=548\n","outputs(x):  $237+773=0001$\n","$\n","wrong  : 237+773=1000\n","correct: 237+773=1010\n","outputs(x):  $742+357=8901$\n","$\n","wrong  : 742+357=1098\n","correct: 742+357=1099\n","outputs(x):  $244+823=769$\n","$5\n","wrong  : 244+823=967\n","correct: 244+823=1067\n","outputs(x):  $245+749=498$\n","$3\n","wrong  : 245+749=894\n","correct: 245+749=994\n","outputs(x):  $502+725=6221$\n","$\n","wrong  : 502+725=1226\n","correct: 502+725=1227\n","outputs(x):  $964+804=7671$\n","$\n","wrong  : 964+804=1767\n","correct: 964+804=1768\n","outputs(x):  $426+368=487$\n","$5\n","wrong  : 426+368=784\n","correct: 426+368=794\n","outputs(x):  $863+816=8761$\n","$\n","wrong  : 863+816=1678\n","correct: 863+816=1679\n","outputs(x):  $392+208=995$\n","$9\n","wrong  : 392+208=599\n","correct: 392+208=600\n","outputs(x):  $702+876=7751$\n","$\n","wrong  : 702+876=1577\n","correct: 702+876=1578\n","outputs(x):  $533+642=5611$\n","$\n","wrong  : 533+642=1165\n","correct: 533+642=1175\n","outputs(x):  $862+366=7221$\n","$\n","wrong  : 862+366=1227\n","correct: 862+366=1228\n","outputs(x):  $241+784=529$\n","$8\n","wrong  : 241+784=925\n","correct: 241+784=1025\n","outputs(x):  $845+763=8951$\n","$\n","wrong  : 845+763=1598\n","correct: 845+763=1608\n","outputs(x):  $981+127=7011$\n","$\n","wrong  : 981+127=1107\n","correct: 981+127=1108\n","outputs(x):  $686+665=1631$\n","$\n","wrong  : 686+665=1361\n","correct: 686+665=1351\n","outputs(x):  $101+445=644$\n","$5\n","wrong  : 101+445=446\n","correct: 101+445=546\n","outputs(x):  $105+273=872$\n","$9\n","wrong  : 105+273=278\n","correct: 105+273=378\n","outputs(x):  $323+915=7321$\n","$\n","wrong  : 323+915=1237\n","correct: 323+915=1238\n","outputs(x):  $809+890=9971$\n","$\n","wrong  : 809+890=1799\n","correct: 809+890=1699\n","outputs(x):  $244+715=958$\n","$9\n","wrong  : 244+715=859\n","correct: 244+715=959\n","outputs(x):  $802+118=919$\n","$4\n","wrong  : 802+118=919\n","correct: 802+118=920\n","outputs(x):  $103+364=663$\n","$8\n","wrong  : 103+364=366\n","correct: 103+364=467\n","outputs(x):  $351+609=068$\n","$8\n","wrong  : 351+609=860\n","correct: 351+609=960\n","outputs(x):  $202+396=794$\n","$1\n","wrong  : 202+396=497\n","correct: 202+396=598\n","outputs(x):  $169+135=402$\n","$8\n","wrong  : 169+135=204\n","correct: 169+135=304\n","outputs(x):  $553+686=8321$\n","$\n","wrong  : 553+686=1238\n","correct: 553+686=1239\n","outputs(x):  $633+674=7921$\n","$\n","wrong  : 633+674=1297\n","correct: 633+674=1307\n","outputs(x):  $913+588=1941$\n","$\n","wrong  : 913+588=1491\n","correct: 913+588=1501\n","outputs(x):  $603+905=7051$\n","$\n","wrong  : 603+905=1507\n","correct: 603+905=1508\n","outputs(x):  $345+842=7801$\n","$\n","wrong  : 345+842=1087\n","correct: 345+842=1187\n","outputs(x):  $198+204=203$\n","$2\n","wrong  : 198+204=302\n","correct: 198+204=402\n","outputs(x):  $672+227=898$\n","$3\n","wrong  : 672+227=898\n","correct: 672+227=899\n","outputs(x):  $268+306=474$\n","$6\n","wrong  : 268+306=474\n","correct: 268+306=574\n","outputs(x):  $711+927=7361$\n","$\n","wrong  : 711+927=1637\n","correct: 711+927=1638\n","outputs(x):  $652+118=967$\n","$9\n","wrong  : 652+118=769\n","correct: 652+118=770\n","outputs(x):  $738+170=898$\n","$6\n","wrong  : 738+170=898\n","correct: 738+170=908\n","outputs(x):  $344+746=099$\n","$3\n","wrong  : 344+746=990\n","correct: 344+746=1090\n","outputs(x):  $729+579=8921$\n","$\n","wrong  : 729+579=1298\n","correct: 729+579=1308\n","outputs(x):  $384+737=1201$\n","$\n","wrong  : 384+737=1021\n","correct: 384+737=1121\n","outputs(x):  $216+194=013$\n","$6\n","wrong  : 216+194=310\n","correct: 216+194=410\n","outputs(x):  $913+664=6751$\n","$\n","wrong  : 913+664=1576\n","correct: 913+664=1577\n","outputs(x):  $243+950=3901$\n","$\n","wrong  : 243+950=1093\n","correct: 243+950=1193\n","outputs(x):  $592+658=9421$\n","$\n","wrong  : 592+658=1249\n","correct: 592+658=1250\n"," 78% 62/80 [00:02<00:00, 20.63it/s]outputs(x):  $106+244=052$\n","$5\n","wrong  : 106+244=250\n","correct: 106+244=350\n","outputs(x):  $730+378=8901$\n","$\n","wrong  : 730+378=1098\n","correct: 730+378=1108\n","outputs(x):  $720+762=2741$\n","$\n","wrong  : 720+762=1472\n","correct: 720+762=1482\n","outputs(x):  $212+294=593$\n","$7\n","wrong  : 212+294=395\n","correct: 212+294=506\n","outputs(x):  $578+774=3631$\n","$\n","wrong  : 578+774=1363\n","correct: 578+774=1352\n","outputs(x):  $185+158=342$\n","$5\n","wrong  : 185+158=243\n","correct: 185+158=343\n","outputs(x):  $673+793=6651$\n","$\n","wrong  : 673+793=1566\n","correct: 673+793=1466\n","outputs(x):  $276+808=489$\n","$2\n","wrong  : 276+808=984\n","correct: 276+808=1084\n","outputs(x):  $221+584=597$\n","$7\n","wrong  : 221+584=795\n","correct: 221+584=805\n","outputs(x):  $115+257=272$\n","$1\n","wrong  : 115+257=272\n","correct: 115+257=372\n","outputs(x):  $662+936=7951$\n","$\n","wrong  : 662+936=1597\n","correct: 662+936=1598\n","outputs(x):  $513+452=559$\n","$4\n","wrong  : 513+452=955\n","correct: 513+452=965\n","outputs(x):  $890+766=6571$\n","$\n","wrong  : 890+766=1756\n","correct: 890+766=1656\n","outputs(x):  $152+271=323$\n","$7\n","wrong  : 152+271=323\n","correct: 152+271=423\n","outputs(x):  $928+580=8941$\n","$\n","wrong  : 928+580=1498\n","correct: 928+580=1508\n","outputs(x):  $653+587=9321$\n","$\n","wrong  : 653+587=1239\n","correct: 653+587=1240\n","outputs(x):  $633+427=9501$\n","$\n","wrong  : 633+427=1059\n","correct: 633+427=1060\n","outputs(x):  $521+861=2731$\n","$\n","wrong  : 521+861=1372\n","correct: 521+861=1382\n","outputs(x):  $277+647=428$\n","$6\n","wrong  : 277+647=824\n","correct: 277+647=924\n","outputs(x):  $741+427=7611$\n","$\n","wrong  : 741+427=1167\n","correct: 741+427=1168\n","outputs(x):  $617+479=6801$\n","$\n","wrong  : 617+479=1086\n","correct: 617+479=1096\n","outputs(x):  $412+616=7201$\n","$\n","wrong  : 412+616=1027\n","correct: 412+616=1028\n","outputs(x):  $822+382=4911$\n","$\n","wrong  : 822+382=1194\n","correct: 822+382=1204\n","outputs(x):  $245+923=8601$\n","$\n","wrong  : 245+923=1068\n","correct: 245+923=1168\n","outputs(x):  $275+611=687$\n","$9\n","wrong  : 275+611=786\n","correct: 275+611=886\n","outputs(x):  $377+780=7501$\n","$\n","wrong  : 377+780=1057\n","correct: 377+780=1157\n","outputs(x):  $532+507=8301$\n","$\n","wrong  : 532+507=1038\n","correct: 532+507=1039\n","outputs(x):  $483+777=9521$\n","$\n","wrong  : 483+777=1259\n","correct: 483+777=1260\n","outputs(x):  $936+676=2061$\n","$\n","wrong  : 936+676=1602\n","correct: 936+676=1612\n","outputs(x):  $174+293=763$\n","$2\n","wrong  : 174+293=367\n","correct: 174+293=467\n","outputs(x):  $683+606=8821$\n","$\n","wrong  : 683+606=1288\n","correct: 683+606=1289\n","outputs(x):  $64+615=876$\n","$4\n","wrong  : 64+615=678\n","correct: 64+615=679\n","outputs(x):  $109+16=421$\n","$8\n","wrong  : 109+16=124\n","correct: 109+16=125\n","outputs(x):  $72+907=879$\n","$3\n","wrong  : 72+907=978\n","correct: 72+907=979\n","outputs(x):  $772+56=728$\n","$1\n","wrong  : 772+56=827\n","correct: 772+56=828\n","outputs(x):  $863+66=829$\n","$6\n","wrong  : 863+66=928\n","correct: 863+66=929\n","outputs(x):  $153+56=802$\n","$7\n","wrong  : 153+56=208\n","correct: 153+56=209\n","outputs(x):  $129+15=431$\n","$1\n","wrong  : 129+15=134\n","correct: 129+15=144\n","outputs(x):  $17+320=723$\n","$2\n","wrong  : 17+320=327\n","correct: 17+320=337\n","outputs(x):  $703+87=987$\n","$3\n","wrong  : 703+87=789\n","correct: 703+87=790\n","outputs(x):  $901+26=629$\n","$2\n","wrong  : 901+26=926\n","correct: 901+26=927\n","outputs(x):  $383+78=064$\n","$2\n","wrong  : 383+78=460\n","correct: 383+78=461\n","outputs(x):  $20+852=268$\n","$3\n","wrong  : 20+852=862\n","correct: 20+852=872\n","outputs(x):  $27+327=443$\n","$5\n","wrong  : 27+327=344\n","correct: 27+327=354\n","outputs(x):  $273+69=242$\n","$5\n","wrong  : 273+69=242\n","correct: 273+69=342\n","outputs(x):  $291+96=683$\n","$1\n","wrong  : 291+96=386\n","correct: 291+96=387\n","outputs(x):  $77+649=628$\n","$2\n","wrong  : 77+649=826\n","correct: 77+649=726\n","outputs(x):  $62+446=705$\n","$6\n","wrong  : 62+446=507\n","correct: 62+446=508\n","outputs(x):  $11+355=653$\n","$3\n","wrong  : 11+355=356\n","correct: 11+355=366\n","outputs(x):  $711+66=677$\n","$8\n","wrong  : 711+66=776\n","correct: 711+66=777\n","outputs(x):  $46+260=602$\n","$5\n","wrong  : 46+260=206\n","correct: 46+260=306\n","outputs(x):  $992+27=8101$\n","$\n","wrong  : 992+27=1018\n","correct: 992+27=1019\n","outputs(x):  $233+78=103$\n","$1\n","wrong  : 233+78=301\n","correct: 233+78=311\n","outputs(x):  $20+390=004$\n","$1\n","wrong  : 20+390=400\n","correct: 20+390=410\n","outputs(x):  $613+21=426$\n","$3\n","wrong  : 613+21=624\n","correct: 613+21=634\n"," 81% 65/80 [00:03<00:00, 21.17it/s]outputs(x):  $613+39=246$\n","$7\n","wrong  : 613+39=642\n","correct: 613+39=652\n","outputs(x):  $992+68=9501$\n","$\n","wrong  : 992+68=1059\n","correct: 992+68=1060\n","outputs(x):  $803+17=918$\n","$3\n","wrong  : 803+17=819\n","correct: 803+17=820\n","outputs(x):  $324+69=383$\n","$9\n","wrong  : 324+69=383\n","correct: 324+69=393\n","outputs(x):  $743+88=038$\n","$7\n","wrong  : 743+88=830\n","correct: 743+88=831\n","outputs(x):  $72+965=6301$\n","$\n","wrong  : 72+965=1036\n","correct: 72+965=1037\n","outputs(x):  $863+16=878$\n","$2\n","wrong  : 863+16=878\n","correct: 863+16=879\n","outputs(x):  $21+438=944$\n","$3\n","wrong  : 21+438=449\n","correct: 21+438=459\n","outputs(x):  $52+908=959$\n","$3\n","wrong  : 52+908=959\n","correct: 52+908=960\n","outputs(x):  $91+958=8301$\n","$\n","wrong  : 91+958=1038\n","correct: 91+958=1049\n","outputs(x):  $29+882=109$\n","$9\n","wrong  : 29+882=901\n","correct: 29+882=911\n","outputs(x):  $942+77=8101$\n","$\n","wrong  : 942+77=1018\n","correct: 942+77=1019\n","outputs(x):  $22+589=106$\n","$7\n","wrong  : 22+589=601\n","correct: 22+589=611\n","outputs(x):  $21+562=375$\n","$9\n","wrong  : 21+562=573\n","correct: 21+562=583\n","outputs(x):  $15+518=325$\n","$7\n","wrong  : 15+518=523\n","correct: 15+518=533\n","outputs(x):  $324+39=353$\n","$3\n","wrong  : 324+39=353\n","correct: 324+39=363\n","outputs(x):  $257+55=212$\n","$5\n","wrong  : 257+55=212\n","correct: 257+55=312\n","outputs(x):  $642+57=896$\n","$4\n","wrong  : 642+57=698\n","correct: 642+57=699\n","outputs(x):  $20+934=449$\n","$5\n","wrong  : 20+934=944\n","correct: 20+934=954\n","outputs(x):  $26+642=856$\n","$7\n","wrong  : 26+642=658\n","correct: 26+642=668\n","outputs(x):  $713+66=877$\n","$7\n","wrong  : 713+66=778\n","correct: 713+66=779\n","outputs(x):  $84+483=775$\n","$8\n","wrong  : 84+483=577\n","correct: 84+483=567\n","outputs(x):  $82+506=785$\n","$9\n","wrong  : 82+506=587\n","correct: 82+506=588\n","outputs(x):  $241+15=651$\n","$8\n","wrong  : 241+15=156\n","correct: 241+15=256\n","outputs(x):  $20+661=176$\n","$6\n","wrong  : 20+661=671\n","correct: 20+661=681\n","outputs(x):  $423+44=754$\n","$2\n","wrong  : 423+44=457\n","correct: 423+44=467\n","outputs(x):  $31+407=734$\n","$4\n","wrong  : 31+407=437\n","correct: 31+407=438\n","outputs(x):  $863+27=988$\n","$2\n","wrong  : 863+27=889\n","correct: 863+27=890\n","outputs(x):  $91+935=5201$\n","$\n","wrong  : 91+935=1025\n","correct: 91+935=1026\n","outputs(x):  $22+755=767$\n","$6\n","wrong  : 22+755=767\n","correct: 22+755=777\n","outputs(x):  $962+56=7101$\n","$\n","wrong  : 962+56=1017\n","correct: 962+56=1018\n","outputs(x):  $336+40=663$\n","$1\n","wrong  : 336+40=366\n","correct: 336+40=376\n","outputs(x):  $567+57=427$\n","$8\n","wrong  : 567+57=724\n","correct: 567+57=624\n","outputs(x):  $663+66=827$\n","$3\n","wrong  : 663+66=728\n","correct: 663+66=729\n","outputs(x):  $244+63=702$\n","$1\n","wrong  : 244+63=207\n","correct: 244+63=307\n","outputs(x):  $122+80=291$\n","$6\n","wrong  : 122+80=192\n","correct: 122+80=202\n","outputs(x):  $25+285=003$\n","$1\n","wrong  : 25+285=300\n","correct: 25+285=310\n","outputs(x):  $779+65=348$\n","$9\n","wrong  : 779+65=843\n","correct: 779+65=844\n","outputs(x):  $22+457=874$\n","$8\n","wrong  : 22+457=478\n","correct: 22+457=479\n","outputs(x):  $452+87=835$\n","$1\n","wrong  : 452+87=538\n","correct: 452+87=539\n","outputs(x):  $593+76=866$\n","$1\n","wrong  : 593+76=668\n","correct: 593+76=669\n","outputs(x):  $734+67=008$\n","$8\n","wrong  : 734+67=800\n","correct: 734+67=801\n","outputs(x):  $32+818=948$\n","$3\n","wrong  : 32+818=849\n","correct: 32+818=850\n","outputs(x):  $823+97=919$\n","$8\n","wrong  : 823+97=919\n","correct: 823+97=920\n","outputs(x):  $642+97=837$\n","$8\n","wrong  : 642+97=738\n","correct: 642+97=739\n","outputs(x):  $435+47=274$\n","$9\n","wrong  : 435+47=472\n","correct: 435+47=482\n","outputs(x):  $28+722=047$\n","$5\n","wrong  : 28+722=740\n","correct: 28+722=750\n","outputs(x):  $293+17=903$\n","$8\n","wrong  : 293+17=309\n","correct: 293+17=310\n","outputs(x):  $113+29=231$\n","$3\n","wrong  : 113+29=132\n","correct: 113+29=142\n","outputs(x):  $582+88=966$\n","$1\n","wrong  : 582+88=669\n","correct: 582+88=670\n","outputs(x):  $33+306=833$\n","$6\n","wrong  : 33+306=338\n","correct: 33+306=339\n","outputs(x):  $813+37=948$\n","$7\n","wrong  : 813+37=849\n","correct: 813+37=850\n","outputs(x):  $892+57=849$\n","$6\n","wrong  : 892+57=948\n","correct: 892+57=949\n","outputs(x):  $207+66=371$\n","$6\n","wrong  : 207+66=173\n","correct: 207+66=273\n","outputs(x):  $472+47=815$\n","$8\n","wrong  : 472+47=518\n","correct: 472+47=519\n","outputs(x):  $126+77=391$\n","$3\n","wrong  : 126+77=193\n","correct: 126+77=203\n","outputs(x):  $611+18=826$\n","$8\n","wrong  : 611+18=628\n","correct: 611+18=629\n","outputs(x):  $24+764=877$\n","$1\n","wrong  : 24+764=778\n","correct: 24+764=788\n","outputs(x):  $82+198=972$\n","$5\n","wrong  : 82+198=279\n","correct: 82+198=280\n","outputs(x):  $34+471=594$\n","$1\n","wrong  : 34+471=495\n","correct: 34+471=505\n","outputs(x):  $972+45=6101$\n","$\n","wrong  : 972+45=1016\n","correct: 972+45=1017\n","outputs(x):  $127+66=381$\n","$2\n","wrong  : 127+66=183\n","correct: 127+66=193\n"," 85% 68/80 [00:03<00:00, 21.47it/s]outputs(x):  $21+150=161$\n","$9\n","wrong  : 21+150=161\n","correct: 21+150=171\n","outputs(x):  $56+262=812$\n","$1\n","wrong  : 56+262=218\n","correct: 56+262=318\n","outputs(x):  $834+63=788$\n","$3\n","wrong  : 834+63=887\n","correct: 834+63=897\n","outputs(x):  $15+642=746$\n","$2\n","wrong  : 15+642=647\n","correct: 15+642=657\n","outputs(x):  $592+28=916$\n","$1\n","wrong  : 592+28=619\n","correct: 592+28=620\n","outputs(x):  $62+366=724$\n","$2\n","wrong  : 62+366=427\n","correct: 62+366=428\n","outputs(x):  $26+550=665$\n","$3\n","wrong  : 26+550=566\n","correct: 26+550=576\n","outputs(x):  $93+805=798$\n","$7\n","wrong  : 93+805=897\n","correct: 93+805=898\n","outputs(x):  $454+26=974$\n","$4\n","wrong  : 454+26=479\n","correct: 454+26=480\n","outputs(x):  $19+880=988$\n","$7\n","wrong  : 19+880=889\n","correct: 19+880=899\n","outputs(x):  $30+745=567$\n","$9\n","wrong  : 30+745=765\n","correct: 30+745=775\n","outputs(x):  $731+16=647$\n","$8\n","wrong  : 731+16=746\n","correct: 731+16=747\n","outputs(x):  $631+56=686$\n","$7\n","wrong  : 631+56=686\n","correct: 631+56=687\n","outputs(x):  $32+879=109$\n","$6\n","wrong  : 32+879=901\n","correct: 32+879=911\n","outputs(x):  $568+34=295$\n","$6\n","wrong  : 568+34=592\n","correct: 568+34=602\n","outputs(x):  $22+160=271$\n","$3\n","wrong  : 22+160=172\n","correct: 22+160=182\n","outputs(x):  $773+78=168$\n","$1\n","wrong  : 773+78=861\n","correct: 773+78=851\n","outputs(x):  $15+422=724$\n","$6\n","wrong  : 15+422=427\n","correct: 15+422=437\n","outputs(x):  $734+70=497$\n","$9\n","wrong  : 734+70=794\n","correct: 734+70=804\n","outputs(x):  $143+47=981$\n","$3\n","wrong  : 143+47=189\n","correct: 143+47=190\n","outputs(x):  $31+318=843$\n","$9\n","wrong  : 31+318=348\n","correct: 31+318=349\n","outputs(x):  $622+67=886$\n","$1\n","wrong  : 622+67=688\n","correct: 622+67=689\n","outputs(x):  $17+127=431$\n","$4\n","wrong  : 17+127=134\n","correct: 17+127=144\n","outputs(x):  $11+439=044$\n","$7\n","wrong  : 11+439=440\n","correct: 11+439=450\n","outputs(x):  $21+896=609$\n","$3\n","wrong  : 21+896=906\n","correct: 21+896=917\n","outputs(x):  $392+75=664$\n","$9\n","wrong  : 392+75=466\n","correct: 392+75=467\n","outputs(x):  $41+918=859$\n","$2\n","wrong  : 41+918=958\n","correct: 41+918=959\n","outputs(x):  $232+15=642$\n","$2\n","wrong  : 232+15=246\n","correct: 232+15=247\n","outputs(x):  $286+18=402$\n","$7\n","wrong  : 286+18=204\n","correct: 286+18=304\n","outputs(x):  $476+94=085$\n","$3\n","wrong  : 476+94=580\n","correct: 476+94=570\n","outputs(x):  $311+16=623$\n","$8\n","wrong  : 311+16=326\n","correct: 311+16=327\n","outputs(x):  $12+743=547$\n","$2\n","wrong  : 12+743=745\n","correct: 12+743=755\n","outputs(x):  $209+34=332$\n","$7\n","wrong  : 209+34=233\n","correct: 209+34=243\n","outputs(x):  $843+68=019$\n","$8\n","wrong  : 843+68=910\n","correct: 843+68=911\n","outputs(x):  $25+399=414$\n","$6\n","wrong  : 25+399=414\n","correct: 25+399=424\n","outputs(x):  $916+28=439$\n","$3\n","wrong  : 916+28=934\n","correct: 916+28=944\n","outputs(x):  $751+18=867$\n","$1\n","wrong  : 751+18=768\n","correct: 751+18=769\n","outputs(x):  $17+541=845$\n","$9\n","wrong  : 17+541=548\n","correct: 17+541=558\n","outputs(x):  $22+178=091$\n","$4\n","wrong  : 22+178=190\n","correct: 22+178=200\n","outputs(x):  $83+376=854$\n","$9\n","wrong  : 83+376=458\n","correct: 83+376=459\n","outputs(x):  $27+731=847$\n","$4\n","wrong  : 27+731=748\n","correct: 27+731=758\n","outputs(x):  $971+47=7101$\n","$\n","wrong  : 971+47=1017\n","correct: 971+47=1018\n","outputs(x):  $66+368=433$\n","$5\n","wrong  : 66+368=334\n","correct: 66+368=434\n","outputs(x):  $901+42=339$\n","$6\n","wrong  : 901+42=933\n","correct: 901+42=943\n","outputs(x):  $271+48=812$\n","$5\n","wrong  : 271+48=218\n","correct: 271+48=319\n","outputs(x):  $853+26=878$\n","$6\n","wrong  : 853+26=878\n","correct: 853+26=879\n","outputs(x):  $220+44=452$\n","$1\n","wrong  : 220+44=254\n","correct: 220+44=264\n","outputs(x):  $17+314=123$\n","$9\n","wrong  : 17+314=321\n","correct: 17+314=331\n","outputs(x):  $661+88=847$\n","$7\n","wrong  : 661+88=748\n","correct: 661+88=749\n","outputs(x):  $26+530=645$\n","$5\n","wrong  : 26+530=546\n","correct: 26+530=556\n","outputs(x):  $12+638=936$\n","$6\n","wrong  : 12+638=639\n","correct: 12+638=650\n","outputs(x):  $921+42=359$\n","$4\n","wrong  : 921+42=953\n","correct: 921+42=963\n","outputs(x):  $237+40=762$\n","$3\n","wrong  : 237+40=267\n","correct: 237+40=277\n","outputs(x):  $272+27=892$\n","$8\n","wrong  : 272+27=298\n","correct: 272+27=299\n","outputs(x):  $222+70=282$\n","$4\n","wrong  : 222+70=282\n","correct: 222+70=292\n","outputs(x):  $17+726=337$\n","$5\n","wrong  : 17+726=733\n","correct: 17+726=743\n","outputs(x):  $294+13=702$\n","$9\n","wrong  : 294+13=207\n","correct: 294+13=307\n","outputs(x):  $62+747=808$\n","$7\n","wrong  : 62+747=808\n","correct: 62+747=809\n","outputs(x):  $163+87=942$\n","$1\n","wrong  : 163+87=249\n","correct: 163+87=250\n","outputs(x):  $53+366=814$\n","$7\n","wrong  : 53+366=418\n","correct: 53+366=419\n","outputs(x):  $21+299=013$\n","$2\n","wrong  : 21+299=310\n","correct: 21+299=320\n","outputs(x):  $18+724=237$\n","$3\n","wrong  : 18+724=732\n","correct: 18+724=742\n","outputs(x):  $26+922=839$\n","$3\n","wrong  : 26+922=938\n","correct: 26+922=948\n","outputs(x):  $433+55=784$\n","$2\n","wrong  : 433+55=487\n","correct: 433+55=488\n","outputs(x):  $42+268=902$\n","$2\n","wrong  : 42+268=209\n","correct: 42+268=310\n","outputs(x):  $32+672=496$\n","$7\n","wrong  : 32+672=694\n","correct: 32+672=704\n","outputs(x):  $237+35=262$\n","$5\n","wrong  : 237+35=262\n","correct: 237+35=272\n","outputs(x):  $891+66=659$\n","$6\n","wrong  : 891+66=956\n","correct: 891+66=957\n","outputs(x):  $472+97=865$\n","$9\n","wrong  : 472+97=568\n","correct: 472+97=569\n","outputs(x):  $534+72=695$\n","$5\n","wrong  : 534+72=596\n","correct: 534+72=606\n","outputs(x):  $25+241=652$\n","$1\n","wrong  : 25+241=256\n","correct: 25+241=266\n"," 89% 71/80 [00:03<00:00, 21.14it/s]outputs(x):  $98+975=2701$\n","$\n","wrong  : 98+975=1072\n","correct: 98+975=1073\n","outputs(x):  $122+46=751$\n","$1\n","wrong  : 122+46=157\n","correct: 122+46=168\n","outputs(x):  $212+58=962$\n","$6\n","wrong  : 212+58=269\n","correct: 212+58=270\n","outputs(x):  $22+287=803$\n","$8\n","wrong  : 22+287=308\n","correct: 22+287=309\n","outputs(x):  $226+31=742$\n","$5\n","wrong  : 226+31=247\n","correct: 226+31=257\n","outputs(x):  $830+75=598$\n","$9\n","wrong  : 830+75=895\n","correct: 830+75=905\n","outputs(x):  $83+587=966$\n","$6\n","wrong  : 83+587=669\n","correct: 83+587=670\n","outputs(x):  $63+727=987$\n","$5\n","wrong  : 63+727=789\n","correct: 63+727=790\n","outputs(x):  $83+718=008$\n","$6\n","wrong  : 83+718=800\n","correct: 83+718=801\n","outputs(x):  $135+31=651$\n","$6\n","wrong  : 135+31=156\n","correct: 135+31=166\n","outputs(x):  $81+265=543$\n","$8\n","wrong  : 81+265=345\n","correct: 81+265=346\n","outputs(x):  $19+329=833$\n","$4\n","wrong  : 19+329=338\n","correct: 19+329=348\n","outputs(x):  $503+95=795$\n","$8\n","wrong  : 503+95=597\n","correct: 503+95=598\n","outputs(x):  $17+429=634$\n","$1\n","wrong  : 17+429=436\n","correct: 17+429=446\n","outputs(x):  $29+889=809$\n","$4\n","wrong  : 29+889=908\n","correct: 29+889=918\n","outputs(x):  $810+74=478$\n","$4\n","wrong  : 810+74=874\n","correct: 810+74=884\n","outputs(x):  $115+39=441$\n","$8\n","wrong  : 115+39=144\n","correct: 115+39=154\n","outputs(x):  $941+38=879$\n","$9\n","wrong  : 941+38=978\n","correct: 941+38=979\n","outputs(x):  $463+66=825$\n","$8\n","wrong  : 463+66=528\n","correct: 463+66=529\n","outputs(x):  $31+806=638$\n","$3\n","wrong  : 31+806=836\n","correct: 31+806=837\n","outputs(x):  $923+38=059$\n","$2\n","wrong  : 923+38=950\n","correct: 923+38=961\n","outputs(x):  $636+70=696$\n","$6\n","wrong  : 636+70=696\n","correct: 636+70=706\n","outputs(x):  $40+733=367$\n","$2\n","wrong  : 40+733=763\n","correct: 40+733=773\n","outputs(x):  $321+34=543$\n","$8\n","wrong  : 321+34=345\n","correct: 321+34=355\n","outputs(x):  $362+66=724$\n","$2\n","wrong  : 362+66=427\n","correct: 362+66=428\n","outputs(x):  $261+58=813$\n","$9\n","wrong  : 261+58=318\n","correct: 261+58=319\n","outputs(x):  $34+481=505$\n","$7\n","wrong  : 34+481=505\n","correct: 34+481=515\n","outputs(x):  $253+77=922$\n","$7\n","wrong  : 253+77=229\n","correct: 253+77=330\n","outputs(x):  $667+69=627$\n","$5\n","wrong  : 667+69=726\n","correct: 667+69=736\n","outputs(x):  $11+982=389$\n","$5\n","wrong  : 11+982=983\n","correct: 11+982=993\n","outputs(x):  $581+33=406$\n","$7\n","wrong  : 581+33=604\n","correct: 581+33=614\n","outputs(x):  $10+967=769$\n","$8\n","wrong  : 10+967=967\n","correct: 10+967=977\n","outputs(x):  $842+78=919$\n","$8\n","wrong  : 842+78=919\n","correct: 842+78=920\n","outputs(x):  $621+98=817$\n","$7\n","wrong  : 621+98=718\n","correct: 621+98=719\n","outputs(x):  $23+365=873$\n","$5\n","wrong  : 23+365=378\n","correct: 23+365=388\n","outputs(x):  $373+97=964$\n","$6\n","wrong  : 373+97=469\n","correct: 373+97=470\n","outputs(x):  $34+373=793$\n","$5\n","wrong  : 34+373=397\n","correct: 34+373=407\n","outputs(x):  $21+176=781$\n","$4\n","wrong  : 21+176=187\n","correct: 21+176=197\n","outputs(x):  $273+34=702$\n","$2\n","wrong  : 273+34=207\n","correct: 273+34=307\n","outputs(x):  $638+37=476$\n","$1\n","wrong  : 638+37=674\n","correct: 638+37=675\n","outputs(x):  $15+830=538$\n","$5\n","wrong  : 15+830=835\n","correct: 15+830=845\n","outputs(x):  $721+45=567$\n","$6\n","wrong  : 721+45=765\n","correct: 721+45=766\n","outputs(x):  $31+367=883$\n","$9\n","wrong  : 31+367=388\n","correct: 31+367=398\n","outputs(x):  $822+98=919$\n","$4\n","wrong  : 822+98=919\n","correct: 822+98=920\n","outputs(x):  $23+686=996$\n","$3\n","wrong  : 23+686=699\n","correct: 23+686=709\n","outputs(x):  $29+570=985$\n","$4\n","wrong  : 29+570=589\n","correct: 29+570=599\n","outputs(x):  $932+69=199$\n","$9\n","wrong  : 932+69=991\n","correct: 932+69=1001\n","outputs(x):  $651+38=886$\n","$2\n","wrong  : 651+38=688\n","correct: 651+38=689\n","outputs(x):  $573+18=095$\n","$4\n","wrong  : 573+18=590\n","correct: 573+18=591\n","outputs(x):  $721+42=357$\n","$8\n","wrong  : 721+42=753\n","correct: 721+42=763\n","outputs(x):  $665+89=467$\n","$4\n","wrong  : 665+89=764\n","correct: 665+89=754\n","outputs(x):  $13+287=982$\n","$4\n","wrong  : 13+287=289\n","correct: 13+287=300\n","outputs(x):  $622+28=946$\n","$8\n","wrong  : 622+28=649\n","correct: 622+28=650\n","outputs(x):  $939+56=589$\n","$7\n","wrong  : 939+56=985\n","correct: 939+56=995\n","outputs(x):  $23+270=382$\n","$1\n","wrong  : 23+270=283\n","correct: 23+270=293\n","outputs(x):  $44+267=112$\n","$2\n","wrong  : 44+267=211\n","correct: 44+267=311\n","outputs(x):  $63+157=912$\n","$1\n","wrong  : 63+157=219\n","correct: 63+157=220\n","outputs(x):  $63+496=855$\n","$1\n","wrong  : 63+496=558\n","correct: 63+496=559\n","outputs(x):  $634+73=796$\n","$1\n","wrong  : 634+73=697\n","correct: 634+73=707\n","outputs(x):  $63+274=633$\n","$6\n","wrong  : 63+274=336\n","correct: 63+274=337\n","outputs(x):  $21+838=948$\n","$3\n","wrong  : 21+838=849\n","correct: 21+838=859\n","outputs(x):  $268+36=402$\n","$6\n","wrong  : 268+36=204\n","correct: 268+36=304\n","outputs(x):  $520+43=355$\n","$6\n","wrong  : 520+43=553\n","correct: 520+43=563\n","outputs(x):  $20+830=048$\n","$5\n","wrong  : 20+830=840\n","correct: 20+830=850\n","outputs(x):  $671+28=896$\n","$9\n","wrong  : 671+28=698\n","correct: 671+28=699\n","outputs(x):  $73+527=995$\n","$8\n","wrong  : 73+527=599\n","correct: 73+527=600\n","outputs(x):  $591+57=746$\n","$6\n","wrong  : 591+57=647\n","correct: 591+57=648\n","outputs(x):  $715+16=027$\n","$5\n","wrong  : 715+16=720\n","correct: 715+16=731\n","outputs(x):  $837+70=798$\n","$9\n","wrong  : 837+70=897\n","correct: 837+70=907\n","outputs(x):  $21+428=934$\n","$5\n","wrong  : 21+428=439\n","correct: 21+428=449\n"," 92% 74/80 [00:03<00:00, 21.26it/s]outputs(x):  $681+58=837$\n","$1\n","wrong  : 681+58=738\n","correct: 681+58=739\n","outputs(x):  $303+46=843$\n","$6\n","wrong  : 303+46=348\n","correct: 303+46=349\n","outputs(x):  $451+37=784$\n","$6\n","wrong  : 451+37=487\n","correct: 451+37=488\n","outputs(x):  $142+35=671$\n","$7\n","wrong  : 142+35=176\n","correct: 142+35=177\n","outputs(x):  $701+88=887$\n","$8\n","wrong  : 701+88=788\n","correct: 701+88=789\n","outputs(x):  $15+480=584$\n","$9\n","wrong  : 15+480=485\n","correct: 15+480=495\n","outputs(x):  $26+206=222$\n","$7\n","wrong  : 26+206=222\n","correct: 26+206=232\n","outputs(x):  $27+174=191$\n","$9\n","wrong  : 27+174=191\n","correct: 27+174=201\n","outputs(x):  $759+98=768$\n","$3\n","wrong  : 759+98=867\n","correct: 759+98=857\n","outputs(x):  $335+11=642$\n","$2\n","wrong  : 335+11=246\n","correct: 335+11=346\n","outputs(x):  $21+462=374$\n","$5\n","wrong  : 21+462=473\n","correct: 21+462=483\n","outputs(x):  $803+25=728$\n","$3\n","wrong  : 803+25=827\n","correct: 803+25=828\n","outputs(x):  $523+74=785$\n","$9\n","wrong  : 523+74=587\n","correct: 523+74=597\n","outputs(x):  $134+63=781$\n","$3\n","wrong  : 134+63=187\n","correct: 134+63=197\n","outputs(x):  $92+587=876$\n","$7\n","wrong  : 92+587=678\n","correct: 92+587=679\n","outputs(x):  $875+89=479$\n","$2\n","wrong  : 875+89=974\n","correct: 875+89=964\n","outputs(x):  $32+847=868$\n","$9\n","wrong  : 32+847=868\n","correct: 32+847=879\n","outputs(x):  $72+667=837$\n","$6\n","wrong  : 72+667=738\n","correct: 72+667=739\n","outputs(x):  $26+619=536$\n","$5\n","wrong  : 26+619=635\n","correct: 26+619=645\n","outputs(x):  $64+106=961$\n","$5\n","wrong  : 64+106=169\n","correct: 64+106=170\n","outputs(x):  $742+66=708$\n","$2\n","wrong  : 742+66=807\n","correct: 742+66=808\n","outputs(x):  $29+461=084$\n","$7\n","wrong  : 29+461=480\n","correct: 29+461=490\n","outputs(x):  $214+39=342$\n","$9\n","wrong  : 214+39=243\n","correct: 214+39=253\n","outputs(x):  $327+72=983$\n","$6\n","wrong  : 327+72=389\n","correct: 327+72=399\n","outputs(x):  $93+905=799$\n","$9\n","wrong  : 93+905=997\n","correct: 93+905=998\n","outputs(x):  $123+17=931$\n","$7\n","wrong  : 123+17=139\n","correct: 123+17=140\n","outputs(x):  $502+38=935$\n","$1\n","wrong  : 502+38=539\n","correct: 502+38=540\n","outputs(x):  $22+790=208$\n","$2\n","wrong  : 22+790=802\n","correct: 22+790=812\n","outputs(x):  $12+738=947$\n","$4\n","wrong  : 12+738=749\n","correct: 12+738=750\n","outputs(x):  $384+31=504$\n","$9\n","wrong  : 384+31=405\n","correct: 384+31=415\n","outputs(x):  $561+15=575$\n","$1\n","wrong  : 561+15=575\n","correct: 561+15=576\n","outputs(x):  $11+598=806$\n","$8\n","wrong  : 11+598=608\n","correct: 11+598=609\n","outputs(x):  $16+587=395$\n","$6\n","wrong  : 16+587=593\n","correct: 16+587=603\n","outputs(x):  $62+267=823$\n","$6\n","wrong  : 62+267=328\n","correct: 62+267=329\n","outputs(x):  $16+542=845$\n","$5\n","wrong  : 16+542=548\n","correct: 16+542=558\n","outputs(x):  $313+37=943$\n","$7\n","wrong  : 313+37=349\n","correct: 313+37=350\n","outputs(x):  $100+56=551$\n","$2\n","wrong  : 100+56=155\n","correct: 100+56=156\n","outputs(x):  $273+20=391$\n","$6\n","wrong  : 273+20=193\n","correct: 273+20=293\n","outputs(x):  $975+78=2501$\n","$\n","wrong  : 975+78=1052\n","correct: 975+78=1053\n","outputs(x):  $636+40=666$\n","$9\n","wrong  : 636+40=666\n","correct: 636+40=676\n","outputs(x):  $526+30=645$\n","$5\n","wrong  : 526+30=546\n","correct: 526+30=556\n","outputs(x):  $411+37=734$\n","$8\n","wrong  : 411+37=437\n","correct: 411+37=448\n","outputs(x):  $63+996=8501$\n","$\n","wrong  : 63+996=1058\n","correct: 63+996=1059\n","outputs(x):  $212+85=692$\n","$4\n","wrong  : 212+85=296\n","correct: 212+85=297\n","outputs(x):  $42+678=917$\n","$1\n","wrong  : 42+678=719\n","correct: 42+678=720\n","outputs(x):  $641+77=717$\n","$3\n","wrong  : 641+77=717\n","correct: 641+77=718\n","outputs(x):  $841+16=658$\n","$2\n","wrong  : 841+16=856\n","correct: 841+16=857\n","outputs(x):  $112+30=231$\n","$7\n","wrong  : 112+30=132\n","correct: 112+30=142\n","outputs(x):  $787+72=968$\n","$1\n","wrong  : 787+72=869\n","correct: 787+72=859\n","outputs(x):  $671+61=227$\n","$8\n","wrong  : 671+61=722\n","correct: 671+61=732\n","outputs(x):  $42+804=548$\n","$8\n","wrong  : 42+804=845\n","correct: 42+804=846\n","outputs(x):  $91+968=8501$\n","$\n","wrong  : 91+968=1058\n","correct: 91+968=1059\n","outputs(x):  $73+178=052$\n","$1\n","wrong  : 73+178=250\n","correct: 73+178=251\n"," 96% 77/80 [00:03<00:00, 21.92it/s]outputs(x):  $34+81=501$\n","$3\n","wrong  : 34+81=105\n","correct: 34+81=115\n","outputs(x):  $2+115=611$\n","$5\n","wrong  : 2+115=116\n","correct: 2+115=117\n","outputs(x):  $83+47=921$\n","$7\n","wrong  : 83+47=129\n","correct: 83+47=130\n","outputs(x):  $63+26=88$\n","$44\n","wrong  : 63+26=88\n","correct: 63+26=89\n","outputs(x):  $24+98=211$\n","$6\n","wrong  : 24+98=112\n","correct: 24+98=122\n","outputs(x):  $237+0=731$\n","$4\n","wrong  : 237+0=137\n","correct: 237+0=237\n","outputs(x):  $9+507=605$\n","$3\n","wrong  : 9+507=506\n","correct: 9+507=516\n","outputs(x):  $346+2=743$\n","$2\n","wrong  : 346+2=347\n","correct: 346+2=348\n","outputs(x):  $31+78=99$\n","$90\n","wrong  : 31+78=99\n","correct: 31+78=109\n","outputs(x):  $9+210=902$\n","$7\n","wrong  : 9+210=209\n","correct: 9+210=219\n","outputs(x):  $623+9=226$\n","$7\n","wrong  : 623+9=622\n","correct: 623+9=632\n","outputs(x):  $7+499=606$\n","$8\n","wrong  : 7+499=606\n","correct: 7+499=506\n","outputs(x):  $24+43=75$\n","$76\n","wrong  : 24+43=57\n","correct: 24+43=67\n","outputs(x):  $28+63=18$\n","$66\n","wrong  : 28+63=81\n","correct: 28+63=91\n","outputs(x):  $209+6=511$\n","$1\n","wrong  : 209+6=115\n","correct: 209+6=215\n","outputs(x):  $26+42=85$\n","$6+\n","wrong  : 26+42=58\n","correct: 26+42=68\n","outputs(x):  $208+4=211$\n","$8\n","wrong  : 208+4=112\n","correct: 208+4=212\n","outputs(x):  $303+2=403$\n","$6\n","wrong  : 303+2=304\n","correct: 303+2=305\n","outputs(x):  $29+49=86$\n","$44\n","wrong  : 29+49=68\n","correct: 29+49=78\n","outputs(x):  $3+442=534$\n","$5\n","wrong  : 3+442=435\n","correct: 3+442=445\n","outputs(x):  $8+758=657$\n","$4\n","wrong  : 8+758=756\n","correct: 8+758=766\n","outputs(x):  $9+750=947$\n","$7\n","wrong  : 9+750=749\n","correct: 9+750=759\n","outputs(x):  $38+81=901$\n","$2\n","wrong  : 38+81=109\n","correct: 38+81=119\n","outputs(x):  $71+67=731$\n","$1\n","wrong  : 71+67=137\n","correct: 71+67=138\n","outputs(x):  $7+869=688$\n","$1\n","wrong  : 7+869=886\n","correct: 7+869=876\n","outputs(x):  $26+37=35$\n","$91\n","wrong  : 26+37=53\n","correct: 26+37=63\n","outputs(x):  $3+576=875$\n","$3\n","wrong  : 3+576=578\n","correct: 3+576=579\n","outputs(x):  $42+38=97$\n","$27\n","wrong  : 42+38=79\n","correct: 42+38=80\n","outputs(x):  $3+457=854$\n","$4\n","wrong  : 3+457=458\n","correct: 3+457=460\n","outputs(x):  $9+845=448$\n","$6\n","wrong  : 9+845=844\n","correct: 9+845=854\n","outputs(x):  $2+878=978$\n","$1\n","wrong  : 2+878=879\n","correct: 2+878=880\n","outputs(x):  $32+86=701$\n","$8\n","wrong  : 32+86=107\n","correct: 32+86=118\n","outputs(x):  $2+899=009$\n","$4\n","wrong  : 2+899=900\n","correct: 2+899=901\n","outputs(x):  $469+2=184$\n","$7\n","wrong  : 469+2=481\n","correct: 469+2=471\n","outputs(x):  $21+85=69$\n","$40\n","wrong  : 21+85=96\n","correct: 21+85=106\n","outputs(x):  $35+33=85$\n","$51\n","wrong  : 35+33=58\n","correct: 35+33=68\n","outputs(x):  $131+6=631$\n","$6\n","wrong  : 131+6=136\n","correct: 131+6=137\n","outputs(x):  $32+28=95$\n","$79\n","wrong  : 32+28=59\n","correct: 32+28=60\n","outputs(x):  $9+201=002$\n","$2\n","wrong  : 9+201=200\n","correct: 9+201=210\n","outputs(x):  $352+8=953$\n","$9\n","wrong  : 352+8=359\n","correct: 352+8=360\n","outputs(x):  $9+219=812$\n","$8\n","wrong  : 9+219=218\n","correct: 9+219=228\n","outputs(x):  $3+849=248$\n","$3\n","wrong  : 3+849=842\n","correct: 3+849=852\n","outputs(x):  $7+250=741$\n","$8\n","wrong  : 7+250=147\n","correct: 7+250=257\n","outputs(x):  $2+817=828$\n","$1\n","wrong  : 2+817=828\n","correct: 2+817=819\n","outputs(x):  $1+985=589$\n","$7\n","wrong  : 1+985=985\n","correct: 1+985=986\n","outputs(x):  $622+2=326$\n","$4\n","wrong  : 622+2=623\n","correct: 622+2=624\n","outputs(x):  $3+229=222$\n","$9\n","wrong  : 3+229=222\n","correct: 3+229=232\n","outputs(x):  $6+989=5901$\n","$\n","wrong  : 6+989=1095\n","correct: 6+989=995\n","outputs(x):  $31+86=701$\n","$1\n","wrong  : 31+86=107\n","correct: 31+86=117\n","outputs(x):  $8+842=048$\n","$9\n","wrong  : 8+842=840\n","correct: 8+842=850\n","outputs(x):  $0+646=636$\n","$5\n","wrong  : 0+646=636\n","correct: 0+646=646\n","outputs(x):  $392+8=993$\n","$9\n","wrong  : 392+8=399\n","correct: 392+8=400\n","outputs(x):  $2+197=881$\n","$7\n","wrong  : 2+197=188\n","correct: 2+197=199\n","outputs(x):  $1+517=715$\n","$9\n","wrong  : 1+517=517\n","correct: 1+517=518\n","outputs(x):  $2+545=645$\n","$6\n","wrong  : 2+545=546\n","correct: 2+545=547\n","outputs(x):  $4+511=505$\n","$2\n","wrong  : 4+511=505\n","correct: 4+511=515\n","outputs(x):  $6+636=236$\n","$9\n","wrong  : 6+636=632\n","correct: 6+636=642\n","outputs(x):  $3+228=121$\n","$3\n","wrong  : 3+228=121\n","correct: 3+228=231\n","outputs(x):  $608+3=106$\n","$8\n","wrong  : 608+3=601\n","correct: 608+3=611\n","outputs(x):  $1+116=611$\n","$7\n","wrong  : 1+116=116\n","correct: 1+116=117\n","outputs(x):  $9+31=03$\n","$31\n","wrong  : 9+31=30\n","correct: 9+31=40\n","outputs(x):  $3+15=71$\n","$67\n","wrong  : 3+15=17\n","correct: 3+15=18\n","outputs(x):  $75+2=67$\n","$96\n","wrong  : 75+2=76\n","correct: 75+2=77\n","outputs(x):  $9+21=02$\n","$94\n","wrong  : 9+21=20\n","correct: 9+21=30\n","outputs(x):  $2+36=72$\n","$42\n","wrong  : 2+36=27\n","correct: 2+36=38\n","outputs(x):  $2+16=71$\n","$99\n","wrong  : 2+16=17\n","correct: 2+16=18\n","100% 80/80 [00:03<00:00, 20.96it/s]\n","accuracy of 9900 examples: 7839/9900 (79.18181818181819%)\n","{'carry0': 69.36708860759494, 'carry1': 77.42470295661785, 'carry2': 83.74296713058928, 'carry3': 84.06344410876133, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 20147.17it/s]\n","100% 80/80 [00:02<00:00, 28.28it/s]\n","accuracy of 10000 examples: 8005/10000 (80.05%)\n","{'carry0': 72.52, 'carry1': 79.60000000000001, 'carry2': 84.64, 'carry3': 83.44, 'carry4': nan, 'carry5': nan}\n","step 1150: train loss 0.9837, val loss 1.0393\n","iter 1150: loss 1.0391, time 38748.28ms, mfu 3.99%\n","iter 1160: loss 1.0035, time 220.61ms, mfu 4.26%\n","iter 1170: loss 1.0068, time 234.74ms, mfu 4.47%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n","  0% 0/80 [00:00<?, ?it/s]outputs(x):  $464+758=1221$\n","$\n","wrong  : 464+758=1221\n","correct: 464+758=1222\n","outputs(x):  $362+637=998$\n","$9\n","wrong  : 362+637=899\n","correct: 362+637=999\n","outputs(x):  $130+355=583$\n","$2\n","wrong  : 130+355=385\n","correct: 130+355=485\n","outputs(x):  $394+607=0001$\n","$\n","wrong  : 394+607=1000\n","correct: 394+607=1001\n","  4% 3/80 [00:00<00:02, 25.78it/s]outputs(x):  $124+788=119$\n","$9\n","wrong  : 124+788=911\n","correct: 124+788=912\n","  8% 6/80 [00:00<00:05, 12.96it/s]outputs(x):  $916+377=3911$\n","$\n","wrong  : 916+377=1193\n","correct: 916+377=1293\n","outputs(x):  $291+175=663$\n","$8\n","wrong  : 291+175=366\n","correct: 291+175=466\n"," 11% 9/80 [00:00<00:04, 16.84it/s]outputs(x):  $253+211=463$\n","$1\n","wrong  : 253+211=364\n","correct: 253+211=464\n"," 15% 12/80 [00:00<00:03, 19.34it/s]outputs(x):  $536+366=109$\n","$6\n","wrong  : 536+366=901\n","correct: 536+366=902\n"," 19% 15/80 [00:00<00:03, 21.62it/s]outputs(x):  $807+788=4951$\n","$\n","wrong  : 807+788=1594\n","correct: 807+788=1595\n","outputs(x):  $836+817=2561$\n","$\n","wrong  : 836+817=1652\n","correct: 836+817=1653\n","outputs(x):  $590+785=5741$\n","$\n","wrong  : 590+785=1475\n","correct: 590+785=1375\n","outputs(x):  $182+276=853$\n","$8\n","wrong  : 182+276=358\n","correct: 182+276=458\n"," 22% 18/80 [00:00<00:02, 22.91it/s]outputs(x):  $980+571=2551$\n","$\n","wrong  : 980+571=1552\n","correct: 980+571=1551\n","outputs(x):  $240+911=1501$\n","$\n","wrong  : 240+911=1051\n","correct: 240+911=1151\n"," 26% 21/80 [00:01<00:02, 23.79it/s]outputs(x):  $676+827=2051$\n","$\n","wrong  : 676+827=1502\n","correct: 676+827=1503\n","outputs(x):  $230+812=249$\n","$1\n","wrong  : 230+812=942\n","correct: 230+812=1042\n","outputs(x):  $475+867=1431$\n","$\n","wrong  : 475+867=1341\n","correct: 475+867=1342\n"," 30% 24/80 [00:01<00:02, 24.27it/s]outputs(x):  $271+234=504$\n","$2\n","wrong  : 271+234=405\n","correct: 271+234=505\n","outputs(x):  $574+773=7441$\n","$\n","wrong  : 574+773=1447\n","correct: 574+773=1347\n","outputs(x):  $911+388=9911$\n","$\n","wrong  : 911+388=1199\n","correct: 911+388=1299\n","outputs(x):  $223+851=479$\n","$4\n","wrong  : 223+851=974\n","correct: 223+851=1074\n","outputs(x):  $493+808=0031$\n","$\n","wrong  : 493+808=1300\n","correct: 493+808=1301\n","outputs(x):  $283+798=0801$\n","$\n","wrong  : 283+798=1080\n","correct: 283+798=1081\n"," 38% 30/80 [00:01<00:01, 25.38it/s]outputs(x):  $731+580=1031$\n","$\n","wrong  : 731+580=1301\n","correct: 731+580=1311\n","outputs(x):  $535+868=2041$\n","$\n","wrong  : 535+868=1402\n","correct: 535+868=1403\n","outputs(x):  $234+334=864$\n","$4\n","wrong  : 234+334=468\n","correct: 234+334=568\n"," 41% 33/80 [00:01<00:01, 25.84it/s]outputs(x):  $225+183=803$\n","$6\n","wrong  : 225+183=308\n","correct: 225+183=408\n","outputs(x):  $259+885=4401$\n","$\n","wrong  : 259+885=1044\n","correct: 259+885=1144\n"," 45% 36/80 [00:01<00:01, 26.16it/s]outputs(x):  $591+242=328$\n","$6\n","wrong  : 591+242=823\n","correct: 591+242=833\n","outputs(x):  $346+927=3711$\n","$\n","wrong  : 346+927=1173\n","correct: 346+927=1273\n"," 49% 39/80 [00:01<00:01, 26.23it/s]outputs(x):  $273+223=693$\n","$7\n","wrong  : 273+223=396\n","correct: 273+223=496\n"," 52% 42/80 [00:01<00:01, 26.36it/s]outputs(x):  $380+710=099$\n","$9\n","wrong  : 380+710=990\n","correct: 380+710=1090\n"," 56% 45/80 [00:01<00:01, 26.74it/s]outputs(x):  $646+218=368$\n","$6\n","wrong  : 646+218=863\n","correct: 646+218=864\n","outputs(x):  $770+901=2761$\n","$\n","wrong  : 770+901=1672\n","correct: 770+901=1671\n","outputs(x):  $634+567=0021$\n","$\n","wrong  : 634+567=1200\n","correct: 634+567=1201\n"," 60% 48/80 [00:02<00:01, 26.22it/s]outputs(x):  $890+235=6211$\n","$\n","wrong  : 890+235=1126\n","correct: 890+235=1125\n","outputs(x):  $644+767=0141$\n","$\n","wrong  : 644+767=1410\n","correct: 644+767=1411\n","outputs(x):  $230+221=153$\n","$1\n","wrong  : 230+221=351\n","correct: 230+221=451\n"," 64% 51/80 [00:02<00:01, 25.91it/s]outputs(x):  $953+418=0731$\n","$\n","wrong  : 953+418=1370\n","correct: 953+418=1371\n"," 68% 54/80 [00:02<00:00, 26.04it/s]outputs(x):  $184+787=079$\n","$4\n","wrong  : 184+787=970\n","correct: 184+787=971\n","outputs(x):  $564+868=1341$\n","$\n","wrong  : 564+868=1431\n","correct: 564+868=1432\n","outputs(x):  $311+591=309$\n","$1\n","wrong  : 311+591=903\n","correct: 311+591=902\n"," 71% 57/80 [00:02<00:00, 25.78it/s]outputs(x):  $913+588=2051$\n","$\n","wrong  : 913+588=1502\n","correct: 913+588=1501\n"," 75% 60/80 [00:02<00:00, 25.72it/s]outputs(x):  $932+474=6051$\n","$\n","wrong  : 932+474=1506\n","correct: 932+474=1406\n","outputs(x):  $904+506=9041$\n","$\n","wrong  : 904+506=1409\n","correct: 904+506=1410\n"," 79% 63/80 [00:02<00:00, 26.49it/s]outputs(x):  $46+260=602$\n","$7\n","wrong  : 46+260=206\n","correct: 46+260=306\n"," 82% 66/80 [00:02<00:00, 26.74it/s]outputs(x):  $803+17=918$\n","$8\n","wrong  : 803+17=819\n","correct: 803+17=820\n","outputs(x):  $76+794=079$\n","$5\n","wrong  : 76+794=970\n","correct: 76+794=870\n","outputs(x):  $70+741=1181$\n","$\n","wrong  : 70+741=1811\n","correct: 70+741=811\n","outputs(x):  $72+230=202$\n","$7\n","wrong  : 72+230=202\n","correct: 72+230=302\n","outputs(x):  $59+756=519$\n","$9\n","wrong  : 59+756=915\n","correct: 59+756=815\n"," 86% 69/80 [00:02<00:00, 25.92it/s]outputs(x):  $84+799=288$\n","$2\n","wrong  : 84+799=882\n","correct: 84+799=883\n","outputs(x):  $667+69=638$\n","$3\n","wrong  : 667+69=836\n","correct: 667+69=736\n","outputs(x):  $73+178=052$\n","$3\n","wrong  : 73+178=250\n","correct: 73+178=251\n","outputs(x):  $66+576=245$\n","$5\n","wrong  : 66+576=542\n","correct: 66+576=642\n","outputs(x):  $76+773=958$\n","$6\n","wrong  : 76+773=859\n","correct: 76+773=849\n"," 90% 72/80 [00:02<00:00, 25.61it/s]outputs(x):  $21+299=022$\n","$6\n","wrong  : 21+299=220\n","correct: 21+299=320\n"," 94% 75/80 [00:03<00:00, 25.73it/s]outputs(x):  $184+98=182$\n","$6\n","wrong  : 184+98=281\n","correct: 184+98=282\n","outputs(x):  $51+223=471$\n","$2\n","wrong  : 51+223=174\n","correct: 51+223=274\n","outputs(x):  $0+646=647$\n","$6\n","wrong  : 0+646=746\n","correct: 0+646=646\n","outputs(x):  $8+758=677$\n","$9\n","wrong  : 8+758=776\n","correct: 8+758=766\n","outputs(x):  $9+845=4521$\n","$\n","wrong  : 9+845=1254\n","correct: 9+845=854\n","outputs(x):  $140+5=551$\n","$1\n","wrong  : 140+5=155\n","correct: 140+5=145\n","outputs(x):  $6+636=256$\n","$4\n","wrong  : 6+636=652\n","correct: 6+636=642\n","outputs(x):  $8+842=059$\n","$3\n","wrong  : 8+842=950\n","correct: 8+842=850\n","outputs(x):  $9+987=6901$\n","$\n","wrong  : 9+987=1096\n","correct: 9+987=996\n","outputs(x):  $3+849=2581$\n","$\n","wrong  : 3+849=1852\n","correct: 3+849=852\n","outputs(x):  $9+219=832$\n","$5\n","wrong  : 9+219=238\n","correct: 9+219=228\n","outputs(x):  $7+977=499$\n","$5\n","wrong  : 7+977=994\n","correct: 7+977=984\n","outputs(x):  $8+572=075$\n","$7\n","wrong  : 8+572=570\n","correct: 8+572=580\n","outputs(x):  $2+899=1091$\n","$\n","wrong  : 2+899=1901\n","correct: 2+899=901\n","outputs(x):  $4+787=1971$\n","$\n","wrong  : 4+787=1791\n","correct: 4+787=791\n"," 98% 78/80 [00:03<00:00, 25.40it/s]outputs(x):  $5+172=781$\n","$6\n","wrong  : 5+172=187\n","correct: 5+172=177\n","outputs(x):  $7+699=608$\n","$5\n","wrong  : 7+699=806\n","correct: 7+699=706\n","outputs(x):  $8+625=3361$\n","$\n","wrong  : 8+625=1633\n","correct: 8+625=633\n","outputs(x):  $4+919=3291$\n","$\n","wrong  : 4+919=1923\n","correct: 4+919=923\n","outputs(x):  $642+9=166$\n","$8\n","wrong  : 642+9=661\n","correct: 642+9=651\n","outputs(x):  $519+7=635$\n","$8\n","wrong  : 519+7=536\n","correct: 519+7=526\n","outputs(x):  $9+617=6161$\n","$\n","wrong  : 9+617=1616\n","correct: 9+617=626\n","outputs(x):  $7+975=2891$\n","$\n","wrong  : 7+975=1982\n","correct: 7+975=982\n","outputs(x):  $0+741=1471$\n","$\n","wrong  : 0+741=1741\n","correct: 0+741=741\n","outputs(x):  $500+8=815$\n","$3\n","wrong  : 500+8=518\n","correct: 500+8=508\n","outputs(x):  $1+116=721$\n","$1\n","wrong  : 1+116=127\n","correct: 1+116=117\n","outputs(x):  $9+162=181$\n","$7\n","wrong  : 9+162=181\n","correct: 9+162=171\n","outputs(x):  $7+785=2971$\n","$\n","wrong  : 7+785=1792\n","correct: 7+785=792\n","outputs(x):  $0+360=073$\n","$7\n","wrong  : 0+360=370\n","correct: 0+360=360\n","outputs(x):  $6+512=825$\n","$9\n","wrong  : 6+512=528\n","correct: 6+512=518\n","outputs(x):  $9+210=922$\n","$5\n","wrong  : 9+210=229\n","correct: 9+210=219\n","outputs(x):  $9+23=22$\n","$96\n","wrong  : 9+23=22\n","correct: 9+23=32\n","outputs(x):  $3+15=82$\n","$39\n","wrong  : 3+15=28\n","correct: 3+15=18\n","outputs(x):  $7+66=38$\n","$67\n","wrong  : 7+66=83\n","correct: 7+66=73\n","outputs(x):  $9+31=03$\n","$46\n","wrong  : 9+31=30\n","correct: 9+31=40\n","outputs(x):  $0+10=02$\n","$26\n","wrong  : 0+10=20\n","correct: 0+10=10\n","100% 80/80 [00:03<00:00, 24.42it/s]\n","accuracy of 9900 examples: 9806/9900 (99.05050505050505%)\n","{'carry0': 98.8607594936709, 'carry1': 98.92235424150317, 'carry2': 99.25969795676636, 'carry3': 99.09365558912387, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 19708.89it/s]\n","100% 80/80 [00:02<00:00, 27.82it/s]\n","accuracy of 10000 examples: 9901/10000 (99.00999999999999%)\n","{'carry0': 98.36, 'carry1': 99.08, 'carry2': 99.4, 'carry3': 99.2, 'carry4': nan, 'carry5': nan}\n","step 1175: train loss 0.9566, val loss 1.0230\n","iter 1180: loss 0.9892, time 219.58ms, mfu 4.71%\n","iter 1190: loss 0.9855, time 225.78ms, mfu 4.90%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n"," 30% 24/80 [00:00<00:02, 27.41it/s]outputs(x):  $920+508=8231$\n","$\n","wrong  : 920+508=1328\n","correct: 920+508=1428\n"," 45% 36/80 [00:01<00:01, 27.36it/s]outputs(x):  $497+810=7041$\n","$\n","wrong  : 497+810=1407\n","correct: 497+810=1307\n"," 64% 51/80 [00:01<00:01, 27.42it/s]outputs(x):  $168+335=405$\n","$6\n","wrong  : 168+335=504\n","correct: 168+335=503\n","outputs(x):  $986+957=2491$\n","$\n","wrong  : 986+957=1942\n","correct: 986+957=1943\n"," 82% 66/80 [00:02<00:00, 27.94it/s]outputs(x):  $198+21=913$\n","$6\n","wrong  : 198+21=319\n","correct: 198+21=219\n"," 90% 72/80 [00:02<00:00, 27.85it/s]outputs(x):  $214+13=822$\n","$2\n","wrong  : 214+13=228\n","correct: 214+13=227\n"," 94% 75/80 [00:02<00:00, 27.84it/s]outputs(x):  $5+172=762$\n","$6\n","wrong  : 5+172=267\n","correct: 5+172=177\n","outputs(x):  $9+987=6901$\n","$\n","wrong  : 9+987=1096\n","correct: 9+987=996\n","outputs(x):  $8+758=657$\n","$5\n","wrong  : 8+758=756\n","correct: 8+758=766\n","outputs(x):  $3+457=084$\n","$6\n","wrong  : 3+457=480\n","correct: 3+457=460\n","outputs(x):  $6+989=5931$\n","$\n","wrong  : 6+989=1395\n","correct: 6+989=995\n","outputs(x):  $7+520=765$\n","$2\n","wrong  : 7+520=567\n","correct: 7+520=527\n","outputs(x):  $4+386=004$\n","$6\n","wrong  : 4+386=400\n","correct: 4+386=390\n","outputs(x):  $7+869=609$\n","$7\n","wrong  : 7+869=906\n","correct: 7+869=876\n","outputs(x):  $1+985=6801$\n","$\n","wrong  : 1+985=1086\n","correct: 1+985=986\n","outputs(x):  $7+699=696$\n","$8\n","wrong  : 7+699=696\n","correct: 7+699=706\n","outputs(x):  $6+636=216$\n","$3\n","wrong  : 6+636=612\n","correct: 6+636=642\n","outputs(x):  $50+82=221$\n","$8\n","wrong  : 50+82=122\n","correct: 50+82=132\n","outputs(x):  $6+957=3301$\n","$\n","wrong  : 6+957=1033\n","correct: 6+957=963\n","outputs(x):  $2+878=098$\n","$8\n","wrong  : 2+878=890\n","correct: 2+878=880\n"," 98% 78/80 [00:02<00:00, 27.68it/s]outputs(x):  $7+977=479$\n","$6\n","wrong  : 7+977=974\n","correct: 7+977=984\n","outputs(x):  $9+483=285$\n","$1\n","wrong  : 9+483=582\n","correct: 9+483=492\n","outputs(x):  $4+456=044$\n","$9\n","wrong  : 4+456=440\n","correct: 4+456=460\n","outputs(x):  $9+201=022$\n","$9\n","wrong  : 9+201=220\n","correct: 9+201=210\n","outputs(x):  $2+650=266$\n","$7\n","wrong  : 2+650=662\n","correct: 2+650=652\n","outputs(x):  $9+750=956$\n","$4\n","wrong  : 9+750=659\n","correct: 9+750=759\n","outputs(x):  $6+631=776$\n","$6\n","wrong  : 6+631=677\n","correct: 6+631=637\n","outputs(x):  $4+712=607$\n","$2\n","wrong  : 4+712=706\n","correct: 4+712=716\n","outputs(x):  $7+250=782$\n","$6\n","wrong  : 7+250=287\n","correct: 7+250=257\n","outputs(x):  $7+996=3901$\n","$\n","wrong  : 7+996=1093\n","correct: 7+996=1003\n","outputs(x):  $3+849=259$\n","$1\n","wrong  : 3+849=952\n","correct: 3+849=852\n","outputs(x):  $0+360=053$\n","$4\n","wrong  : 0+360=350\n","correct: 0+360=360\n","outputs(x):  $6+598=495$\n","$4\n","wrong  : 6+598=594\n","correct: 6+598=604\n","outputs(x):  $9+507=605$\n","$8\n","wrong  : 9+507=506\n","correct: 9+507=516\n","outputs(x):  $6+555=155$\n","$4\n","wrong  : 6+555=551\n","correct: 6+555=561\n","outputs(x):  $9+210=992$\n","$7\n","wrong  : 9+210=299\n","correct: 9+210=219\n","outputs(x):  $6+512=855$\n","$2\n","wrong  : 6+512=558\n","correct: 6+512=518\n","100% 80/80 [00:02<00:00, 27.35it/s]\n","accuracy of 9900 examples: 9863/9900 (99.62626262626263%)\n","{'carry0': 99.24050632911391, 'carry1': 99.47499309201436, 'carry2': 99.88155167308261, 'carry3': 99.8489425981873, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 19449.39it/s]\n","100% 80/80 [00:02<00:00, 27.32it/s]\n","accuracy of 10000 examples: 9951/10000 (99.51%)\n","{'carry0': 99.08, 'carry1': 99.03999999999999, 'carry2': 99.96000000000001, 'carry3': 99.96000000000001, 'carry4': nan, 'carry5': nan}\n","step 1200: train loss 0.9474, val loss 1.0298\n","iter 1200: loss 0.9785, time 38005.68ms, mfu 4.41%\n","iter 1210: loss 0.9795, time 220.36ms, mfu 4.65%\n","iter 1220: loss 0.9728, time 233.58ms, mfu 4.82%\n","iter 1230: loss 0.9715, time 228.88ms, mfu 4.99%\n","iter 1240: loss 0.9711, time 229.56ms, mfu 5.14%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n","  4% 3/80 [00:00<00:02, 26.85it/s]outputs(x):  $331+220=255$\n","$8\n","wrong  : 331+220=552\n","correct: 331+220=551\n"," 64% 51/80 [00:01<00:01, 27.98it/s]outputs(x):  $355+368=427$\n","$9\n","wrong  : 355+368=724\n","correct: 355+368=723\n"," 82% 66/80 [00:02<00:00, 28.29it/s]outputs(x):  $568+34=295$\n","$9\n","wrong  : 568+34=592\n","correct: 568+34=602\n"," 90% 72/80 [00:02<00:00, 28.24it/s]outputs(x):  $95+726=129$\n","$8\n","wrong  : 95+726=921\n","correct: 95+726=821\n"," 94% 75/80 [00:02<00:00, 28.01it/s]outputs(x):  $9+210=922$\n","$5\n","wrong  : 9+210=229\n","correct: 9+210=219\n","outputs(x):  $9+201=022$\n","$1\n","wrong  : 9+201=220\n","correct: 9+201=210\n","outputs(x):  $6+301=713$\n","$2\n","wrong  : 6+301=317\n","correct: 6+301=307\n","outputs(x):  $6+989=5901$\n","$\n","wrong  : 6+989=1095\n","correct: 6+989=995\n"," 98% 78/80 [00:02<00:00, 28.07it/s]outputs(x):  $209+6=522$\n","$2\n","wrong  : 209+6=225\n","correct: 209+6=215\n","100% 80/80 [00:02<00:00, 27.77it/s]\n","accuracy of 9900 examples: 9891/9900 (99.90909090909092%)\n","{'carry0': 99.81012658227849, 'carry1': 99.91710417242332, 'carry2': 99.91116375481197, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 13943.44it/s]\n","100% 80/80 [00:02<00:00, 27.58it/s]\n","accuracy of 10000 examples: 9987/10000 (99.87%)\n","{'carry0': 99.64, 'carry1': 99.92, 'carry2': 99.92, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 1250: train loss 0.9232, val loss 1.0400\n","saving checkpoint to out2/addition_reverse_more_early_eval_v2/ckpt_teaching_addition_reverse_10000.pt\n","iter 1250: loss 0.9627, time 38414.79ms, mfu 4.63%\n","iter 1260: loss 0.9663, time 222.34ms, mfu 4.84%\n","iter 1270: loss 0.9578, time 234.11ms, mfu 4.99%\n","iter 1280: loss 0.9536, time 232.35ms, mfu 5.13%\n","iter 1290: loss 0.9457, time 227.99ms, mfu 5.28%\n","iter 1300: loss 0.9407, time 227.12ms, mfu 5.41%\n","iter 1310: loss 0.9313, time 228.88ms, mfu 5.52%\n","iter 1320: loss 0.9270, time 228.62ms, mfu 5.62%\n","iter 1330: loss 0.9624, time 229.12ms, mfu 5.71%\n","iter 1340: loss 0.9592, time 231.23ms, mfu 5.78%\n","iter 1350: loss 0.9189, time 229.07ms, mfu 5.85%\n","iter 1360: loss 0.9043, time 231.49ms, mfu 5.91%\n","iter 1370: loss 0.8921, time 230.76ms, mfu 5.97%\n","iter 1380: loss 0.8845, time 231.03ms, mfu 6.02%\n","iter 1390: loss 0.8749, time 232.04ms, mfu 6.06%\n","iter 1400: loss 0.8591, time 230.55ms, mfu 6.10%\n","iter 1410: loss 0.8502, time 232.12ms, mfu 6.13%\n","iter 1420: loss 0.8340, time 231.71ms, mfu 6.16%\n","iter 1430: loss 0.8774, time 229.06ms, mfu 6.20%\n","iter 1440: loss 0.8607, time 232.88ms, mfu 6.22%\n","iter 1450: loss 0.8278, time 233.36ms, mfu 6.24%\n","iter 1460: loss 0.8058, time 230.80ms, mfu 6.26%\n","iter 1470: loss 0.7800, time 232.76ms, mfu 6.27%\n","iter 1480: loss 0.7608, time 232.33ms, mfu 6.29%\n","iter 1490: loss 0.7378, time 229.20ms, mfu 6.31%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n"," 94% 75/80 [00:02<00:00, 28.11it/s]outputs(x):  $5+564=975$\n","$9\n","wrong  : 5+564=579\n","correct: 5+564=569\n","outputs(x):  $7+699=617$\n","$5\n","wrong  : 7+699=716\n","correct: 7+699=706\n"," 98% 78/80 [00:02<00:00, 28.22it/s]outputs(x):  $9+550=965$\n","$2\n","wrong  : 9+550=569\n","correct: 9+550=559\n","100% 80/80 [00:02<00:00, 28.17it/s]\n","accuracy of 9900 examples: 9897/9900 (99.96969696969697%)\n","{'carry0': 99.87341772151899, 'carry1': 100.0, 'carry2': 99.97038791827066, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 20090.24it/s]\n","100% 80/80 [00:02<00:00, 28.11it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 99.96000000000001, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 1500: train loss 0.5635, val loss 1.3753\n","saving checkpoint to out2/addition_reverse_more_early_eval_v2/ckpt_teaching_addition_reverse_10000.pt\n","iter 1500: loss 0.7219, time 38097.86ms, mfu 5.68%\n","iter 1510: loss 0.7104, time 220.79ms, mfu 5.79%\n","iter 1520: loss 0.6955, time 231.38ms, mfu 5.86%\n","iter 1530: loss 0.6807, time 233.77ms, mfu 5.91%\n","iter 1540: loss 0.6543, time 225.89ms, mfu 5.98%\n","iter 1550: loss 0.6459, time 228.83ms, mfu 6.03%\n","iter 1560: loss 0.6362, time 230.95ms, mfu 6.08%\n","iter 1570: loss 0.6192, time 228.17ms, mfu 6.12%\n","iter 1580: loss 0.5867, time 228.52ms, mfu 6.16%\n","iter 1590: loss 0.5866, time 231.88ms, mfu 6.19%\n","iter 1600: loss 0.5496, time 229.99ms, mfu 6.22%\n","iter 1610: loss 0.5413, time 227.38ms, mfu 6.25%\n","iter 1620: loss 0.5211, time 231.57ms, mfu 6.27%\n","iter 1630: loss 0.5195, time 231.98ms, mfu 6.29%\n","iter 1640: loss 0.5063, time 227.58ms, mfu 6.32%\n","iter 1650: loss 0.4767, time 232.87ms, mfu 6.32%\n","iter 1660: loss 0.4599, time 231.34ms, mfu 6.34%\n","iter 1670: loss 0.4441, time 231.26ms, mfu 6.35%\n","iter 1680: loss 0.4223, time 229.33ms, mfu 6.36%\n","iter 1690: loss 0.4137, time 231.51ms, mfu 6.37%\n","iter 1700: loss 0.3974, time 232.17ms, mfu 6.38%\n","iter 1710: loss 0.3836, time 231.47ms, mfu 6.38%\n","iter 1720: loss 0.3769, time 229.89ms, mfu 6.40%\n","iter 1730: loss 0.3604, time 229.79ms, mfu 6.40%\n","iter 1740: loss 0.3530, time 230.35ms, mfu 6.41%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n"," 79% 63/80 [00:02<00:00, 26.97it/s]outputs(x):  $9+162=181$\n","$2\n","wrong  : 9+162=181\n","correct: 9+162=171\n","outputs(x):  $9+845=448$\n","$7\n","wrong  : 9+845=844\n","correct: 9+845=854\n","outputs(x):  $7+869=688$\n","$1\n","wrong  : 7+869=886\n","correct: 7+869=876\n","outputs(x):  $6+989=5001$\n","$\n","wrong  : 6+989=1005\n","correct: 6+989=995\n"," 86% 69/80 [00:02<00:00, 26.96it/s]outputs(x):  $861+62=339$\n","$8\n","wrong  : 861+62=933\n","correct: 861+62=923\n"," 98% 78/80 [00:02<00:00, 26.93it/s]outputs(x):  $9+31=05$\n","$72\n","wrong  : 9+31=50\n","correct: 9+31=40\n","outputs(x):  $0+10=02$\n","$70\n","wrong  : 0+10=20\n","correct: 0+10=10\n","100% 80/80 [00:02<00:00, 27.48it/s]\n","accuracy of 9900 examples: 9893/9900 (99.92929292929293%)\n","{'carry0': 99.9367088607595, 'carry1': 99.83420834484664, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 20046.45it/s]\n","100% 80/80 [00:02<00:00, 27.97it/s]\n","accuracy of 10000 examples: 9996/10000 (99.96000000000001%)\n","{'carry0': 99.96000000000001, 'carry1': 99.88, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 1750: train loss 0.1608, val loss 2.2069\n","iter 1750: loss 0.3366, time 37935.54ms, mfu 5.77%\n","iter 1760: loss 0.3264, time 219.58ms, mfu 5.88%\n","iter 1770: loss 0.3124, time 231.58ms, mfu 5.93%\n","iter 1780: loss 0.2993, time 232.43ms, mfu 5.98%\n","iter 1790: loss 0.2952, time 228.23ms, mfu 6.04%\n","iter 1800: loss 0.2862, time 230.19ms, mfu 6.08%\n","iter 1810: loss 0.2783, time 229.84ms, mfu 6.12%\n","iter 1820: loss 0.2646, time 227.02ms, mfu 6.17%\n","iter 1830: loss 0.2618, time 229.78ms, mfu 6.20%\n","iter 1840: loss 0.2504, time 229.89ms, mfu 6.23%\n","iter 1850: loss 0.2404, time 230.36ms, mfu 6.25%\n","iter 1860: loss 0.2357, time 230.76ms, mfu 6.28%\n","iter 1870: loss 0.2297, time 231.12ms, mfu 6.29%\n","iter 1880: loss 0.2235, time 231.59ms, mfu 6.31%\n","iter 1890: loss 0.2182, time 231.94ms, mfu 6.32%\n","iter 1900: loss 0.2121, time 230.15ms, mfu 6.34%\n","iter 1910: loss 0.2067, time 228.45ms, mfu 6.36%\n","iter 1920: loss 0.2002, time 230.66ms, mfu 6.37%\n","iter 1930: loss 0.1941, time 229.93ms, mfu 6.38%\n","iter 1940: loss 0.1932, time 226.91ms, mfu 6.40%\n","iter 1950: loss 0.1872, time 231.87ms, mfu 6.40%\n","iter 1960: loss 0.1813, time 231.33ms, mfu 6.41%\n","iter 1970: loss 0.1782, time 230.89ms, mfu 6.41%\n","iter 1980: loss 0.1741, time 231.71ms, mfu 6.42%\n","iter 1990: loss 0.1718, time 230.55ms, mfu 6.42%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n","  4% 3/80 [00:00<00:02, 27.56it/s]outputs(x):  $154+61=511$\n","$8\n","wrong  : 154+61=115\n","correct: 154+61=215\n"," 86% 69/80 [00:02<00:00, 27.61it/s]outputs(x):  $184+607=196$\n","$2\n","wrong  : 184+607=691\n","correct: 184+607=791\n"," 94% 75/80 [00:02<00:00, 27.56it/s]outputs(x):  $4+919=3221$\n","$\n","wrong  : 4+919=1223\n","correct: 4+919=923\n","outputs(x):  $5+564=961$\n","$3\n","wrong  : 5+564=169\n","correct: 5+564=569\n","outputs(x):  $2+712=416$\n","$6\n","wrong  : 2+712=614\n","correct: 2+712=714\n","outputs(x):  $9+845=429$\n","$6\n","wrong  : 9+845=924\n","correct: 9+845=854\n"," 98% 78/80 [00:02<00:00, 27.68it/s]outputs(x):  $4+447=153$\n","$8\n","wrong  : 4+447=351\n","correct: 4+447=451\n","outputs(x):  $3+723=626$\n","$1\n","wrong  : 3+723=626\n","correct: 3+723=726\n","outputs(x):  $6+555=164$\n","$6\n","wrong  : 6+555=461\n","correct: 6+555=561\n","outputs(x):  $3+576=971$\n","$3\n","wrong  : 3+576=179\n","correct: 3+576=579\n","100% 80/80 [00:02<00:00, 27.61it/s]\n","accuracy of 9900 examples: 9890/9900 (99.8989898989899%)\n","{'carry0': 99.74683544303798, 'carry1': 99.83420834484664, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 19660.37it/s]\n","100% 80/80 [00:02<00:00, 27.75it/s]\n","accuracy of 10000 examples: 9998/10000 (99.98%)\n","{'carry0': 99.96000000000001, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 99.96000000000001, 'carry4': nan, 'carry5': nan}\n","step 2000: train loss 0.0795, val loss 2.8399\n","iter 2000: loss 0.1689, time 38088.94ms, mfu 5.78%\n","iter 2010: loss 0.1653, time 222.99ms, mfu 5.87%\n","iter 2020: loss 0.1634, time 233.72ms, mfu 5.92%\n","iter 2030: loss 0.1610, time 231.32ms, mfu 5.98%\n","iter 2040: loss 0.1553, time 227.81ms, mfu 6.03%\n","iter 2050: loss 0.1485, time 227.96ms, mfu 6.09%\n","iter 2060: loss 0.1498, time 229.84ms, mfu 6.13%\n","iter 2070: loss 0.1505, time 230.16ms, mfu 6.16%\n","iter 2080: loss 0.1511, time 227.44ms, mfu 6.20%\n","iter 2090: loss 0.1425, time 229.95ms, mfu 6.23%\n","iter 2100: loss 0.1415, time 229.77ms, mfu 6.26%\n","iter 2110: loss 0.1418, time 230.34ms, mfu 6.28%\n","iter 2120: loss 0.1361, time 231.36ms, mfu 6.30%\n","iter 2130: loss 0.1348, time 231.16ms, mfu 6.31%\n","iter 2140: loss 0.1363, time 231.86ms, mfu 6.32%\n","iter 2150: loss 0.1383, time 231.64ms, mfu 6.34%\n","iter 2160: loss 0.1347, time 229.13ms, mfu 6.35%\n","iter 2170: loss 0.1326, time 228.64ms, mfu 6.37%\n","iter 2180: loss 0.1335, time 232.75ms, mfu 6.37%\n","iter 2190: loss 0.1306, time 229.95ms, mfu 6.39%\n","iter 2200: loss 0.1240, time 232.70ms, mfu 6.39%\n","iter 2210: loss 0.1259, time 228.65ms, mfu 6.40%\n","iter 2220: loss 0.1235, time 228.32ms, mfu 6.42%\n","iter 2230: loss 0.1247, time 230.69ms, mfu 6.42%\n","iter 2240: loss 0.1248, time 228.36ms, mfu 6.43%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n"," 41% 33/80 [00:01<00:01, 27.99it/s]outputs(x):  $361+858=9131$\n","$\n","wrong  : 361+858=1319\n","correct: 361+858=1219\n"," 64% 51/80 [00:01<00:01, 27.51it/s]outputs(x):  $671+919=0851$\n","$\n","wrong  : 671+919=1580\n","correct: 671+919=1590\n"," 68% 54/80 [00:01<00:00, 27.50it/s]outputs(x):  $782+259=149$\n","$7\n","wrong  : 782+259=941\n","correct: 782+259=1041\n"," 79% 63/80 [00:02<00:00, 27.51it/s]outputs(x):  $196+28=322$\n","$6\n","wrong  : 196+28=223\n","correct: 196+28=224\n","100% 80/80 [00:02<00:00, 27.66it/s]\n","accuracy of 9900 examples: 9896/9900 (99.95959595959596%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 99.91116375481197, 'carry3': 99.92447129909365, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 19857.73it/s]\n","100% 80/80 [00:02<00:00, 27.71it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","{'carry0': 99.96000000000001, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 2250: train loss 0.0700, val loss 3.1561\n","iter 2250: loss 0.1224, time 37989.65ms, mfu 5.79%\n","iter 2260: loss 0.1188, time 219.71ms, mfu 5.89%\n","iter 2270: loss 0.1216, time 230.36ms, mfu 5.95%\n","iter 2280: loss 0.1200, time 230.83ms, mfu 6.00%\n","iter 2290: loss 0.1205, time 228.86ms, mfu 6.05%\n","iter 2300: loss 0.1189, time 229.02ms, mfu 6.10%\n","iter 2310: loss 0.1162, time 231.64ms, mfu 6.13%\n","iter 2320: loss 0.1149, time 225.90ms, mfu 6.18%\n","iter 2330: loss 0.1119, time 228.89ms, mfu 6.21%\n","iter 2340: loss 0.1113, time 225.34ms, mfu 6.26%\n","iter 2350: loss 0.1134, time 230.06ms, mfu 6.28%\n","iter 2360: loss 0.1124, time 229.44ms, mfu 6.30%\n","iter 2370: loss 0.1118, time 229.66ms, mfu 6.32%\n","iter 2380: loss 0.1129, time 229.09ms, mfu 6.34%\n","iter 2390: loss 0.1097, time 231.54ms, mfu 6.35%\n","iter 2400: loss 0.1079, time 229.81ms, mfu 6.36%\n","iter 2410: loss 0.1082, time 231.15ms, mfu 6.37%\n","iter 2420: loss 0.1074, time 231.08ms, mfu 6.38%\n","iter 2430: loss 0.1070, time 229.87ms, mfu 6.39%\n","iter 2440: loss 0.1058, time 231.10ms, mfu 6.40%\n","iter 2450: loss 0.1086, time 229.86ms, mfu 6.41%\n","iter 2460: loss 0.1040, time 228.73ms, mfu 6.42%\n","iter 2470: loss 0.1041, time 231.53ms, mfu 6.42%\n","iter 2480: loss 0.1045, time 229.14ms, mfu 6.43%\n","iter 2490: loss 0.1041, time 230.22ms, mfu 6.44%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n"," 94% 75/80 [00:02<00:00, 28.25it/s]outputs(x):  $6+636=236$\n","$7\n","wrong  : 6+636=632\n","correct: 6+636=642\n"," 98% 78/80 [00:02<00:00, 28.27it/s]outputs(x):  $5+902=7001$\n","$\n","wrong  : 5+902=1007\n","correct: 5+902=907\n","outputs(x):  $4+294=893$\n","$8\n","wrong  : 4+294=398\n","correct: 4+294=298\n","outputs(x):  $3+228=131$\n","$9\n","wrong  : 3+228=131\n","correct: 3+228=231\n","outputs(x):  $2+878=089$\n","$5\n","wrong  : 2+878=980\n","correct: 2+878=880\n","outputs(x):  $3+457=054$\n","$9\n","wrong  : 3+457=450\n","correct: 3+457=460\n","outputs(x):  $7+977=48$\n","$83\n","wrong  : 7+977=84\n","correct: 7+977=984\n","100% 80/80 [00:02<00:00, 28.02it/s]\n","accuracy of 9900 examples: 9893/9900 (99.92929292929293%)\n","{'carry0': 99.87341772151899, 'carry1': 99.8618402873722, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 14277.61it/s]\n","100% 80/80 [00:02<00:00, 27.55it/s]\n","accuracy of 10000 examples: 9993/10000 (99.92999999999999%)\n","{'carry0': 99.83999999999999, 'carry1': 99.92, 'carry2': 100.0, 'carry3': 99.96000000000001, 'carry4': nan, 'carry5': nan}\n","step 2500: train loss 0.0654, val loss 3.4306\n","iter 2500: loss 0.1027, time 38109.86ms, mfu 5.80%\n","iter 2510: loss 0.1034, time 218.81ms, mfu 5.90%\n","iter 2520: loss 0.1027, time 231.63ms, mfu 5.95%\n","iter 2530: loss 0.0994, time 230.61ms, mfu 6.00%\n","iter 2540: loss 0.0994, time 222.19ms, mfu 6.08%\n","iter 2550: loss 0.1045, time 224.78ms, mfu 6.13%\n","iter 2560: loss 0.1021, time 226.54ms, mfu 6.18%\n","iter 2570: loss 0.0993, time 231.98ms, mfu 6.20%\n","iter 2580: loss 0.0991, time 227.55ms, mfu 6.24%\n","iter 2590: loss 0.0990, time 227.34ms, mfu 6.27%\n","iter 2600: loss 0.0987, time 227.79ms, mfu 6.30%\n","iter 2610: loss 0.0952, time 229.32ms, mfu 6.32%\n","iter 2620: loss 0.0964, time 226.33ms, mfu 6.35%\n","iter 2630: loss 0.0987, time 230.43ms, mfu 6.36%\n","iter 2640: loss 0.0948, time 230.18ms, mfu 6.37%\n","iter 2650: loss 0.0927, time 226.32ms, mfu 6.39%\n","iter 2660: loss 0.0939, time 229.05ms, mfu 6.41%\n","iter 2670: loss 0.0922, time 230.72ms, mfu 6.41%\n","iter 2680: loss 0.0946, time 229.65ms, mfu 6.42%\n","iter 2690: loss 0.0948, time 231.23ms, mfu 6.42%\n","iter 2700: loss 0.0911, time 228.38ms, mfu 6.43%\n","iter 2710: loss 0.0934, time 231.79ms, mfu 6.43%\n","iter 2720: loss 0.0916, time 228.18ms, mfu 6.45%\n","iter 2730: loss 0.0928, time 228.34ms, mfu 6.45%\n","iter 2740: loss 0.0911, time 229.26ms, mfu 6.46%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n"," 94% 75/80 [00:02<00:00, 27.55it/s]outputs(x):  $3+457=054$\n","$9\n","wrong  : 3+457=450\n","correct: 3+457=460\n","100% 80/80 [00:02<00:00, 27.58it/s]\n","accuracy of 9900 examples: 9899/9900 (99.98989898989899%)\n","{'carry0': 100.0, 'carry1': 99.97236805747444, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 19326.13it/s]\n","100% 80/80 [00:02<00:00, 27.83it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 2750: train loss 0.0627, val loss 3.5565\n","saving checkpoint to out2/addition_reverse_more_early_eval_v2/ckpt_teaching_addition_reverse_10000.pt\n","iter 2750: loss 0.0916, time 38228.92ms, mfu 5.82%\n","iter 2760: loss 0.0913, time 216.91ms, mfu 5.92%\n","iter 2770: loss 0.0898, time 226.31ms, mfu 5.99%\n","iter 2780: loss 0.0915, time 228.74ms, mfu 6.04%\n","iter 2790: loss 0.0903, time 227.35ms, mfu 6.10%\n","iter 2800: loss 0.0894, time 227.44ms, mfu 6.14%\n","iter 2810: loss 0.0897, time 229.52ms, mfu 6.18%\n","iter 2820: loss 0.0930, time 229.76ms, mfu 6.21%\n","iter 2830: loss 0.0894, time 226.89ms, mfu 6.25%\n","iter 2840: loss 0.0878, time 223.37ms, mfu 6.29%\n","iter 2850: loss 0.0867, time 224.16ms, mfu 6.33%\n","iter 2860: loss 0.0845, time 227.27ms, mfu 6.35%\n","iter 2870: loss 0.0857, time 229.98ms, mfu 6.36%\n","iter 2880: loss 0.0844, time 229.02ms, mfu 6.38%\n","iter 2890: loss 0.0864, time 227.20ms, mfu 6.40%\n","iter 2900: loss 0.0841, time 229.73ms, mfu 6.41%\n","iter 2910: loss 0.0882, time 225.03ms, mfu 6.43%\n","iter 2920: loss 0.0888, time 231.88ms, mfu 6.43%\n","iter 2930: loss 0.0840, time 228.38ms, mfu 6.44%\n","iter 2940: loss 0.0844, time 225.78ms, mfu 6.46%\n","iter 2950: loss 0.0824, time 228.17ms, mfu 6.47%\n","iter 2960: loss 0.0836, time 227.77ms, mfu 6.47%\n","iter 2970: loss 0.0820, time 231.28ms, mfu 6.47%\n","iter 2980: loss 0.0824, time 226.34ms, mfu 6.48%\n","iter 2990: loss 0.0835, time 228.80ms, mfu 6.49%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n","100% 80/80 [00:02<00:00, 28.02it/s]\n","accuracy of 9900 examples: 9900/9900 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 19856.02it/s]\n","100% 80/80 [00:02<00:00, 28.01it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 3000: train loss 0.0600, val loss 3.6676\n","saving checkpoint to out2/addition_reverse_more_early_eval_v2/ckpt_teaching_addition_reverse_10000.pt\n","iter 3000: loss 0.0851, time 38196.58ms, mfu 5.84%\n","iter 3010: loss 0.0855, time 216.08ms, mfu 5.95%\n","iter 3020: loss 0.0810, time 227.18ms, mfu 6.01%\n","iter 3030: loss 0.0841, time 229.22ms, mfu 6.06%\n","iter 3040: loss 0.0838, time 222.58ms, mfu 6.12%\n","iter 3050: loss 0.0801, time 222.38ms, mfu 6.18%\n","iter 3060: loss 0.0824, time 226.85ms, mfu 6.22%\n","iter 3070: loss 0.0806, time 225.04ms, mfu 6.26%\n","iter 3080: loss 0.0802, time 229.67ms, mfu 6.29%\n","iter 3090: loss 0.0802, time 225.02ms, mfu 6.32%\n","iter 3100: loss 0.0791, time 230.75ms, mfu 6.34%\n","iter 3110: loss 0.0808, time 223.91ms, mfu 6.37%\n","iter 3120: loss 0.0799, time 226.88ms, mfu 6.39%\n","iter 3130: loss 0.0809, time 229.26ms, mfu 6.40%\n","iter 3140: loss 0.0759, time 229.29ms, mfu 6.41%\n","iter 3150: loss 0.0774, time 227.18ms, mfu 6.43%\n","iter 3160: loss 0.0813, time 228.25ms, mfu 6.44%\n","iter 3170: loss 0.0782, time 225.23ms, mfu 6.46%\n","iter 3180: loss 0.0771, time 226.79ms, mfu 6.47%\n","iter 3190: loss 0.0780, time 230.36ms, mfu 6.47%\n","iter 3200: loss 0.0777, time 228.32ms, mfu 6.48%\n","iter 3210: loss 0.0797, time 227.09ms, mfu 6.49%\n","iter 3220: loss 0.0791, time 228.14ms, mfu 6.49%\n","iter 3230: loss 0.0769, time 226.00ms, mfu 6.50%\n","iter 3240: loss 0.0781, time 229.31ms, mfu 6.50%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n"," 94% 75/80 [00:02<00:00, 28.00it/s]outputs(x):  $4+919=3241$\n","$\n","wrong  : 4+919=1423\n","correct: 4+919=923\n","outputs(x):  $9+617=627$\n","$8\n","wrong  : 9+617=726\n","correct: 9+617=626\n"," 98% 78/80 [00:02<00:00, 27.88it/s]outputs(x):  $4+456=068$\n","$6\n","wrong  : 4+456=860\n","correct: 4+456=460\n","100% 80/80 [00:02<00:00, 27.70it/s]\n","accuracy of 9900 examples: 9897/9900 (99.96969696969697%)\n","{'carry0': 100.0, 'carry1': 99.91710417242332, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 19473.33it/s]\n","100% 80/80 [00:02<00:00, 27.22it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","{'carry0': 99.96000000000001, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 3250: train loss 0.0583, val loss 3.7707\n","iter 3250: loss 0.0777, time 38230.15ms, mfu 5.86%\n","iter 3260: loss 0.0771, time 216.89ms, mfu 5.96%\n","iter 3270: loss 0.0764, time 222.38ms, mfu 6.03%\n","iter 3280: loss 0.0759, time 229.29ms, mfu 6.08%\n","iter 3290: loss 0.0764, time 225.04ms, mfu 6.14%\n","iter 3300: loss 0.0759, time 225.97ms, mfu 6.18%\n","iter 3310: loss 0.0775, time 225.71ms, mfu 6.23%\n","iter 3320: loss 0.0746, time 226.17ms, mfu 6.26%\n","iter 3330: loss 0.0775, time 226.12ms, mfu 6.30%\n","iter 3340: loss 0.0740, time 227.18ms, mfu 6.32%\n","iter 3350: loss 0.0737, time 227.85ms, mfu 6.35%\n","iter 3360: loss 0.0752, time 225.13ms, mfu 6.37%\n","iter 3370: loss 0.0758, time 227.05ms, mfu 6.39%\n","iter 3380: loss 0.0739, time 231.61ms, mfu 6.40%\n","iter 3390: loss 0.0769, time 225.30ms, mfu 6.42%\n","iter 3400: loss 0.0738, time 227.55ms, mfu 6.43%\n","iter 3410: loss 0.0731, time 229.05ms, mfu 6.44%\n","iter 3420: loss 0.0732, time 225.50ms, mfu 6.46%\n","iter 3430: loss 0.0724, time 227.00ms, mfu 6.47%\n","iter 3440: loss 0.0727, time 227.82ms, mfu 6.48%\n","iter 3450: loss 0.0719, time 229.12ms, mfu 6.48%\n","iter 3460: loss 0.0725, time 228.36ms, mfu 6.49%\n","iter 3470: loss 0.0734, time 226.50ms, mfu 6.50%\n","iter 3480: loss 0.0729, time 226.74ms, mfu 6.51%\n","iter 3490: loss 0.0728, time 229.88ms, mfu 6.50%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n"," 15% 12/80 [00:00<00:02, 26.81it/s]outputs(x):  $632+567=9921$\n","$\n","wrong  : 632+567=1299\n","correct: 632+567=1199\n","100% 80/80 [00:02<00:00, 27.43it/s]\n","accuracy of 9900 examples: 9899/9900 (99.98989898989899%)\n","{'carry0': 100.0, 'carry1': 99.97236805747444, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 19705.21it/s]\n","100% 80/80 [00:02<00:00, 28.03it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 3500: train loss 0.0564, val loss 3.8779\n","iter 3500: loss 0.0716, time 37853.64ms, mfu 5.86%\n","iter 3510: loss 0.0716, time 218.06ms, mfu 5.96%\n","iter 3520: loss 0.0729, time 225.58ms, mfu 6.02%\n","iter 3530: loss 0.0717, time 224.92ms, mfu 6.08%\n","iter 3540: loss 0.0714, time 225.60ms, mfu 6.14%\n","iter 3550: loss 0.0712, time 227.06ms, mfu 6.18%\n","iter 3560: loss 0.0707, time 221.51ms, mfu 6.24%\n","iter 3570: loss 0.0719, time 225.75ms, mfu 6.27%\n","iter 3580: loss 0.0718, time 226.39ms, mfu 6.30%\n","iter 3590: loss 0.0718, time 224.61ms, mfu 6.34%\n","iter 3600: loss 0.0688, time 223.70ms, mfu 6.37%\n","iter 3610: loss 0.0705, time 225.73ms, mfu 6.40%\n","iter 3620: loss 0.0708, time 228.09ms, mfu 6.41%\n","iter 3630: loss 0.0701, time 226.20ms, mfu 6.43%\n","iter 3640: loss 0.0693, time 228.88ms, mfu 6.44%\n","iter 3650: loss 0.0698, time 226.12ms, mfu 6.45%\n","iter 3660: loss 0.0696, time 227.39ms, mfu 6.46%\n","iter 3670: loss 0.0679, time 228.33ms, mfu 6.47%\n","iter 3680: loss 0.0697, time 230.40ms, mfu 6.47%\n","iter 3690: loss 0.0693, time 228.21ms, mfu 6.48%\n","iter 3700: loss 0.0676, time 226.89ms, mfu 6.49%\n","iter 3710: loss 0.0693, time 223.29ms, mfu 6.51%\n","iter 3720: loss 0.0679, time 226.17ms, mfu 6.52%\n","iter 3730: loss 0.0679, time 227.53ms, mfu 6.52%\n","iter 3740: loss 0.0691, time 223.70ms, mfu 6.54%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n","100% 80/80 [00:02<00:00, 27.99it/s]\n","accuracy of 9900 examples: 9900/9900 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 14376.12it/s]\n","100% 80/80 [00:02<00:00, 28.12it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 3750: train loss 0.0551, val loss 3.9280\n","iter 3750: loss 0.0689, time 38103.74ms, mfu 5.89%\n","iter 3760: loss 0.0690, time 215.48ms, mfu 5.99%\n","iter 3770: loss 0.0683, time 224.08ms, mfu 6.06%\n","iter 3780: loss 0.0685, time 228.85ms, mfu 6.10%\n","iter 3790: loss 0.0674, time 225.85ms, mfu 6.15%\n","iter 3800: loss 0.0671, time 223.59ms, mfu 6.21%\n","iter 3810: loss 0.0677, time 224.22ms, mfu 6.25%\n","iter 3820: loss 0.0678, time 223.19ms, mfu 6.29%\n","iter 3830: loss 0.0680, time 224.20ms, mfu 6.33%\n","iter 3840: loss 0.0678, time 227.44ms, mfu 6.35%\n","iter 3850: loss 0.0686, time 227.00ms, mfu 6.37%\n","iter 3860: loss 0.0677, time 221.16ms, mfu 6.41%\n","iter 3870: loss 0.0677, time 224.53ms, mfu 6.44%\n","iter 3880: loss 0.0673, time 226.86ms, mfu 6.45%\n","iter 3890: loss 0.0656, time 225.09ms, mfu 6.47%\n","iter 3900: loss 0.0668, time 226.08ms, mfu 6.48%\n","iter 3910: loss 0.0658, time 226.21ms, mfu 6.49%\n","iter 3920: loss 0.0656, time 227.42ms, mfu 6.50%\n","iter 3930: loss 0.0654, time 226.49ms, mfu 6.51%\n","iter 3940: loss 0.0645, time 226.36ms, mfu 6.52%\n","iter 3950: loss 0.0661, time 226.17ms, mfu 6.52%\n","iter 3960: loss 0.0654, time 227.23ms, mfu 6.53%\n","iter 3970: loss 0.0666, time 225.61ms, mfu 6.54%\n","iter 3980: loss 0.0665, time 227.81ms, mfu 6.54%\n","iter 3990: loss 0.0646, time 224.90ms, mfu 6.55%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n","100% 80/80 [00:02<00:00, 27.91it/s]\n","accuracy of 9900 examples: 9900/9900 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 19529.06it/s]\n","100% 80/80 [00:02<00:00, 27.94it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 4000: train loss 0.0540, val loss 4.0196\n","iter 4000: loss 0.0652, time 38051.36ms, mfu 5.90%\n","iter 4010: loss 0.0663, time 220.02ms, mfu 5.99%\n","iter 4020: loss 0.0665, time 225.20ms, mfu 6.05%\n","iter 4030: loss 0.0649, time 225.60ms, mfu 6.11%\n","iter 4040: loss 0.0637, time 226.94ms, mfu 6.15%\n","iter 4050: loss 0.0652, time 224.28ms, mfu 6.20%\n","iter 4060: loss 0.0647, time 226.80ms, mfu 6.24%\n","iter 4070: loss 0.0663, time 223.85ms, mfu 6.28%\n","iter 4080: loss 0.0651, time 225.42ms, mfu 6.32%\n","iter 4090: loss 0.0653, time 224.34ms, mfu 6.35%\n","iter 4100: loss 0.0660, time 228.67ms, mfu 6.37%\n","iter 4110: loss 0.0654, time 227.83ms, mfu 6.39%\n","iter 4120: loss 0.0642, time 225.96ms, mfu 6.41%\n","iter 4130: loss 0.0652, time 227.57ms, mfu 6.42%\n","iter 4140: loss 0.0652, time 226.53ms, mfu 6.44%\n","iter 4150: loss 0.0654, time 228.53ms, mfu 6.45%\n","iter 4160: loss 0.0640, time 227.90ms, mfu 6.46%\n","iter 4170: loss 0.0638, time 229.29ms, mfu 6.46%\n","iter 4180: loss 0.0635, time 227.66ms, mfu 6.47%\n","iter 4190: loss 0.0639, time 226.36ms, mfu 6.48%\n","iter 4200: loss 0.0648, time 229.00ms, mfu 6.49%\n","iter 4210: loss 0.0646, time 222.82ms, mfu 6.51%\n","iter 4220: loss 0.0635, time 228.90ms, mfu 6.51%\n","iter 4230: loss 0.0644, time 223.40ms, mfu 6.53%\n","iter 4240: loss 0.0625, time 227.29ms, mfu 6.53%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n"," 94% 75/80 [00:02<00:00, 27.96it/s]outputs(x):  $1+985=6831$\n","$\n","wrong  : 1+985=1386\n","correct: 1+985=986\n","100% 80/80 [00:02<00:00, 27.43it/s]\n","accuracy of 9900 examples: 9899/9900 (99.98989898989899%)\n","{'carry0': 99.9367088607595, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 20312.70it/s]\n","100% 80/80 [00:03<00:00, 26.35it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 4250: train loss 0.0530, val loss 4.0701\n","iter 4250: loss 0.0633, time 38105.60ms, mfu 5.88%\n","iter 4260: loss 0.0638, time 217.80ms, mfu 5.98%\n","iter 4270: loss 0.0634, time 223.72ms, mfu 6.05%\n","iter 4280: loss 0.0633, time 228.51ms, mfu 6.09%\n","iter 4290: loss 0.0629, time 225.08ms, mfu 6.15%\n","iter 4300: loss 0.0633, time 223.87ms, mfu 6.20%\n","iter 4310: loss 0.0636, time 225.73ms, mfu 6.24%\n","iter 4320: loss 0.0638, time 223.45ms, mfu 6.28%\n","iter 4330: loss 0.0624, time 226.71ms, mfu 6.31%\n","iter 4340: loss 0.0626, time 225.57ms, mfu 6.34%\n","iter 4350: loss 0.0623, time 224.73ms, mfu 6.37%\n","iter 4360: loss 0.0608, time 227.56ms, mfu 6.39%\n","iter 4370: loss 0.0626, time 227.63ms, mfu 6.41%\n","iter 4380: loss 0.0618, time 229.54ms, mfu 6.42%\n","iter 4390: loss 0.0623, time 229.83ms, mfu 6.42%\n","iter 4400: loss 0.0631, time 226.07ms, mfu 6.44%\n","iter 4410: loss 0.0628, time 229.06ms, mfu 6.45%\n","iter 4420: loss 0.0636, time 225.92ms, mfu 6.46%\n","iter 4430: loss 0.0627, time 224.97ms, mfu 6.48%\n","iter 4440: loss 0.0617, time 226.91ms, mfu 6.49%\n","iter 4450: loss 0.0625, time 224.53ms, mfu 6.51%\n","iter 4460: loss 0.0607, time 226.34ms, mfu 6.51%\n","iter 4470: loss 0.0615, time 228.09ms, mfu 6.52%\n","iter 4480: loss 0.0614, time 225.04ms, mfu 6.53%\n","iter 4490: loss 0.0627, time 228.24ms, mfu 6.53%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n"," 94% 75/80 [00:02<00:00, 27.98it/s]outputs(x):  $9+507=625$\n","$8\n","wrong  : 9+507=526\n","correct: 9+507=516\n","outputs(x):  $3+457=054$\n","$9\n","wrong  : 3+457=450\n","correct: 3+457=460\n","100% 80/80 [00:02<00:00, 27.72it/s]\n","accuracy of 9900 examples: 9898/9900 (99.97979797979798%)\n","{'carry0': 100.0, 'carry1': 99.94473611494888, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 20213.52it/s]\n","100% 80/80 [00:02<00:00, 27.74it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 4500: train loss 0.0523, val loss 4.1873\n","iter 4500: loss 0.0626, time 37897.95ms, mfu 5.88%\n","iter 4510: loss 0.0608, time 216.11ms, mfu 5.98%\n","iter 4520: loss 0.0628, time 223.69ms, mfu 6.05%\n","iter 4530: loss 0.0627, time 224.20ms, mfu 6.11%\n","iter 4540: loss 0.0620, time 222.79ms, mfu 6.17%\n","iter 4550: loss 0.0612, time 222.15ms, mfu 6.23%\n","iter 4560: loss 0.0620, time 223.21ms, mfu 6.27%\n","iter 4570: loss 0.0613, time 224.43ms, mfu 6.31%\n","iter 4580: loss 0.0636, time 223.98ms, mfu 6.34%\n","iter 4590: loss 0.0611, time 224.90ms, mfu 6.37%\n","iter 4600: loss 0.0613, time 223.50ms, mfu 6.40%\n","iter 4610: loss 0.0613, time 225.04ms, mfu 6.43%\n","iter 4620: loss 0.0594, time 224.78ms, mfu 6.45%\n","iter 4630: loss 0.0607, time 224.40ms, mfu 6.47%\n","iter 4640: loss 0.0611, time 224.63ms, mfu 6.48%\n","iter 4650: loss 0.0616, time 228.42ms, mfu 6.49%\n","iter 4660: loss 0.0595, time 228.53ms, mfu 6.49%\n","iter 4670: loss 0.0616, time 225.90ms, mfu 6.50%\n","iter 4680: loss 0.0600, time 226.34ms, mfu 6.51%\n","iter 4690: loss 0.0615, time 229.45ms, mfu 6.51%\n","iter 4700: loss 0.0604, time 228.08ms, mfu 6.52%\n","iter 4710: loss 0.0607, time 224.00ms, mfu 6.53%\n","iter 4720: loss 0.0611, time 226.54ms, mfu 6.54%\n","iter 4730: loss 0.0605, time 226.26ms, mfu 6.54%\n","iter 4740: loss 0.0617, time 225.27ms, mfu 6.55%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n","100% 80/80 [00:02<00:00, 27.46it/s]\n","accuracy of 9900 examples: 9900/9900 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 20352.86it/s]\n","100% 80/80 [00:02<00:00, 28.07it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 4750: train loss 0.0516, val loss 4.1811\n","iter 4750: loss 0.0619, time 37950.86ms, mfu 5.90%\n","iter 4760: loss 0.0597, time 216.39ms, mfu 6.00%\n","iter 4770: loss 0.0613, time 225.23ms, mfu 6.06%\n","iter 4780: loss 0.0608, time 226.16ms, mfu 6.11%\n","iter 4790: loss 0.0617, time 223.60ms, mfu 6.17%\n","iter 4800: loss 0.0606, time 223.86ms, mfu 6.22%\n","iter 4810: loss 0.0612, time 223.83ms, mfu 6.26%\n","iter 4820: loss 0.0612, time 222.43ms, mfu 6.31%\n","iter 4830: loss 0.0599, time 225.59ms, mfu 6.34%\n","iter 4840: loss 0.0597, time 226.86ms, mfu 6.36%\n","iter 4850: loss 0.0607, time 220.33ms, mfu 6.40%\n","iter 4860: loss 0.0604, time 220.59ms, mfu 6.44%\n","iter 4870: loss 0.0602, time 223.74ms, mfu 6.46%\n","iter 4880: loss 0.0600, time 227.63ms, mfu 6.47%\n","iter 4890: loss 0.0618, time 225.76ms, mfu 6.49%\n","iter 4900: loss 0.0595, time 226.71ms, mfu 6.50%\n","iter 4910: loss 0.0596, time 221.88ms, mfu 6.52%\n","iter 4920: loss 0.0607, time 228.47ms, mfu 6.52%\n","iter 4930: loss 0.0610, time 231.00ms, mfu 6.51%\n","iter 4940: loss 0.0607, time 225.14ms, mfu 6.52%\n","iter 4950: loss 0.0589, time 224.89ms, mfu 6.54%\n","iter 4960: loss 0.0598, time 226.47ms, mfu 6.54%\n","iter 4970: loss 0.0592, time 225.53ms, mfu 6.55%\n","iter 4980: loss 0.0581, time 227.07ms, mfu 6.55%\n","iter 4990: loss 0.0588, time 224.69ms, mfu 6.56%\n","evaluating addition from: FILE:data/bal/test_10000.txt\n","Evaluating Addition using test data file: data/bal/test_10000.txt\n","100% 80/80 [00:03<00:00, 25.87it/s]\n","accuracy of 9900 examples: 9900/9900 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","/content/drive/MyDrive/teaching_arithmetic/train.py:477: DtypeWarning: Columns (45,46,47,50,51,52,53,57,59,62,63,64,67,68,69,70,71,72,74,77,79,81,82,83,85,86,87,88,89,90,92,94,95,97,99,100,101,103,104,105,106,107,110,111,112,113,114,115,116,117,119,120,122,124,125,127,128,130,132,133,134,135,136,137,138,139,140,141,142,144,145,146,148,149,150,151,152,153,155,157,158,160,174,176,181,183) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","evaluating addition from: FILE:data/bal/train_3digit_10000.txt\n","Evaluating Addition using test data file: data/bal/train_3digit_10000.txt\n","100% 10000/10000 [00:00<00:00, 20186.12it/s]\n","100% 80/80 [00:02<00:00, 27.84it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 5000: train loss 0.0511, val loss 4.2324\n","iter 5000: loss 0.0601, time 38122.74ms, mfu 5.91%\n","saving final checkpoint to out2/addition_reverse_more_early_eval_v2\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mteaching_addition_reverse_more_early_eval_v2\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/ocu9vb0b\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250624_154420-ocu9vb0b/logs\u001b[0m\n"]}],"source":["!python train.py config2/addition/reverse/train_addition_bal.py"]},{"cell_type":"markdown","metadata":{"id":"ZRY8TECoOEvN"},"source":["## Reproduce 2 Operands 0-999 Addition (Our Code, Their Data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":120,"status":"ok","timestamp":1752348377751,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"Kby1YW7ZOTIK","outputId":"614fb7b9-d7f3-4bf0-f302-51f6704d6c32"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/addition\n"," 0_to_99999_times_1_digit\n"," 0_to_six_digit_times_1_digit\n"," 2_operands_0_to_999_balanced_digit_plain\n"," 2_operands_0_to_999_balanced_digit_reversed\n"," 2_operands_0_to_999_output_wo_leading_digit\n"," 2_operands_0_to_999_their_data\n"," 2_operands_0_to_999_their_training_data_only_more_eval\n"," 2_operands_0_to_999_their_training_our_testing\n"," 2_operands_0_to_999_uniform\n"," 2_operands_0_to_999_uniform_using_new_code\n"," 2_operands_3_digit_output_1000+\n"," 2_operands_3_digit_output_padding\n"," 2_operands_addition_plain.txt\n"," 2_operands_addition_reversed.txt\n"," 2_operands_mul_plain.txt\n"," 2_operands_mul_reversed.txt\n"," 3_operands_addition.txt\n"," 4_operands_0_to_999_balanced_digit\n"," 4_operands_0_to_999_uniform_wo_padding\n"," 4_operands_0_to_999_uniform_w_padding\n"," 4_operands_3_digit_uniform_output_padding\n"," 4_operands_addition_3_digit.txt\n"," 4_operands_addition_plain.txt\n"," 4_operands_addition_reversed.txt\n"," 5_operands_addition.txt\n"," 6_operands_0_to_999_balanced_digit\n"," 6_operands_addition_reversed.txt\n"," 7_operands_addition.txt\n"," ckpt_iter_145100_acc.pt\n"," configurator.py\n","'Copy of test_v4.ipynb'\n"," data\n"," eight_operand_examples.txt\n"," evaluation_by_reading_groundtruth.py\n"," evaluation.py\n"," generate_addition_data.py\n"," main_utilities.py\n"," model.py\n"," myStart_New.ipynb\n"," original.txt\n"," playTest.ipynb\n"," __pycache__\n"," README.md\n"," result_analysis.ipynb\n"," reversed_new.txt\n"," reversed.txt\n"," simple_test.ipynb\n"," test_checkpoint.ipynb\n"," train_end_padding_auto_val.py\n"," train_end_padding_more_early_eval.py\n"," train_end_padding_NoPE.py\n"," train_end_padding.py\n"," train_NoPE.py\n"," train.py\n"," train_with_eval_by_reading.py\n"," triple_operand_examples.txt\n"]}],"source":["\n","# Change directory to your code\n","%cd /content/drive/MyDrive/addition\n","%pwd   # verify you’re in the right place\n","!ls    # should show train.py, 4_operands_addition.txt, etc."]},{"cell_type":"markdown","metadata":{"id":"VcqlWGAlTIO7"},"source":["### Plain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1176,"status":"ok","timestamp":1750540821599,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"stAvBPyMXQgZ","outputId":"6e28a9af-56e1-4a58-d738-efc2407ad0e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["0+10=\n","0+360=\n","0+536=\n","0+551=\n","0+646=\n","0+741=\n","1+116=\n","1+517=\n","1+685=\n","1+985=\n","2+16=\n","2+36=\n","2+115=\n","2+120=\n","2+197=\n","2+545=\n","2+650=\n","2+712=\n","2+817=\n","2+878=\n","2+899=\n","2+980=\n","3+15=\n","3+228=\n","3+229=\n","3+323=\n","3+442=\n","3+453=\n","3+457=\n","3+510=\n","3+553=\n","3+576=\n","3+635=\n","3+723=\n","3+849=\n","4+39=\n","4+233=\n","4+294=\n","4+353=\n","4+386=\n","4+447=\n","4+456=\n","4+511=\n","4+636=\n","4+644=\n","4+712=\n","4+787=\n","4+829=\n","4+919=\n","5+120=\n","5+172=\n","5+564=\n","5+902=\n","6+213=\n","6+301=\n","6+512=\n","6+555=\n","6+598=\n","6+631=\n","6+636=\n","6+791=\n","6+957=\n","6+989=\n","7+66=\n","7+106=\n","7+161=\n","7+250=\n","7+287=\n","7+499=\n","7+520=\n","7+699=\n","7+785=\n","7+869=\n","7+975=\n","7+977=\n","7+996=\n","7+999=\n","8+16=\n","8+118=\n","8+488=\n","8+572=\n","8+625=\n","8+758=\n","8+842=\n","9+21=\n","9+23=\n","9+31=\n","9+162=\n","9+201=\n","9+210=\n","9+219=\n","9+483=\n","9+507=\n","9+550=\n","9+617=\n","9+750=\n","9+845=\n","9+987=\n","10+42=\n","10+286=\n","10+622=\n","10+721=\n","10+967=\n","11+99=\n","11+149=\n","11+292=\n","11+355=\n","11+439=\n","11+598=\n","11+873=\n","11+982=\n","12+194=\n","12+375=\n","12+393=\n","12+476=\n","12+595=\n","12+638=\n","12+682=\n","12+738=\n","12+743=\n","12+982=\n","13+101=\n","13+213=\n","13+287=\n","13+354=\n","13+440=\n","13+579=\n","13+783=\n","13+929=\n","13+951=\n","13+993=\n","14+379=\n","14+448=\n","14+485=\n","14+495=\n","14+611=\n","14+624=\n","14+783=\n","15+58=\n","15+109=\n","15+146=\n","15+211=\n","15+362=\n","15+422=\n","15+480=\n","15+518=\n","15+642=\n","15+645=\n","15+667=\n","15+733=\n","15+830=\n","15+886=\n","15+895=\n","16+84=\n","16+271=\n","16+542=\n","16+544=\n","16+587=\n","16+613=\n","16+760=\n","16+874=\n","17+46=\n","17+127=\n","17+158=\n","17+268=\n","17+314=\n","17+317=\n","17+320=\n","17+408=\n","17+429=\n","17+449=\n","17+541=\n","17+568=\n","17+611=\n","17+647=\n","17+661=\n","17+717=\n","17+726=\n","17+768=\n","17+812=\n","17+880=\n","17+915=\n","18+146=\n","18+194=\n","18+207=\n","18+297=\n","18+315=\n","18+461=\n","18+538=\n","18+544=\n","18+597=\n","18+653=\n","18+724=\n","18+904=\n","19+40=\n","19+122=\n","19+193=\n","19+243=\n","19+283=\n","19+329=\n","19+436=\n","19+488=\n","19+505=\n","19+544=\n","19+563=\n","19+579=\n","19+669=\n","19+689=\n","19+880=\n","19+945=\n","20+25=\n","20+390=\n","20+661=\n","20+830=\n","20+852=\n","20+934=\n","21+85=\n","21+150=\n","21+155=\n","21+176=\n","21+299=\n","21+401=\n","21+428=\n","21+438=\n","21+462=\n","21+562=\n","21+838=\n","21+896=\n","22+160=\n","22+178=\n","22+287=\n","22+415=\n","22+457=\n","22+589=\n","22+749=\n","22+755=\n","22+790=\n","22+821=\n","23+270=\n","23+365=\n","23+396=\n","23+686=\n","23+721=\n","24+43=\n","24+94=\n","24+98=\n","24+423=\n","24+590=\n","24+599=\n","24+622=\n","24+764=\n","25+241=\n","25+285=\n","25+399=\n","25+901=\n","26+3=\n","26+37=\n","26+42=\n","26+206=\n","26+335=\n","26+423=\n","26+530=\n","26+550=\n","26+619=\n","26+642=\n","26+646=\n","26+922=\n","27+174=\n","27+327=\n","27+363=\n","27+458=\n","27+731=\n","27+790=\n","28+63=\n","28+122=\n","28+515=\n","28+709=\n","28+712=\n","28+722=\n","28+755=\n","28+756=\n","28+802=\n","28+810=\n","28+904=\n","28+954=\n","29+49=\n","29+208=\n","29+282=\n","29+461=\n","29+570=\n","29+624=\n","29+726=\n","29+827=\n","29+882=\n","29+889=\n","29+923=\n","30+130=\n","30+327=\n","30+554=\n","30+745=\n","30+930=\n","31+44=\n","31+78=\n","31+86=\n","31+182=\n","31+318=\n","31+367=\n","31+407=\n","31+750=\n","31+791=\n","31+806=\n","31+904=\n","32+28=\n","32+86=\n","32+102=\n","32+350=\n","32+549=\n","32+672=\n","32+818=\n","32+847=\n","32+848=\n","32+879=\n","32+990=\n","32+991=\n","33+159=\n","33+306=\n","33+350=\n","33+485=\n","34+49=\n","34+81=\n","34+373=\n","34+446=\n","34+471=\n","34+481=\n","34+531=\n","34+583=\n","34+643=\n","34+713=\n","34+828=\n","34+888=\n","34+951=\n","35+33=\n","35+120=\n","35+146=\n","35+277=\n","35+359=\n","35+417=\n","35+791=\n","36+16=\n","36+99=\n","36+144=\n","36+418=\n","36+674=\n","36+921=\n","36+939=\n","36+997=\n","37+298=\n","37+424=\n","37+647=\n","38+13=\n","38+81=\n","38+605=\n","38+663=\n","38+702=\n","38+968=\n","38+991=\n","39+257=\n","39+387=\n","39+429=\n","39+506=\n","39+609=\n","39+727=\n","39+755=\n","39+797=\n","39+894=\n","39+948=\n","39+986=\n","40+2=\n","40+67=\n","40+184=\n","40+492=\n","40+706=\n","40+733=\n","40+882=\n","40+994=\n","41+2=\n","41+132=\n","41+144=\n","41+246=\n","41+343=\n","41+409=\n","41+541=\n","41+733=\n","41+811=\n","41+879=\n","41+918=\n","42+38=\n","42+190=\n","42+192=\n","42+268=\n","42+301=\n","42+590=\n","42+678=\n","42+699=\n","42+725=\n","42+804=\n","42+895=\n","43+61=\n","43+110=\n","43+325=\n","43+441=\n","43+639=\n","43+754=\n","43+792=\n","43+825=\n","43+922=\n","44+24=\n","44+156=\n","44+166=\n","44+189=\n","44+267=\n","44+824=\n","44+836=\n","44+839=\n","44+853=\n","44+922=\n","44+981=\n","45+38=\n","45+52=\n","45+105=\n","45+180=\n","45+261=\n","45+312=\n","45+315=\n","45+578=\n","45+587=\n","45+597=\n","45+695=\n","45+748=\n","46+27=\n","46+78=\n","46+235=\n","46+260=\n","46+509=\n","46+521=\n","46+776=\n","46+944=\n","47+68=\n","47+212=\n","47+388=\n","47+495=\n","47+577=\n","47+697=\n","47+971=\n","48+115=\n","48+153=\n","48+214=\n","48+277=\n","48+303=\n","48+360=\n","48+571=\n","48+685=\n","48+752=\n","49+27=\n","49+98=\n","49+255=\n","49+302=\n","49+570=\n","49+721=\n","49+757=\n","49+820=\n","49+858=\n","49+906=\n","50+82=\n","50+86=\n","50+123=\n","50+309=\n","50+442=\n","50+481=\n","50+599=\n","50+656=\n","50+739=\n","50+745=\n","50+977=\n","51+116=\n","51+214=\n","51+223=\n","51+268=\n","51+340=\n","51+421=\n","51+511=\n","51+533=\n","51+809=\n","51+853=\n","51+951=\n","52+31=\n","52+501=\n","52+572=\n","52+641=\n","52+675=\n","52+908=\n","52+972=\n","53+52=\n","53+60=\n","53+179=\n","53+295=\n","53+366=\n","53+514=\n","53+548=\n","53+674=\n","53+861=\n","53+930=\n","54+219=\n","54+528=\n","54+722=\n","54+872=\n","55+41=\n","55+106=\n","55+203=\n","55+339=\n","55+441=\n","55+458=\n","55+553=\n","55+668=\n","55+734=\n","55+819=\n","55+977=\n","56+54=\n","56+262=\n","56+300=\n","56+422=\n","56+497=\n","56+594=\n","56+749=\n","56+772=\n","56+883=\n","56+969=\n","57+145=\n","57+159=\n","57+225=\n","57+308=\n","57+393=\n","57+410=\n","57+439=\n","57+446=\n","57+452=\n","57+538=\n","57+599=\n","57+643=\n","57+693=\n","57+758=\n","57+796=\n","57+821=\n","57+826=\n","57+884=\n","57+907=\n","57+982=\n","58+112=\n","58+113=\n","58+195=\n","58+400=\n","58+472=\n","58+487=\n","58+518=\n","58+574=\n","58+608=\n","58+729=\n","58+739=\n","59+115=\n","59+149=\n","59+178=\n","59+296=\n","59+315=\n","59+444=\n","59+597=\n","59+692=\n","59+706=\n","59+756=\n","59+769=\n","59+847=\n","59+915=\n","60+141=\n","60+258=\n","60+437=\n","60+446=\n","60+478=\n","60+646=\n","60+679=\n","60+805=\n","60+887=\n","60+904=\n","60+929=\n","60+992=\n","61+249=\n","61+282=\n","61+487=\n","61+521=\n","61+534=\n","61+644=\n","61+841=\n","61+965=\n","61+978=\n","62+250=\n","62+267=\n","62+331=\n","62+366=\n","62+446=\n","62+747=\n","62+879=\n","62+941=\n","63+26=\n","63+157=\n","63+274=\n","63+322=\n","63+360=\n","63+415=\n","63+495=\n","63+496=\n","63+660=\n","63+727=\n","63+996=\n","64+100=\n","64+106=\n","64+165=\n","64+228=\n","64+403=\n","64+422=\n","64+615=\n","64+712=\n","64+809=\n","64+849=\n","64+986=\n","65+41=\n","65+136=\n","65+162=\n","65+211=\n","65+326=\n","65+483=\n","65+501=\n","65+996=\n","66+25=\n","66+123=\n","66+368=\n","66+370=\n","66+448=\n","66+576=\n","66+615=\n","66+617=\n","66+745=\n","66+907=\n","66+990=\n","67+378=\n","67+563=\n","67+615=\n","67+636=\n","67+814=\n","68+43=\n","68+52=\n","68+71=\n","68+123=\n","68+189=\n","68+556=\n","68+685=\n","68+705=\n","68+755=\n","68+789=\n","68+804=\n","68+837=\n","69+164=\n","69+392=\n","69+722=\n","69+734=\n","69+942=\n","70+268=\n","70+526=\n","70+573=\n","70+582=\n","70+644=\n","70+741=\n","70+751=\n","70+921=\n","70+952=\n","71+10=\n","71+67=\n","71+165=\n","71+172=\n","71+387=\n","71+516=\n","71+565=\n","71+646=\n","71+760=\n","71+932=\n","72+58=\n","72+230=\n","72+399=\n","72+650=\n","72+667=\n","72+907=\n","72+965=\n","72+990=\n","73+99=\n","73+145=\n","73+178=\n","73+253=\n","73+299=\n","73+372=\n","73+438=\n","73+527=\n","73+584=\n","73+900=\n","73+963=\n","74+73=\n","74+127=\n","74+168=\n","74+226=\n","74+244=\n","74+414=\n","74+590=\n","74+816=\n","75+2=\n","75+268=\n","75+361=\n","75+513=\n","75+618=\n","75+625=\n","75+695=\n","75+761=\n","75+882=\n","76+3=\n","76+41=\n","76+139=\n","76+155=\n","76+433=\n","76+645=\n","76+672=\n","76+734=\n","76+773=\n","76+794=\n","76+953=\n","77+125=\n","77+140=\n","77+301=\n","77+351=\n","77+615=\n","77+649=\n","77+717=\n","77+878=\n","78+5=\n","78+338=\n","78+362=\n","78+524=\n","78+545=\n","78+777=\n","78+979=\n","79+175=\n","79+233=\n","79+242=\n","79+376=\n","79+406=\n","79+531=\n","79+563=\n","79+671=\n","79+755=\n","79+908=\n","79+975=\n","80+39=\n","80+125=\n","80+135=\n","80+165=\n","80+323=\n","80+533=\n","80+558=\n","80+789=\n","80+909=\n","81+212=\n","81+265=\n","81+282=\n","81+504=\n","81+525=\n","81+740=\n","81+965=\n","82+198=\n","82+336=\n","82+447=\n","82+506=\n","82+551=\n","82+695=\n","82+719=\n","82+852=\n","82+924=\n","83+47=\n","83+65=\n","83+376=\n","83+587=\n","83+663=\n","83+676=\n","83+718=\n","83+729=\n","83+862=\n","84+121=\n","84+150=\n","84+177=\n","84+216=\n","84+380=\n","84+397=\n","84+483=\n","84+563=\n","84+578=\n","84+799=\n","84+800=\n","85+27=\n","85+32=\n","85+83=\n","85+161=\n","85+243=\n","85+412=\n","85+485=\n","85+697=\n","85+753=\n","85+950=\n","86+42=\n","86+308=\n","86+313=\n","86+391=\n","86+503=\n","86+558=\n","86+624=\n","86+744=\n","86+754=\n","86+780=\n","86+977=\n","87+84=\n","87+130=\n","87+237=\n","87+436=\n","87+519=\n","87+593=\n","87+624=\n","87+639=\n","87+763=\n","87+795=\n","87+901=\n","87+972=\n","88+37=\n","88+194=\n","88+221=\n","88+236=\n","88+369=\n","88+438=\n","88+695=\n","88+830=\n","88+832=\n","88+994=\n","89+28=\n","89+115=\n","89+208=\n","89+300=\n","89+401=\n","89+414=\n","89+521=\n","89+528=\n","89+549=\n","89+666=\n","89+817=\n","89+874=\n","89+970=\n","90+365=\n","90+534=\n","90+541=\n","90+638=\n","90+647=\n","90+744=\n","90+766=\n","90+789=\n","90+949=\n","90+965=\n","91+104=\n","91+191=\n","91+260=\n","91+375=\n","91+521=\n","91+579=\n","91+592=\n","91+754=\n","91+804=\n","91+870=\n","91+935=\n","91+958=\n","91+968=\n","92+4=\n","92+195=\n","92+285=\n","92+288=\n","92+420=\n","92+436=\n","92+561=\n","92+587=\n","92+712=\n","92+840=\n","92+925=\n","93+336=\n","93+505=\n","93+552=\n","93+664=\n","93+740=\n","93+753=\n","93+805=\n","93+863=\n","93+905=\n","93+922=\n","93+929=\n","94+328=\n","94+333=\n","94+558=\n","94+701=\n","94+812=\n","94+856=\n","94+902=\n","95+80=\n","95+131=\n","95+168=\n","95+173=\n","95+305=\n","95+577=\n","95+592=\n","95+726=\n","95+843=\n","95+953=\n","96+106=\n","96+196=\n","96+246=\n","96+262=\n","96+364=\n","96+377=\n","96+888=\n","97+269=\n","97+445=\n","97+448=\n","97+499=\n","97+621=\n","97+686=\n","98+123=\n","98+189=\n","98+633=\n","98+705=\n","98+805=\n","98+856=\n","98+884=\n","98+975=\n","98+984=\n","99+109=\n","99+155=\n","99+221=\n","99+269=\n","99+453=\n","99+560=\n","99+705=\n","99+798=\n","99+808=\n","99+865=\n","100+16=\n","100+37=\n","100+56=\n","100+283=\n","100+312=\n","100+479=\n","100+667=\n","100+762=\n","100+838=\n","100+939=\n","101+230=\n","101+360=\n","101+445=\n","101+488=\n","101+506=\n","101+595=\n","101+704=\n","101+843=\n","101+905=\n","101+959=\n","102+50=\n","102+133=\n","102+181=\n","102+279=\n","102+336=\n","102+586=\n","102+611=\n","102+686=\n","102+693=\n","102+783=\n","102+837=\n","102+854=\n","102+924=\n","103+60=\n","103+183=\n","103+364=\n","103+489=\n","103+643=\n","103+748=\n","103+846=\n","104+89=\n","104+146=\n","104+366=\n","104+442=\n","104+520=\n","104+582=\n","104+591=\n","104+601=\n","104+652=\n","105+273=\n","105+514=\n","105+787=\n","105+813=\n","105+908=\n","105+965=\n","106+150=\n","106+175=\n","106+244=\n","106+390=\n","106+472=\n","106+574=\n","106+649=\n","106+843=\n","107+44=\n","107+125=\n","107+280=\n","107+541=\n","107+554=\n","107+719=\n","107+725=\n","107+827=\n","107+902=\n","108+154=\n","108+192=\n","108+243=\n","108+279=\n","108+303=\n","108+386=\n","108+435=\n","108+475=\n","108+560=\n","108+658=\n","108+662=\n","109+16=\n","109+134=\n","109+264=\n","109+341=\n","109+356=\n","109+393=\n","109+567=\n","109+655=\n","109+947=\n","109+991=\n","110+14=\n","110+127=\n","110+162=\n","110+337=\n","110+575=\n","110+606=\n","110+729=\n","110+918=\n","111+397=\n","111+546=\n","111+582=\n","111+675=\n","111+834=\n","111+875=\n","112+9=\n","112+30=\n","112+261=\n","112+375=\n","112+556=\n","112+586=\n","112+678=\n","112+695=\n","112+740=\n","112+796=\n","112+827=\n","112+851=\n","113+29=\n","113+78=\n","113+149=\n","113+182=\n","113+472=\n","113+504=\n","113+763=\n","113+822=\n","113+833=\n","113+841=\n","113+843=\n","113+864=\n","113+965=\n","113+967=\n","114+55=\n","114+141=\n","114+308=\n","114+316=\n","114+380=\n","114+645=\n","114+677=\n","114+726=\n","115+39=\n","115+57=\n","115+132=\n","115+257=\n","115+299=\n","115+311=\n","115+525=\n","115+840=\n","115+845=\n","115+848=\n","115+853=\n","115+978=\n","116+123=\n","116+150=\n","116+200=\n","116+301=\n","116+652=\n","116+659=\n","116+767=\n","116+779=\n","117+283=\n","117+344=\n","117+364=\n","117+400=\n","117+441=\n","117+555=\n","117+602=\n","117+668=\n","117+684=\n","117+695=\n","118+207=\n","118+247=\n","118+275=\n","118+319=\n","118+382=\n","118+463=\n","118+532=\n","118+614=\n","118+719=\n","118+900=\n","118+959=\n","119+167=\n","119+174=\n","119+376=\n","119+419=\n","119+513=\n","119+661=\n","119+881=\n","119+929=\n","119+945=\n","120+106=\n","120+133=\n","120+296=\n","120+383=\n","120+393=\n","120+409=\n","120+516=\n","120+605=\n","120+616=\n","120+618=\n","120+627=\n","120+697=\n","120+749=\n","121+5=\n","121+109=\n","121+229=\n","121+246=\n","121+267=\n","121+428=\n","121+596=\n","121+619=\n","121+861=\n","122+46=\n","122+80=\n","122+150=\n","122+413=\n","122+628=\n","122+651=\n","122+817=\n","122+993=\n","123+17=\n","123+229=\n","123+268=\n","123+284=\n","123+322=\n","123+389=\n","123+493=\n","123+535=\n","123+591=\n","123+903=\n","123+950=\n","124+26=\n","124+78=\n","124+127=\n","124+248=\n","124+788=\n","124+842=\n","124+874=\n","124+921=\n","124+945=\n","125+59=\n","125+122=\n","125+141=\n","125+465=\n","125+614=\n","125+627=\n","125+673=\n","125+751=\n","125+766=\n","125+967=\n","126+77=\n","126+178=\n","126+230=\n","126+272=\n","126+273=\n","126+565=\n","126+657=\n","126+841=\n","126+888=\n","126+992=\n","126+999=\n","127+66=\n","127+136=\n","127+338=\n","127+353=\n","127+385=\n","127+432=\n","127+469=\n","127+545=\n","127+868=\n","127+914=\n","128+272=\n","128+487=\n","128+527=\n","128+552=\n","128+626=\n","128+765=\n","129+13=\n","129+15=\n","129+259=\n","129+289=\n","129+413=\n","129+452=\n","129+466=\n","129+534=\n","129+616=\n","129+889=\n","129+993=\n","130+210=\n","130+222=\n","130+355=\n","130+453=\n","130+459=\n","130+747=\n","130+788=\n","130+933=\n","130+982=\n","131+6=\n","131+59=\n","131+244=\n","131+334=\n","131+368=\n","131+675=\n","131+773=\n","131+792=\n","131+896=\n","131+917=\n","132+140=\n","132+284=\n","132+416=\n","132+508=\n","132+580=\n","132+610=\n","132+755=\n","132+832=\n","132+844=\n","133+132=\n","133+140=\n","133+222=\n","133+474=\n","133+478=\n","133+564=\n","133+581=\n","133+715=\n","133+726=\n","133+790=\n","133+917=\n","134+63=\n","134+157=\n","134+220=\n","134+246=\n","134+260=\n","134+332=\n","134+409=\n","134+499=\n","134+538=\n","134+553=\n","134+833=\n","134+847=\n","134+856=\n","134+892=\n","134+922=\n","134+963=\n","134+973=\n","135+31=\n","135+105=\n","135+184=\n","135+280=\n","135+417=\n","135+499=\n","135+532=\n","135+539=\n","135+677=\n","135+712=\n","135+782=\n","135+836=\n","135+849=\n","135+882=\n","135+891=\n","135+932=\n","135+969=\n","136+13=\n","136+103=\n","136+168=\n","136+247=\n","136+272=\n","136+332=\n","136+521=\n","136+581=\n","136+769=\n","136+828=\n","136+994=\n","137+26=\n","137+93=\n","137+278=\n","137+454=\n","137+569=\n","137+677=\n","137+974=\n","137+977=\n","138+116=\n","138+166=\n","138+254=\n","138+480=\n","138+608=\n","138+627=\n","138+696=\n","138+790=\n","138+829=\n","138+950=\n","138+984=\n","139+22=\n","139+188=\n","139+426=\n","139+427=\n","139+765=\n","139+788=\n","139+919=\n","140+5=\n","140+186=\n","140+198=\n","140+348=\n","140+362=\n","140+385=\n","140+755=\n","140+841=\n","140+873=\n","140+904=\n","140+982=\n","141+0=\n","141+94=\n","141+214=\n","141+471=\n","141+645=\n","141+777=\n","141+887=\n","141+906=\n","142+35=\n","142+137=\n","142+139=\n","142+216=\n","142+246=\n","142+664=\n","142+735=\n","142+985=\n","143+41=\n","143+47=\n","143+64=\n","143+327=\n","143+348=\n","143+525=\n","143+538=\n","143+942=\n","143+998=\n","144+117=\n","144+273=\n","144+297=\n","144+307=\n","144+436=\n","144+510=\n","144+605=\n","144+650=\n","144+680=\n","144+782=\n","144+846=\n","144+900=\n","144+926=\n","144+947=\n","145+90=\n","145+150=\n","145+242=\n","145+275=\n","145+305=\n","145+352=\n","145+543=\n","145+580=\n","145+713=\n","145+823=\n","145+848=\n","146+318=\n","146+351=\n","146+542=\n","146+613=\n","146+775=\n","146+884=\n","146+899=\n","147+130=\n","147+181=\n","147+239=\n","147+647=\n","147+666=\n","147+728=\n","147+766=\n","147+803=\n","147+877=\n","148+93=\n","148+116=\n","148+152=\n","148+197=\n","148+222=\n","148+442=\n","148+458=\n","148+527=\n","148+574=\n","148+582=\n","148+710=\n","148+731=\n","148+794=\n","149+148=\n","149+464=\n","149+478=\n","149+701=\n","149+783=\n","150+36=\n","150+143=\n","150+304=\n","150+440=\n","150+468=\n","150+577=\n","151+6=\n","151+318=\n","151+452=\n","151+669=\n","151+694=\n","151+808=\n","152+44=\n","152+271=\n","152+288=\n","152+346=\n","152+347=\n","152+514=\n","152+562=\n","152+594=\n","152+618=\n","152+634=\n","152+670=\n","152+895=\n","152+902=\n","153+56=\n","153+178=\n","153+393=\n","153+458=\n","153+478=\n","153+528=\n","153+558=\n","153+583=\n","153+704=\n","153+944=\n","154+7=\n","154+21=\n","154+57=\n","154+61=\n","154+396=\n","154+557=\n","154+625=\n","154+686=\n","154+697=\n","154+726=\n","154+774=\n","155+263=\n","155+439=\n","155+512=\n","155+689=\n","155+705=\n","155+834=\n","155+904=\n","156+9=\n","156+96=\n","156+233=\n","156+309=\n","156+369=\n","156+379=\n","156+518=\n","156+581=\n","156+784=\n","156+868=\n","156+879=\n","157+77=\n","157+92=\n","157+133=\n","157+157=\n","157+237=\n","157+327=\n","157+373=\n","157+498=\n","157+522=\n","157+626=\n","157+717=\n","157+730=\n","157+787=\n","157+928=\n","157+932=\n","158+314=\n","158+385=\n","158+520=\n","158+622=\n","158+675=\n","158+752=\n","158+885=\n","159+149=\n","159+173=\n","159+200=\n","159+228=\n","159+285=\n","159+351=\n","159+445=\n","159+530=\n","159+916=\n","160+152=\n","160+157=\n","160+208=\n","160+305=\n","160+712=\n","160+809=\n","160+938=\n","161+240=\n","162+20=\n","162+64=\n","162+214=\n","162+229=\n","162+317=\n","162+334=\n","162+396=\n","162+476=\n","162+495=\n","162+603=\n","162+697=\n","162+723=\n","162+804=\n","162+839=\n","162+855=\n","162+987=\n","163+87=\n","163+165=\n","163+307=\n","163+383=\n","163+420=\n","163+660=\n","163+662=\n","163+940=\n","164+4=\n","164+53=\n","164+333=\n","164+403=\n","164+446=\n","164+467=\n","164+554=\n","164+637=\n","164+670=\n","164+685=\n","164+753=\n","164+933=\n","164+944=\n","165+117=\n","165+148=\n","165+166=\n","165+341=\n","165+453=\n","165+540=\n","165+543=\n","165+575=\n","165+588=\n","165+669=\n","165+683=\n","165+706=\n","165+719=\n","165+850=\n","165+930=\n","166+7=\n","166+116=\n","166+177=\n","166+281=\n","166+503=\n","166+643=\n","166+810=\n","166+811=\n","166+820=\n","166+949=\n","166+962=\n","167+104=\n","167+105=\n","167+116=\n","167+160=\n","167+288=\n","167+320=\n","167+383=\n","167+498=\n","167+548=\n","167+639=\n","167+754=\n","167+762=\n","167+787=\n","167+814=\n","167+987=\n","167+993=\n","168+221=\n","168+335=\n","168+384=\n","168+385=\n","168+399=\n","168+541=\n","168+558=\n","168+577=\n","168+720=\n","168+753=\n","168+757=\n","169+11=\n","169+93=\n","169+135=\n","169+205=\n","169+303=\n","169+309=\n","169+475=\n","169+484=\n","169+512=\n","169+623=\n","169+654=\n","169+880=\n","169+936=\n","170+121=\n","170+148=\n","170+174=\n","170+177=\n","170+197=\n","170+279=\n","170+304=\n","170+453=\n","170+481=\n","170+548=\n","170+575=\n","170+600=\n","170+766=\n","171+58=\n","171+120=\n","171+258=\n","171+261=\n","171+274=\n","171+352=\n","171+416=\n","171+504=\n","171+597=\n","171+953=\n","172+131=\n","172+280=\n","172+335=\n","172+463=\n","172+599=\n","172+637=\n","172+731=\n","172+967=\n","172+982=\n","173+157=\n","173+574=\n","173+699=\n","173+882=\n","173+960=\n","174+81=\n","174+293=\n","174+395=\n","174+492=\n","174+703=\n","174+716=\n","174+784=\n","174+794=\n","175+145=\n","175+174=\n","175+374=\n","175+487=\n","175+587=\n","175+591=\n","175+601=\n","175+820=\n","176+174=\n","176+235=\n","176+275=\n","176+463=\n","176+465=\n","176+476=\n","176+511=\n","176+572=\n","176+621=\n","176+624=\n","176+696=\n","176+746=\n","176+747=\n","176+794=\n","176+833=\n","176+917=\n","177+297=\n","177+310=\n","177+375=\n","177+493=\n","177+607=\n","177+752=\n","177+919=\n","178+75=\n","178+101=\n","178+122=\n","178+204=\n","178+224=\n","178+232=\n","178+241=\n","178+394=\n","178+602=\n","178+644=\n","178+764=\n","178+972=\n","178+996=\n","178+999=\n","179+12=\n","179+137=\n","179+214=\n","179+252=\n","179+258=\n","179+283=\n","179+445=\n","179+510=\n","179+595=\n","179+739=\n","179+751=\n","179+813=\n","179+894=\n","179+944=\n","180+85=\n","180+94=\n","180+103=\n","180+147=\n","180+294=\n","180+338=\n","180+439=\n","180+596=\n","180+798=\n","180+831=\n","180+973=\n","181+61=\n","181+136=\n","181+294=\n","181+378=\n","181+444=\n","181+448=\n","181+480=\n","181+950=\n","182+103=\n","182+276=\n","182+295=\n","182+563=\n","182+674=\n","182+710=\n","182+771=\n","182+807=\n","182+866=\n","182+875=\n","183+83=\n","183+116=\n","183+212=\n","183+235=\n","183+243=\n","183+462=\n","183+897=\n","183+918=\n","183+956=\n","183+969=\n","184+31=\n","184+89=\n","184+98=\n","184+594=\n","184+607=\n","184+649=\n","184+654=\n","184+787=\n","185+10=\n","185+138=\n","185+147=\n","185+158=\n","185+286=\n","185+471=\n","185+603=\n","185+778=\n","185+884=\n","185+928=\n","186+47=\n","186+134=\n","186+196=\n","186+530=\n","186+602=\n","186+691=\n","186+741=\n","186+788=\n","186+945=\n","187+136=\n","187+288=\n","187+353=\n","187+685=\n","187+916=\n","188+14=\n","188+95=\n","188+325=\n","188+377=\n","188+395=\n","188+399=\n","188+454=\n","188+545=\n","188+597=\n","188+704=\n","188+723=\n","188+774=\n","189+224=\n","189+227=\n","189+272=\n","189+281=\n","189+655=\n","189+760=\n","189+851=\n","189+873=\n","190+139=\n","190+278=\n","190+427=\n","190+456=\n","190+557=\n","190+706=\n","190+734=\n","190+866=\n","190+928=\n","190+951=\n","190+982=\n","191+5=\n","191+94=\n","191+230=\n","191+632=\n","191+673=\n","191+739=\n","191+838=\n","191+877=\n","191+926=\n","191+976=\n","192+110=\n","192+237=\n","192+466=\n","192+502=\n","192+578=\n","192+648=\n","193+24=\n","193+61=\n","193+161=\n","193+266=\n","193+357=\n","193+432=\n","193+619=\n","193+663=\n","194+457=\n","194+496=\n","194+536=\n","194+722=\n","194+803=\n","194+909=\n","194+975=\n","194+997=\n","195+25=\n","195+112=\n","195+144=\n","195+212=\n","195+277=\n","195+385=\n","195+528=\n","195+558=\n","195+579=\n","195+583=\n","195+611=\n","195+614=\n","195+661=\n","195+720=\n","195+999=\n","196+1=\n","196+5=\n","196+23=\n","196+28=\n","196+303=\n","196+329=\n","196+470=\n","196+596=\n","196+625=\n","196+729=\n","196+753=\n","196+844=\n","197+95=\n","197+161=\n","197+182=\n","197+305=\n","197+341=\n","197+358=\n","197+476=\n","197+507=\n","197+533=\n","197+760=\n","197+790=\n","197+901=\n","197+931=\n","197+935=\n","197+999=\n","198+17=\n","198+21=\n","198+54=\n","198+139=\n","198+204=\n","198+314=\n","198+351=\n","198+363=\n","198+419=\n","198+594=\n","198+674=\n","198+728=\n","198+899=\n","199+433=\n","199+528=\n","199+548=\n","199+585=\n","199+692=\n","199+701=\n","199+842=\n","199+880=\n","199+940=\n","200+21=\n","200+254=\n","200+700=\n","200+758=\n","201+108=\n","201+225=\n","201+484=\n","201+495=\n","201+583=\n","201+781=\n","201+899=\n","201+952=\n","202+138=\n","202+322=\n","202+379=\n","202+396=\n","202+666=\n","202+718=\n","202+755=\n","202+954=\n","203+175=\n","203+345=\n","203+358=\n","203+452=\n","203+523=\n","203+632=\n","203+646=\n","203+696=\n","203+785=\n","203+832=\n","204+14=\n","204+15=\n","204+220=\n","204+578=\n","204+650=\n","204+840=\n","204+861=\n","205+71=\n","205+192=\n","205+286=\n","205+711=\n","205+900=\n","205+996=\n","206+20=\n","206+61=\n","206+184=\n","206+330=\n","206+416=\n","206+540=\n","206+829=\n","206+847=\n","206+955=\n","206+979=\n","207+0=\n","207+66=\n","207+126=\n","207+135=\n","207+181=\n","207+332=\n","207+482=\n","207+577=\n","207+750=\n","207+924=\n","207+996=\n","208+4=\n","208+72=\n","208+121=\n","208+356=\n","208+389=\n","208+405=\n","208+566=\n","208+589=\n","208+643=\n","208+705=\n","208+910=\n","208+945=\n","209+6=\n","209+34=\n","209+70=\n","209+107=\n","209+151=\n","209+167=\n","209+195=\n","209+197=\n","209+243=\n","209+299=\n","209+436=\n","209+441=\n","210+155=\n","210+402=\n","210+450=\n","210+592=\n","210+697=\n","210+913=\n","211+184=\n","211+204=\n","211+210=\n","211+275=\n","211+293=\n","211+407=\n","211+493=\n","211+605=\n","211+913=\n","211+924=\n","211+929=\n","211+937=\n","211+940=\n","211+983=\n","212+58=\n","212+85=\n","212+294=\n","212+379=\n","212+686=\n","212+911=\n","213+44=\n","213+122=\n","213+188=\n","213+245=\n","213+325=\n","213+419=\n","213+479=\n","213+640=\n","213+641=\n","213+727=\n","213+858=\n","214+13=\n","214+39=\n","214+206=\n","214+263=\n","214+408=\n","214+429=\n","214+489=\n","214+681=\n","214+686=\n","214+739=\n","214+791=\n","214+921=\n","215+114=\n","215+181=\n","215+576=\n","215+705=\n","215+791=\n","215+885=\n","215+898=\n","216+68=\n","216+194=\n","216+217=\n","216+488=\n","216+606=\n","216+645=\n","216+698=\n","216+722=\n","216+819=\n","216+845=\n","216+867=\n","216+870=\n","216+942=\n","216+971=\n","216+991=\n","217+217=\n","217+310=\n","217+363=\n","217+481=\n","217+764=\n","217+805=\n","217+932=\n","218+217=\n","218+254=\n","218+265=\n","218+300=\n","218+314=\n","218+324=\n","218+426=\n","218+559=\n","218+698=\n","218+736=\n","218+941=\n","218+986=\n","219+190=\n","219+201=\n","219+335=\n","219+380=\n","219+390=\n","219+743=\n","219+750=\n","219+946=\n","220+44=\n","220+196=\n","220+480=\n","220+489=\n","220+618=\n","220+627=\n","221+422=\n","221+473=\n","221+475=\n","221+483=\n","221+494=\n","221+513=\n","221+576=\n","221+584=\n","221+601=\n","221+769=\n","221+793=\n","221+796=\n","221+916=\n","222+70=\n","222+203=\n","222+271=\n","222+464=\n","222+537=\n","222+722=\n","222+867=\n","222+913=\n","223+319=\n","223+328=\n","223+335=\n","223+359=\n","223+422=\n","223+851=\n","223+879=\n","223+887=\n","223+907=\n","223+974=\n","224+66=\n","224+242=\n","224+403=\n","224+589=\n","224+610=\n","224+740=\n","224+753=\n","225+183=\n","225+193=\n","225+199=\n","225+244=\n","225+312=\n","225+529=\n","225+536=\n","225+601=\n","225+621=\n","225+626=\n","225+635=\n","225+878=\n","225+900=\n","225+932=\n","225+963=\n","226+27=\n","226+31=\n","226+285=\n","226+595=\n","226+605=\n","226+683=\n","226+796=\n","226+875=\n","226+885=\n","226+927=\n","227+39=\n","227+63=\n","227+82=\n","227+92=\n","227+584=\n","227+911=\n","228+75=\n","228+177=\n","228+199=\n","228+217=\n","228+256=\n","228+355=\n","228+431=\n","228+520=\n","228+523=\n","228+547=\n","228+728=\n","228+753=\n","228+933=\n","229+64=\n","229+419=\n","229+525=\n","229+574=\n","229+648=\n","229+733=\n","229+766=\n","229+777=\n","229+874=\n","229+973=\n","230+47=\n","230+192=\n","230+221=\n","230+289=\n","230+379=\n","230+397=\n","230+565=\n","230+812=\n","230+894=\n","230+905=\n","231+198=\n","231+385=\n","231+658=\n","231+703=\n","231+792=\n","231+893=\n","231+919=\n","231+930=\n","232+15=\n","232+178=\n","232+279=\n","232+358=\n","232+484=\n","232+500=\n","232+735=\n","232+751=\n","232+923=\n","232+999=\n","233+78=\n","233+101=\n","233+127=\n","233+133=\n","233+188=\n","233+316=\n","233+335=\n","233+390=\n","233+582=\n","233+603=\n","233+890=\n","233+944=\n","234+61=\n","234+107=\n","234+196=\n","234+283=\n","234+334=\n","234+468=\n","234+769=\n","234+813=\n","235+17=\n","235+240=\n","235+544=\n","235+573=\n","235+771=\n","235+890=\n","235+974=\n","236+149=\n","236+235=\n","236+268=\n","236+289=\n","236+584=\n","236+636=\n","236+671=\n","236+728=\n","236+858=\n","236+907=\n","237+0=\n","237+35=\n","237+40=\n","237+113=\n","237+241=\n","237+339=\n","237+387=\n","237+395=\n","237+492=\n","237+621=\n","237+623=\n","237+765=\n","237+773=\n","237+844=\n","237+932=\n","238+85=\n","238+120=\n","238+139=\n","238+195=\n","238+290=\n","238+315=\n","238+412=\n","238+492=\n","238+694=\n","238+827=\n","238+982=\n","239+89=\n","239+289=\n","239+495=\n","239+594=\n","239+598=\n","239+809=\n","240+158=\n","240+541=\n","240+911=\n","241+15=\n","241+118=\n","241+320=\n","241+439=\n","241+450=\n","241+560=\n","241+688=\n","241+733=\n","241+784=\n","241+843=\n","241+896=\n","241+919=\n","241+983=\n","242+41=\n","242+135=\n","242+281=\n","242+339=\n","242+450=\n","242+507=\n","242+568=\n","242+582=\n","242+634=\n","242+677=\n","242+867=\n","242+917=\n","242+999=\n","243+212=\n","243+230=\n","243+301=\n","243+418=\n","243+420=\n","243+484=\n","243+514=\n","243+700=\n","243+757=\n","243+912=\n","243+950=\n","244+63=\n","244+258=\n","244+406=\n","244+506=\n","244+524=\n","244+636=\n","244+715=\n","244+723=\n","244+794=\n","244+823=\n","244+892=\n","245+95=\n","245+216=\n","245+303=\n","245+349=\n","245+435=\n","245+489=\n","245+499=\n","245+573=\n","245+616=\n","245+749=\n","245+850=\n","245+851=\n","245+923=\n","245+997=\n","246+237=\n","246+257=\n","246+447=\n","246+504=\n","246+739=\n","246+844=\n","246+865=\n","246+909=\n","246+937=\n","246+976=\n","246+981=\n","247+67=\n","247+90=\n","247+220=\n","247+231=\n","247+279=\n","247+412=\n","247+425=\n","247+544=\n","247+628=\n","247+677=\n","247+729=\n","247+901=\n","247+934=\n","247+978=\n","248+136=\n","248+166=\n","248+271=\n","248+416=\n","248+551=\n","248+552=\n","248+691=\n","248+708=\n","248+713=\n","248+817=\n","248+905=\n","249+228=\n","249+255=\n","249+310=\n","249+344=\n","249+371=\n","249+510=\n","249+606=\n","249+661=\n","249+689=\n","249+697=\n","249+713=\n","249+743=\n","250+39=\n","250+90=\n","250+100=\n","250+146=\n","250+234=\n","250+287=\n","250+392=\n","250+401=\n","250+436=\n","250+457=\n","250+466=\n","250+815=\n","250+898=\n","251+60=\n","251+65=\n","251+245=\n","251+626=\n","251+893=\n","251+968=\n","251+983=\n","252+120=\n","252+338=\n","252+382=\n","252+413=\n","252+695=\n","252+965=\n","253+77=\n","253+133=\n","253+152=\n","253+188=\n","253+211=\n","253+333=\n","253+387=\n","253+461=\n","253+580=\n","253+743=\n","253+991=\n","254+254=\n","254+436=\n","254+517=\n","254+590=\n","254+633=\n","254+938=\n","254+948=\n","254+980=\n","255+22=\n","255+333=\n","255+638=\n","255+654=\n","255+774=\n","255+957=\n","255+970=\n","256+3=\n","256+135=\n","256+262=\n","256+394=\n","256+463=\n","256+545=\n","256+566=\n","256+617=\n","256+838=\n","256+861=\n","257+55=\n","257+178=\n","257+216=\n","257+246=\n","257+299=\n","257+301=\n","257+383=\n","257+400=\n","257+413=\n","257+427=\n","257+592=\n","257+725=\n","257+733=\n","257+991=\n","258+31=\n","258+183=\n","258+196=\n","258+331=\n","258+345=\n","258+349=\n","258+404=\n","258+442=\n","258+658=\n","258+679=\n","258+805=\n","259+57=\n","259+375=\n","259+414=\n","259+422=\n","259+676=\n","259+885=\n","259+930=\n","260+248=\n","260+361=\n","260+395=\n","260+617=\n","260+654=\n","260+747=\n","260+869=\n","260+937=\n","260+994=\n","261+58=\n","261+545=\n","261+565=\n","261+626=\n","261+666=\n","261+699=\n","261+791=\n","262+116=\n","262+454=\n","262+461=\n","262+535=\n","262+652=\n","262+663=\n","262+729=\n","262+770=\n","262+871=\n","262+961=\n","262+987=\n","263+133=\n","263+391=\n","263+418=\n","263+661=\n","263+717=\n","263+931=\n","264+234=\n","264+362=\n","264+436=\n","264+480=\n","264+568=\n","264+609=\n","264+789=\n","264+893=\n","265+63=\n","265+363=\n","265+614=\n","265+718=\n","265+756=\n","265+833=\n","265+963=\n","266+197=\n","266+258=\n","266+302=\n","266+470=\n","266+497=\n","266+510=\n","266+534=\n","266+611=\n","266+648=\n","266+665=\n","266+807=\n","266+888=\n","266+909=\n","267+273=\n","267+329=\n","267+340=\n","267+488=\n","267+644=\n","267+766=\n","267+855=\n","267+858=\n","267+947=\n","268+36=\n","268+210=\n","268+242=\n","268+287=\n","268+306=\n","268+475=\n","268+651=\n","268+658=\n","268+688=\n","268+833=\n","269+92=\n","269+264=\n","269+380=\n","269+474=\n","269+499=\n","269+673=\n","269+768=\n","269+781=\n","269+885=\n","269+968=\n","270+161=\n","270+493=\n","270+569=\n","270+577=\n","270+667=\n","270+785=\n","270+788=\n","270+812=\n","270+871=\n","271+48=\n","271+102=\n","271+121=\n","271+234=\n","271+301=\n","271+415=\n","271+454=\n","271+633=\n","271+657=\n","271+764=\n","271+812=\n","272+27=\n","272+149=\n","272+161=\n","272+230=\n","272+379=\n","272+409=\n","272+512=\n","272+586=\n","272+639=\n","272+674=\n","272+686=\n","272+739=\n","272+813=\n","272+856=\n","273+20=\n","273+34=\n","273+65=\n","273+69=\n","273+223=\n","273+247=\n","273+365=\n","273+474=\n","273+679=\n","273+749=\n","273+923=\n","273+977=\n","274+267=\n","274+374=\n","274+744=\n","274+785=\n","275+5=\n","275+23=\n","275+114=\n","275+328=\n","275+406=\n","275+455=\n","275+611=\n","275+707=\n","275+798=\n","275+910=\n","276+217=\n","276+227=\n","276+374=\n","276+375=\n","276+548=\n","276+808=\n","276+938=\n","276+969=\n","276+975=\n","277+19=\n","277+41=\n","277+588=\n","277+647=\n","277+744=\n","277+797=\n","277+881=\n","277+929=\n","277+982=\n","278+225=\n","278+346=\n","278+375=\n","278+810=\n","278+815=\n","278+900=\n","278+923=\n","279+239=\n","279+314=\n","279+376=\n","279+595=\n","279+599=\n","279+862=\n","280+7=\n","280+100=\n","280+122=\n","280+235=\n","280+347=\n","280+639=\n","280+736=\n","280+765=\n","281+165=\n","281+199=\n","281+213=\n","281+257=\n","281+308=\n","281+323=\n","281+354=\n","281+517=\n","281+720=\n","282+381=\n","282+729=\n","282+740=\n","282+824=\n","283+320=\n","283+380=\n","283+433=\n","283+545=\n","283+563=\n","283+615=\n","283+798=\n","283+910=\n","283+936=\n","284+122=\n","284+471=\n","284+715=\n","284+996=\n","285+95=\n","285+320=\n","285+329=\n","285+383=\n","285+496=\n","285+639=\n","285+696=\n","285+739=\n","285+863=\n","285+890=\n","286+18=\n","286+64=\n","286+163=\n","286+192=\n","286+237=\n","286+263=\n","286+368=\n","286+500=\n","286+519=\n","286+540=\n","286+564=\n","286+589=\n","286+705=\n","286+883=\n","286+894=\n","287+179=\n","287+331=\n","287+463=\n","287+506=\n","287+540=\n","287+642=\n","287+671=\n","287+805=\n","287+898=\n","287+899=\n","287+902=\n","288+103=\n","288+120=\n","288+136=\n","288+201=\n","288+288=\n","288+576=\n","288+673=\n","288+723=\n","288+783=\n","288+969=\n","289+152=\n","289+377=\n","289+383=\n","289+435=\n","289+491=\n","289+509=\n","289+557=\n","289+636=\n","289+638=\n","289+751=\n","289+791=\n","289+811=\n","289+950=\n","289+953=\n","290+165=\n","290+166=\n","290+420=\n","290+466=\n","290+571=\n","290+597=\n","290+623=\n","290+628=\n","290+655=\n","290+925=\n","291+96=\n","291+161=\n","291+175=\n","291+374=\n","291+379=\n","291+519=\n","291+560=\n","291+763=\n","291+891=\n","292+8=\n","292+21=\n","292+163=\n","292+310=\n","292+431=\n","292+476=\n","292+653=\n","292+676=\n","292+741=\n","292+905=\n","292+992=\n","293+17=\n","293+219=\n","293+286=\n","293+314=\n","293+460=\n","293+524=\n","293+525=\n","293+536=\n","293+589=\n","293+664=\n","293+685=\n","293+806=\n","293+982=\n","294+13=\n","294+59=\n","294+174=\n","294+498=\n","294+581=\n","294+584=\n","294+735=\n","294+742=\n","294+933=\n","295+166=\n","295+201=\n","295+246=\n","295+251=\n","295+445=\n","295+563=\n","295+583=\n","295+591=\n","295+789=\n","295+848=\n","295+902=\n","295+953=\n","296+485=\n","296+610=\n","296+663=\n","296+982=\n","297+3=\n","297+186=\n","297+189=\n","297+330=\n","297+570=\n","297+656=\n","297+666=\n","297+734=\n","298+96=\n","298+236=\n","298+348=\n","298+484=\n","298+538=\n","298+547=\n","298+618=\n","298+651=\n","298+776=\n","298+813=\n","298+975=\n","299+167=\n","299+240=\n","299+333=\n","299+429=\n","299+872=\n","299+951=\n","300+179=\n","300+238=\n","300+272=\n","300+323=\n","300+381=\n","300+415=\n","300+637=\n","300+655=\n","300+839=\n","300+893=\n","300+950=\n","301+71=\n","301+277=\n","301+410=\n","301+415=\n","301+469=\n","301+485=\n","301+490=\n","301+503=\n","301+535=\n","301+727=\n","301+753=\n","301+781=\n","301+845=\n","302+298=\n","302+309=\n","302+379=\n","302+675=\n","302+783=\n","302+869=\n","302+899=\n","302+923=\n","303+2=\n","303+19=\n","303+46=\n","303+80=\n","303+186=\n","303+218=\n","303+379=\n","303+568=\n","303+782=\n","303+959=\n","304+78=\n","304+246=\n","304+338=\n","304+884=\n","305+126=\n","305+170=\n","305+215=\n","305+364=\n","305+412=\n","305+449=\n","305+473=\n","306+11=\n","306+222=\n","306+407=\n","306+474=\n","307+54=\n","307+77=\n","307+185=\n","307+232=\n","307+407=\n","307+506=\n","307+508=\n","307+876=\n","307+908=\n","308+7=\n","308+12=\n","308+65=\n","308+114=\n","308+142=\n","308+168=\n","308+249=\n","308+281=\n","308+427=\n","308+451=\n","308+695=\n","308+755=\n","309+73=\n","309+105=\n","309+375=\n","309+595=\n","309+667=\n","309+731=\n","309+922=\n","310+15=\n","310+154=\n","310+237=\n","310+259=\n","310+450=\n","310+630=\n","310+671=\n","310+759=\n","310+793=\n","310+927=\n","310+965=\n","310+979=\n","311+16=\n","311+113=\n","311+154=\n","311+173=\n","311+340=\n","311+366=\n","311+555=\n","311+591=\n","311+608=\n","311+628=\n","311+660=\n","311+824=\n","312+99=\n","312+188=\n","312+333=\n","312+346=\n","312+368=\n","312+409=\n","312+522=\n","312+688=\n","312+912=\n","312+937=\n","313+37=\n","313+180=\n","313+210=\n","313+255=\n","313+329=\n","313+359=\n","313+483=\n","313+593=\n","313+710=\n","313+868=\n","313+993=\n","314+57=\n","314+202=\n","314+586=\n","314+685=\n","314+908=\n","314+969=\n","315+49=\n","315+91=\n","315+169=\n","315+248=\n","315+250=\n","315+871=\n","316+153=\n","316+277=\n","316+319=\n","316+582=\n","316+640=\n","316+822=\n","316+879=\n","316+890=\n","316+902=\n","316+917=\n","317+49=\n","317+165=\n","317+236=\n","317+315=\n","317+409=\n","317+449=\n","317+536=\n","317+553=\n","317+656=\n","317+788=\n","317+793=\n","317+876=\n","317+896=\n","317+931=\n","317+935=\n","318+42=\n","318+167=\n","318+196=\n","318+387=\n","318+458=\n","318+706=\n","318+745=\n","318+834=\n","319+104=\n","319+336=\n","319+397=\n","319+459=\n","319+476=\n","319+564=\n","319+672=\n","319+745=\n","319+831=\n","319+885=\n","320+10=\n","320+89=\n","320+208=\n","320+339=\n","320+394=\n","320+483=\n","320+583=\n","320+630=\n","320+770=\n","320+872=\n","320+931=\n","320+977=\n","321+34=\n","321+137=\n","321+175=\n","321+299=\n","321+309=\n","321+395=\n","321+690=\n","321+942=\n","322+70=\n","322+114=\n","322+130=\n","322+174=\n","322+213=\n","322+331=\n","322+400=\n","322+451=\n","322+493=\n","322+535=\n","322+538=\n","322+611=\n","322+620=\n","322+628=\n","322+634=\n","322+648=\n","322+660=\n","322+671=\n","322+677=\n","323+118=\n","323+250=\n","323+318=\n","323+386=\n","323+461=\n","323+569=\n","323+600=\n","323+617=\n","323+663=\n","323+915=\n","323+990=\n","324+39=\n","324+69=\n","324+99=\n","324+134=\n","324+135=\n","324+170=\n","324+197=\n","324+274=\n","324+389=\n","324+454=\n","324+572=\n","324+706=\n","324+788=\n","324+855=\n","324+900=\n","324+911=\n","325+28=\n","325+172=\n","325+190=\n","325+252=\n","325+347=\n","325+591=\n","325+699=\n","325+860=\n","325+968=\n","326+62=\n","326+140=\n","326+244=\n","326+267=\n","326+399=\n","326+516=\n","326+625=\n","326+641=\n","326+947=\n","326+949=\n","327+6=\n","327+21=\n","327+50=\n","327+62=\n","327+72=\n","327+227=\n","327+255=\n","327+357=\n","327+391=\n","327+408=\n","327+427=\n","327+430=\n","327+512=\n","327+658=\n","327+676=\n","327+906=\n","327+915=\n","327+963=\n","327+983=\n","328+114=\n","328+124=\n","328+126=\n","328+167=\n","328+434=\n","328+796=\n","328+878=\n","328+905=\n","329+98=\n","329+105=\n","329+176=\n","329+245=\n","329+252=\n","329+333=\n","329+342=\n","329+558=\n","329+616=\n","329+759=\n","329+838=\n","329+933=\n","329+993=\n","330+137=\n","330+139=\n","330+152=\n","330+162=\n","330+247=\n","330+542=\n","330+694=\n","330+702=\n","330+794=\n","330+817=\n","330+887=\n","330+969=\n","331+120=\n","331+220=\n","331+327=\n","331+365=\n","331+398=\n","331+399=\n","331+415=\n","331+457=\n","331+705=\n","331+720=\n","331+781=\n","331+967=\n","331+969=\n","332+41=\n","332+334=\n","332+425=\n","332+548=\n","332+652=\n","332+657=\n","332+666=\n","332+940=\n","333+100=\n","333+175=\n","333+549=\n","333+651=\n","333+866=\n","333+956=\n","333+979=\n","333+980=\n","334+59=\n","334+118=\n","334+143=\n","334+171=\n","334+350=\n","334+374=\n","334+394=\n","334+399=\n","334+430=\n","334+476=\n","334+571=\n","334+676=\n","334+742=\n","334+783=\n","334+943=\n","334+986=\n","335+11=\n","335+69=\n","335+96=\n","335+139=\n","335+227=\n","335+245=\n","335+311=\n","335+453=\n","335+624=\n","335+694=\n","335+758=\n","336+7=\n","336+35=\n","336+40=\n","336+122=\n","336+226=\n","336+283=\n","336+421=\n","336+425=\n","336+438=\n","336+490=\n","336+741=\n","336+764=\n","336+907=\n","336+910=\n","336+951=\n","337+163=\n","337+269=\n","337+297=\n","337+335=\n","337+483=\n","337+489=\n","337+601=\n","337+689=\n","337+736=\n","337+754=\n","337+771=\n","337+940=\n","337+996=\n","338+5=\n","338+16=\n","338+137=\n","338+415=\n","338+433=\n","338+588=\n","338+715=\n","338+784=\n","338+835=\n","338+890=\n","339+54=\n","339+176=\n","339+462=\n","339+466=\n","339+539=\n","339+585=\n","339+593=\n","339+619=\n","339+678=\n","339+762=\n","339+861=\n","340+284=\n","340+297=\n","340+416=\n","340+547=\n","340+592=\n","340+788=\n","340+806=\n","340+923=\n","340+956=\n","341+25=\n","341+154=\n","341+400=\n","341+413=\n","341+499=\n","341+557=\n","341+606=\n","341+677=\n","341+737=\n","341+855=\n","342+110=\n","342+234=\n","342+295=\n","342+427=\n","342+444=\n","342+483=\n","342+692=\n","342+771=\n","342+800=\n","342+816=\n","342+860=\n","343+134=\n","343+165=\n","343+322=\n","343+327=\n","343+341=\n","343+473=\n","343+502=\n","343+532=\n","343+786=\n","343+971=\n","344+15=\n","344+29=\n","344+70=\n","344+79=\n","344+572=\n","344+718=\n","344+746=\n","344+772=\n","344+889=\n","344+908=\n","344+959=\n","344+980=\n","344+993=\n","345+160=\n","345+236=\n","345+378=\n","345+424=\n","345+460=\n","345+484=\n","345+524=\n","345+553=\n","345+689=\n","345+842=\n","345+926=\n","345+929=\n","346+2=\n","346+4=\n","346+145=\n","346+313=\n","346+351=\n","346+367=\n","346+468=\n","346+494=\n","346+550=\n","346+695=\n","346+927=\n","347+125=\n","347+152=\n","347+241=\n","347+359=\n","347+450=\n","347+610=\n","347+619=\n","347+653=\n","347+676=\n","347+694=\n","347+994=\n","348+58=\n","348+76=\n","348+131=\n","348+156=\n","348+298=\n","348+350=\n","348+461=\n","348+467=\n","348+474=\n","348+618=\n","348+691=\n","348+701=\n","348+889=\n","349+73=\n","349+248=\n","349+374=\n","349+378=\n","349+582=\n","349+686=\n","349+711=\n","350+89=\n","350+252=\n","350+320=\n","350+532=\n","350+759=\n","350+954=\n","350+987=\n","350+994=\n","351+73=\n","351+305=\n","351+435=\n","351+534=\n","351+609=\n","351+617=\n","351+762=\n","351+790=\n","351+846=\n","351+895=\n","351+970=\n","352+8=\n","352+99=\n","352+101=\n","352+126=\n","352+128=\n","352+142=\n","352+169=\n","352+580=\n","352+703=\n","352+740=\n","352+889=\n","352+956=\n","352+973=\n","352+989=\n","353+169=\n","353+384=\n","353+458=\n","353+652=\n","353+690=\n","353+756=\n","353+780=\n","353+912=\n","353+925=\n","353+944=\n","353+992=\n","354+113=\n","354+245=\n","354+289=\n","354+316=\n","354+367=\n","354+436=\n","354+590=\n","354+761=\n","354+964=\n","355+112=\n","355+140=\n","355+368=\n","355+522=\n","355+706=\n","355+748=\n","355+888=\n","355+949=\n","355+998=\n","356+278=\n","356+331=\n","356+333=\n","356+389=\n","356+443=\n","356+474=\n","356+489=\n","356+620=\n","356+661=\n","356+700=\n","356+931=\n","356+978=\n","357+40=\n","357+155=\n","357+262=\n","357+782=\n","357+857=\n","358+24=\n","358+94=\n","358+133=\n","358+224=\n","358+278=\n","358+432=\n","358+753=\n","358+792=\n","358+930=\n","358+967=\n","359+52=\n","359+83=\n","359+141=\n","359+153=\n","359+163=\n","359+403=\n","359+592=\n","360+221=\n","360+259=\n","360+344=\n","360+366=\n","360+558=\n","360+673=\n","360+761=\n","361+0=\n","361+188=\n","361+191=\n","361+197=\n","361+212=\n","361+231=\n","361+328=\n","361+472=\n","361+503=\n","361+750=\n","361+796=\n","361+858=\n","361+960=\n","362+66=\n","362+150=\n","362+151=\n","362+214=\n","362+309=\n","362+363=\n","362+384=\n","362+439=\n","362+637=\n","362+694=\n","362+771=\n","362+864=\n","362+924=\n","362+927=\n","362+933=\n","363+62=\n","363+224=\n","363+253=\n","363+391=\n","363+508=\n","363+728=\n","363+867=\n","364+641=\n","364+666=\n","364+780=\n","364+792=\n","364+793=\n","365+179=\n","365+604=\n","365+624=\n","365+628=\n","365+651=\n","365+660=\n","365+930=\n","365+964=\n","366+198=\n","366+389=\n","366+497=\n","366+511=\n","367+141=\n","367+160=\n","367+187=\n","367+327=\n","368+309=\n","368+413=\n","368+414=\n","368+416=\n","368+427=\n","368+507=\n","368+550=\n","368+610=\n","368+904=\n","369+151=\n","369+324=\n","369+399=\n","369+639=\n","369+648=\n","369+670=\n","370+75=\n","370+153=\n","370+159=\n","370+505=\n","370+737=\n","371+210=\n","371+309=\n","371+412=\n","371+533=\n","371+553=\n","371+744=\n","371+934=\n","372+43=\n","372+216=\n","372+219=\n","372+296=\n","372+511=\n","372+586=\n","372+613=\n","372+883=\n","372+937=\n","373+32=\n","373+97=\n","373+98=\n","373+154=\n","373+236=\n","373+477=\n","373+517=\n","373+905=\n","373+919=\n","374+19=\n","374+27=\n","374+43=\n","374+64=\n","374+110=\n","374+134=\n","374+187=\n","374+254=\n","374+323=\n","374+556=\n","374+625=\n","374+998=\n","375+25=\n","375+61=\n","375+114=\n","375+268=\n","375+430=\n","375+596=\n","375+700=\n","375+871=\n","375+924=\n","376+13=\n","376+311=\n","376+475=\n","376+550=\n","376+895=\n","376+978=\n","376+984=\n","377+390=\n","377+482=\n","377+508=\n","377+566=\n","377+571=\n","377+628=\n","377+780=\n","377+823=\n","377+939=\n","377+999=\n","378+104=\n","378+260=\n","378+325=\n","378+628=\n","378+646=\n","378+820=\n","379+55=\n","379+131=\n","379+147=\n","379+175=\n","379+327=\n","379+422=\n","379+596=\n","379+602=\n","379+661=\n","379+740=\n","379+985=\n","380+253=\n","380+269=\n","380+421=\n","380+710=\n","380+824=\n","380+889=\n","380+927=\n","380+997=\n","381+105=\n","381+307=\n","381+349=\n","381+404=\n","381+447=\n","381+456=\n","381+468=\n","381+644=\n","381+709=\n","381+837=\n","381+878=\n","381+928=\n","382+190=\n","382+216=\n","382+219=\n","382+291=\n","382+392=\n","382+418=\n","382+560=\n","382+706=\n","382+787=\n","382+795=\n","382+965=\n","383+51=\n","383+78=\n","383+132=\n","383+236=\n","383+250=\n","383+326=\n","383+627=\n","383+844=\n","383+865=\n","384+31=\n","384+42=\n","384+123=\n","384+242=\n","384+466=\n","384+543=\n","384+692=\n","384+697=\n","384+737=\n","384+745=\n","384+781=\n","384+864=\n","384+883=\n","385+60=\n","385+64=\n","385+90=\n","385+339=\n","385+727=\n","385+757=\n","385+905=\n","385+990=\n","386+33=\n","386+230=\n","386+361=\n","386+594=\n","386+609=\n","386+915=\n","386+955=\n","386+979=\n","386+980=\n","387+93=\n","387+250=\n","387+379=\n","387+477=\n","387+519=\n","387+589=\n","387+672=\n","387+822=\n","388+106=\n","388+265=\n","388+341=\n","388+496=\n","388+677=\n","388+812=\n","388+830=\n","389+100=\n","389+174=\n","389+218=\n","389+476=\n","389+636=\n","389+940=\n","389+987=\n","390+70=\n","390+191=\n","390+376=\n","390+385=\n","390+450=\n","390+477=\n","390+534=\n","390+676=\n","390+812=\n","390+921=\n","390+951=\n","390+957=\n","390+994=\n","391+36=\n","391+62=\n","391+348=\n","391+447=\n","391+631=\n","391+787=\n","391+899=\n","391+966=\n","392+8=\n","392+75=\n","392+180=\n","392+192=\n","392+208=\n","392+209=\n","392+225=\n","392+337=\n","392+501=\n","392+735=\n","392+786=\n","392+912=\n","392+923=\n","393+40=\n","393+64=\n","393+269=\n","393+422=\n","393+458=\n","393+514=\n","393+595=\n","393+597=\n","393+631=\n","393+762=\n","393+835=\n","393+963=\n","394+102=\n","394+105=\n","394+141=\n","394+220=\n","394+243=\n","394+363=\n","394+376=\n","394+607=\n","394+639=\n","394+795=\n","394+959=\n","394+972=\n","395+35=\n","395+129=\n","395+231=\n","395+658=\n","395+855=\n","396+8=\n","396+161=\n","396+263=\n","396+414=\n","396+419=\n","396+609=\n","396+884=\n","397+253=\n","397+370=\n","397+605=\n","397+756=\n","397+826=\n","397+880=\n","397+917=\n","397+955=\n","398+184=\n","398+259=\n","398+275=\n","398+466=\n","398+494=\n","398+591=\n","398+772=\n","398+823=\n","398+914=\n","398+950=\n","399+66=\n","399+137=\n","399+138=\n","399+318=\n","399+346=\n","399+430=\n","399+517=\n","399+526=\n","399+718=\n","399+808=\n","400+156=\n","400+186=\n","400+497=\n","400+511=\n","400+556=\n","400+585=\n","400+883=\n","401+137=\n","401+170=\n","401+176=\n","401+230=\n","401+399=\n","401+584=\n","401+627=\n","401+791=\n","401+885=\n","401+930=\n","402+69=\n","402+157=\n","402+171=\n","402+243=\n","402+350=\n","402+437=\n","402+589=\n","402+630=\n","402+674=\n","402+692=\n","402+827=\n","403+31=\n","403+249=\n","403+364=\n","403+396=\n","403+418=\n","403+510=\n","403+542=\n","403+547=\n","403+729=\n","403+772=\n","403+859=\n","403+860=\n","403+915=\n","403+940=\n","404+97=\n","404+293=\n","404+510=\n","404+559=\n","404+586=\n","404+687=\n","404+696=\n","404+863=\n","404+911=\n","405+103=\n","405+266=\n","405+286=\n","405+333=\n","405+539=\n","405+565=\n","405+993=\n","406+53=\n","406+63=\n","406+115=\n","406+148=\n","406+167=\n","406+361=\n","406+370=\n","406+478=\n","406+779=\n","406+944=\n","406+986=\n","407+5=\n","407+28=\n","407+136=\n","407+381=\n","407+486=\n","407+586=\n","407+609=\n","407+888=\n","407+995=\n","408+190=\n","408+262=\n","408+382=\n","408+489=\n","408+790=\n","408+971=\n","409+84=\n","409+98=\n","409+482=\n","409+554=\n","409+592=\n","409+759=\n","409+795=\n","409+900=\n","409+912=\n","409+953=\n","410+361=\n","410+379=\n","410+528=\n","410+703=\n","410+749=\n","410+773=\n","410+820=\n","411+37=\n","411+216=\n","411+218=\n","411+261=\n","411+290=\n","411+345=\n","411+398=\n","411+444=\n","411+827=\n","411+828=\n","411+849=\n","411+965=\n","412+89=\n","412+184=\n","412+193=\n","412+226=\n","412+298=\n","412+362=\n","412+377=\n","412+616=\n","412+824=\n","412+962=\n","413+190=\n","413+195=\n","413+244=\n","413+274=\n","413+430=\n","413+528=\n","413+793=\n","413+807=\n","413+827=\n","413+922=\n","414+15=\n","414+65=\n","414+125=\n","414+241=\n","414+287=\n","414+398=\n","414+533=\n","414+801=\n","415+108=\n","415+334=\n","415+348=\n","415+368=\n","415+451=\n","415+465=\n","415+505=\n","415+514=\n","415+939=\n","415+970=\n","416+1=\n","416+224=\n","416+243=\n","416+381=\n","416+406=\n","416+418=\n","416+519=\n","416+554=\n","416+566=\n","416+637=\n","416+747=\n","416+844=\n","416+922=\n","417+7=\n","417+160=\n","417+172=\n","417+224=\n","417+339=\n","417+431=\n","417+451=\n","417+510=\n","417+573=\n","417+574=\n","417+672=\n","417+700=\n","417+775=\n","417+795=\n","417+819=\n","417+912=\n","418+142=\n","418+287=\n","418+470=\n","418+622=\n","419+206=\n","419+277=\n","419+290=\n","419+412=\n","419+508=\n","419+512=\n","419+617=\n","419+781=\n","419+999=\n","420+21=\n","420+111=\n","420+248=\n","420+323=\n","420+343=\n","420+358=\n","420+603=\n","420+730=\n","420+813=\n","420+890=\n","420+939=\n","421+10=\n","421+189=\n","421+267=\n","421+310=\n","421+363=\n","421+400=\n","421+402=\n","421+414=\n","421+580=\n","421+595=\n","421+805=\n","421+879=\n","421+882=\n","422+202=\n","422+215=\n","422+320=\n","422+370=\n","422+490=\n","422+533=\n","422+591=\n","422+744=\n","422+958=\n","422+974=\n","423+22=\n","423+44=\n","423+73=\n","423+264=\n","423+355=\n","423+505=\n","423+578=\n","423+660=\n","423+702=\n","423+768=\n","423+783=\n","423+793=\n","423+940=\n","423+946=\n","423+976=\n","424+124=\n","424+198=\n","424+344=\n","424+541=\n","424+597=\n","424+622=\n","424+639=\n","424+737=\n","424+807=\n","424+871=\n","424+990=\n","425+35=\n","425+155=\n","425+324=\n","425+453=\n","425+461=\n","425+475=\n","425+536=\n","425+728=\n","425+791=\n","425+804=\n","426+252=\n","426+368=\n","426+424=\n","426+466=\n","426+611=\n","426+857=\n","426+923=\n","427+74=\n","427+294=\n","427+383=\n","427+418=\n","427+491=\n","427+566=\n","427+712=\n","427+783=\n","427+829=\n","427+835=\n","427+898=\n","427+969=\n","428+357=\n","428+774=\n","428+948=\n","429+11=\n","429+233=\n","429+292=\n","429+372=\n","429+549=\n","429+685=\n","429+718=\n","429+839=\n","430+10=\n","430+72=\n","430+87=\n","430+282=\n","430+357=\n","430+376=\n","430+423=\n","430+425=\n","430+464=\n","430+652=\n","430+662=\n","430+724=\n","431+220=\n","431+290=\n","431+296=\n","431+335=\n","431+516=\n","431+517=\n","431+598=\n","431+722=\n","432+209=\n","432+636=\n","432+654=\n","432+683=\n","432+883=\n","432+965=\n","433+11=\n","433+55=\n","433+184=\n","433+196=\n","433+235=\n","433+299=\n","433+375=\n","433+593=\n","433+613=\n","433+660=\n","433+706=\n","433+809=\n","433+862=\n","433+864=\n","433+978=\n","433+995=\n","434+235=\n","434+354=\n","434+415=\n","434+435=\n","434+511=\n","434+566=\n","434+817=\n","434+908=\n","434+928=\n","434+958=\n","434+960=\n","435+11=\n","435+47=\n","435+158=\n","435+171=\n","435+450=\n","435+657=\n","435+805=\n","436+368=\n","436+394=\n","436+526=\n","436+645=\n","436+826=\n","437+13=\n","437+18=\n","437+232=\n","437+359=\n","437+389=\n","437+468=\n","437+553=\n","437+699=\n","437+736=\n","437+809=\n","437+865=\n","437+898=\n","437+899=\n","438+192=\n","438+428=\n","438+431=\n","438+470=\n","438+642=\n","438+669=\n","438+719=\n","438+910=\n","439+46=\n","439+223=\n","439+358=\n","439+639=\n","439+997=\n","440+310=\n","440+374=\n","440+385=\n","440+470=\n","440+484=\n","440+645=\n","440+766=\n","440+929=\n","440+941=\n","441+44=\n","441+110=\n","441+376=\n","441+480=\n","441+828=\n","441+892=\n","441+971=\n","442+22=\n","442+51=\n","442+580=\n","442+637=\n","442+810=\n","442+890=\n","442+960=\n","443+110=\n","443+127=\n","443+134=\n","443+138=\n","443+367=\n","443+379=\n","443+511=\n","443+573=\n","443+575=\n","443+792=\n","443+928=\n","443+986=\n","444+456=\n","444+571=\n","444+619=\n","444+752=\n","444+815=\n","444+833=\n","444+871=\n","444+883=\n","445+232=\n","445+360=\n","445+383=\n","445+384=\n","445+487=\n","445+781=\n","445+809=\n","445+832=\n","445+948=\n","446+82=\n","446+286=\n","446+435=\n","446+616=\n","446+663=\n","446+783=\n","446+803=\n","446+839=\n","446+899=\n","446+950=\n","447+72=\n","447+146=\n","447+170=\n","447+236=\n","447+264=\n","447+322=\n","447+358=\n","447+413=\n","447+564=\n","447+638=\n","447+709=\n","447+830=\n","447+910=\n","448+23=\n","448+45=\n","448+108=\n","448+134=\n","448+441=\n","448+556=\n","448+573=\n","448+607=\n","448+625=\n","448+782=\n","448+907=\n","448+921=\n","448+963=\n","448+977=\n","449+155=\n","449+197=\n","449+244=\n","449+779=\n","449+871=\n","449+951=\n","450+50=\n","450+231=\n","450+235=\n","450+275=\n","450+440=\n","450+606=\n","450+648=\n","450+720=\n","450+794=\n","451+37=\n","451+182=\n","451+313=\n","451+346=\n","451+356=\n","451+371=\n","451+483=\n","451+599=\n","451+729=\n","451+793=\n","451+837=\n","451+896=\n","451+929=\n","452+50=\n","452+87=\n","452+99=\n","452+263=\n","452+510=\n","452+544=\n","452+593=\n","452+684=\n","452+719=\n","452+798=\n","452+816=\n","452+875=\n","452+933=\n","452+984=\n","453+115=\n","453+393=\n","453+523=\n","454+4=\n","454+26=\n","454+43=\n","454+172=\n","454+459=\n","454+716=\n","454+931=\n","455+29=\n","455+77=\n","455+271=\n","455+491=\n","455+524=\n","455+533=\n","455+631=\n","455+892=\n","455+913=\n","455+958=\n","455+970=\n","456+4=\n","456+132=\n","456+136=\n","456+357=\n","456+380=\n","456+840=\n","456+867=\n","456+919=\n","457+31=\n","457+121=\n","457+181=\n","457+207=\n","457+213=\n","457+656=\n","457+771=\n","457+799=\n","457+869=\n","457+947=\n","458+1=\n","458+50=\n","458+280=\n","458+569=\n","458+579=\n","458+633=\n","458+663=\n","458+755=\n","458+963=\n","459+53=\n","459+134=\n","459+190=\n","459+233=\n","459+326=\n","459+502=\n","459+654=\n","459+686=\n","459+687=\n","459+737=\n","459+795=\n","459+862=\n","459+949=\n","460+73=\n","460+82=\n","460+115=\n","460+134=\n","460+148=\n","460+388=\n","460+546=\n","460+619=\n","460+641=\n","460+689=\n","460+707=\n","460+971=\n","460+982=\n","461+100=\n","461+174=\n","461+175=\n","461+231=\n","461+427=\n","461+469=\n","461+496=\n","461+617=\n","461+629=\n","461+687=\n","461+838=\n","461+857=\n","462+29=\n","462+79=\n","462+425=\n","462+489=\n","462+511=\n","462+614=\n","462+695=\n","462+791=\n","462+892=\n","463+5=\n","463+66=\n","463+94=\n","463+203=\n","463+248=\n","463+338=\n","463+352=\n","463+354=\n","463+445=\n","463+572=\n","463+631=\n","463+681=\n","463+769=\n","463+795=\n","463+806=\n","463+838=\n","463+854=\n","463+977=\n","464+45=\n","464+82=\n","464+110=\n","464+258=\n","464+294=\n","464+316=\n","464+382=\n","464+427=\n","464+533=\n","464+730=\n","464+749=\n","464+758=\n","464+771=\n","465+18=\n","465+247=\n","465+414=\n","465+422=\n","465+439=\n","465+589=\n","465+661=\n","465+720=\n","465+814=\n","465+864=\n","465+982=\n","465+995=\n","466+56=\n","466+131=\n","466+151=\n","466+352=\n","466+364=\n","466+432=\n","466+502=\n","466+628=\n","466+872=\n","466+924=\n","467+138=\n","467+219=\n","467+379=\n","467+390=\n","467+407=\n","467+500=\n","467+604=\n","467+682=\n","468+275=\n","468+745=\n","468+763=\n","468+798=\n","468+829=\n","469+2=\n","469+21=\n","469+24=\n","469+59=\n","469+75=\n","469+311=\n","469+319=\n","469+576=\n","469+653=\n","469+654=\n","469+764=\n","469+910=\n","470+85=\n","470+203=\n","470+324=\n","470+576=\n","470+606=\n","470+857=\n","470+858=\n","470+891=\n","470+892=\n","470+984=\n","471+42=\n","471+71=\n","471+83=\n","471+327=\n","471+358=\n","471+539=\n","471+585=\n","471+649=\n","471+874=\n","472+47=\n","472+97=\n","472+126=\n","472+151=\n","472+227=\n","472+314=\n","472+379=\n","472+506=\n","472+530=\n","472+547=\n","473+23=\n","473+190=\n","473+204=\n","473+311=\n","473+433=\n","473+494=\n","473+571=\n","473+801=\n","473+844=\n","474+270=\n","474+478=\n","474+534=\n","474+541=\n","474+711=\n","474+771=\n","474+789=\n","474+973=\n","475+28=\n","475+241=\n","475+257=\n","475+290=\n","475+321=\n","475+445=\n","475+598=\n","475+638=\n","475+779=\n","475+867=\n","475+940=\n","476+94=\n","476+131=\n","476+256=\n","476+297=\n","476+327=\n","476+358=\n","476+427=\n","476+450=\n","476+557=\n","476+636=\n","476+757=\n","476+863=\n","476+864=\n","476+891=\n","476+907=\n","476+925=\n","476+963=\n","476+985=\n","477+16=\n","477+169=\n","477+257=\n","477+265=\n","477+288=\n","477+327=\n","477+354=\n","477+420=\n","477+444=\n","477+527=\n","477+580=\n","477+636=\n","477+718=\n","477+784=\n","477+814=\n","477+916=\n","478+19=\n","478+73=\n","478+138=\n","478+168=\n","478+235=\n","478+428=\n","478+596=\n","478+603=\n","478+695=\n","478+748=\n","478+757=\n","478+834=\n","478+847=\n","478+922=\n","479+65=\n","479+92=\n","479+301=\n","479+334=\n","479+484=\n","479+529=\n","479+571=\n","479+577=\n","479+692=\n","480+54=\n","480+244=\n","480+261=\n","480+280=\n","480+401=\n","480+463=\n","480+642=\n","480+668=\n","480+750=\n","480+769=\n","480+817=\n","480+909=\n","481+25=\n","481+133=\n","481+158=\n","481+167=\n","481+260=\n","481+599=\n","481+718=\n","481+782=\n","481+966=\n","482+203=\n","482+223=\n","482+280=\n","482+345=\n","482+400=\n","482+417=\n","482+435=\n","482+666=\n","482+807=\n","483+169=\n","483+480=\n","483+670=\n","483+746=\n","483+777=\n","484+217=\n","484+263=\n","484+274=\n","484+280=\n","484+546=\n","484+659=\n","485+18=\n","485+397=\n","485+458=\n","485+567=\n","485+604=\n","485+860=\n","486+64=\n","486+100=\n","486+294=\n","486+299=\n","486+536=\n","486+592=\n","486+612=\n","486+868=\n","487+140=\n","487+151=\n","487+260=\n","487+272=\n","487+406=\n","487+579=\n","487+610=\n","487+661=\n","487+674=\n","487+704=\n","487+709=\n","487+845=\n","488+43=\n","488+123=\n","488+132=\n","488+148=\n","488+393=\n","488+421=\n","488+497=\n","488+623=\n","488+943=\n","488+956=\n","488+980=\n","489+205=\n","489+216=\n","489+278=\n","489+508=\n","489+550=\n","489+551=\n","489+757=\n","489+878=\n","489+917=\n","489+968=\n","490+97=\n","490+107=\n","490+136=\n","490+137=\n","490+189=\n","490+273=\n","490+304=\n","490+430=\n","490+437=\n","490+447=\n","490+487=\n","490+768=\n","491+104=\n","491+153=\n","491+230=\n","491+494=\n","491+613=\n","491+757=\n","491+949=\n","492+94=\n","492+207=\n","492+241=\n","492+330=\n","492+366=\n","492+510=\n","492+821=\n","492+969=\n","493+33=\n","493+246=\n","493+522=\n","493+671=\n","493+808=\n","493+812=\n","493+872=\n","493+952=\n","494+216=\n","494+266=\n","494+310=\n","494+319=\n","494+449=\n","494+472=\n","494+655=\n","494+784=\n","494+890=\n","494+892=\n","495+68=\n","495+161=\n","495+201=\n","495+350=\n","495+506=\n","495+551=\n","495+680=\n","495+695=\n","495+720=\n","495+765=\n","495+827=\n","495+945=\n","495+968=\n","495+982=\n","496+54=\n","496+148=\n","496+207=\n","496+262=\n","496+333=\n","496+441=\n","496+457=\n","496+463=\n","496+574=\n","496+785=\n","497+10=\n","497+58=\n","497+144=\n","497+184=\n","497+384=\n","497+491=\n","497+529=\n","497+678=\n","497+688=\n","497+716=\n","497+754=\n","497+810=\n","497+983=\n","498+85=\n","498+228=\n","498+308=\n","498+330=\n","498+339=\n","498+356=\n","498+437=\n","498+535=\n","498+845=\n","498+902=\n","498+921=\n","498+991=\n","499+177=\n","499+183=\n","499+282=\n","499+327=\n","499+335=\n","499+346=\n","499+404=\n","499+432=\n","499+518=\n","499+661=\n","499+736=\n","499+762=\n","499+870=\n","499+917=\n","499+946=\n","500+8=\n","500+39=\n","500+56=\n","500+95=\n","500+101=\n","500+249=\n","500+427=\n","500+457=\n","500+467=\n","500+612=\n","500+673=\n","500+714=\n","501+141=\n","501+408=\n","501+505=\n","501+534=\n","501+568=\n","501+626=\n","502+38=\n","502+82=\n","502+171=\n","502+222=\n","502+268=\n","502+272=\n","502+281=\n","502+499=\n","502+548=\n","502+653=\n","502+667=\n","502+725=\n","503+95=\n","503+371=\n","503+456=\n","503+578=\n","503+618=\n","503+770=\n","503+996=\n","504+205=\n","504+515=\n","504+550=\n","504+557=\n","504+603=\n","504+775=\n","504+787=\n","504+826=\n","505+277=\n","505+278=\n","505+341=\n","505+346=\n","505+386=\n","505+394=\n","505+611=\n","505+741=\n","505+843=\n","506+0=\n","506+141=\n","506+207=\n","506+235=\n","506+315=\n","506+349=\n","506+413=\n","506+492=\n","506+501=\n","506+577=\n","506+599=\n","506+670=\n","506+841=\n","506+861=\n","506+865=\n","507+30=\n","507+34=\n","507+93=\n","507+230=\n","507+313=\n","507+369=\n","507+554=\n","507+659=\n","507+722=\n","508+72=\n","508+132=\n","508+218=\n","508+238=\n","508+542=\n","508+563=\n","508+581=\n","508+756=\n","508+774=\n","508+895=\n","509+91=\n","509+110=\n","509+193=\n","509+222=\n","509+293=\n","509+448=\n","509+584=\n","509+703=\n","509+794=\n","509+795=\n","509+891=\n","509+917=\n","509+937=\n","510+265=\n","510+510=\n","510+577=\n","510+651=\n","510+696=\n","510+934=\n","511+20=\n","511+271=\n","511+273=\n","511+777=\n","511+835=\n","511+917=\n","512+350=\n","512+391=\n","512+903=\n","513+119=\n","513+129=\n","513+141=\n","513+151=\n","513+155=\n","513+323=\n","513+338=\n","513+452=\n","513+527=\n","513+550=\n","513+599=\n","513+629=\n","513+731=\n","513+740=\n","513+852=\n","513+873=\n","513+910=\n","514+51=\n","514+54=\n","514+137=\n","514+186=\n","514+248=\n","514+272=\n","514+335=\n","514+445=\n","514+659=\n","514+665=\n","514+695=\n","514+772=\n","514+930=\n","514+993=\n","515+179=\n","515+604=\n","515+689=\n","515+710=\n","515+722=\n","515+826=\n","515+951=\n","516+34=\n","516+98=\n","516+141=\n","516+393=\n","516+438=\n","516+767=\n","516+830=\n","516+928=\n","516+947=\n","516+951=\n","517+18=\n","517+38=\n","517+185=\n","517+268=\n","517+402=\n","517+477=\n","517+821=\n","517+871=\n","517+877=\n","517+937=\n","517+952=\n","518+27=\n","518+133=\n","518+172=\n","518+257=\n","518+302=\n","518+364=\n","518+397=\n","518+770=\n","518+799=\n","518+868=\n","518+873=\n","519+7=\n","519+62=\n","519+131=\n","519+154=\n","519+302=\n","519+345=\n","519+377=\n","519+400=\n","519+669=\n","519+952=\n","520+43=\n","520+473=\n","520+482=\n","520+573=\n","520+815=\n","520+928=\n","521+143=\n","521+168=\n","521+307=\n","521+861=\n","521+993=\n","522+138=\n","522+149=\n","522+190=\n","522+340=\n","522+435=\n","522+442=\n","522+444=\n","522+551=\n","522+587=\n","522+626=\n","522+708=\n","522+719=\n","522+821=\n","522+867=\n","522+875=\n","523+74=\n","523+121=\n","523+187=\n","523+194=\n","523+238=\n","523+263=\n","523+347=\n","523+416=\n","523+488=\n","523+534=\n","523+559=\n","523+808=\n","524+163=\n","524+311=\n","524+396=\n","524+408=\n","524+451=\n","524+560=\n","524+638=\n","524+839=\n","525+24=\n","525+276=\n","525+578=\n","525+592=\n","525+677=\n","525+689=\n","525+873=\n","525+932=\n","525+997=\n","526+0=\n","526+30=\n","526+286=\n","526+329=\n","526+400=\n","526+494=\n","526+501=\n","526+608=\n","526+662=\n","526+762=\n","526+802=\n","527+9=\n","527+220=\n","527+443=\n","527+480=\n","527+489=\n","527+763=\n","528+396=\n","528+414=\n","528+455=\n","528+459=\n","528+549=\n","528+580=\n","528+583=\n","528+606=\n","528+890=\n","529+83=\n","529+102=\n","529+409=\n","529+454=\n","529+550=\n","529+908=\n","530+128=\n","530+138=\n","530+317=\n","530+319=\n","530+348=\n","530+377=\n","530+409=\n","530+550=\n","530+665=\n","530+817=\n","530+856=\n","530+918=\n","530+926=\n","531+337=\n","531+676=\n","531+847=\n","532+11=\n","532+116=\n","532+221=\n","532+277=\n","532+323=\n","532+338=\n","532+416=\n","532+507=\n","532+521=\n","532+558=\n","532+691=\n","532+803=\n","532+855=\n","532+957=\n","533+163=\n","533+317=\n","533+532=\n","533+543=\n","533+642=\n","533+739=\n","533+921=\n","533+955=\n","534+72=\n","534+82=\n","534+241=\n","534+297=\n","534+344=\n","534+364=\n","534+409=\n","534+588=\n","534+600=\n","534+634=\n","534+663=\n","534+693=\n","535+198=\n","535+244=\n","535+449=\n","535+525=\n","535+592=\n","535+613=\n","535+622=\n","535+644=\n","535+682=\n","535+806=\n","535+868=\n","536+54=\n","536+366=\n","536+389=\n","536+451=\n","536+468=\n","536+537=\n","536+607=\n","536+765=\n","536+794=\n","536+855=\n","537+80=\n","537+94=\n","537+102=\n","537+269=\n","537+514=\n","537+542=\n","537+604=\n","537+613=\n","537+691=\n","537+704=\n","537+712=\n","537+736=\n","537+768=\n","537+847=\n","537+919=\n","537+920=\n","537+961=\n","538+254=\n","538+346=\n","538+369=\n","538+401=\n","538+516=\n","538+709=\n","538+839=\n","538+845=\n","538+898=\n","539+128=\n","539+208=\n","539+345=\n","539+356=\n","539+359=\n","539+374=\n","539+376=\n","539+466=\n","539+523=\n","539+909=\n","539+988=\n","540+240=\n","540+294=\n","540+308=\n","540+440=\n","540+465=\n","540+530=\n","540+557=\n","540+692=\n","540+833=\n","540+890=\n","540+915=\n","541+4=\n","541+93=\n","541+101=\n","541+133=\n","541+347=\n","541+379=\n","541+513=\n","541+538=\n","541+571=\n","541+579=\n","541+643=\n","541+871=\n","542+118=\n","542+289=\n","542+308=\n","542+423=\n","542+500=\n","542+594=\n","542+653=\n","542+732=\n","543+232=\n","543+300=\n","543+303=\n","543+404=\n","543+433=\n","543+652=\n","543+811=\n","543+877=\n","543+935=\n","543+965=\n","544+23=\n","544+213=\n","544+536=\n","544+576=\n","544+831=\n","544+869=\n","544+913=\n","545+91=\n","545+291=\n","545+333=\n","545+459=\n","545+549=\n","545+566=\n","545+730=\n","545+769=\n","545+960=\n","546+14=\n","546+18=\n","546+103=\n","546+185=\n","546+224=\n","546+289=\n","546+375=\n","546+441=\n","546+501=\n","546+520=\n","546+614=\n","546+798=\n","546+819=\n","546+823=\n","546+842=\n","546+879=\n","546+943=\n","546+988=\n","547+38=\n","547+59=\n","547+172=\n","547+204=\n","547+273=\n","547+364=\n","547+370=\n","547+446=\n","547+531=\n","547+594=\n","547+606=\n","547+711=\n","547+724=\n","547+810=\n","547+910=\n","548+19=\n","548+89=\n","548+100=\n","548+298=\n","548+317=\n","548+442=\n","548+635=\n","548+834=\n","548+968=\n","549+8=\n","549+21=\n","549+47=\n","549+71=\n","549+165=\n","549+308=\n","549+519=\n","549+617=\n","549+834=\n","550+16=\n","550+267=\n","550+611=\n","550+631=\n","550+640=\n","550+644=\n","550+735=\n","550+737=\n","550+782=\n","550+994=\n","551+12=\n","551+108=\n","551+269=\n","551+293=\n","551+421=\n","551+520=\n","551+636=\n","551+644=\n","551+687=\n","551+768=\n","552+23=\n","552+171=\n","552+574=\n","552+581=\n","552+630=\n","552+707=\n","552+761=\n","552+867=\n","553+377=\n","553+454=\n","553+552=\n","553+656=\n","553+664=\n","553+686=\n","553+746=\n","553+847=\n","553+893=\n","553+908=\n","553+973=\n","554+91=\n","554+103=\n","554+142=\n","554+211=\n","554+266=\n","554+357=\n","554+449=\n","554+507=\n","554+511=\n","554+563=\n","554+621=\n","555+119=\n","555+144=\n","555+188=\n","555+251=\n","555+277=\n","555+280=\n","555+317=\n","555+324=\n","555+397=\n","555+553=\n","555+607=\n","555+618=\n","555+695=\n","555+728=\n","555+856=\n","555+872=\n","555+993=\n","556+27=\n","556+75=\n","556+409=\n","556+413=\n","556+446=\n","556+548=\n","556+613=\n","556+625=\n","556+683=\n","556+700=\n","556+733=\n","556+800=\n","556+846=\n","556+909=\n","557+65=\n","557+151=\n","557+311=\n","557+429=\n","557+529=\n","557+614=\n","557+835=\n","557+875=\n","557+917=\n","557+943=\n","558+108=\n","558+200=\n","558+222=\n","558+334=\n","558+438=\n","558+506=\n","558+712=\n","558+949=\n","559+2=\n","559+199=\n","559+324=\n","559+469=\n","559+488=\n","559+528=\n","559+605=\n","559+626=\n","559+640=\n","559+813=\n","559+837=\n","559+899=\n","560+266=\n","560+300=\n","560+373=\n","560+437=\n","560+610=\n","560+653=\n","560+858=\n","560+887=\n","560+974=\n","561+15=\n","561+177=\n","561+240=\n","561+302=\n","561+319=\n","561+449=\n","561+472=\n","561+657=\n","561+874=\n","562+251=\n","562+530=\n","562+534=\n","562+538=\n","562+671=\n","562+682=\n","562+734=\n","562+774=\n","563+50=\n","563+52=\n","563+252=\n","563+274=\n","563+358=\n","563+545=\n","563+614=\n","563+825=\n","563+940=\n","563+989=\n","564+352=\n","564+569=\n","564+635=\n","564+709=\n","564+868=\n","565+101=\n","565+161=\n","565+244=\n","565+431=\n","565+519=\n","565+534=\n","565+643=\n","565+711=\n","565+888=\n","565+948=\n","566+29=\n","566+117=\n","566+305=\n","566+862=\n","566+903=\n","566+904=\n","567+8=\n","567+51=\n","567+57=\n","567+561=\n","567+630=\n","567+638=\n","567+917=\n","568+34=\n","568+208=\n","568+228=\n","568+235=\n","568+363=\n","568+403=\n","568+410=\n","568+424=\n","568+516=\n","568+600=\n","568+704=\n","568+785=\n","569+74=\n","569+182=\n","569+277=\n","569+394=\n","569+402=\n","569+531=\n","569+771=\n","570+1=\n","570+201=\n","570+398=\n","570+538=\n","570+752=\n","570+773=\n","571+31=\n","571+49=\n","571+214=\n","571+335=\n","571+367=\n","571+376=\n","571+505=\n","571+578=\n","571+704=\n","571+705=\n","571+730=\n","571+817=\n","571+823=\n","571+826=\n","571+887=\n","571+999=\n","572+99=\n","572+221=\n","572+367=\n","572+407=\n","572+627=\n","572+891=\n","572+927=\n","572+951=\n","572+990=\n","573+18=\n","573+49=\n","573+70=\n","573+81=\n","573+286=\n","573+351=\n","573+397=\n","573+528=\n","573+544=\n","573+746=\n","574+4=\n","574+146=\n","574+218=\n","574+361=\n","574+392=\n","574+549=\n","574+603=\n","574+647=\n","574+773=\n","575+127=\n","575+144=\n","575+160=\n","575+211=\n","575+259=\n","575+443=\n","575+594=\n","575+644=\n","575+676=\n","575+718=\n","575+811=\n","576+199=\n","576+216=\n","576+364=\n","576+403=\n","576+491=\n","576+493=\n","576+510=\n","576+730=\n","576+808=\n","576+905=\n","576+910=\n","577+201=\n","577+340=\n","577+465=\n","577+493=\n","577+636=\n","577+868=\n","577+944=\n","578+86=\n","578+144=\n","578+196=\n","578+223=\n","578+287=\n","578+371=\n","578+704=\n","578+758=\n","578+771=\n","578+773=\n","578+774=\n","578+775=\n","578+790=\n","578+834=\n","578+933=\n","578+987=\n","579+9=\n","579+18=\n","579+126=\n","579+146=\n","579+236=\n","579+244=\n","579+338=\n","579+440=\n","579+479=\n","579+529=\n","579+858=\n","579+970=\n","580+137=\n","580+276=\n","580+365=\n","580+576=\n","580+712=\n","580+721=\n","580+817=\n","580+888=\n","581+33=\n","581+119=\n","581+138=\n","581+165=\n","581+262=\n","581+320=\n","581+358=\n","581+384=\n","581+402=\n","581+404=\n","581+421=\n","581+430=\n","581+563=\n","581+624=\n","581+657=\n","581+682=\n","581+726=\n","581+811=\n","581+928=\n","582+30=\n","582+88=\n","582+119=\n","582+122=\n","582+144=\n","582+172=\n","582+211=\n","582+281=\n","582+436=\n","582+462=\n","582+720=\n","582+904=\n","583+257=\n","583+339=\n","583+446=\n","583+535=\n","583+746=\n","583+958=\n","583+969=\n","584+32=\n","584+68=\n","584+107=\n","584+393=\n","584+433=\n","584+466=\n","584+502=\n","584+629=\n","584+744=\n","584+984=\n","585+37=\n","585+458=\n","585+571=\n","585+647=\n","585+675=\n","585+697=\n","585+889=\n","585+939=\n","586+223=\n","586+249=\n","586+268=\n","586+380=\n","586+406=\n","586+438=\n","586+541=\n","586+633=\n","586+665=\n","586+667=\n","586+790=\n","586+817=\n","586+977=\n","587+76=\n","587+127=\n","587+445=\n","587+572=\n","587+736=\n","587+778=\n","587+790=\n","588+185=\n","588+318=\n","588+368=\n","588+437=\n","588+461=\n","588+535=\n","588+733=\n","588+769=\n","588+897=\n","588+915=\n","589+27=\n","589+244=\n","589+384=\n","589+525=\n","589+534=\n","589+589=\n","589+665=\n","589+733=\n","589+735=\n","589+806=\n","590+74=\n","590+108=\n","590+156=\n","590+167=\n","590+232=\n","590+413=\n","590+579=\n","590+785=\n","590+807=\n","590+889=\n","591+12=\n","591+57=\n","591+242=\n","591+363=\n","591+510=\n","591+528=\n","591+689=\n","591+728=\n","591+731=\n","591+899=\n","591+989=\n","592+0=\n","592+28=\n","592+40=\n","592+209=\n","592+391=\n","592+543=\n","592+561=\n","592+571=\n","592+658=\n","592+667=\n","592+894=\n","593+23=\n","593+76=\n","593+213=\n","593+467=\n","593+489=\n","593+606=\n","593+722=\n","593+752=\n","594+100=\n","594+165=\n","594+492=\n","594+523=\n","594+664=\n","594+960=\n","595+211=\n","595+223=\n","595+257=\n","595+459=\n","595+517=\n","595+801=\n","595+842=\n","596+63=\n","596+89=\n","596+242=\n","596+243=\n","596+273=\n","596+461=\n","596+586=\n","596+788=\n","596+911=\n","597+25=\n","597+37=\n","597+105=\n","597+187=\n","597+397=\n","597+543=\n","597+729=\n","597+771=\n","597+788=\n","597+795=\n","597+896=\n","598+26=\n","598+200=\n","598+207=\n","598+322=\n","598+554=\n","598+598=\n","598+628=\n","598+780=\n","598+821=\n","599+37=\n","599+92=\n","599+102=\n","599+118=\n","599+301=\n","599+341=\n","599+400=\n","599+412=\n","599+506=\n","599+572=\n","599+632=\n","600+20=\n","600+122=\n","600+201=\n","600+280=\n","600+362=\n","600+550=\n","600+769=\n","600+856=\n","600+903=\n","601+132=\n","601+300=\n","601+335=\n","601+694=\n","601+699=\n","601+863=\n","602+155=\n","602+437=\n","602+494=\n","602+540=\n","602+665=\n","602+732=\n","602+735=\n","603+60=\n","603+163=\n","603+268=\n","603+284=\n","603+359=\n","603+372=\n","603+504=\n","603+735=\n","603+776=\n","603+820=\n","603+891=\n","603+897=\n","603+905=\n","603+965=\n","604+97=\n","604+128=\n","604+159=\n","604+359=\n","604+385=\n","604+513=\n","604+622=\n","604+778=\n","604+793=\n","604+837=\n","604+852=\n","604+920=\n","605+28=\n","605+340=\n","605+356=\n","605+398=\n","605+510=\n","605+532=\n","605+575=\n","605+665=\n","605+809=\n","605+817=\n","605+822=\n","605+918=\n","606+110=\n","606+152=\n","606+518=\n","606+674=\n","607+113=\n","607+142=\n","607+164=\n","607+349=\n","607+515=\n","607+544=\n","607+603=\n","607+662=\n","607+839=\n","607+907=\n","607+998=\n","608+3=\n","608+82=\n","608+175=\n","608+248=\n","608+250=\n","608+362=\n","608+364=\n","608+547=\n","608+552=\n","608+557=\n","608+695=\n","608+869=\n","608+920=\n","609+73=\n","609+82=\n","609+106=\n","609+179=\n","609+203=\n","609+444=\n","609+536=\n","609+687=\n","609+834=\n","609+878=\n","609+898=\n","609+917=\n","609+943=\n","609+950=\n","610+241=\n","610+322=\n","610+334=\n","610+543=\n","610+730=\n","611+18=\n","611+106=\n","611+148=\n","611+248=\n","611+251=\n","611+253=\n","611+288=\n","611+317=\n","611+403=\n","611+612=\n","611+771=\n","611+892=\n","612+217=\n","612+311=\n","612+542=\n","612+580=\n","612+848=\n","612+875=\n","612+921=\n","613+21=\n","613+39=\n","613+263=\n","613+271=\n","613+291=\n","613+307=\n","613+346=\n","613+370=\n","613+449=\n","613+507=\n","613+770=\n","613+805=\n","614+60=\n","614+172=\n","614+335=\n","614+370=\n","614+514=\n","614+606=\n","614+741=\n","614+796=\n","614+842=\n","614+850=\n","614+865=\n","614+892=\n","614+902=\n","615+156=\n","615+322=\n","615+592=\n","615+731=\n","615+960=\n","616+146=\n","616+716=\n","616+753=\n","616+774=\n","617+126=\n","617+332=\n","617+356=\n","617+388=\n","617+402=\n","617+479=\n","617+594=\n","617+671=\n","617+684=\n","617+761=\n","617+802=\n","618+50=\n","618+122=\n","618+127=\n","618+244=\n","618+288=\n","618+463=\n","618+668=\n","618+815=\n","618+876=\n","618+907=\n","619+26=\n","619+170=\n","619+507=\n","619+734=\n","619+807=\n","619+872=\n","619+966=\n","620+44=\n","620+238=\n","620+380=\n","620+546=\n","620+559=\n","620+991=\n","621+98=\n","621+165=\n","621+230=\n","621+330=\n","621+366=\n","621+370=\n","621+515=\n","621+843=\n","622+2=\n","622+28=\n","622+67=\n","622+70=\n","622+99=\n","622+107=\n","622+136=\n","622+154=\n","622+206=\n","622+239=\n","622+319=\n","622+391=\n","622+588=\n","622+595=\n","622+685=\n","622+765=\n","622+843=\n","622+864=\n","622+952=\n","623+9=\n","623+91=\n","623+205=\n","623+308=\n","623+377=\n","623+449=\n","623+475=\n","623+557=\n","623+572=\n","623+600=\n","623+608=\n","623+685=\n","623+837=\n","623+873=\n","623+875=\n","623+889=\n","624+114=\n","624+547=\n","624+854=\n","625+122=\n","625+137=\n","625+232=\n","625+529=\n","625+546=\n","625+636=\n","625+697=\n","625+764=\n","625+848=\n","625+875=\n","625+950=\n","625+993=\n","626+92=\n","626+183=\n","626+254=\n","626+303=\n","626+489=\n","626+547=\n","626+759=\n","626+850=\n","626+979=\n","627+28=\n","627+48=\n","627+158=\n","627+171=\n","627+238=\n","627+546=\n","627+558=\n","627+729=\n","627+817=\n","627+826=\n","627+911=\n","627+921=\n","628+23=\n","628+83=\n","628+230=\n","628+301=\n","628+305=\n","628+313=\n","628+318=\n","628+374=\n","628+396=\n","628+453=\n","628+640=\n","628+929=\n","628+950=\n","629+172=\n","629+294=\n","629+295=\n","629+350=\n","629+363=\n","629+474=\n","629+499=\n","629+511=\n","629+527=\n","629+703=\n","630+95=\n","630+290=\n","630+370=\n","630+408=\n","630+840=\n","630+895=\n","631+56=\n","631+64=\n","631+79=\n","631+151=\n","631+160=\n","631+265=\n","631+367=\n","631+479=\n","631+515=\n","631+675=\n","631+744=\n","631+767=\n","631+792=\n","631+889=\n","632+158=\n","632+318=\n","632+567=\n","632+581=\n","632+589=\n","632+598=\n","632+709=\n","632+738=\n","632+847=\n","632+848=\n","632+961=\n","632+974=\n","632+978=\n","632+984=\n","633+108=\n","633+187=\n","633+244=\n","633+395=\n","633+427=\n","633+540=\n","633+548=\n","633+616=\n","633+652=\n","633+674=\n","633+719=\n","633+742=\n","634+19=\n","634+73=\n","634+89=\n","634+114=\n","634+195=\n","634+217=\n","634+220=\n","634+221=\n","634+224=\n","634+276=\n","634+447=\n","634+511=\n","634+567=\n","634+587=\n","634+994=\n","635+18=\n","635+197=\n","635+439=\n","635+449=\n","635+606=\n","635+869=\n","636+40=\n","636+70=\n","636+71=\n","636+555=\n","636+645=\n","636+731=\n","636+742=\n","636+789=\n","637+121=\n","637+165=\n","637+327=\n","637+399=\n","637+588=\n","637+662=\n","637+696=\n","637+910=\n","638+27=\n","638+37=\n","638+133=\n","638+153=\n","638+165=\n","638+188=\n","638+200=\n","638+237=\n","638+254=\n","638+354=\n","638+412=\n","638+562=\n","638+609=\n","638+749=\n","638+884=\n","639+174=\n","639+262=\n","639+415=\n","639+571=\n","639+595=\n","639+638=\n","639+665=\n","639+725=\n","639+793=\n","640+57=\n","640+78=\n","640+102=\n","640+153=\n","640+390=\n","640+401=\n","640+632=\n","640+635=\n","640+671=\n","640+710=\n","641+77=\n","641+120=\n","641+434=\n","641+794=\n","641+827=\n","641+983=\n","642+9=\n","642+57=\n","642+97=\n","642+110=\n","642+141=\n","642+159=\n","642+253=\n","642+452=\n","642+497=\n","643+43=\n","643+59=\n","643+323=\n","643+682=\n","643+732=\n","643+769=\n","643+792=\n","643+873=\n","644+222=\n","644+328=\n","644+421=\n","644+429=\n","644+470=\n","644+491=\n","644+596=\n","644+600=\n","644+659=\n","644+693=\n","644+767=\n","644+783=\n","645+14=\n","645+65=\n","645+117=\n","645+165=\n","645+213=\n","645+274=\n","645+400=\n","645+651=\n","645+668=\n","645+768=\n","645+821=\n","645+928=\n","645+931=\n","645+954=\n","646+15=\n","646+88=\n","646+131=\n","646+170=\n","646+218=\n","646+304=\n","646+446=\n","646+470=\n","646+557=\n","646+563=\n","646+564=\n","646+750=\n","646+939=\n","646+945=\n","647+33=\n","647+132=\n","647+148=\n","647+196=\n","647+537=\n","647+558=\n","647+611=\n","647+724=\n","647+780=\n","647+874=\n","647+905=\n","647+978=\n","647+998=\n","648+114=\n","648+156=\n","648+223=\n","648+235=\n","648+244=\n","648+478=\n","648+518=\n","648+803=\n","648+898=\n","648+906=\n","648+915=\n","648+920=\n","648+966=\n","649+61=\n","649+106=\n","649+167=\n","649+168=\n","649+223=\n","649+434=\n","649+466=\n","649+638=\n","649+641=\n","649+756=\n","649+778=\n","650+25=\n","650+40=\n","650+62=\n","650+98=\n","650+102=\n","650+276=\n","650+278=\n","650+289=\n","650+417=\n","650+433=\n","650+558=\n","650+767=\n","650+897=\n","651+38=\n","651+358=\n","651+426=\n","651+491=\n","651+498=\n","651+698=\n","651+741=\n","651+758=\n","651+966=\n","651+975=\n","652+81=\n","652+118=\n","652+124=\n","652+249=\n","652+520=\n","652+971=\n","653+69=\n","653+154=\n","653+208=\n","653+260=\n","653+344=\n","653+370=\n","653+403=\n","653+433=\n","653+443=\n","653+534=\n","653+587=\n","653+597=\n","653+638=\n","653+688=\n","653+742=\n","653+841=\n","653+848=\n","653+938=\n","654+26=\n","654+39=\n","654+203=\n","654+493=\n","654+523=\n","654+574=\n","654+575=\n","654+901=\n","654+939=\n","655+171=\n","655+279=\n","655+395=\n","655+400=\n","655+521=\n","655+544=\n","655+581=\n","655+688=\n","655+734=\n","655+776=\n","655+798=\n","655+939=\n","655+952=\n","656+22=\n","656+189=\n","656+207=\n","656+243=\n","656+338=\n","656+354=\n","656+510=\n","656+516=\n","656+636=\n","656+761=\n","656+823=\n","656+959=\n","656+991=\n","657+239=\n","657+248=\n","657+477=\n","657+541=\n","657+597=\n","657+606=\n","657+682=\n","657+716=\n","658+78=\n","658+199=\n","658+407=\n","658+484=\n","658+774=\n","658+844=\n","658+857=\n","658+867=\n","658+882=\n","658+932=\n","659+134=\n","659+189=\n","659+263=\n","659+269=\n","659+275=\n","659+284=\n","659+295=\n","659+355=\n","659+365=\n","659+410=\n","659+492=\n","659+672=\n","659+765=\n","659+787=\n","659+888=\n","659+967=\n","660+34=\n","660+150=\n","660+271=\n","660+283=\n","660+476=\n","660+752=\n","660+874=\n","661+0=\n","661+88=\n","661+260=\n","661+446=\n","661+755=\n","661+759=\n","661+980=\n","661+989=\n","662+403=\n","662+422=\n","662+509=\n","662+637=\n","662+639=\n","662+727=\n","662+740=\n","662+814=\n","662+936=\n","662+960=\n","663+66=\n","663+72=\n","663+99=\n","663+178=\n","663+218=\n","663+228=\n","663+257=\n","663+295=\n","663+351=\n","663+372=\n","663+620=\n","663+633=\n","663+703=\n","663+737=\n","663+810=\n","663+825=\n","663+851=\n","664+85=\n","664+118=\n","664+135=\n","664+207=\n","664+264=\n","664+487=\n","665+78=\n","665+89=\n","665+118=\n","665+214=\n","665+352=\n","665+394=\n","665+398=\n","665+470=\n","665+550=\n","665+596=\n","665+737=\n","665+846=\n","665+906=\n","666+455=\n","666+579=\n","666+611=\n","666+817=\n","666+836=\n","666+842=\n","666+873=\n","666+903=\n","667+69=\n","667+232=\n","667+307=\n","667+361=\n","667+443=\n","667+676=\n","667+702=\n","667+854=\n","667+882=\n","667+917=\n","667+933=\n","667+945=\n","667+952=\n","668+138=\n","668+159=\n","668+174=\n","668+218=\n","668+262=\n","668+341=\n","668+375=\n","668+397=\n","668+511=\n","668+571=\n","668+661=\n","668+806=\n","668+898=\n","668+917=\n","668+949=\n","669+265=\n","669+301=\n","669+326=\n","669+331=\n","669+481=\n","669+622=\n","669+670=\n","669+753=\n","669+850=\n","670+52=\n","670+393=\n","670+477=\n","670+598=\n","670+739=\n","670+774=\n","670+856=\n","670+972=\n","671+28=\n","671+61=\n","671+677=\n","671+829=\n","671+917=\n","671+919=\n","671+975=\n","672+70=\n","672+148=\n","672+210=\n","672+227=\n","672+280=\n","672+413=\n","672+460=\n","672+474=\n","672+495=\n","672+878=\n","672+889=\n","672+990=\n","673+20=\n","673+71=\n","673+79=\n","673+115=\n","673+153=\n","673+289=\n","673+298=\n","673+332=\n","673+369=\n","673+483=\n","673+495=\n","673+540=\n","673+575=\n","673+704=\n","673+746=\n","673+793=\n","673+829=\n","673+837=\n","673+863=\n","674+440=\n","674+593=\n","674+616=\n","674+668=\n","674+773=\n","674+881=\n","674+905=\n","674+992=\n","675+48=\n","675+116=\n","675+220=\n","675+341=\n","675+467=\n","675+611=\n","675+879=\n","675+932=\n","675+992=\n","676+120=\n","676+132=\n","676+352=\n","676+747=\n","676+825=\n","676+827=\n","676+883=\n","676+996=\n","677+1=\n","677+92=\n","677+307=\n","677+365=\n","677+430=\n","677+605=\n","677+726=\n","677+817=\n","677+945=\n","677+946=\n","677+985=\n","678+384=\n","678+453=\n","678+575=\n","678+579=\n","678+665=\n","678+688=\n","678+958=\n","679+72=\n","679+386=\n","679+695=\n","679+880=\n","679+937=\n","679+945=\n","680+1=\n","680+164=\n","680+198=\n","680+317=\n","680+325=\n","680+410=\n","680+434=\n","680+451=\n","680+697=\n","680+819=\n","680+872=\n","681+58=\n","681+104=\n","681+153=\n","681+477=\n","681+498=\n","681+547=\n","681+588=\n","681+598=\n","681+643=\n","681+795=\n","681+865=\n","681+932=\n","682+117=\n","682+138=\n","682+274=\n","682+407=\n","682+476=\n","682+479=\n","682+593=\n","682+767=\n","682+774=\n","682+989=\n","683+13=\n","683+33=\n","683+178=\n","683+324=\n","683+553=\n","683+606=\n","683+747=\n","683+785=\n","684+90=\n","684+113=\n","684+294=\n","684+301=\n","684+311=\n","684+327=\n","684+480=\n","684+852=\n","684+883=\n","685+161=\n","685+170=\n","685+176=\n","685+346=\n","685+404=\n","685+588=\n","685+718=\n","685+795=\n","685+869=\n","685+961=\n","686+101=\n","686+132=\n","686+171=\n","686+337=\n","686+428=\n","686+493=\n","686+524=\n","686+648=\n","686+665=\n","686+722=\n","686+808=\n","687+208=\n","687+455=\n","687+560=\n","687+652=\n","687+700=\n","687+778=\n","687+807=\n","687+842=\n","687+844=\n","688+66=\n","688+216=\n","688+444=\n","688+577=\n","688+828=\n","688+976=\n","689+54=\n","689+97=\n","689+290=\n","689+516=\n","689+530=\n","689+653=\n","689+676=\n","689+679=\n","689+694=\n","689+903=\n","689+975=\n","690+115=\n","690+250=\n","690+254=\n","690+366=\n","690+390=\n","690+606=\n","690+720=\n","690+812=\n","690+814=\n","690+894=\n","690+974=\n","691+182=\n","691+272=\n","691+342=\n","691+347=\n","691+408=\n","691+472=\n","691+635=\n","691+639=\n","691+648=\n","691+720=\n","691+753=\n","691+813=\n","691+814=\n","691+835=\n","691+839=\n","691+875=\n","691+922=\n","691+952=\n","692+43=\n","692+180=\n","692+379=\n","692+413=\n","692+526=\n","692+676=\n","692+711=\n","692+836=\n","692+902=\n","692+942=\n","692+952=\n","693+212=\n","693+242=\n","693+441=\n","693+492=\n","693+664=\n","693+762=\n","693+854=\n","693+904=\n","693+920=\n","693+950=\n","694+244=\n","694+350=\n","694+494=\n","694+510=\n","694+548=\n","694+620=\n","694+663=\n","694+664=\n","694+688=\n","694+731=\n","694+748=\n","694+786=\n","694+981=\n","695+379=\n","695+876=\n","695+918=\n","695+976=\n","695+984=\n","696+2=\n","696+159=\n","696+418=\n","696+647=\n","696+980=\n","697+64=\n","697+98=\n","697+175=\n","697+227=\n","697+228=\n","697+445=\n","697+516=\n","697+551=\n","697+557=\n","697+757=\n","697+770=\n","697+774=\n","697+850=\n","697+872=\n","697+940=\n","698+108=\n","698+123=\n","698+128=\n","698+143=\n","698+179=\n","698+567=\n","698+645=\n","698+672=\n","699+86=\n","699+136=\n","699+148=\n","699+370=\n","699+466=\n","699+500=\n","699+515=\n","699+582=\n","699+643=\n","699+652=\n","699+704=\n","699+958=\n","699+972=\n","700+48=\n","700+316=\n","700+392=\n","700+397=\n","700+447=\n","700+537=\n","700+700=\n","700+894=\n","700+952=\n","701+14=\n","701+22=\n","701+88=\n","701+107=\n","701+186=\n","701+216=\n","701+375=\n","701+842=\n","701+866=\n","702+149=\n","702+176=\n","702+401=\n","702+688=\n","702+816=\n","702+876=\n","703+87=\n","703+118=\n","703+291=\n","703+465=\n","703+539=\n","703+564=\n","703+659=\n","703+660=\n","703+763=\n","703+795=\n","703+797=\n","703+828=\n","703+853=\n","704+278=\n","704+386=\n","704+533=\n","704+585=\n","704+692=\n","704+785=\n","704+813=\n","704+903=\n","704+936=\n","704+996=\n","704+997=\n","705+40=\n","705+110=\n","705+227=\n","705+327=\n","705+354=\n","705+409=\n","705+446=\n","705+540=\n","705+656=\n","705+659=\n","705+841=\n","705+902=\n","705+903=\n","706+34=\n","706+43=\n","706+161=\n","706+269=\n","706+271=\n","706+274=\n","706+351=\n","706+514=\n","706+579=\n","706+597=\n","706+744=\n","706+847=\n","706+850=\n","706+946=\n","707+206=\n","707+215=\n","707+272=\n","707+312=\n","707+449=\n","707+606=\n","707+641=\n","707+666=\n","707+715=\n","707+815=\n","707+867=\n","708+99=\n","708+179=\n","708+222=\n","708+529=\n","708+538=\n","709+41=\n","709+43=\n","709+100=\n","709+240=\n","709+246=\n","709+347=\n","709+419=\n","710+70=\n","710+363=\n","710+540=\n","710+585=\n","710+619=\n","710+655=\n","710+717=\n","710+792=\n","710+816=\n","710+982=\n","711+4=\n","711+60=\n","711+66=\n","711+94=\n","711+124=\n","711+134=\n","711+186=\n","711+278=\n","711+290=\n","711+362=\n","711+378=\n","711+413=\n","711+427=\n","711+435=\n","711+494=\n","711+515=\n","711+877=\n","711+927=\n","712+385=\n","712+426=\n","712+465=\n","712+488=\n","712+590=\n","712+621=\n","712+748=\n","712+785=\n","712+801=\n","712+806=\n","712+819=\n","713+24=\n","713+30=\n","713+66=\n","713+125=\n","713+132=\n","713+143=\n","713+292=\n","713+311=\n","713+474=\n","713+581=\n","713+669=\n","713+935=\n","713+944=\n","714+262=\n","714+386=\n","714+388=\n","714+472=\n","714+490=\n","714+621=\n","714+628=\n","714+763=\n","714+893=\n","715+16=\n","715+98=\n","715+157=\n","715+185=\n","715+344=\n","715+518=\n","715+723=\n","715+753=\n","716+19=\n","716+31=\n","716+235=\n","716+296=\n","716+378=\n","716+422=\n","716+661=\n","716+697=\n","716+718=\n","716+736=\n","716+763=\n","716+941=\n","717+141=\n","717+304=\n","717+351=\n","717+787=\n","717+811=\n","717+897=\n","717+919=\n","717+979=\n","718+196=\n","718+227=\n","718+286=\n","718+369=\n","718+382=\n","718+507=\n","718+848=\n","719+117=\n","719+166=\n","719+171=\n","719+252=\n","719+377=\n","719+400=\n","719+494=\n","719+538=\n","719+717=\n","719+983=\n","720+37=\n","720+38=\n","720+216=\n","720+228=\n","720+249=\n","720+489=\n","720+581=\n","720+674=\n","720+705=\n","720+711=\n","720+716=\n","720+762=\n","721+1=\n","721+42=\n","721+45=\n","721+94=\n","721+191=\n","721+377=\n","721+456=\n","721+467=\n","721+501=\n","721+611=\n","721+886=\n","721+959=\n","722+92=\n","722+94=\n","722+416=\n","722+429=\n","722+514=\n","722+520=\n","722+574=\n","722+617=\n","722+699=\n","722+911=\n","722+935=\n","722+978=\n","722+986=\n","723+194=\n","723+361=\n","723+540=\n","723+654=\n","723+659=\n","723+777=\n","723+798=\n","723+925=\n","723+971=\n","724+157=\n","724+182=\n","724+281=\n","724+307=\n","724+357=\n","724+381=\n","724+412=\n","724+433=\n","724+675=\n","724+839=\n","725+0=\n","725+320=\n","725+448=\n","725+450=\n","725+479=\n","725+565=\n","725+576=\n","726+212=\n","726+289=\n","726+418=\n","726+484=\n","726+691=\n","726+933=\n","727+81=\n","727+159=\n","727+221=\n","727+245=\n","727+394=\n","727+520=\n","727+546=\n","727+579=\n","727+662=\n","727+805=\n","727+890=\n","727+942=\n","728+173=\n","728+180=\n","728+347=\n","728+388=\n","728+458=\n","728+553=\n","728+984=\n","729+62=\n","729+209=\n","729+241=\n","729+308=\n","729+377=\n","729+400=\n","729+422=\n","729+579=\n","729+721=\n","729+891=\n","729+902=\n","729+938=\n","730+19=\n","730+155=\n","730+276=\n","730+378=\n","730+477=\n","730+540=\n","730+643=\n","730+660=\n","730+757=\n","730+820=\n","730+823=\n","730+995=\n","730+996=\n","731+16=\n","731+62=\n","731+187=\n","731+211=\n","731+479=\n","731+515=\n","731+528=\n","731+580=\n","731+597=\n","731+705=\n","731+744=\n","731+745=\n","731+871=\n","732+9=\n","732+164=\n","732+178=\n","732+272=\n","732+338=\n","732+387=\n","732+411=\n","732+726=\n","733+25=\n","733+400=\n","733+550=\n","733+611=\n","733+707=\n","733+742=\n","733+744=\n","733+815=\n","733+850=\n","733+960=\n","733+981=\n","734+67=\n","734+70=\n","734+181=\n","734+207=\n","734+377=\n","734+397=\n","734+442=\n","734+533=\n","734+554=\n","734+569=\n","734+665=\n","734+668=\n","734+725=\n","734+762=\n","734+898=\n","734+903=\n","734+935=\n","735+69=\n","735+92=\n","735+282=\n","735+441=\n","735+502=\n","735+751=\n","735+897=\n","735+938=\n","736+365=\n","736+430=\n","736+595=\n","736+771=\n","736+967=\n","737+116=\n","737+194=\n","737+288=\n","737+352=\n","737+498=\n","737+609=\n","737+833=\n","738+104=\n","738+170=\n","738+230=\n","738+317=\n","738+389=\n","738+736=\n","738+980=\n","739+127=\n","739+223=\n","739+303=\n","739+326=\n","739+337=\n","739+439=\n","739+466=\n","739+754=\n","739+874=\n","739+987=\n","740+19=\n","740+255=\n","740+328=\n","740+403=\n","740+465=\n","740+634=\n","740+656=\n","740+672=\n","740+952=\n","740+965=\n","741+427=\n","741+436=\n","741+472=\n","741+613=\n","741+621=\n","741+703=\n","741+763=\n","741+864=\n","741+941=\n","742+66=\n","742+128=\n","742+183=\n","742+253=\n","742+357=\n","742+589=\n","742+662=\n","742+686=\n","742+945=\n","743+62=\n","743+88=\n","743+328=\n","743+542=\n","743+756=\n","744+17=\n","744+76=\n","744+281=\n","744+326=\n","744+337=\n","744+650=\n","744+656=\n","744+689=\n","744+731=\n","744+744=\n","744+836=\n","744+929=\n","745+64=\n","745+169=\n","745+359=\n","745+478=\n","745+606=\n","745+627=\n","745+693=\n","745+856=\n","745+895=\n","746+6=\n","746+181=\n","746+192=\n","746+245=\n","746+376=\n","746+380=\n","746+476=\n","746+479=\n","746+493=\n","746+537=\n","746+601=\n","746+609=\n","746+643=\n","746+648=\n","746+799=\n","746+983=\n","747+0=\n","747+64=\n","747+136=\n","747+253=\n","747+406=\n","747+557=\n","747+592=\n","747+644=\n","747+890=\n","747+943=\n","748+91=\n","748+168=\n","748+570=\n","748+676=\n","748+968=\n","749+246=\n","749+383=\n","749+388=\n","749+490=\n","749+520=\n","749+710=\n","749+816=\n","749+881=\n","749+920=\n","749+944=\n","749+990=\n","750+175=\n","750+500=\n","750+583=\n","750+601=\n","750+622=\n","750+935=\n","750+952=\n","751+18=\n","751+267=\n","751+431=\n","751+527=\n","751+580=\n","751+676=\n","751+898=\n","751+911=\n","751+923=\n","752+187=\n","752+210=\n","752+244=\n","752+375=\n","752+388=\n","752+444=\n","752+530=\n","752+682=\n","752+718=\n","752+828=\n","752+842=\n","753+62=\n","753+118=\n","753+143=\n","753+169=\n","753+187=\n","753+224=\n","753+320=\n","753+607=\n","753+667=\n","753+891=\n","754+13=\n","754+178=\n","754+212=\n","754+546=\n","754+585=\n","754+800=\n","754+876=\n","754+999=\n","755+50=\n","755+274=\n","755+282=\n","755+299=\n","755+358=\n","755+368=\n","755+667=\n","755+674=\n","756+11=\n","756+13=\n","756+39=\n","756+153=\n","756+366=\n","756+687=\n","756+832=\n","756+959=\n","756+995=\n","757+103=\n","757+457=\n","757+459=\n","757+508=\n","757+528=\n","757+861=\n","757+897=\n","757+932=\n","757+947=\n","758+69=\n","758+125=\n","758+176=\n","758+251=\n","758+269=\n","758+289=\n","758+307=\n","758+442=\n","758+478=\n","758+629=\n","758+653=\n","758+870=\n","758+930=\n","759+98=\n","759+136=\n","759+160=\n","759+554=\n","759+693=\n","759+768=\n","759+776=\n","759+922=\n","759+958=\n","759+975=\n","760+0=\n","760+150=\n","760+226=\n","760+311=\n","760+665=\n","760+717=\n","760+787=\n","760+828=\n","760+870=\n","760+963=\n","761+151=\n","761+180=\n","761+190=\n","761+199=\n","761+314=\n","761+332=\n","761+366=\n","761+375=\n","761+648=\n","761+763=\n","761+894=\n","761+914=\n","762+262=\n","762+277=\n","762+467=\n","762+576=\n","762+718=\n","762+887=\n","762+931=\n","763+479=\n","763+784=\n","763+794=\n","763+857=\n","763+909=\n","764+21=\n","764+111=\n","764+166=\n","764+178=\n","764+581=\n","764+644=\n","764+750=\n","764+834=\n","764+874=\n","764+929=\n","765+64=\n","765+393=\n","765+434=\n","765+588=\n","765+651=\n","765+653=\n","765+869=\n","765+977=\n","766+133=\n","766+777=\n","767+36=\n","767+420=\n","767+646=\n","767+918=\n","768+29=\n","768+113=\n","768+429=\n","768+435=\n","768+560=\n","768+578=\n","768+757=\n","768+765=\n","768+819=\n","769+454=\n","769+459=\n","769+664=\n","769+714=\n","769+716=\n","769+727=\n","769+915=\n","769+986=\n","770+5=\n","770+37=\n","770+83=\n","770+108=\n","770+337=\n","770+431=\n","770+478=\n","770+528=\n","770+818=\n","770+821=\n","770+901=\n","770+905=\n","770+934=\n","770+973=\n","771+107=\n","771+149=\n","771+206=\n","771+377=\n","771+417=\n","771+491=\n","771+573=\n","771+592=\n","771+599=\n","771+711=\n","771+821=\n","771+827=\n","771+837=\n","771+839=\n","771+843=\n","772+5=\n","772+56=\n","772+283=\n","772+809=\n","772+860=\n","773+78=\n","773+142=\n","773+308=\n","773+384=\n","773+392=\n","773+739=\n","773+827=\n","773+967=\n","773+988=\n","774+100=\n","774+410=\n","774+474=\n","774+620=\n","774+674=\n","774+689=\n","774+728=\n","774+938=\n","774+951=\n","774+989=\n","775+298=\n","776+64=\n","776+169=\n","776+260=\n","776+387=\n","776+433=\n","776+456=\n","776+666=\n","776+800=\n","776+823=\n","776+986=\n","777+101=\n","777+120=\n","777+283=\n","777+584=\n","777+589=\n","777+640=\n","778+35=\n","778+112=\n","778+508=\n","778+531=\n","778+839=\n","778+967=\n","779+65=\n","779+365=\n","779+500=\n","779+628=\n","779+761=\n","779+818=\n","780+60=\n","780+139=\n","780+337=\n","780+362=\n","780+438=\n","780+524=\n","780+557=\n","780+604=\n","780+711=\n","780+733=\n","780+873=\n","780+971=\n","780+993=\n","781+131=\n","781+308=\n","781+312=\n","781+361=\n","781+362=\n","781+541=\n","781+658=\n","781+963=\n","782+132=\n","782+172=\n","782+259=\n","782+276=\n","782+399=\n","782+544=\n","782+760=\n","782+873=\n","782+919=\n","782+989=\n","783+60=\n","783+240=\n","783+439=\n","783+508=\n","783+511=\n","783+589=\n","783+594=\n","783+856=\n","783+961=\n","784+94=\n","784+115=\n","784+147=\n","784+161=\n","784+530=\n","784+751=\n","784+766=\n","784+961=\n","785+211=\n","785+326=\n","785+381=\n","785+631=\n","785+856=\n","786+160=\n","786+258=\n","786+280=\n","786+292=\n","786+335=\n","786+444=\n","786+450=\n","786+585=\n","786+637=\n","786+778=\n","787+72=\n","787+74=\n","787+173=\n","787+222=\n","787+267=\n","787+312=\n","787+513=\n","787+622=\n","787+636=\n","787+679=\n","787+871=\n","788+118=\n","788+163=\n","788+194=\n","788+274=\n","788+283=\n","788+324=\n","788+367=\n","788+488=\n","788+801=\n","788+835=\n","788+902=\n","789+4=\n","789+44=\n","789+414=\n","789+431=\n","789+496=\n","789+499=\n","789+659=\n","789+840=\n","790+142=\n","790+165=\n","790+183=\n","790+199=\n","790+262=\n","790+469=\n","790+507=\n","790+648=\n","790+715=\n","790+724=\n","790+903=\n","790+950=\n","791+587=\n","791+720=\n","791+737=\n","791+768=\n","791+850=\n","791+902=\n","791+926=\n","791+980=\n","792+114=\n","792+231=\n","792+298=\n","792+333=\n","792+335=\n","792+449=\n","792+648=\n","792+664=\n","792+898=\n","792+921=\n","793+60=\n","793+107=\n","793+129=\n","793+188=\n","793+704=\n","793+762=\n","793+817=\n","793+866=\n","793+867=\n","793+941=\n","794+73=\n","794+151=\n","794+164=\n","794+200=\n","794+525=\n","794+582=\n","794+702=\n","794+748=\n","794+924=\n","794+938=\n","795+258=\n","795+365=\n","795+394=\n","795+442=\n","795+475=\n","795+638=\n","795+652=\n","795+727=\n","796+5=\n","796+142=\n","796+286=\n","796+310=\n","796+392=\n","796+465=\n","796+525=\n","796+576=\n","796+744=\n","796+751=\n","796+951=\n","796+989=\n","797+41=\n","797+47=\n","797+67=\n","797+154=\n","797+290=\n","797+299=\n","797+429=\n","797+867=\n","798+326=\n","798+399=\n","798+631=\n","798+686=\n","799+86=\n","799+104=\n","799+113=\n","799+487=\n","799+722=\n","799+930=\n","799+936=\n","799+999=\n","800+199=\n","800+551=\n","800+784=\n","800+914=\n","801+60=\n","801+105=\n","801+111=\n","801+372=\n","801+427=\n","801+438=\n","801+722=\n","801+807=\n","801+927=\n","802+54=\n","802+83=\n","802+118=\n","802+163=\n","802+203=\n","802+219=\n","802+235=\n","802+245=\n","802+340=\n","802+405=\n","802+624=\n","802+635=\n","802+701=\n","802+716=\n","802+893=\n","803+17=\n","803+25=\n","803+52=\n","803+222=\n","803+226=\n","803+250=\n","803+258=\n","803+285=\n","803+371=\n","803+559=\n","803+580=\n","803+599=\n","803+610=\n","803+794=\n","803+870=\n","803+884=\n","803+885=\n","804+10=\n","804+49=\n","804+771=\n","804+837=\n","804+844=\n","804+852=\n","804+930=\n","804+945=\n","804+989=\n","805+28=\n","805+62=\n","805+158=\n","805+166=\n","805+297=\n","805+301=\n","805+320=\n","805+335=\n","805+391=\n","805+433=\n","805+475=\n","805+585=\n","805+591=\n","805+639=\n","805+816=\n","806+11=\n","806+35=\n","806+135=\n","806+144=\n","806+333=\n","806+349=\n","806+354=\n","806+484=\n","806+543=\n","806+563=\n","806+656=\n","806+730=\n","806+850=\n","807+145=\n","807+331=\n","807+354=\n","807+389=\n","807+410=\n","807+447=\n","807+478=\n","807+538=\n","807+574=\n","807+578=\n","807+788=\n","807+877=\n","807+904=\n","808+197=\n","808+204=\n","808+226=\n","808+257=\n","808+401=\n","808+416=\n","808+478=\n","808+503=\n","808+534=\n","808+739=\n","808+789=\n","808+911=\n","809+47=\n","809+231=\n","809+288=\n","809+299=\n","809+394=\n","809+564=\n","809+582=\n","809+684=\n","809+759=\n","809+882=\n","809+890=\n","810+74=\n","810+168=\n","810+185=\n","810+199=\n","810+200=\n","810+204=\n","810+215=\n","810+288=\n","810+303=\n","810+330=\n","810+640=\n","811+195=\n","811+205=\n","811+265=\n","811+458=\n","811+501=\n","811+816=\n","811+880=\n","812+175=\n","812+192=\n","812+213=\n","812+226=\n","812+326=\n","812+452=\n","812+479=\n","812+654=\n","812+887=\n","812+970=\n","813+37=\n","813+215=\n","813+342=\n","813+439=\n","813+658=\n","813+702=\n","813+708=\n","813+733=\n","813+763=\n","813+800=\n","813+904=\n","813+976=\n","814+103=\n","814+113=\n","814+185=\n","814+386=\n","814+791=\n","814+800=\n","814+970=\n","815+100=\n","815+216=\n","815+220=\n","815+222=\n","815+240=\n","815+290=\n","815+328=\n","815+358=\n","815+385=\n","815+513=\n","815+602=\n","815+639=\n","815+728=\n","815+755=\n","815+821=\n","815+862=\n","816+10=\n","816+35=\n","816+228=\n","816+292=\n","816+351=\n","816+605=\n","816+650=\n","816+734=\n","816+946=\n","816+947=\n","817+106=\n","817+146=\n","817+332=\n","817+383=\n","817+443=\n","817+463=\n","817+626=\n","817+653=\n","817+717=\n","817+760=\n","817+797=\n","818+68=\n","818+119=\n","818+122=\n","818+142=\n","818+171=\n","818+357=\n","818+393=\n","818+458=\n","818+612=\n","818+837=\n","818+880=\n","818+895=\n","818+950=\n","818+969=\n","818+996=\n","819+111=\n","819+201=\n","819+307=\n","819+351=\n","819+595=\n","819+610=\n","819+615=\n","819+679=\n","819+686=\n","819+774=\n","819+815=\n","820+8=\n","820+206=\n","820+256=\n","820+329=\n","820+360=\n","820+546=\n","820+551=\n","820+722=\n","820+733=\n","820+755=\n","820+763=\n","820+774=\n","820+846=\n","820+890=\n","820+905=\n","821+239=\n","821+428=\n","821+528=\n","821+561=\n","821+606=\n","821+703=\n","821+782=\n","821+955=\n","821+963=\n","821+999=\n","822+12=\n","822+98=\n","822+155=\n","822+381=\n","822+382=\n","822+420=\n","822+431=\n","822+466=\n","822+676=\n","822+718=\n","822+754=\n","822+795=\n","823+97=\n","823+280=\n","823+527=\n","823+606=\n","823+626=\n","823+808=\n","823+891=\n","823+923=\n","823+939=\n","823+965=\n","824+97=\n","824+115=\n","824+139=\n","824+162=\n","824+267=\n","824+411=\n","824+533=\n","824+573=\n","824+613=\n","824+802=\n","824+899=\n","825+18=\n","825+67=\n","825+292=\n","825+327=\n","825+481=\n","825+505=\n","825+561=\n","825+652=\n","825+787=\n","825+914=\n","825+928=\n","825+930=\n","826+63=\n","826+318=\n","826+354=\n","826+385=\n","826+521=\n","826+527=\n","826+761=\n","826+845=\n","826+877=\n","826+911=\n","826+996=\n","827+141=\n","827+170=\n","827+290=\n","827+347=\n","827+431=\n","827+539=\n","827+632=\n","827+638=\n","827+736=\n","828+11=\n","828+24=\n","828+47=\n","828+92=\n","828+103=\n","828+163=\n","828+190=\n","828+214=\n","828+394=\n","828+412=\n","828+740=\n","828+812=\n","828+935=\n","828+946=\n","828+974=\n","828+998=\n","829+30=\n","829+50=\n","829+193=\n","829+225=\n","829+247=\n","829+417=\n","829+560=\n","829+744=\n","829+753=\n","829+762=\n","829+858=\n","829+860=\n","829+925=\n","829+993=\n","830+74=\n","830+75=\n","830+268=\n","830+452=\n","830+453=\n","830+480=\n","830+501=\n","830+548=\n","830+562=\n","830+633=\n","830+674=\n","830+822=\n","830+876=\n","830+899=\n","830+976=\n","831+344=\n","831+434=\n","831+474=\n","831+569=\n","831+577=\n","831+933=\n","831+940=\n","831+952=\n","832+62=\n","832+352=\n","832+468=\n","832+555=\n","832+574=\n","832+615=\n","832+667=\n","832+718=\n","832+851=\n","833+179=\n","833+370=\n","833+493=\n","833+556=\n","833+657=\n","833+824=\n","833+830=\n","833+841=\n","833+861=\n","833+947=\n","834+63=\n","834+344=\n","834+482=\n","834+523=\n","834+533=\n","834+655=\n","834+709=\n","834+797=\n","834+837=\n","834+988=\n","834+994=\n","835+178=\n","835+341=\n","835+493=\n","835+606=\n","835+806=\n","836+23=\n","836+123=\n","836+134=\n","836+192=\n","836+354=\n","836+438=\n","836+652=\n","836+817=\n","836+884=\n","836+914=\n","836+915=\n","836+990=\n","837+11=\n","837+70=\n","837+84=\n","837+130=\n","837+145=\n","837+201=\n","837+207=\n","837+222=\n","837+248=\n","837+307=\n","837+478=\n","837+673=\n","837+785=\n","837+811=\n","838+32=\n","838+221=\n","838+314=\n","838+349=\n","838+390=\n","838+431=\n","838+444=\n","838+702=\n","838+936=\n","839+27=\n","839+149=\n","839+157=\n","839+253=\n","839+421=\n","839+492=\n","839+574=\n","839+844=\n","839+847=\n","839+903=\n","839+944=\n","840+105=\n","840+108=\n","840+177=\n","840+273=\n","840+461=\n","840+484=\n","840+593=\n","840+875=\n","840+997=\n","841+16=\n","841+79=\n","841+90=\n","841+131=\n","841+340=\n","841+453=\n","841+540=\n","841+551=\n","841+673=\n","841+677=\n","841+725=\n","841+944=\n","842+43=\n","842+78=\n","842+143=\n","842+323=\n","842+363=\n","842+371=\n","842+497=\n","842+547=\n","842+590=\n","842+621=\n","842+832=\n","843+68=\n","843+131=\n","843+219=\n","843+253=\n","843+684=\n","843+894=\n","844+126=\n","844+160=\n","844+341=\n","844+361=\n","844+546=\n","844+650=\n","844+775=\n","844+784=\n","844+831=\n","844+848=\n","844+873=\n","845+31=\n","845+73=\n","845+101=\n","845+133=\n","845+224=\n","845+296=\n","845+352=\n","845+495=\n","845+541=\n","845+763=\n","845+851=\n","845+943=\n","846+110=\n","846+132=\n","846+176=\n","846+184=\n","846+353=\n","846+425=\n","846+502=\n","846+592=\n","846+610=\n","846+673=\n","846+677=\n","847+270=\n","847+421=\n","847+453=\n","847+474=\n","847+563=\n","847+648=\n","847+734=\n","847+838=\n","847+927=\n","848+51=\n","848+108=\n","848+420=\n","848+421=\n","848+485=\n","848+897=\n","849+17=\n","849+101=\n","849+113=\n","849+390=\n","849+674=\n","849+675=\n","849+718=\n","849+732=\n","849+736=\n","849+798=\n","849+874=\n","849+924=\n","849+937=\n","850+1=\n","850+10=\n","850+17=\n","850+27=\n","850+116=\n","850+160=\n","850+250=\n","850+364=\n","850+384=\n","850+470=\n","850+480=\n","850+486=\n","850+509=\n","850+608=\n","850+700=\n","850+815=\n","850+979=\n","851+15=\n","851+113=\n","851+117=\n","851+196=\n","851+225=\n","851+403=\n","851+704=\n","851+803=\n","851+837=\n","851+936=\n","851+950=\n","852+148=\n","852+316=\n","852+340=\n","852+438=\n","852+544=\n","852+760=\n","852+796=\n","852+857=\n","852+944=\n","853+26=\n","853+44=\n","853+90=\n","853+149=\n","853+448=\n","853+471=\n","853+538=\n","853+588=\n","853+633=\n","853+652=\n","853+676=\n","853+687=\n","853+778=\n","853+840=\n","853+848=\n","853+854=\n","853+912=\n","853+931=\n","853+956=\n","854+9=\n","854+31=\n","854+69=\n","854+280=\n","854+372=\n","854+453=\n","854+558=\n","854+606=\n","854+736=\n","854+796=\n","854+914=\n","855+186=\n","855+230=\n","855+261=\n","855+327=\n","855+383=\n","855+399=\n","855+651=\n","855+676=\n","855+682=\n","855+728=\n","855+842=\n","855+947=\n","856+90=\n","856+175=\n","856+318=\n","856+415=\n","856+458=\n","856+460=\n","856+479=\n","856+543=\n","856+587=\n","856+623=\n","856+733=\n","856+986=\n","857+68=\n","857+140=\n","857+306=\n","857+360=\n","857+486=\n","857+575=\n","857+928=\n","858+9=\n","858+98=\n","858+350=\n","858+355=\n","858+430=\n","858+548=\n","858+549=\n","858+657=\n","859+77=\n","859+142=\n","859+178=\n","859+190=\n","859+213=\n","859+275=\n","859+331=\n","859+403=\n","859+458=\n","859+478=\n","859+630=\n","859+816=\n","859+860=\n","859+922=\n","860+156=\n","860+211=\n","860+230=\n","860+316=\n","860+410=\n","860+534=\n","860+580=\n","860+586=\n","860+645=\n","860+693=\n","860+723=\n","860+733=\n","860+755=\n","860+758=\n","860+932=\n","861+62=\n","861+144=\n","861+310=\n","861+690=\n","861+743=\n","861+769=\n","861+980=\n","862+139=\n","862+243=\n","862+247=\n","862+279=\n","862+319=\n","862+357=\n","862+366=\n","862+407=\n","862+543=\n","862+551=\n","862+611=\n","862+655=\n","862+684=\n","862+737=\n","862+780=\n","862+840=\n","862+860=\n","863+16=\n","863+27=\n","863+66=\n","863+129=\n","863+233=\n","863+569=\n","863+628=\n","863+657=\n","863+759=\n","863+810=\n","863+816=\n","864+29=\n","864+148=\n","864+155=\n","864+195=\n","864+481=\n","864+811=\n","864+923=\n","865+15=\n","865+144=\n","865+290=\n","865+341=\n","865+342=\n","865+351=\n","865+623=\n","865+798=\n","866+312=\n","866+397=\n","866+410=\n","866+501=\n","866+610=\n","866+720=\n","866+770=\n","866+776=\n","866+815=\n","866+887=\n","866+928=\n","866+984=\n","867+95=\n","867+156=\n","867+204=\n","867+278=\n","867+300=\n","867+305=\n","867+497=\n","867+518=\n","867+533=\n","867+619=\n","867+681=\n","867+998=\n","868+297=\n","868+490=\n","868+509=\n","868+529=\n","868+540=\n","868+603=\n","868+866=\n","868+925=\n","868+940=\n","869+10=\n","869+165=\n","869+192=\n","869+245=\n","869+277=\n","869+278=\n","869+404=\n","869+577=\n","869+712=\n","869+790=\n","869+795=\n","869+850=\n","869+979=\n","870+75=\n","870+424=\n","870+428=\n","870+819=\n","870+863=\n","870+958=\n","870+971=\n","871+138=\n","871+322=\n","871+410=\n","871+430=\n","871+497=\n","871+555=\n","871+565=\n","871+659=\n","871+716=\n","871+732=\n","871+737=\n","871+765=\n","871+792=\n","871+873=\n","871+926=\n","871+969=\n","872+393=\n","872+570=\n","872+740=\n","872+798=\n","872+856=\n","872+934=\n","873+274=\n","873+317=\n","873+503=\n","873+827=\n","873+861=\n","873+987=\n","874+171=\n","874+297=\n","874+657=\n","874+750=\n","874+798=\n","874+973=\n","875+89=\n","875+126=\n","875+383=\n","875+432=\n","876+87=\n","876+132=\n","876+337=\n","876+451=\n","876+502=\n","876+691=\n","876+700=\n","876+704=\n","876+752=\n","877+51=\n","877+322=\n","877+345=\n","877+348=\n","877+488=\n","877+658=\n","877+681=\n","877+741=\n","877+812=\n","877+826=\n","877+839=\n","877+942=\n","877+975=\n","878+555=\n","878+682=\n","878+720=\n","879+43=\n","879+153=\n","879+612=\n","879+615=\n","879+641=\n","879+722=\n","879+780=\n","879+818=\n","879+829=\n","879+908=\n","879+957=\n","879+964=\n","879+966=\n","879+972=\n","880+3=\n","880+90=\n","880+97=\n","880+112=\n","880+136=\n","880+218=\n","880+586=\n","880+618=\n","880+639=\n","880+652=\n","880+682=\n","880+816=\n","881+75=\n","881+94=\n","881+153=\n","881+392=\n","881+698=\n","881+742=\n","881+806=\n","881+916=\n","882+43=\n","882+64=\n","882+338=\n","882+587=\n","882+668=\n","882+982=\n","883+9=\n","883+164=\n","883+400=\n","883+414=\n","883+792=\n","883+896=\n","883+957=\n","884+170=\n","884+299=\n","884+461=\n","884+475=\n","884+606=\n","884+954=\n","885+60=\n","885+161=\n","885+187=\n","885+395=\n","885+425=\n","885+433=\n","885+633=\n","885+766=\n","885+775=\n","885+906=\n","885+908=\n","885+913=\n","885+955=\n","886+70=\n","886+430=\n","886+523=\n","886+545=\n","886+676=\n","886+916=\n","886+951=\n","887+310=\n","887+313=\n","887+319=\n","887+390=\n","887+555=\n","887+625=\n","887+752=\n","887+985=\n","888+119=\n","888+122=\n","888+145=\n","888+384=\n","888+738=\n","888+972=\n","889+16=\n","889+165=\n","889+168=\n","889+186=\n","889+241=\n","889+256=\n","889+258=\n","889+307=\n","889+310=\n","889+384=\n","889+587=\n","889+591=\n","889+739=\n","889+799=\n","889+803=\n","889+913=\n","890+211=\n","890+235=\n","890+245=\n","890+291=\n","890+347=\n","890+505=\n","890+562=\n","890+563=\n","890+616=\n","890+636=\n","890+766=\n","891+66=\n","891+464=\n","891+529=\n","891+551=\n","891+558=\n","891+587=\n","891+738=\n","891+794=\n","891+963=\n","892+57=\n","892+73=\n","892+112=\n","892+247=\n","892+269=\n","892+422=\n","892+436=\n","892+462=\n","892+505=\n","892+704=\n","892+742=\n","892+856=\n","892+934=\n","892+951=\n","893+88=\n","893+107=\n","893+178=\n","893+220=\n","893+404=\n","893+460=\n","893+477=\n","893+506=\n","893+553=\n","893+665=\n","893+790=\n","893+815=\n","893+859=\n","893+971=\n","893+996=\n","893+998=\n","894+123=\n","894+196=\n","894+298=\n","894+358=\n","894+361=\n","894+385=\n","894+450=\n","894+508=\n","894+632=\n","894+716=\n","894+783=\n","894+805=\n","894+823=\n","894+832=\n","895+9=\n","895+72=\n","895+189=\n","895+258=\n","895+275=\n","895+427=\n","895+472=\n","895+553=\n","895+728=\n","895+842=\n","895+944=\n","895+987=\n","896+78=\n","896+144=\n","896+447=\n","896+461=\n","896+503=\n","896+563=\n","896+636=\n","896+844=\n","896+864=\n","897+290=\n","897+568=\n","897+599=\n","897+684=\n","897+886=\n","897+950=\n","897+970=\n","897+992=\n","898+22=\n","898+293=\n","898+369=\n","898+397=\n","898+422=\n","898+607=\n","898+666=\n","898+713=\n","899+128=\n","899+252=\n","899+528=\n","899+748=\n","899+931=\n","900+31=\n","900+49=\n","900+424=\n","900+682=\n","900+846=\n","900+849=\n","900+910=\n","901+26=\n","901+42=\n","901+52=\n","901+300=\n","901+314=\n","901+478=\n","901+563=\n","901+623=\n","901+665=\n","901+694=\n","901+696=\n","902+151=\n","902+343=\n","902+396=\n","902+430=\n","902+500=\n","902+514=\n","902+515=\n","902+531=\n","902+606=\n","902+703=\n","902+729=\n","902+914=\n","903+409=\n","903+559=\n","903+677=\n","903+887=\n","904+118=\n","904+160=\n","904+211=\n","904+304=\n","904+366=\n","904+506=\n","904+556=\n","904+570=\n","905+54=\n","905+245=\n","905+373=\n","905+415=\n","905+422=\n","905+443=\n","905+767=\n","905+866=\n","906+71=\n","906+72=\n","906+81=\n","906+138=\n","906+149=\n","906+166=\n","906+286=\n","906+455=\n","906+563=\n","906+686=\n","906+796=\n","906+801=\n","906+853=\n","906+956=\n","906+984=\n","907+131=\n","907+236=\n","907+333=\n","907+422=\n","907+550=\n","907+653=\n","907+721=\n","907+882=\n","908+24=\n","908+232=\n","908+338=\n","908+345=\n","908+516=\n","908+532=\n","908+643=\n","908+703=\n","908+732=\n","908+855=\n","908+926=\n","908+939=\n","909+34=\n","909+65=\n","909+229=\n","909+255=\n","909+334=\n","909+340=\n","909+411=\n","909+494=\n","909+610=\n","909+864=\n","909+924=\n","909+931=\n","909+983=\n","909+991=\n","910+218=\n","910+325=\n","910+341=\n","910+537=\n","910+546=\n","910+803=\n","911+10=\n","911+388=\n","911+512=\n","911+624=\n","911+786=\n","911+835=\n","911+860=\n","911+894=\n","911+908=\n","911+909=\n","911+965=\n","912+29=\n","912+147=\n","912+330=\n","912+479=\n","912+512=\n","912+534=\n","912+539=\n","912+562=\n","912+574=\n","912+648=\n","912+692=\n","912+705=\n","912+872=\n","912+949=\n","913+69=\n","913+114=\n","913+243=\n","913+372=\n","913+373=\n","913+562=\n","913+588=\n","913+617=\n","913+664=\n","913+794=\n","913+817=\n","913+888=\n","913+913=\n","913+928=\n","914+347=\n","914+929=\n","915+136=\n","915+242=\n","915+273=\n","915+290=\n","915+324=\n","915+351=\n","915+431=\n","915+435=\n","915+443=\n","915+477=\n","915+775=\n","915+914=\n","915+958=\n","916+28=\n","916+111=\n","916+260=\n","916+299=\n","916+377=\n","916+384=\n","916+494=\n","916+594=\n","916+742=\n","917+46=\n","917+141=\n","917+253=\n","917+282=\n","917+286=\n","917+344=\n","917+400=\n","917+453=\n","917+502=\n","917+519=\n","917+528=\n","917+692=\n","917+736=\n","918+69=\n","918+566=\n","918+651=\n","918+783=\n","918+784=\n","918+887=\n","918+941=\n","919+202=\n","919+417=\n","919+423=\n","919+500=\n","919+646=\n","919+652=\n","919+687=\n","919+705=\n","919+996=\n","920+69=\n","920+222=\n","920+310=\n","920+439=\n","920+508=\n","920+512=\n","920+530=\n","920+876=\n","920+938=\n","920+988=\n","921+42=\n","921+282=\n","921+350=\n","921+535=\n","921+689=\n","921+842=\n","921+845=\n","921+940=\n","922+135=\n","922+218=\n","922+275=\n","922+322=\n","922+389=\n","922+427=\n","922+534=\n","922+551=\n","922+629=\n","922+639=\n","922+724=\n","922+730=\n","922+737=\n","922+745=\n","922+792=\n","922+862=\n","922+998=\n","923+38=\n","923+180=\n","923+515=\n","923+639=\n","923+665=\n","923+712=\n","923+725=\n","923+803=\n","923+979=\n","924+145=\n","924+467=\n","924+534=\n","924+772=\n","924+834=\n","924+999=\n","925+101=\n","925+219=\n","925+244=\n","925+245=\n","925+329=\n","925+546=\n","925+728=\n","925+755=\n","925+811=\n","926+119=\n","926+139=\n","926+149=\n","926+333=\n","926+616=\n","926+645=\n","926+646=\n","926+853=\n","927+8=\n","927+79=\n","927+722=\n","927+745=\n","927+762=\n","928+15=\n","928+130=\n","928+162=\n","928+196=\n","928+219=\n","928+225=\n","928+226=\n","928+243=\n","928+299=\n","928+435=\n","928+446=\n","928+462=\n","928+481=\n","928+580=\n","928+763=\n","928+772=\n","928+778=\n","928+868=\n","929+222=\n","929+244=\n","929+255=\n","929+282=\n","929+395=\n","929+498=\n","929+616=\n","929+786=\n","929+990=\n","930+178=\n","930+324=\n","930+368=\n","930+538=\n","930+543=\n","930+602=\n","930+972=\n","931+23=\n","931+208=\n","931+238=\n","931+241=\n","931+246=\n","931+254=\n","931+356=\n","931+399=\n","931+414=\n","931+491=\n","931+502=\n","931+569=\n","931+605=\n","931+698=\n","931+822=\n","932+20=\n","932+69=\n","932+234=\n","932+333=\n","932+474=\n","932+502=\n","932+566=\n","932+721=\n","932+731=\n","932+883=\n","932+922=\n","933+30=\n","933+118=\n","933+304=\n","933+335=\n","933+348=\n","933+758=\n","933+863=\n","933+866=\n","934+588=\n","934+805=\n","935+231=\n","935+288=\n","935+354=\n","935+474=\n","935+571=\n","935+586=\n","935+904=\n","935+969=\n","936+145=\n","936+197=\n","936+254=\n","936+300=\n","936+324=\n","936+498=\n","936+579=\n","936+586=\n","936+676=\n","936+898=\n","937+30=\n","937+67=\n","937+158=\n","937+369=\n","937+377=\n","937+418=\n","937+504=\n","937+641=\n","938+2=\n","938+238=\n","938+356=\n","938+454=\n","938+496=\n","938+568=\n","938+651=\n","938+715=\n","938+798=\n","939+56=\n","939+317=\n","939+380=\n","939+648=\n","939+700=\n","939+897=\n","939+917=\n","939+924=\n","939+957=\n","939+958=\n","940+55=\n","940+85=\n","940+111=\n","940+115=\n","940+183=\n","940+226=\n","940+241=\n","940+295=\n","940+682=\n","940+774=\n","940+870=\n","941+15=\n","941+24=\n","941+38=\n","941+52=\n","941+114=\n","941+156=\n","941+477=\n","941+551=\n","941+583=\n","941+635=\n","941+700=\n","941+727=\n","941+991=\n","942+77=\n","942+623=\n","942+646=\n","943+73=\n","943+443=\n","943+785=\n","943+914=\n","944+233=\n","944+267=\n","944+373=\n","944+530=\n","944+824=\n","945+35=\n","945+46=\n","945+81=\n","945+179=\n","945+224=\n","945+296=\n","945+382=\n","945+550=\n","945+596=\n","945+691=\n","945+721=\n","945+808=\n","945+846=\n","945+940=\n","946+27=\n","946+103=\n","946+141=\n","946+192=\n","946+219=\n","946+257=\n","946+278=\n","946+288=\n","946+330=\n","946+612=\n","946+933=\n","947+21=\n","947+103=\n","947+219=\n","947+444=\n","947+453=\n","947+525=\n","947+538=\n","947+802=\n","947+965=\n","948+158=\n","948+534=\n","948+634=\n","948+706=\n","948+729=\n","948+789=\n","948+947=\n","948+990=\n","949+118=\n","949+176=\n","949+233=\n","949+281=\n","949+315=\n","949+318=\n","949+328=\n","949+397=\n","949+486=\n","949+499=\n","949+545=\n","949+585=\n","949+761=\n","950+14=\n","950+62=\n","950+77=\n","950+151=\n","950+245=\n","950+267=\n","950+315=\n","950+488=\n","950+546=\n","950+696=\n","950+804=\n","950+874=\n","950+896=\n","951+35=\n","951+104=\n","951+200=\n","951+231=\n","951+288=\n","951+349=\n","951+423=\n","951+431=\n","951+505=\n","951+635=\n","951+820=\n","951+915=\n","951+938=\n","951+949=\n","952+197=\n","952+375=\n","952+413=\n","952+806=\n","953+160=\n","953+180=\n","953+376=\n","953+377=\n","953+418=\n","953+522=\n","953+825=\n","953+938=\n","954+13=\n","954+123=\n","954+153=\n","954+205=\n","954+443=\n","954+508=\n","954+534=\n","954+551=\n","954+553=\n","954+578=\n","954+810=\n","954+827=\n","955+289=\n","955+411=\n","955+491=\n","955+575=\n","955+620=\n","955+737=\n","955+899=\n","956+81=\n","956+129=\n","956+149=\n","956+175=\n","956+199=\n","956+282=\n","956+381=\n","956+523=\n","956+607=\n","956+610=\n","956+967=\n","957+177=\n","957+234=\n","957+252=\n","957+283=\n","957+427=\n","957+681=\n","957+768=\n","957+901=\n","958+46=\n","958+122=\n","958+153=\n","958+338=\n","958+476=\n","958+536=\n","958+561=\n","958+606=\n","958+762=\n","958+765=\n","958+833=\n","958+856=\n","958+947=\n","959+133=\n","959+248=\n","959+370=\n","959+423=\n","959+488=\n","959+618=\n","959+657=\n","959+662=\n","959+727=\n","959+785=\n","959+788=\n","959+841=\n","959+890=\n","959+990=\n","960+88=\n","960+105=\n","960+353=\n","960+602=\n","960+681=\n","961+29=\n","961+329=\n","961+335=\n","961+373=\n","961+475=\n","961+493=\n","961+568=\n","961+619=\n","961+715=\n","961+892=\n","962+56=\n","962+189=\n","962+258=\n","962+353=\n","962+464=\n","962+505=\n","962+524=\n","962+539=\n","962+574=\n","962+655=\n","963+141=\n","963+378=\n","963+459=\n","963+538=\n","963+545=\n","963+604=\n","963+621=\n","963+995=\n","964+161=\n","964+279=\n","964+355=\n","964+443=\n","964+446=\n","964+460=\n","964+506=\n","964+524=\n","964+528=\n","964+693=\n","964+774=\n","964+804=\n","964+852=\n","965+346=\n","965+560=\n","965+717=\n","965+908=\n","965+913=\n","966+113=\n","966+114=\n","966+170=\n","966+202=\n","966+257=\n","966+355=\n","966+472=\n","966+622=\n","966+669=\n","966+820=\n","966+825=\n","966+945=\n","967+399=\n","967+562=\n","967+591=\n","967+700=\n","967+738=\n","967+935=\n","968+200=\n","968+425=\n","968+603=\n","968+610=\n","968+617=\n","968+654=\n","968+657=\n","969+88=\n","969+156=\n","969+177=\n","969+361=\n","969+395=\n","969+503=\n","969+538=\n","969+859=\n","969+865=\n","969+952=\n","970+228=\n","970+524=\n","970+527=\n","970+617=\n","971+5=\n","971+47=\n","971+140=\n","971+231=\n","971+305=\n","971+317=\n","971+429=\n","971+738=\n","971+962=\n","971+971=\n","972+21=\n","972+24=\n","972+45=\n","972+189=\n","972+219=\n","972+246=\n","972+376=\n","972+452=\n","972+927=\n","972+990=\n","973+82=\n","973+122=\n","973+141=\n","973+294=\n","973+368=\n","973+480=\n","973+534=\n","973+595=\n","973+743=\n","973+784=\n","973+950=\n","973+995=\n","974+323=\n","974+450=\n","974+558=\n","974+742=\n","974+876=\n","974+949=\n","974+993=\n","975+78=\n","975+102=\n","975+112=\n","975+161=\n","975+246=\n","975+334=\n","975+388=\n","975+399=\n","975+484=\n","975+485=\n","975+565=\n","975+650=\n","975+831=\n","975+852=\n","976+101=\n","976+113=\n","976+361=\n","976+491=\n","976+660=\n","976+769=\n","976+904=\n","977+8=\n","977+12=\n","977+110=\n","977+139=\n","977+237=\n","977+273=\n","977+288=\n","977+309=\n","977+382=\n","977+412=\n","977+440=\n","977+617=\n","977+771=\n","977+784=\n","977+958=\n","977+994=\n","978+151=\n","978+241=\n","978+261=\n","978+701=\n","978+750=\n","978+755=\n","978+756=\n","978+933=\n","979+18=\n","979+614=\n","979+711=\n","979+738=\n","979+800=\n","979+884=\n","979+907=\n","980+39=\n","980+291=\n","980+311=\n","980+483=\n","980+512=\n","980+554=\n","980+571=\n","980+799=\n","980+829=\n","980+867=\n","980+868=\n","980+901=\n","980+927=\n","981+2=\n","981+102=\n","981+127=\n","981+235=\n","981+282=\n","981+318=\n","981+351=\n","981+370=\n","981+488=\n","981+604=\n","981+792=\n","981+855=\n","981+866=\n","981+995=\n","982+213=\n","982+747=\n","982+749=\n","982+847=\n","983+49=\n","983+73=\n","983+281=\n","983+371=\n","983+513=\n","983+567=\n","983+701=\n","984+127=\n","984+540=\n","984+550=\n","984+882=\n","984+930=\n","984+940=\n","984+980=\n","985+159=\n","985+326=\n","985+381=\n","985+407=\n","985+707=\n","985+925=\n","986+42=\n","986+107=\n","986+462=\n","986+569=\n","986+606=\n","986+693=\n","986+881=\n","986+910=\n","986+933=\n","986+934=\n","986+957=\n","986+972=\n","987+91=\n","987+193=\n","987+255=\n","987+327=\n","987+553=\n","987+569=\n","987+956=\n","988+84=\n","988+109=\n","988+221=\n","988+257=\n","988+351=\n","988+398=\n","988+435=\n","988+577=\n","988+790=\n","989+13=\n","989+34=\n","989+49=\n","989+54=\n","989+116=\n","989+235=\n","989+438=\n","989+729=\n","989+849=\n","990+26=\n","990+315=\n","990+356=\n","990+429=\n","990+440=\n","990+470=\n","990+550=\n","990+562=\n","990+789=\n","990+979=\n","991+73=\n","991+195=\n","991+239=\n","991+506=\n","991+617=\n","991+638=\n","991+780=\n","991+794=\n","991+795=\n","992+27=\n","992+52=\n","992+68=\n","992+221=\n","992+232=\n","992+378=\n","992+466=\n","992+482=\n","992+555=\n","992+561=\n","992+596=\n","992+641=\n","992+776=\n","992+842=\n","992+850=\n","992+876=\n","992+901=\n","992+939=\n","993+93=\n","993+332=\n","993+505=\n","993+688=\n","993+838=\n","993+905=\n","994+19=\n","994+70=\n","994+161=\n","994+187=\n","994+252=\n","994+346=\n","994+398=\n","994+412=\n","994+822=\n","994+836=\n","994+860=\n","994+875=\n","994+880=\n","994+912=\n","994+956=\n","995+95=\n","995+114=\n","995+189=\n","995+434=\n","995+480=\n","995+544=\n","995+576=\n","995+620=\n","995+869=\n","995+912=\n","995+924=\n","995+929=\n","995+959=\n","996+15=\n","996+30=\n","996+124=\n","996+397=\n","996+515=\n","996+540=\n","996+592=\n","996+708=\n","996+843=\n","997+63=\n","997+134=\n","997+136=\n","997+143=\n","997+222=\n","997+429=\n","997+431=\n","997+449=\n","997+473=\n","997+507=\n","997+528=\n","997+585=\n","997+681=\n","997+720=\n","997+723=\n","997+738=\n","997+751=\n","997+757=\n","997+837=\n","997+904=\n","997+940=\n","998+233=\n","998+252=\n","998+255=\n","998+259=\n","998+306=\n","998+368=\n","998+378=\n","998+426=\n","998+490=\n","998+530=\n","998+575=\n","998+639=\n","998+653=\n","998+895=\n","998+982=\n","999+75=\n","999+106=\n","999+208=\n","999+395=\n","999+396=\n","999+496=\n","999+853=\n","999+885=\n"]}],"source":["%cat ./data/2_operands_0_to_999_balanced_digit_original_data/test/test_10000_stripped.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":682656,"status":"ok","timestamp":1750541509218,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"W7nWQ9KETLIu","outputId":"c4ea2a56-05d8-4340-f93b-0dd983c580d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=219\n","Skipping y_hat=229\n","Skipping y_hat=260\n","Skipping y_hat=396\n","Skipping y_hat=305\n","Skipping y_hat=392\n","Skipping y_hat=209\n","Skipping y_hat=330\n","Skipping y_hat=211\n","Skipping y_hat=280\n","Skipping y_hat=300\n","Skipping y_hat=399\n","Skipping y_hat=393\n","Skipping y_hat=300\n","Skipping y_hat=348\n","Skipping y_hat=347\n","Skipping y_hat=397\n","Skipping y_hat=312\n","Skipping y_hat=204\n","Skipping y_hat=306\n","Skipping y_hat=300\n","Skipping y_hat=404\n","Skipping y_hat=340\n","Skipping y_hat=401\n","Skipping y_hat=491\n","Skipping y_hat=421\n","Skipping y_hat=493\n","Skipping y_hat=441\n","Skipping y_hat=497\n","Skipping y_hat=312\n","Skipping y_hat=448\n","Skipping y_hat=345\n"," 14% 11/80 [00:00<00:02, 32.42it/s]Skipping y_hat=496\n","Skipping y_hat=509\n","Skipping y_hat=449\n","Skipping y_hat=582\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=541\n","Skipping y_hat=515\n","Skipping y_hat=404\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=514\n","Skipping y_hat=697\n","Skipping y_hat=695\n","Skipping y_hat=629\n","Skipping y_hat=694\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=667\n","Skipping y_hat=699\n","Skipping y_hat=719\n","Skipping y_hat=797\n","Skipping y_hat=799\n","Skipping y_hat=717\n","Skipping y_hat=671\n","Skipping y_hat=793\n","Skipping y_hat=795\n","Skipping y_hat=720\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=779\n","Skipping y_hat=782\n","Skipping y_hat=809\n","Skipping y_hat=819\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=840\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=880\n","Skipping y_hat=850\n","Skipping y_hat=801\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=889\n","Skipping y_hat=998\n","Skipping y_hat=889\n","Skipping y_hat=989\n","Skipping y_hat=870\n","Skipping y_hat=920\n","Skipping y_hat=988\n","Skipping y_hat=934\n","Skipping y_hat=188\n","Skipping y_hat=189\n","Skipping y_hat=906\n","Skipping y_hat=195\n","Skipping y_hat=1029\n","Skipping y_hat=1017\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=1001\n","Skipping y_hat=1021\n","Skipping y_hat=999\n","Skipping y_hat=196\n"," 19% 15/80 [00:00<00:02, 31.46it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","Skipping y_hat=1093\n","Skipping y_hat=1036\n","Skipping y_hat=1067\n","100% 80/80 [00:02<00:00, 34.25it/s]\n","accuracy of 9900 examples: 9315/9900 (94.0909090909091%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 39.50it/s]\n","accuracy of 10000 examples: 9987/10000 (99.87%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=20\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=47\n","Skipping y_hat=39\n","Skipping y_hat=47\n","Skipping y_hat=92\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=626\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=784\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=941\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=302\n","Skipping y_hat=396\n","Skipping y_hat=475\n","Skipping y_hat=476\n","Skipping y_hat=400\n","Skipping y_hat=523\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=842\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=360\n","Skipping y_hat=481\n","Skipping y_hat=470\n","Skipping y_hat=555\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1001\n","Skipping y_hat=165\n","Skipping y_hat=127\n","Skipping y_hat=579\n","Skipping y_hat=947\n","Skipping y_hat=239\n","Skipping y_hat=347\n","Skipping y_hat=578\n","Skipping y_hat=621\n","Skipping y_hat=624\n","Skipping y_hat=657\n","Skipping y_hat=622\n","Skipping y_hat=787\n","Skipping y_hat=1003\n","Skipping y_hat=975\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1002\n","Skipping y_hat=964\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=673\n","Skipping y_hat=746\n","Skipping y_hat=810\n","Skipping y_hat=131\n","Skipping y_hat=240\n","Skipping y_hat=249\n","Skipping y_hat=248\n","Skipping y_hat=582\n","Skipping y_hat=526\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=779\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=119\n","Skipping y_hat=149\n","Skipping y_hat=120\n","Skipping y_hat=111\n","Skipping y_hat=109\n","Skipping y_hat=159\n","Skipping y_hat=109\n","Skipping y_hat=162\n","Skipping y_hat=213\n","Skipping y_hat=161\n","Skipping y_hat=111\n","Skipping y_hat=129\n","Skipping y_hat=109\n","Skipping y_hat=101\n","Skipping y_hat=120\n","Skipping y_hat=218\n","Skipping y_hat=298\n","Skipping y_hat=254\n","Skipping y_hat=311\n","Skipping y_hat=326\n","Skipping y_hat=339\n","Skipping y_hat=463\n","Skipping y_hat=432\n","Skipping y_hat=361\n","Skipping y_hat=459\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=416\n","Skipping y_hat=462\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=531\n","Skipping y_hat=555\n","Skipping y_hat=594\n","Skipping y_hat=551\n","Skipping y_hat=569\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=655\n","Skipping y_hat=686\n","Skipping y_hat=689\n","Skipping y_hat=719\n","Skipping y_hat=711\n","Skipping y_hat=729\n","Skipping y_hat=707\n","Skipping y_hat=775\n","Skipping y_hat=758\n","Skipping y_hat=751\n","Skipping y_hat=784\n","Skipping y_hat=897\n","Skipping y_hat=832\n","Skipping y_hat=850\n","Skipping y_hat=940\n","Skipping y_hat=881\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=988\n","Skipping y_hat=100\n","  4% 3/80 [00:00<00:02, 29.43it/s]Skipping y_hat=134\n","  9% 7/80 [00:00<00:02, 32.75it/s]Skipping y_hat=506\n","Skipping y_hat=139\n","Skipping y_hat=110\n","Skipping y_hat=142\n","Skipping y_hat=120\n","Skipping y_hat=159\n","Skipping y_hat=209\n","Skipping y_hat=140\n","Skipping y_hat=220\n","Skipping y_hat=199\n","Skipping y_hat=291\n","Skipping y_hat=295\n","Skipping y_hat=228\n","Skipping y_hat=260\n","Skipping y_hat=396\n","Skipping y_hat=299\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=211\n","Skipping y_hat=380\n","Skipping y_hat=300\n","Skipping y_hat=399\n","Skipping y_hat=305\n","Skipping y_hat=328\n","Skipping y_hat=200\n","Skipping y_hat=306\n","Skipping y_hat=321\n","Skipping y_hat=417\n","Skipping y_hat=499\n","Skipping y_hat=491\n","Skipping y_hat=421\n","Skipping y_hat=493\n","Skipping y_hat=448\n","Skipping y_hat=345\n","Skipping y_hat=469\n"," 14% 11/80 [00:00<00:02, 32.84it/s]Skipping y_hat=496\n","Skipping y_hat=505\n","Skipping y_hat=581\n","Skipping y_hat=404\n","Skipping y_hat=405\n","Skipping y_hat=515\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=530\n","Skipping y_hat=549\n","Skipping y_hat=514\n","Skipping y_hat=695\n","Skipping y_hat=629\n","Skipping y_hat=616\n","Skipping y_hat=615\n","Skipping y_hat=699\n","Skipping y_hat=680\n","Skipping y_hat=719\n","Skipping y_hat=765\n","Skipping y_hat=798\n","Skipping y_hat=799\n","Skipping y_hat=795\n","Skipping y_hat=790\n","Skipping y_hat=793\n","Skipping y_hat=782\n","Skipping y_hat=809\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=800\n","Skipping y_hat=869\n","Skipping y_hat=856\n","Skipping y_hat=840\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=889\n","Skipping y_hat=809\n","Skipping y_hat=909\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=999\n","Skipping y_hat=998\n","Skipping y_hat=889\n","Skipping y_hat=988\n","Skipping y_hat=934\n","Skipping y_hat=906\n","Skipping y_hat=195\n","Skipping y_hat=1029\n","Skipping y_hat=1017\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=1001\n","Skipping y_hat=1021\n","Skipping y_hat=1050\n","Skipping y_hat=999\n","Skipping y_hat=196\n"," 19% 15/80 [00:00<00:02, 32.19it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=900\n","Skipping y_hat=1093\n","Skipping y_hat=1031\n","100% 80/80 [00:02<00:00, 34.28it/s]\n","accuracy of 9900 examples: 9313/9900 (94.07070707070707%)\n","\n","Test Results:\n","test_10000_stripped: 94.07%\n","\n","iter 4200: train loss 0.6729, val loss 1.8132\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=20\n","Skipping y_hat=38\n","Skipping y_hat=37\n","Skipping y_hat=49\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=721\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=632\n","Skipping y_hat=784\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=841\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=475\n","Skipping y_hat=476\n","Skipping y_hat=480\n","Skipping y_hat=523\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=802\n","Skipping y_hat=318\n","Skipping y_hat=337\n","Skipping y_hat=360\n","Skipping y_hat=481\n","Skipping y_hat=470\n","Skipping y_hat=575\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=843\n","Skipping y_hat=1003\n","Skipping y_hat=165\n","Skipping y_hat=127\n","Skipping y_hat=579\n","Skipping y_hat=947\n","Skipping y_hat=239\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=621\n","Skipping y_hat=684\n","Skipping y_hat=627\n","Skipping y_hat=622\n","Skipping y_hat=787\n","Skipping y_hat=1003\n","Skipping y_hat=1005\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=656\n","Skipping y_hat=846\n","Skipping y_hat=1002\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=603\n","Skipping y_hat=746\n","Skipping y_hat=810\n","Skipping y_hat=131\n","Skipping y_hat=340\n","Skipping y_hat=229\n","Skipping y_hat=348\n","Skipping y_hat=582\n","Skipping y_hat=526\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=729\n","Skipping y_hat=834\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=119\n","Skipping y_hat=50\n","Skipping y_hat=120\n","Skipping y_hat=126\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=101\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=160\n","Skipping y_hat=124\n","Skipping y_hat=109\n","Skipping y_hat=100\n","Skipping y_hat=260\n","Skipping y_hat=208\n","Skipping y_hat=208\n","Skipping y_hat=259\n","Skipping y_hat=281\n","Skipping y_hat=304\n","Skipping y_hat=339\n","Skipping y_hat=364\n","Skipping y_hat=430\n","Skipping y_hat=350\n","Skipping y_hat=491\n","Skipping y_hat=496\n","Skipping y_hat=416\n","Skipping y_hat=451\n","Skipping y_hat=460\n","Skipping y_hat=516\n","Skipping y_hat=531\n","Skipping y_hat=559\n","Skipping y_hat=594\n","Skipping y_hat=569\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=665\n","Skipping y_hat=689\n","Skipping y_hat=718\n","Skipping y_hat=711\n","Skipping y_hat=729\n","Skipping y_hat=705\n","Skipping y_hat=775\n","Skipping y_hat=788\n","Skipping y_hat=778\n","Skipping y_hat=807\n","Skipping y_hat=812\n","Skipping y_hat=857\n","Skipping y_hat=948\n","Skipping y_hat=888\n","Skipping y_hat=881\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=988\n","Skipping y_hat=1001\n","  9% 7/80 [00:00<00:02, 30.49it/s]Skipping y_hat=506\n","Skipping y_hat=110\n","Skipping y_hat=120\n","Skipping y_hat=163\n","Skipping y_hat=109\n","Skipping y_hat=159\n","Skipping y_hat=209\n","Skipping y_hat=140\n","Skipping y_hat=220\n","Skipping y_hat=299\n","Skipping y_hat=292\n","Skipping y_hat=260\n","Skipping y_hat=295\n","Skipping y_hat=261\n","Skipping y_hat=396\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=349\n","Skipping y_hat=211\n","Skipping y_hat=280\n","Skipping y_hat=300\n","Skipping y_hat=399\n","Skipping y_hat=347\n","Skipping y_hat=205\n","Skipping y_hat=312\n","Skipping y_hat=200\n","Skipping y_hat=306\n","Skipping y_hat=321\n","Skipping y_hat=362\n","Skipping y_hat=491\n","Skipping y_hat=372\n","Skipping y_hat=369\n","Skipping y_hat=447\n","Skipping y_hat=312\n","Skipping y_hat=300\n","Skipping y_hat=447\n","Skipping y_hat=409\n","Skipping y_hat=345\n"," 14% 11/80 [00:00<00:02, 31.08it/s]Skipping y_hat=496\n","Skipping y_hat=401\n","Skipping y_hat=405\n","Skipping y_hat=449\n","Skipping y_hat=403\n","Skipping y_hat=404\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=405\n","Skipping y_hat=520\n","Skipping y_hat=514\n","Skipping y_hat=403\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=530\n","Skipping y_hat=514\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=579\n","Skipping y_hat=696\n","Skipping y_hat=684\n","Skipping y_hat=616\n","Skipping y_hat=696\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=667\n","Skipping y_hat=699\n","Skipping y_hat=684\n","Skipping y_hat=719\n","Skipping y_hat=775\n","Skipping y_hat=795\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=779\n","Skipping y_hat=759\n","Skipping y_hat=796\n","Skipping y_hat=708\n","Skipping y_hat=706\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=840\n","Skipping y_hat=841\n","Skipping y_hat=871\n","Skipping y_hat=801\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=819\n","Skipping y_hat=996\n","Skipping y_hat=920\n","Skipping y_hat=845\n","Skipping y_hat=855\n","Skipping y_hat=988\n","Skipping y_hat=932\n","Skipping y_hat=189\n","Skipping y_hat=906\n","Skipping y_hat=1029\n","Skipping y_hat=1018\n","Skipping y_hat=1029\n","Skipping y_hat=1012\n","Skipping y_hat=196\n","Skipping y_hat=1021\n","Skipping y_hat=1051\n","Skipping y_hat=196\n"," 19% 15/80 [00:00<00:02, 30.68it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","Skipping y_hat=1093\n","Skipping y_hat=1036\n","Skipping y_hat=1067\n","100% 80/80 [00:02<00:00, 33.89it/s]\n","accuracy of 9900 examples: 9331/9900 (94.25252525252526%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 39.32it/s]\n","accuracy of 10000 examples: 9980/10000 (99.8%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=37\n","Skipping y_hat=38\n","Skipping y_hat=49\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=976\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=784\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=841\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=302\n","Skipping y_hat=396\n","Skipping y_hat=475\n","Skipping y_hat=476\n","Skipping y_hat=480\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=842\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=380\n","Skipping y_hat=481\n","Skipping y_hat=470\n","Skipping y_hat=575\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1003\n","Skipping y_hat=165\n","Skipping y_hat=127\n","Skipping y_hat=579\n","Skipping y_hat=977\n","Skipping y_hat=239\n","Skipping y_hat=347\n","Skipping y_hat=578\n","Skipping y_hat=621\n","Skipping y_hat=684\n","Skipping y_hat=627\n","Skipping y_hat=622\n","Skipping y_hat=717\n","Skipping y_hat=1003\n","Skipping y_hat=1005\n","Skipping y_hat=173\n","Skipping y_hat=128\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=626\n","Skipping y_hat=846\n","Skipping y_hat=1002\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=673\n","Skipping y_hat=796\n","Skipping y_hat=820\n","Skipping y_hat=131\n","Skipping y_hat=340\n","Skipping y_hat=229\n","Skipping y_hat=238\n","Skipping y_hat=482\n","Skipping y_hat=526\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=729\n","Skipping y_hat=834\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=79\n","Skipping y_hat=149\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=111\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=101\n","Skipping y_hat=161\n","Skipping y_hat=113\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=134\n","Skipping y_hat=129\n","Skipping y_hat=102\n","Skipping y_hat=100\n","Skipping y_hat=218\n","Skipping y_hat=208\n","Skipping y_hat=258\n","Skipping y_hat=281\n","Skipping y_hat=306\n","Skipping y_hat=319\n","Skipping y_hat=339\n","Skipping y_hat=460\n","Skipping y_hat=331\n","Skipping y_hat=450\n","Skipping y_hat=491\n","Skipping y_hat=494\n","Skipping y_hat=416\n","Skipping y_hat=460\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=518\n","Skipping y_hat=555\n","Skipping y_hat=594\n","Skipping y_hat=569\n","Skipping y_hat=588\n","Skipping y_hat=596\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=665\n","Skipping y_hat=718\n","Skipping y_hat=711\n","Skipping y_hat=729\n","Skipping y_hat=705\n","Skipping y_hat=775\n","Skipping y_hat=757\n","Skipping y_hat=751\n","Skipping y_hat=788\n","Skipping y_hat=897\n","Skipping y_hat=832\n","Skipping y_hat=850\n","Skipping y_hat=858\n","Skipping y_hat=888\n","Skipping y_hat=881\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=988\n","Skipping y_hat=1001\n","  4% 3/80 [00:00<00:02, 27.03it/s]Skipping y_hat=608\n","  9% 7/80 [00:00<00:02, 30.69it/s]Skipping y_hat=506\n","Skipping y_hat=110\n","Skipping y_hat=163\n","Skipping y_hat=159\n","Skipping y_hat=209\n","Skipping y_hat=140\n","Skipping y_hat=289\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=291\n","Skipping y_hat=261\n","Skipping y_hat=396\n","Skipping y_hat=299\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=330\n","Skipping y_hat=211\n","Skipping y_hat=380\n","Skipping y_hat=300\n","Skipping y_hat=348\n","Skipping y_hat=205\n","Skipping y_hat=200\n","Skipping y_hat=321\n","Skipping y_hat=419\n","Skipping y_hat=340\n","Skipping y_hat=362\n","Skipping y_hat=301\n","Skipping y_hat=372\n","Skipping y_hat=493\n","Skipping y_hat=447\n","Skipping y_hat=409\n","Skipping y_hat=345\n","Skipping y_hat=469\n"," 14% 11/80 [00:00<00:02, 31.46it/s]Skipping y_hat=409\n","Skipping y_hat=407\n","Skipping y_hat=451\n","Skipping y_hat=405\n","Skipping y_hat=449\n","Skipping y_hat=403\n","Skipping y_hat=404\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=405\n","Skipping y_hat=514\n","Skipping y_hat=492\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=530\n","Skipping y_hat=514\n","Skipping y_hat=513\n","Skipping y_hat=579\n","Skipping y_hat=696\n","Skipping y_hat=694\n","Skipping y_hat=616\n","Skipping y_hat=696\n","Skipping y_hat=661\n","Skipping y_hat=699\n","Skipping y_hat=684\n","Skipping y_hat=616\n","Skipping y_hat=719\n","Skipping y_hat=775\n","Skipping y_hat=797\n","Skipping y_hat=719\n","Skipping y_hat=793\n","Skipping y_hat=795\n","Skipping y_hat=739\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=779\n","Skipping y_hat=796\n","Skipping y_hat=749\n","Skipping y_hat=708\n","Skipping y_hat=806\n","Skipping y_hat=709\n","Skipping y_hat=799\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=707\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=840\n","Skipping y_hat=841\n","Skipping y_hat=880\n","Skipping y_hat=889\n","Skipping y_hat=809\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=819\n","Skipping y_hat=996\n","Skipping y_hat=899\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=845\n","Skipping y_hat=855\n","Skipping y_hat=988\n","Skipping y_hat=906\n","Skipping y_hat=195\n","Skipping y_hat=1029\n","Skipping y_hat=999\n","Skipping y_hat=1018\n","Skipping y_hat=1031\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=196\n","Skipping y_hat=1021\n","Skipping y_hat=999\n"," 19% 15/80 [00:00<00:02, 31.58it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","Skipping y_hat=1092\n","Skipping y_hat=1032\n","Skipping y_hat=1067\n","100% 80/80 [00:02<00:00, 34.43it/s]\n","accuracy of 9900 examples: 9337/9900 (94.31313131313132%)\n","\n","Test Results:\n","test_10000_stripped: 94.31%\n","\n","iter 4300: train loss 0.6750, val loss 1.8312\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=22\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=38\n","Skipping y_hat=37\n","Skipping y_hat=47\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=656\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=632\n","Skipping y_hat=704\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=861\n","Skipping y_hat=972\n","Skipping y_hat=301\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=455\n","Skipping y_hat=466\n","Skipping y_hat=480\n","Skipping y_hat=523\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=842\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=380\n","Skipping y_hat=481\n","Skipping y_hat=470\n","Skipping y_hat=555\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1003\n","Skipping y_hat=165\n","Skipping y_hat=127\n","Skipping y_hat=579\n","Skipping y_hat=977\n","Skipping y_hat=249\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=621\n","Skipping y_hat=684\n","Skipping y_hat=677\n","Skipping y_hat=622\n","Skipping y_hat=787\n","Skipping y_hat=1003\n","Skipping y_hat=1005\n","Skipping y_hat=173\n","Skipping y_hat=128\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=557\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1001\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=603\n","Skipping y_hat=796\n","Skipping y_hat=810\n","Skipping y_hat=131\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=248\n","Skipping y_hat=582\n","Skipping y_hat=526\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=779\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=79\n","Skipping y_hat=80\n","Skipping y_hat=149\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=101\n","Skipping y_hat=167\n","Skipping y_hat=123\n","Skipping y_hat=160\n","Skipping y_hat=160\n","Skipping y_hat=124\n","Skipping y_hat=109\n","Skipping y_hat=162\n","Skipping y_hat=160\n","Skipping y_hat=218\n","Skipping y_hat=297\n","Skipping y_hat=258\n","Skipping y_hat=281\n","Skipping y_hat=305\n","Skipping y_hat=339\n","Skipping y_hat=432\n","Skipping y_hat=332\n","Skipping y_hat=361\n","Skipping y_hat=460\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=416\n","Skipping y_hat=460\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=517\n","Skipping y_hat=531\n","Skipping y_hat=555\n","Skipping y_hat=550\n","Skipping y_hat=569\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=665\n","Skipping y_hat=618\n","Skipping y_hat=711\n","Skipping y_hat=729\n","Skipping y_hat=705\n","Skipping y_hat=775\n","Skipping y_hat=757\n","Skipping y_hat=751\n","Skipping y_hat=786\n","Skipping y_hat=897\n","Skipping y_hat=812\n","Skipping y_hat=857\n","Skipping y_hat=958\n","Skipping y_hat=888\n","Skipping y_hat=881\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=988\n","Skipping y_hat=1001\n","  4% 3/80 [00:00<00:02, 26.12it/s]Skipping y_hat=608\n","  9% 7/80 [00:00<00:02, 30.85it/s]Skipping y_hat=506\n","Skipping y_hat=104\n","Skipping y_hat=403\n","Skipping y_hat=734\n","Skipping y_hat=159\n","Skipping y_hat=209\n","Skipping y_hat=140\n","Skipping y_hat=114\n","Skipping y_hat=238\n","Skipping y_hat=292\n","Skipping y_hat=295\n","Skipping y_hat=396\n","Skipping y_hat=392\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=332\n","Skipping y_hat=211\n","Skipping y_hat=214\n","Skipping y_hat=300\n","Skipping y_hat=207\n","Skipping y_hat=200\n","Skipping y_hat=306\n","Skipping y_hat=362\n","Skipping y_hat=401\n","Skipping y_hat=301\n","Skipping y_hat=421\n","Skipping y_hat=457\n","Skipping y_hat=312\n","Skipping y_hat=329\n","Skipping y_hat=447\n","Skipping y_hat=409\n"," 14% 11/80 [00:00<00:02, 31.36it/s]Skipping y_hat=409\n","Skipping y_hat=407\n","Skipping y_hat=405\n","Skipping y_hat=449\n","Skipping y_hat=403\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=405\n","Skipping y_hat=521\n","Skipping y_hat=415\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=530\n","Skipping y_hat=514\n","Skipping y_hat=509\n","Skipping y_hat=563\n","Skipping y_hat=579\n","Skipping y_hat=629\n","Skipping y_hat=616\n","Skipping y_hat=696\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=623\n","Skipping y_hat=775\n","Skipping y_hat=795\n","Skipping y_hat=751\n","Skipping y_hat=799\n","Skipping y_hat=750\n","Skipping y_hat=708\n","Skipping y_hat=806\n","Skipping y_hat=709\n","Skipping y_hat=809\n","Skipping y_hat=848\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=916\n","Skipping y_hat=916\n","Skipping y_hat=880\n","Skipping y_hat=869\n","Skipping y_hat=889\n","Skipping y_hat=801\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=998\n","Skipping y_hat=920\n","Skipping y_hat=806\n","Skipping y_hat=977\n","Skipping y_hat=988\n","Skipping y_hat=189\n","Skipping y_hat=1029\n","Skipping y_hat=989\n","Skipping y_hat=981\n","Skipping y_hat=1017\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=196\n","Skipping y_hat=1021\n","Skipping y_hat=196\n"," 19% 15/80 [00:00<00:02, 31.08it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","Skipping y_hat=1092\n","Skipping y_hat=1021\n","100% 80/80 [00:02<00:00, 33.87it/s]\n","accuracy of 9900 examples: 9324/9900 (94.18181818181817%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 39.66it/s]\n","accuracy of 10000 examples: 9984/10000 (99.83999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=20\n","Skipping y_hat=38\n","Skipping y_hat=37\n","Skipping y_hat=47\n","Skipping y_hat=49\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=781\n","Skipping y_hat=528\n","Skipping y_hat=656\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=632\n","Skipping y_hat=784\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=841\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=202\n","Skipping y_hat=336\n","Skipping y_hat=455\n","Skipping y_hat=466\n","Skipping y_hat=480\n","Skipping y_hat=573\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=842\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=481\n","Skipping y_hat=480\n","Skipping y_hat=525\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1001\n","Skipping y_hat=165\n","Skipping y_hat=127\n","Skipping y_hat=579\n","Skipping y_hat=947\n","Skipping y_hat=249\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=621\n","Skipping y_hat=684\n","Skipping y_hat=627\n","Skipping y_hat=622\n","Skipping y_hat=717\n","Skipping y_hat=1003\n","Skipping y_hat=1005\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=746\n","Skipping y_hat=846\n","Skipping y_hat=1001\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=603\n","Skipping y_hat=746\n","Skipping y_hat=810\n","Skipping y_hat=131\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=248\n","Skipping y_hat=582\n","Skipping y_hat=526\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=779\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=79\n","Skipping y_hat=81\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=159\n","Skipping y_hat=101\n","Skipping y_hat=151\n","Skipping y_hat=113\n","Skipping y_hat=161\n","Skipping y_hat=160\n","Skipping y_hat=123\n","Skipping y_hat=129\n","Skipping y_hat=162\n","Skipping y_hat=160\n","Skipping y_hat=218\n","Skipping y_hat=294\n","Skipping y_hat=258\n","Skipping y_hat=281\n","Skipping y_hat=381\n","Skipping y_hat=206\n","Skipping y_hat=339\n","Skipping y_hat=462\n","Skipping y_hat=332\n","Skipping y_hat=361\n","Skipping y_hat=450\n","Skipping y_hat=491\n","Skipping y_hat=494\n","Skipping y_hat=416\n","Skipping y_hat=450\n","Skipping y_hat=466\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=517\n","Skipping y_hat=555\n","Skipping y_hat=569\n","Skipping y_hat=568\n","Skipping y_hat=576\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=645\n","Skipping y_hat=655\n","Skipping y_hat=718\n","Skipping y_hat=711\n","Skipping y_hat=729\n","Skipping y_hat=705\n","Skipping y_hat=775\n","Skipping y_hat=758\n","Skipping y_hat=751\n","Skipping y_hat=788\n","Skipping y_hat=870\n","Skipping y_hat=832\n","Skipping y_hat=858\n","Skipping y_hat=958\n","Skipping y_hat=881\n","Skipping y_hat=891\n","Skipping y_hat=100\n","Skipping y_hat=108\n","Skipping y_hat=100\n","  4% 3/80 [00:00<00:02, 28.20it/s]Skipping y_hat=608\n","  9% 7/80 [00:00<00:02, 32.30it/s]Skipping y_hat=506\n","Skipping y_hat=403\n","Skipping y_hat=110\n","Skipping y_hat=101\n","Skipping y_hat=159\n","Skipping y_hat=209\n","Skipping y_hat=140\n","Skipping y_hat=107\n","Skipping y_hat=292\n","Skipping y_hat=295\n","Skipping y_hat=396\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=330\n","Skipping y_hat=211\n","Skipping y_hat=214\n","Skipping y_hat=380\n","Skipping y_hat=300\n","Skipping y_hat=399\n","Skipping y_hat=305\n","Skipping y_hat=328\n","Skipping y_hat=300\n","Skipping y_hat=306\n","Skipping y_hat=417\n","Skipping y_hat=362\n","Skipping y_hat=301\n","Skipping y_hat=421\n","Skipping y_hat=493\n","Skipping y_hat=457\n","Skipping y_hat=312\n","Skipping y_hat=447\n","Skipping y_hat=409\n"," 14% 11/80 [00:00<00:02, 32.46it/s]Skipping y_hat=409\n","Skipping y_hat=401\n","Skipping y_hat=405\n","Skipping y_hat=449\n","Skipping y_hat=403\n","Skipping y_hat=459\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=405\n","Skipping y_hat=415\n","Skipping y_hat=514\n","Skipping y_hat=403\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=519\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=51=\n","Skipping y_hat=579\n","Skipping y_hat=695\n","Skipping y_hat=608\n","Skipping y_hat=616\n","Skipping y_hat=696\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=765\n","Skipping y_hat=717\n","Skipping y_hat=671\n","Skipping y_hat=795\n","Skipping y_hat=751\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=741\n","Skipping y_hat=750\n","Skipping y_hat=706\n","Skipping y_hat=709\n","Skipping y_hat=781\n","Skipping y_hat=809\n","Skipping y_hat=848\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=916\n","Skipping y_hat=882\n","Skipping y_hat=801\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=998\n","Skipping y_hat=880\n","Skipping y_hat=988\n","Skipping y_hat=932\n","Skipping y_hat=188\n","Skipping y_hat=189\n","Skipping y_hat=1029\n","Skipping y_hat=989\n","Skipping y_hat=1018\n","Skipping y_hat=1031\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=196\n","Skipping y_hat=1021\n","Skipping y_hat=999\n","Skipping y_hat=196\n"," 19% 15/80 [00:00<00:02, 31.97it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","100% 80/80 [00:02<00:00, 34.30it/s]\n","accuracy of 9900 examples: 9336/9900 (94.3030303030303%)\n","\n","Test Results:\n","test_10000_stripped: 94.30%\n","\n","iter 4400: train loss 0.6662, val loss 1.8444\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=20\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=47\n","Skipping y_hat=49\n","Skipping y_hat=92\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=137\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=704\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=972\n","Skipping y_hat=301\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=475\n","Skipping y_hat=466\n","Skipping y_hat=480\n","Skipping y_hat=586\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=892\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=360\n","Skipping y_hat=481\n","Skipping y_hat=470\n","Skipping y_hat=525\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1001\n","Skipping y_hat=165\n","Skipping y_hat=137\n","Skipping y_hat=579\n","Skipping y_hat=977\n","Skipping y_hat=239\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=621\n","Skipping y_hat=684\n","Skipping y_hat=622\n","Skipping y_hat=717\n","Skipping y_hat=1003\n","Skipping y_hat=1005\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1004\n","Skipping y_hat=1004\n","Skipping y_hat=187\n","Skipping y_hat=486\n","Skipping y_hat=673\n","Skipping y_hat=746\n","Skipping y_hat=810\n","Skipping y_hat=136\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=308\n","Skipping y_hat=582\n","Skipping y_hat=626\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=729\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=119\n","Skipping y_hat=79\n","Skipping y_hat=149\n","Skipping y_hat=120\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=109\n","Skipping y_hat=161\n","Skipping y_hat=133\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=164\n","Skipping y_hat=129\n","Skipping y_hat=214\n","Skipping y_hat=260\n","Skipping y_hat=218\n","Skipping y_hat=294\n","Skipping y_hat=258\n","Skipping y_hat=381\n","Skipping y_hat=304\n","Skipping y_hat=315\n","Skipping y_hat=319\n","Skipping y_hat=339\n","Skipping y_hat=460\n","Skipping y_hat=330\n","Skipping y_hat=361\n","Skipping y_hat=350\n","Skipping y_hat=491\n","Skipping y_hat=494\n","Skipping y_hat=416\n","Skipping y_hat=466\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=517\n","Skipping y_hat=559\n","Skipping y_hat=550\n","Skipping y_hat=569\n","Skipping y_hat=579\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=665\n","Skipping y_hat=719\n","Skipping y_hat=72\n","Skipping y_hat=729\n","Skipping y_hat=704\n","Skipping y_hat=785\n","Skipping y_hat=758\n","Skipping y_hat=751\n","Skipping y_hat=788\n","Skipping y_hat=776\n","Skipping y_hat=877\n","Skipping y_hat=832\n","Skipping y_hat=858\n","Skipping y_hat=948\n","Skipping y_hat=881\n","Skipping y_hat=891\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=407\n","Skipping y_hat=608\n","  5% 4/80 [00:00<00:02, 31.06it/s]Skipping y_hat=603\n"," 10% 8/80 [00:00<00:02, 33.05it/s]Skipping y_hat=506\n","Skipping y_hat=403\n","Skipping y_hat=806\n","Skipping y_hat=139\n","Skipping y_hat=140\n","Skipping y_hat=220\n","Skipping y_hat=199\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=291\n","Skipping y_hat=260\n","Skipping y_hat=397\n","Skipping y_hat=299\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=211\n","Skipping y_hat=217\n","Skipping y_hat=280\n","Skipping y_hat=300\n","Skipping y_hat=205\n","Skipping y_hat=204\n","Skipping y_hat=306\n","Skipping y_hat=321\n","Skipping y_hat=417\n","Skipping y_hat=340\n","Skipping y_hat=362\n","Skipping y_hat=491\n","Skipping y_hat=301\n","Skipping y_hat=421\n","Skipping y_hat=493\n","Skipping y_hat=441\n","Skipping y_hat=311\n","Skipping y_hat=448\n","Skipping y_hat=309\n","Skipping y_hat=496\n","Skipping y_hat=400\n","Skipping y_hat=405\n","Skipping y_hat=449\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=541\n","Skipping y_hat=405\n","Skipping y_hat=515\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=519\n","Skipping y_hat=509\n"," 15% 12/80 [00:00<00:02, 32.75it/s]Skipping y_hat=579\n","Skipping y_hat=695\n","Skipping y_hat=616\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=680\n","Skipping y_hat=775\n","Skipping y_hat=717\n","Skipping y_hat=720\n","Skipping y_hat=751\n","Skipping y_hat=798\n","Skipping y_hat=793\n","Skipping y_hat=779\n","Skipping y_hat=708\n","Skipping y_hat=709\n","Skipping y_hat=781\n","Skipping y_hat=895\n","Skipping y_hat=898\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=840\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=916\n","Skipping y_hat=916\n","Skipping y_hat=801\n","Skipping y_hat=909\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=999\n","Skipping y_hat=998\n","Skipping y_hat=899\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=988\n","Skipping y_hat=1029\n","Skipping y_hat=1018\n","Skipping y_hat=195\n","Skipping y_hat=1021\n","Skipping y_hat=1050\n","Skipping y_hat=196\n","Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1070\n","Skipping y_hat=900\n","Skipping y_hat=1092\n","Skipping y_hat=1021\n","100% 80/80 [00:02<00:00, 34.21it/s]\n","accuracy of 9900 examples: 9324/9900 (94.18181818181817%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 39.23it/s]\n","accuracy of 10000 examples: 9983/10000 (99.83%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=38\n","Skipping y_hat=37\n","Skipping y_hat=47\n","Skipping y_hat=4\n","Skipping y_hat=92\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=137\n","Skipping y_hat=129\n","Skipping y_hat=517\n","Skipping y_hat=672\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=841\n","Skipping y_hat=972\n","Skipping y_hat=301\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=455\n","Skipping y_hat=480\n","Skipping y_hat=523\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=802\n","Skipping y_hat=218\n","Skipping y_hat=337\n","Skipping y_hat=360\n","Skipping y_hat=481\n","Skipping y_hat=480\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1003\n","Skipping y_hat=165\n","Skipping y_hat=127\n","Skipping y_hat=579\n","Skipping y_hat=947\n","Skipping y_hat=239\n","Skipping y_hat=347\n","Skipping y_hat=578\n","Skipping y_hat=681\n","Skipping y_hat=674\n","Skipping y_hat=627\n","Skipping y_hat=622\n","Skipping y_hat=787\n","Skipping y_hat=1003\n","Skipping y_hat=1005\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1004\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=570\n","Skipping y_hat=673\n","Skipping y_hat=796\n","Skipping y_hat=136\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=348\n","Skipping y_hat=582\n","Skipping y_hat=626\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=779\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=149\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=101\n","Skipping y_hat=162\n","Skipping y_hat=113\n","Skipping y_hat=161\n","Skipping y_hat=160\n","Skipping y_hat=134\n","Skipping y_hat=169\n","Skipping y_hat=204\n","Skipping y_hat=260\n","Skipping y_hat=218\n","Skipping y_hat=297\n","Skipping y_hat=258\n","Skipping y_hat=381\n","Skipping y_hat=306\n","Skipping y_hat=339\n","Skipping y_hat=462\n","Skipping y_hat=330\n","Skipping y_hat=361\n","Skipping y_hat=450\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=416\n","Skipping y_hat=466\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=531\n","Skipping y_hat=559\n","Skipping y_hat=550\n","Skipping y_hat=569\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=645\n","Skipping y_hat=655\n","Skipping y_hat=719\n","Skipping y_hat=721\n","Skipping y_hat=739\n","Skipping y_hat=703\n","Skipping y_hat=785\n","Skipping y_hat=757\n","Skipping y_hat=788\n","Skipping y_hat=877\n","Skipping y_hat=832\n","Skipping y_hat=851\n","Skipping y_hat=958\n","Skipping y_hat=881\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=100\n","Skipping y_hat=608\n","  5% 4/80 [00:00<00:02, 31.55it/s]Skipping y_hat=603\n"," 10% 8/80 [00:00<00:02, 33.32it/s]Skipping y_hat=506\n","Skipping y_hat=403\n","Skipping y_hat=806\n","Skipping y_hat=139\n","Skipping y_hat=162\n","Skipping y_hat=159\n","Skipping y_hat=140\n","Skipping y_hat=220\n","Skipping y_hat=199\n","Skipping y_hat=220\n","Skipping y_hat=291\n","Skipping y_hat=397\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=211\n","Skipping y_hat=214\n","Skipping y_hat=280\n","Skipping y_hat=300\n","Skipping y_hat=205\n","Skipping y_hat=313\n","Skipping y_hat=204\n","Skipping y_hat=306\n","Skipping y_hat=321\n","Skipping y_hat=417\n","Skipping y_hat=340\n","Skipping y_hat=361\n","Skipping y_hat=301\n","Skipping y_hat=421\n","Skipping y_hat=493\n","Skipping y_hat=441\n","Skipping y_hat=329\n","Skipping y_hat=447\n","Skipping y_hat=309\n","Skipping y_hat=409\n","Skipping y_hat=401\n","Skipping y_hat=579\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=541\n","Skipping y_hat=502\n","Skipping y_hat=515\n","Skipping y_hat=404\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=514\n"," 15% 12/80 [00:00<00:02, 32.70it/s]Skipping y_hat=579\n","Skipping y_hat=695\n","Skipping y_hat=616\n","Skipping y_hat=696\n","Skipping y_hat=616\n","Skipping y_hat=661\n","Skipping y_hat=650\n","Skipping y_hat=699\n","Skipping y_hat=680\n","Skipping y_hat=671\n","Skipping y_hat=720\n","Skipping y_hat=751\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=749\n","Skipping y_hat=708\n","Skipping y_hat=709\n","Skipping y_hat=781\n","Skipping y_hat=809\n","Skipping y_hat=895\n","Skipping y_hat=820\n","Skipping y_hat=898\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=840\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=869\n","Skipping y_hat=801\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=999\n","Skipping y_hat=998\n","Skipping y_hat=889\n","Skipping y_hat=899\n","Skipping y_hat=920\n","Skipping y_hat=870\n","Skipping y_hat=920\n","Skipping y_hat=988\n","Skipping y_hat=932\n","Skipping y_hat=189\n","Skipping y_hat=1029\n","Skipping y_hat=1018\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=196\n","Skipping y_hat=1021\n","Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","Skipping y_hat=1093\n","Skipping y_hat=1021\n","100% 80/80 [00:02<00:00, 34.26it/s]\n","accuracy of 9900 examples: 9322/9900 (94.16161616161615%)\n","\n","Test Results:\n","test_10000_stripped: 94.16%\n","\n","iter 4500: train loss 0.6672, val loss 1.8129\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=20\n","Skipping y_hat=38\n","Skipping y_hat=37\n","Skipping y_hat=47\n","Skipping y_hat=49\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=139\n","Skipping y_hat=587\n","Skipping y_hat=632\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=971\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=475\n","Skipping y_hat=436\n","Skipping y_hat=480\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=892\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=380\n","Skipping y_hat=481\n","Skipping y_hat=480\n","Skipping y_hat=525\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1003\n","Skipping y_hat=127\n","Skipping y_hat=579\n","Skipping y_hat=917\n","Skipping y_hat=249\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=621\n","Skipping y_hat=684\n","Skipping y_hat=627\n","Skipping y_hat=622\n","Skipping y_hat=717\n","Skipping y_hat=1003\n","Skipping y_hat=975\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1002\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=603\n","Skipping y_hat=796\n","Skipping y_hat=840\n","Skipping y_hat=131\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=348\n","Skipping y_hat=582\n","Skipping y_hat=526\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=779\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=119\n","Skipping y_hat=79\n","Skipping y_hat=149\n","Skipping y_hat=111\n","Skipping y_hat=129\n","Skipping y_hat=154\n","Skipping y_hat=101\n","Skipping y_hat=166\n","Skipping y_hat=133\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=124\n","Skipping y_hat=169\n","Skipping y_hat=206\n","Skipping y_hat=260\n","Skipping y_hat=218\n","Skipping y_hat=298\n","Skipping y_hat=258\n","Skipping y_hat=381\n","Skipping y_hat=306\n","Skipping y_hat=319\n","Skipping y_hat=339\n","Skipping y_hat=462\n","Skipping y_hat=330\n","Skipping y_hat=450\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=416\n","Skipping y_hat=460\n","Skipping y_hat=466\n","Skipping y_hat=516\n","Skipping y_hat=519\n","Skipping y_hat=531\n","Skipping y_hat=559\n","Skipping y_hat=594\n","Skipping y_hat=569\n","Skipping y_hat=576\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=645\n","Skipping y_hat=655\n","Skipping y_hat=689\n","Skipping y_hat=719\n","Skipping y_hat=721\n","Skipping y_hat=729\n","Skipping y_hat=705\n","Skipping y_hat=715\n","Skipping y_hat=757\n","Skipping y_hat=748\n","Skipping y_hat=897\n","Skipping y_hat=832\n","Skipping y_hat=850\n","Skipping y_hat=948\n","Skipping y_hat=888\n","Skipping y_hat=941\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=1001\n"," 10% 8/80 [00:00<00:02, 33.80it/s]Skipping y_hat=506\n","Skipping y_hat=139\n","Skipping y_hat=292\n","Skipping y_hat=162\n","Skipping y_hat=159\n","Skipping y_hat=209\n","Skipping y_hat=140\n","Skipping y_hat=299\n","Skipping y_hat=107\n","Skipping y_hat=291\n","Skipping y_hat=260\n","Skipping y_hat=295\n","Skipping y_hat=260\n","Skipping y_hat=396\n","Skipping y_hat=260\n","Skipping y_hat=299\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=211\n","Skipping y_hat=280\n","Skipping y_hat=300\n","Skipping y_hat=326\n","Skipping y_hat=399\n","Skipping y_hat=205\n","Skipping y_hat=200\n","Skipping y_hat=306\n","Skipping y_hat=417\n","Skipping y_hat=340\n","Skipping y_hat=362\n","Skipping y_hat=401\n","Skipping y_hat=301\n","Skipping y_hat=493\n","Skipping y_hat=441\n","Skipping y_hat=447\n","Skipping y_hat=497\n","Skipping y_hat=407\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=541\n","Skipping y_hat=500\n","Skipping y_hat=513\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=530\n","Skipping y_hat=590\n","Skipping y_hat=514\n","Skipping y_hat=511\n"," 15% 12/80 [00:00<00:02, 33.59it/s]Skipping y_hat=579\n","Skipping y_hat=695\n","Skipping y_hat=616\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=650\n","Skipping y_hat=684\n","Skipping y_hat=719\n","Skipping y_hat=775\n","Skipping y_hat=717\n","Skipping y_hat=671\n","Skipping y_hat=751\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=796\n","Skipping y_hat=706\n","Skipping y_hat=740\n","Skipping y_hat=731\n","Skipping y_hat=799\n","Skipping y_hat=809\n","Skipping y_hat=705\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=880\n","Skipping y_hat=889\n","Skipping y_hat=801\n","Skipping y_hat=909\n","Skipping y_hat=996\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=999\n","Skipping y_hat=998\n","Skipping y_hat=889\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=988\n","Skipping y_hat=933\n","Skipping y_hat=900\n","Skipping y_hat=1029\n","Skipping y_hat=999\n","Skipping y_hat=1018\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=1021\n","Skipping y_hat=1050\n","Skipping y_hat=999\n","Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","Skipping y_hat=1093\n","Skipping y_hat=1067\n","100% 80/80 [00:02<00:00, 34.67it/s]\n","accuracy of 9900 examples: 9334/9900 (94.28282828282828%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 39.08it/s]\n","accuracy of 10000 examples: 9978/10000 (99.78%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=20\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=47\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=632\n","Skipping y_hat=849\n","Skipping y_hat=860\n","Skipping y_hat=971\n","Skipping y_hat=972\n","Skipping y_hat=301\n","Skipping y_hat=302\n","Skipping y_hat=386\n","Skipping y_hat=465\n","Skipping y_hat=466\n","Skipping y_hat=480\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=802\n","Skipping y_hat=218\n","Skipping y_hat=337\n","Skipping y_hat=380\n","Skipping y_hat=481\n","Skipping y_hat=480\n","Skipping y_hat=585\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1003\n","Skipping y_hat=137\n","Skipping y_hat=579\n","Skipping y_hat=947\n","Skipping y_hat=239\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=601\n","Skipping y_hat=684\n","Skipping y_hat=657\n","Skipping y_hat=622\n","Skipping y_hat=717\n","Skipping y_hat=1003\n","Skipping y_hat=985\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1002\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=673\n","Skipping y_hat=796\n","Skipping y_hat=810\n","Skipping y_hat=132\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=348\n","Skipping y_hat=482\n","Skipping y_hat=526\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=749\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=119\n","Skipping y_hat=79\n","Skipping y_hat=149\n","Skipping y_hat=111\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=101\n","Skipping y_hat=166\n","Skipping y_hat=113\n","Skipping y_hat=161\n","Skipping y_hat=160\n","Skipping y_hat=109\n","Skipping y_hat=205\n","Skipping y_hat=161\n","Skipping y_hat=218\n","Skipping y_hat=298\n","Skipping y_hat=259\n","Skipping y_hat=306\n","Skipping y_hat=319\n","Skipping y_hat=339\n","Skipping y_hat=462\n","Skipping y_hat=330\n","Skipping y_hat=460\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=419\n","Skipping y_hat=466\n","Skipping y_hat=516\n","Skipping y_hat=519\n","Skipping y_hat=531\n","Skipping y_hat=559\n","Skipping y_hat=550\n","Skipping y_hat=569\n","Skipping y_hat=579\n","Skipping y_hat=568\n","Skipping y_hat=576\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=655\n","Skipping y_hat=619\n","Skipping y_hat=721\n","Skipping y_hat=705\n","Skipping y_hat=775\n","Skipping y_hat=757\n","Skipping y_hat=786\n","Skipping y_hat=877\n","Skipping y_hat=832\n","Skipping y_hat=857\n","Skipping y_hat=848\n","Skipping y_hat=888\n","Skipping y_hat=941\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=988\n","Skipping y_hat=1001\n","  9% 7/80 [00:00<00:02, 32.79it/s]Skipping y_hat=506\n","Skipping y_hat=139\n","Skipping y_hat=208\n","Skipping y_hat=140\n","Skipping y_hat=238\n","Skipping y_hat=292\n","Skipping y_hat=260\n","Skipping y_hat=396\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=211\n","Skipping y_hat=214\n","Skipping y_hat=300\n","Skipping y_hat=399\n","Skipping y_hat=348\n","Skipping y_hat=205\n","Skipping y_hat=328\n","Skipping y_hat=200\n","Skipping y_hat=306\n","Skipping y_hat=417\n","Skipping y_hat=362\n","Skipping y_hat=405\n","Skipping y_hat=491\n","Skipping y_hat=301\n","Skipping y_hat=493\n","Skipping y_hat=441\n","Skipping y_hat=447\n","Skipping y_hat=309\n"," 14% 11/80 [00:00<00:02, 32.93it/s]Skipping y_hat=496\n","Skipping y_hat=400\n","Skipping y_hat=405\n","Skipping y_hat=402\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=541\n","Skipping y_hat=405\n","Skipping y_hat=500\n","Skipping y_hat=515\n","Skipping y_hat=404\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=514\n","Skipping y_hat=513\n","Skipping y_hat=505\n","Skipping y_hat=695\n","Skipping y_hat=616\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=699\n","Skipping y_hat=680\n","Skipping y_hat=719\n","Skipping y_hat=717\n","Skipping y_hat=720\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=759\n","Skipping y_hat=706\n","Skipping y_hat=740\n","Skipping y_hat=709\n","Skipping y_hat=799\n","Skipping y_hat=809\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=880\n","Skipping y_hat=869\n","Skipping y_hat=801\n","Skipping y_hat=909\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=999\n","Skipping y_hat=996\n","Skipping y_hat=889\n","Skipping y_hat=899\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=988\n","Skipping y_hat=1029\n","Skipping y_hat=1018\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=196\n","Skipping y_hat=1021\n","Skipping y_hat=1051\n","Skipping y_hat=999\n","Skipping y_hat=196\n"," 19% 15/80 [00:00<00:02, 32.08it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","100% 80/80 [00:02<00:00, 34.42it/s]\n","accuracy of 9900 examples: 9332/9900 (94.26262626262626%)\n","\n","Test Results:\n","test_10000_stripped: 94.26%\n","\n","iter 4600: train loss 0.6686, val loss 1.8357\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=20\n","Skipping y_hat=37\n","Skipping y_hat=38\n","Skipping y_hat=47\n","Skipping y_hat=49\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=704\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=941\n","Skipping y_hat=972\n","Skipping y_hat=301\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=465\n","Skipping y_hat=476\n","Skipping y_hat=480\n","Skipping y_hat=526\n","Skipping y_hat=618\n","Skipping y_hat=716\n","Skipping y_hat=892\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=380\n","Skipping y_hat=481\n","Skipping y_hat=480\n","Skipping y_hat=525\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1003\n","Skipping y_hat=127\n","Skipping y_hat=579\n","Skipping y_hat=957\n","Skipping y_hat=239\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=621\n","Skipping y_hat=684\n","Skipping y_hat=677\n","Skipping y_hat=622\n","Skipping y_hat=787\n","Skipping y_hat=1003\n","Skipping y_hat=1005\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1004\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=673\n","Skipping y_hat=796\n","Skipping y_hat=810\n","Skipping y_hat=130\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=348\n","Skipping y_hat=582\n","Skipping y_hat=626\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=779\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=79\n","Skipping y_hat=149\n","Skipping y_hat=112\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=101\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=124\n","Skipping y_hat=109\n","Skipping y_hat=102\n","Skipping y_hat=260\n","Skipping y_hat=218\n","Skipping y_hat=298\n","Skipping y_hat=258\n","Skipping y_hat=291\n","Skipping y_hat=306\n","Skipping y_hat=319\n","Skipping y_hat=339\n","Skipping y_hat=462\n","Skipping y_hat=430\n","Skipping y_hat=361\n","Skipping y_hat=460\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=416\n","Skipping y_hat=451\n","Skipping y_hat=466\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=559\n","Skipping y_hat=551\n","Skipping y_hat=569\n","Skipping y_hat=568\n","Skipping y_hat=576\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=665\n","Skipping y_hat=719\n","Skipping y_hat=726\n","Skipping y_hat=728\n","Skipping y_hat=705\n","Skipping y_hat=775\n","Skipping y_hat=757\n","Skipping y_hat=751\n","Skipping y_hat=728\n","Skipping y_hat=877\n","Skipping y_hat=812\n","Skipping y_hat=857\n","Skipping y_hat=948\n","Skipping y_hat=888\n","Skipping y_hat=831\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=988\n","Skipping y_hat=1001\n","  9% 7/80 [00:00<00:02, 31.32it/s]Skipping y_hat=506\n","Skipping y_hat=139\n","Skipping y_hat=159\n","Skipping y_hat=108\n","Skipping y_hat=140\n","Skipping y_hat=220\n","Skipping y_hat=291\n","Skipping y_hat=295\n","Skipping y_hat=397\n","Skipping y_hat=299\n","Skipping y_hat=305\n","Skipping y_hat=324\n","Skipping y_hat=205\n","Skipping y_hat=330\n","Skipping y_hat=211\n","Skipping y_hat=214\n","Skipping y_hat=280\n","Skipping y_hat=300\n","Skipping y_hat=329\n","Skipping y_hat=205\n","Skipping y_hat=328\n","Skipping y_hat=200\n","Skipping y_hat=306\n","Skipping y_hat=321\n","Skipping y_hat=407\n","Skipping y_hat=362\n","Skipping y_hat=301\n","Skipping y_hat=493\n","Skipping y_hat=447\n","Skipping y_hat=300\n","Skipping y_hat=409\n"," 14% 11/80 [00:00<00:02, 31.87it/s]Skipping y_hat=420\n","Skipping y_hat=426\n","Skipping y_hat=499\n","Skipping y_hat=401\n","Skipping y_hat=490\n","Skipping y_hat=403\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=500\n","Skipping y_hat=514\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=530\n","Skipping y_hat=514\n","Skipping y_hat=511\n","Skipping y_hat=579\n","Skipping y_hat=695\n","Skipping y_hat=616\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=623\n","Skipping y_hat=661\n","Skipping y_hat=640\n","Skipping y_hat=719\n","Skipping y_hat=765\n","Skipping y_hat=775\n","Skipping y_hat=751\n","Skipping y_hat=798\n","Skipping y_hat=793\n","Skipping y_hat=779\n","Skipping y_hat=750\n","Skipping y_hat=708\n","Skipping y_hat=759\n","Skipping y_hat=809\n","Skipping y_hat=709\n","Skipping y_hat=799\n","Skipping y_hat=709\n","Skipping y_hat=819\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=916\n","Skipping y_hat=869\n","Skipping y_hat=801\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=996\n","Skipping y_hat=889\n","Skipping y_hat=988\n","Skipping y_hat=932\n","Skipping y_hat=1001\n","Skipping y_hat=900\n","Skipping y_hat=1029\n","Skipping y_hat=981\n","Skipping y_hat=1022\n","Skipping y_hat=1017\n","Skipping y_hat=1031\n","Skipping y_hat=900\n","Skipping y_hat=986\n","Skipping y_hat=1012\n","Skipping y_hat=1021\n","Skipping y_hat=1051\n","Skipping y_hat=999\n"," 19% 15/80 [00:00<00:02, 31.49it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1070\n","Skipping y_hat=900\n","Skipping y_hat=1051\n","Skipping y_hat=1015\n","100% 80/80 [00:02<00:00, 33.46it/s]\n","accuracy of 9900 examples: 9319/9900 (94.13131313131314%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.79it/s]\n","accuracy of 10000 examples: 9984/10000 (99.83999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=38\n","Skipping y_hat=37\n","Skipping y_hat=47\n","Skipping y_hat=49\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=784\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=941\n","Skipping y_hat=972\n","Skipping y_hat=301\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=455\n","Skipping y_hat=466\n","Skipping y_hat=480\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=802\n","Skipping y_hat=218\n","Skipping y_hat=337\n","Skipping y_hat=380\n","Skipping y_hat=481\n","Skipping y_hat=480\n","Skipping y_hat=525\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1001\n","Skipping y_hat=127\n","Skipping y_hat=579\n","Skipping y_hat=947\n","Skipping y_hat=239\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=621\n","Skipping y_hat=684\n","Skipping y_hat=657\n","Skipping y_hat=622\n","Skipping y_hat=717\n","Skipping y_hat=1003\n","Skipping y_hat=1005\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=364\n","Skipping y_hat=486\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1002\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=673\n","Skipping y_hat=796\n","Skipping y_hat=810\n","Skipping y_hat=132\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=348\n","Skipping y_hat=582\n","Skipping y_hat=626\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=779\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=149\n","Skipping y_hat=120\n","Skipping y_hat=111\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=101\n","Skipping y_hat=161\n","Skipping y_hat=143\n","Skipping y_hat=161\n","Skipping y_hat=160\n","Skipping y_hat=122\n","Skipping y_hat=109\n","Skipping y_hat=163\n","Skipping y_hat=161\n","Skipping y_hat=218\n","Skipping y_hat=298\n","Skipping y_hat=266\n","Skipping y_hat=258\n","Skipping y_hat=297\n","Skipping y_hat=381\n","Skipping y_hat=306\n","Skipping y_hat=319\n","Skipping y_hat=339\n","Skipping y_hat=364\n","Skipping y_hat=452\n","Skipping y_hat=361\n","Skipping y_hat=460\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=416\n","Skipping y_hat=451\n","Skipping y_hat=466\n","Skipping y_hat=516\n","Skipping y_hat=519\n","Skipping y_hat=518\n","Skipping y_hat=531\n","Skipping y_hat=555\n","Skipping y_hat=594\n","Skipping y_hat=567\n","Skipping y_hat=568\n","Skipping y_hat=576\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=665\n","Skipping y_hat=718\n","Skipping y_hat=726\n","Skipping y_hat=729\n","Skipping y_hat=758\n","Skipping y_hat=757\n","Skipping y_hat=888\n","Skipping y_hat=877\n","Skipping y_hat=832\n","Skipping y_hat=851\n","Skipping y_hat=958\n","Skipping y_hat=888\n","Skipping y_hat=991\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=988\n","Skipping y_hat=1001\n","  9% 7/80 [00:00<00:02, 31.61it/s]Skipping y_hat=506\n","Skipping y_hat=139\n","Skipping y_hat=120\n","Skipping y_hat=161\n","Skipping y_hat=291\n","Skipping y_hat=295\n","Skipping y_hat=228\n","Skipping y_hat=229\n","Skipping y_hat=397\n","Skipping y_hat=305\n","Skipping y_hat=324\n","Skipping y_hat=205\n","Skipping y_hat=211\n","Skipping y_hat=380\n","Skipping y_hat=300\n","Skipping y_hat=329\n","Skipping y_hat=205\n","Skipping y_hat=328\n","Skipping y_hat=204\n","Skipping y_hat=306\n","Skipping y_hat=321\n","Skipping y_hat=417\n","Skipping y_hat=362\n","Skipping y_hat=491\n","Skipping y_hat=301\n","Skipping y_hat=493\n","Skipping y_hat=447\n","Skipping y_hat=311\n","Skipping y_hat=329\n","Skipping y_hat=479\n","Skipping y_hat=300\n","Skipping y_hat=307\n"," 14% 11/80 [00:00<00:02, 32.11it/s]Skipping y_hat=499\n","Skipping y_hat=401\n","Skipping y_hat=449\n","Skipping y_hat=403\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=500\n","Skipping y_hat=515\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=530\n","Skipping y_hat=514\n","Skipping y_hat=511\n","Skipping y_hat=505\n","Skipping y_hat=579\n","Skipping y_hat=695\n","Skipping y_hat=629\n","Skipping y_hat=614\n","Skipping y_hat=607\n","Skipping y_hat=616\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=623\n","Skipping y_hat=699\n","Skipping y_hat=775\n","Skipping y_hat=671\n","Skipping y_hat=724\n","Skipping y_hat=751\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=779\n","Skipping y_hat=750\n","Skipping y_hat=709\n","Skipping y_hat=809\n","Skipping y_hat=819\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=916\n","Skipping y_hat=869\n","Skipping y_hat=871\n","Skipping y_hat=801\n","Skipping y_hat=809\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=998\n","Skipping y_hat=889\n","Skipping y_hat=871\n","Skipping y_hat=988\n","Skipping y_hat=933\n","Skipping y_hat=195\n","Skipping y_hat=1029\n","Skipping y_hat=1017\n","Skipping y_hat=1031\n","Skipping y_hat=900\n","Skipping y_hat=986\n","Skipping y_hat=196\n","Skipping y_hat=1021\n","Skipping y_hat=1051\n","Skipping y_hat=999\n"," 19% 15/80 [00:00<00:02, 31.21it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1070\n","Skipping y_hat=900\n","Skipping y_hat=1082\n","Skipping y_hat=1095\n","Skipping y_hat=1067\n","100% 80/80 [00:02<00:00, 33.60it/s]\n","accuracy of 9900 examples: 9326/9900 (94.2020202020202%)\n","\n","Test Results:\n","test_10000_stripped: 94.20%\n","\n","iter 4700: train loss 0.6666, val loss 1.8375\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=20\n","Skipping y_hat=38\n","Skipping y_hat=37\n","Skipping y_hat=47\n","Skipping y_hat=49\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=137\n","Skipping y_hat=139\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=784\n","Skipping y_hat=839\n","Skipping y_hat=860\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=202\n","Skipping y_hat=336\n","Skipping y_hat=465\n","Skipping y_hat=480\n","Skipping y_hat=533\n","Skipping y_hat=618\n","Skipping y_hat=786\n","Skipping y_hat=802\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=360\n","Skipping y_hat=481\n","Skipping y_hat=470\n","Skipping y_hat=555\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1003\n","Skipping y_hat=165\n","Skipping y_hat=137\n","Skipping y_hat=579\n","Skipping y_hat=947\n","Skipping y_hat=249\n","Skipping y_hat=347\n","Skipping y_hat=578\n","Skipping y_hat=681\n","Skipping y_hat=684\n","Skipping y_hat=657\n","Skipping y_hat=622\n","Skipping y_hat=787\n","Skipping y_hat=1003\n","Skipping y_hat=975\n","Skipping y_hat=273\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=746\n","Skipping y_hat=846\n","Skipping y_hat=1004\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=683\n","Skipping y_hat=796\n","Skipping y_hat=810\n","Skipping y_hat=131\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=348\n","Skipping y_hat=582\n","Skipping y_hat=526\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=729\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=149\n","Skipping y_hat=111\n","Skipping y_hat=149\n","Skipping y_hat=159\n","Skipping y_hat=101\n","Skipping y_hat=166\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=124\n","Skipping y_hat=129\n","Skipping y_hat=102\n","Skipping y_hat=162\n","Skipping y_hat=218\n","Skipping y_hat=298\n","Skipping y_hat=258\n","Skipping y_hat=297\n","Skipping y_hat=381\n","Skipping y_hat=306\n","Skipping y_hat=319\n","Skipping y_hat=339\n","Skipping y_hat=362\n","Skipping y_hat=330\n","Skipping y_hat=361\n","Skipping y_hat=450\n","Skipping y_hat=491\n","Skipping y_hat=494\n","Skipping y_hat=416\n","Skipping y_hat=451\n","Skipping y_hat=466\n","Skipping y_hat=519\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=569\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=619\n","Skipping y_hat=644\n","Skipping y_hat=665\n","Skipping y_hat=719\n","Skipping y_hat=721\n","Skipping y_hat=729\n","Skipping y_hat=755\n","Skipping y_hat=757\n","Skipping y_hat=789\n","Skipping y_hat=788\n","Skipping y_hat=890\n","Skipping y_hat=812\n","Skipping y_hat=858\n","Skipping y_hat=858\n","Skipping y_hat=991\n","Skipping y_hat=1001\n","Skipping y_hat=100\n","  4% 3/80 [00:00<00:02, 26.74it/s]Skipping y_hat=608\n","  9% 7/80 [00:00<00:02, 30.71it/s]Skipping y_hat=506\n","Skipping y_hat=139\n","Skipping y_hat=120\n","Skipping y_hat=209\n","Skipping y_hat=159\n","Skipping y_hat=209\n","Skipping y_hat=140\n","Skipping y_hat=238\n","Skipping y_hat=291\n","Skipping y_hat=260\n","Skipping y_hat=396\n","Skipping y_hat=319\n","Skipping y_hat=262\n","Skipping y_hat=205\n","Skipping y_hat=211\n","Skipping y_hat=380\n","Skipping y_hat=300\n","Skipping y_hat=328\n","Skipping y_hat=204\n","Skipping y_hat=306\n","Skipping y_hat=329\n","Skipping y_hat=321\n","Skipping y_hat=417\n","Skipping y_hat=301\n","Skipping y_hat=447\n","Skipping y_hat=497\n","Skipping y_hat=479\n","Skipping y_hat=499\n","Skipping y_hat=447\n","Skipping y_hat=309\n"," 14% 11/80 [00:00<00:02, 30.59it/s]Skipping y_hat=499\n","Skipping y_hat=401\n","Skipping y_hat=405\n","Skipping y_hat=449\n","Skipping y_hat=403\n","Skipping y_hat=459\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=415\n","Skipping y_hat=515\n","Skipping y_hat=404\n","Skipping y_hat=407\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=514\n","Skipping y_hat=511\n","Skipping y_hat=695\n","Skipping y_hat=629\n","Skipping y_hat=608\n","Skipping y_hat=616\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=699\n","Skipping y_hat=684\n","Skipping y_hat=775\n","Skipping y_hat=707\n","Skipping y_hat=758\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=759\n","Skipping y_hat=708\n","Skipping y_hat=759\n","Skipping y_hat=809\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=859\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=880\n","Skipping y_hat=801\n","Skipping y_hat=909\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=999\n","Skipping y_hat=996\n","Skipping y_hat=899\n","Skipping y_hat=920\n","Skipping y_hat=906\n","Skipping y_hat=1029\n","Skipping y_hat=1017\n","Skipping y_hat=1031\n","Skipping y_hat=900\n","Skipping y_hat=1021\n"," 19% 15/80 [00:00<00:02, 29.48it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","Skipping y_hat=1093\n","100% 80/80 [00:02<00:00, 33.62it/s]\n","accuracy of 9900 examples: 9341/9900 (94.35353535353535%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.05it/s]\n","accuracy of 10000 examples: 9986/10000 (99.86%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=38\n","Skipping y_hat=37\n","Skipping y_hat=47\n","Skipping y_hat=39\n","Skipping y_hat=49\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=656\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=784\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=971\n","Skipping y_hat=972\n","Skipping y_hat=301\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=475\n","Skipping y_hat=476\n","Skipping y_hat=480\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=802\n","Skipping y_hat=218\n","Skipping y_hat=337\n","Skipping y_hat=360\n","Skipping y_hat=481\n","Skipping y_hat=470\n","Skipping y_hat=555\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1003\n","Skipping y_hat=165\n","Skipping y_hat=127\n","Skipping y_hat=579\n","Skipping y_hat=947\n","Skipping y_hat=349\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=681\n","Skipping y_hat=684\n","Skipping y_hat=657\n","Skipping y_hat=682\n","Skipping y_hat=717\n","Skipping y_hat=1003\n","Skipping y_hat=975\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1004\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=589\n","Skipping y_hat=683\n","Skipping y_hat=796\n","Skipping y_hat=810\n","Skipping y_hat=131\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=348\n","Skipping y_hat=582\n","Skipping y_hat=526\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=729\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=79\n","Skipping y_hat=149\n","Skipping y_hat=120\n","Skipping y_hat=111\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=109\n","Skipping y_hat=162\n","Skipping y_hat=123\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=124\n","Skipping y_hat=169\n","Skipping y_hat=102\n","Skipping y_hat=101\n","Skipping y_hat=218\n","Skipping y_hat=297\n","Skipping y_hat=264\n","Skipping y_hat=258\n","Skipping y_hat=297\n","Skipping y_hat=281\n","Skipping y_hat=206\n","Skipping y_hat=319\n","Skipping y_hat=339\n","Skipping y_hat=364\n","Skipping y_hat=330\n","Skipping y_hat=361\n","Skipping y_hat=460\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=416\n","Skipping y_hat=450\n","Skipping y_hat=466\n","Skipping y_hat=516\n","Skipping y_hat=519\n","Skipping y_hat=551\n","Skipping y_hat=567\n","Skipping y_hat=579\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=645\n","Skipping y_hat=665\n","Skipping y_hat=689\n","Skipping y_hat=719\n","Skipping y_hat=726\n","Skipping y_hat=729\n","Skipping y_hat=705\n","Skipping y_hat=775\n","Skipping y_hat=758\n","Skipping y_hat=789\n","Skipping y_hat=726\n","Skipping y_hat=807\n","Skipping y_hat=812\n","Skipping y_hat=858\n","Skipping y_hat=848\n","Skipping y_hat=888\n","Skipping y_hat=881\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=988\n","Skipping y_hat=1001\n","  4% 3/80 [00:00<00:02, 26.49it/s]Skipping y_hat=149\n","Skipping y_hat=608\n","  9% 7/80 [00:00<00:02, 30.68it/s]Skipping y_hat=506\n","Skipping y_hat=139\n","Skipping y_hat=1063\n","Skipping y_hat=120\n","Skipping y_hat=209\n","Skipping y_hat=159\n","Skipping y_hat=209\n","Skipping y_hat=108\n","Skipping y_hat=140\n","Skipping y_hat=107\n","Skipping y_hat=291\n","Skipping y_hat=219\n","Skipping y_hat=206\n","Skipping y_hat=260\n","Skipping y_hat=396\n","Skipping y_hat=299\n","Skipping y_hat=305\n","Skipping y_hat=315\n","Skipping y_hat=209\n","Skipping y_hat=349\n","Skipping y_hat=211\n","Skipping y_hat=280\n","Skipping y_hat=300\n","Skipping y_hat=205\n","Skipping y_hat=328\n","Skipping y_hat=204\n","Skipping y_hat=306\n","Skipping y_hat=329\n","Skipping y_hat=321\n","Skipping y_hat=417\n","Skipping y_hat=301\n","Skipping y_hat=493\n","Skipping y_hat=441\n","Skipping y_hat=311\n","Skipping y_hat=405\n","Skipping y_hat=448\n","Skipping y_hat=309\n"," 14% 11/80 [00:00<00:02, 31.63it/s]Skipping y_hat=496\n","Skipping y_hat=401\n","Skipping y_hat=490\n","Skipping y_hat=449\n","Skipping y_hat=403\n","Skipping y_hat=459\n","Skipping y_hat=404\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=415\n","Skipping y_hat=515\n","Skipping y_hat=404\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=530\n","Skipping y_hat=514\n","Skipping y_hat=511\n","Skipping y_hat=579\n","Skipping y_hat=695\n","Skipping y_hat=629\n","Skipping y_hat=608\n","Skipping y_hat=616\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=699\n","Skipping y_hat=719\n","Skipping y_hat=775\n","Skipping y_hat=717\n","Skipping y_hat=748\n","Skipping y_hat=758\n","Skipping y_hat=798\n","Skipping y_hat=779\n","Skipping y_hat=708\n","Skipping y_hat=709\n","Skipping y_hat=809\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=800\n","Skipping y_hat=869\n","Skipping y_hat=816\n","Skipping y_hat=859\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=911\n","Skipping y_hat=889\n","Skipping y_hat=801\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=999\n","Skipping y_hat=998\n","Skipping y_hat=889\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=988\n","Skipping y_hat=195\n","Skipping y_hat=1029\n","Skipping y_hat=1022\n","Skipping y_hat=1017\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=196\n","Skipping y_hat=1021\n"," 19% 15/80 [00:00<00:02, 30.93it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1082\n","Skipping y_hat=900\n","Skipping y_hat=1093\n","Skipping y_hat=1036\n","100% 80/80 [00:02<00:00, 33.96it/s]\n","accuracy of 9900 examples: 9316/9900 (94.10101010101009%)\n","\n","Test Results:\n","test_10000_stripped: 94.10%\n","\n","iter 4800: train loss 0.6626, val loss 1.8772\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=20\n","Skipping y_hat=38\n","Skipping y_hat=37\n","Skipping y_hat=47\n","Skipping y_hat=39\n","Skipping y_hat=47\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=784\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=841\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=465\n","Skipping y_hat=476\n","Skipping y_hat=420\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=892\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=380\n","Skipping y_hat=481\n","Skipping y_hat=480\n","Skipping y_hat=555\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1001\n","Skipping y_hat=165\n","Skipping y_hat=127\n","Skipping y_hat=579\n","Skipping y_hat=977\n","Skipping y_hat=239\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=681\n","Skipping y_hat=684\n","Skipping y_hat=627\n","Skipping y_hat=622\n","Skipping y_hat=787\n","Skipping y_hat=1003\n","Skipping y_hat=975\n","Skipping y_hat=273\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=557\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1001\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=589\n","Skipping y_hat=683\n","Skipping y_hat=746\n","Skipping y_hat=840\n","Skipping y_hat=131\n","Skipping y_hat=240\n","Skipping y_hat=249\n","Skipping y_hat=348\n","Skipping y_hat=582\n","Skipping y_hat=526\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=779\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=79\n","Skipping y_hat=149\n","Skipping y_hat=120\n","Skipping y_hat=111\n","Skipping y_hat=149\n","Skipping y_hat=159\n","Skipping y_hat=109\n","Skipping y_hat=152\n","Skipping y_hat=143\n","Skipping y_hat=160\n","Skipping y_hat=160\n","Skipping y_hat=123\n","Skipping y_hat=169\n","Skipping y_hat=162\n","Skipping y_hat=161\n","Skipping y_hat=208\n","Skipping y_hat=294\n","Skipping y_hat=258\n","Skipping y_hat=281\n","Skipping y_hat=306\n","Skipping y_hat=319\n","Skipping y_hat=339\n","Skipping y_hat=462\n","Skipping y_hat=330\n","Skipping y_hat=361\n","Skipping y_hat=450\n","Skipping y_hat=491\n","Skipping y_hat=493\n","Skipping y_hat=416\n","Skipping y_hat=451\n","Skipping y_hat=462\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=517\n","Skipping y_hat=555\n","Skipping y_hat=569\n","Skipping y_hat=567\n","Skipping y_hat=579\n","Skipping y_hat=568\n","Skipping y_hat=556\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=645\n","Skipping y_hat=655\n","Skipping y_hat=689\n","Skipping y_hat=719\n","Skipping y_hat=721\n","Skipping y_hat=729\n","Skipping y_hat=755\n","Skipping y_hat=758\n","Skipping y_hat=784\n","Skipping y_hat=788\n","Skipping y_hat=877\n","Skipping y_hat=832\n","Skipping y_hat=857\n","Skipping y_hat=958\n","Skipping y_hat=831\n","Skipping y_hat=891\n","Skipping y_hat=100\n","Skipping y_hat=1001\n","  9% 7/80 [00:00<00:02, 31.82it/s]Skipping y_hat=506\n","Skipping y_hat=139\n","Skipping y_hat=159\n","Skipping y_hat=140\n","Skipping y_hat=106\n","Skipping y_hat=107\n","Skipping y_hat=238\n","Skipping y_hat=291\n","Skipping y_hat=260\n","Skipping y_hat=219\n","Skipping y_hat=260\n","Skipping y_hat=397\n","Skipping y_hat=299\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=211\n","Skipping y_hat=214\n","Skipping y_hat=389\n","Skipping y_hat=300\n","Skipping y_hat=347\n","Skipping y_hat=328\n","Skipping y_hat=200\n","Skipping y_hat=306\n","Skipping y_hat=321\n","Skipping y_hat=407\n","Skipping y_hat=362\n","Skipping y_hat=401\n","Skipping y_hat=491\n","Skipping y_hat=301\n","Skipping y_hat=493\n","Skipping y_hat=497\n","Skipping y_hat=409\n"," 14% 11/80 [00:00<00:02, 30.79it/s]Skipping y_hat=409\n","Skipping y_hat=407\n","Skipping y_hat=451\n","Skipping y_hat=405\n","Skipping y_hat=449\n","Skipping y_hat=403\n","Skipping y_hat=459\n","Skipping y_hat=497\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=415\n","Skipping y_hat=500\n","Skipping y_hat=513\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=549\n","Skipping y_hat=514\n","Skipping y_hat=509\n","Skipping y_hat=505\n","Skipping y_hat=579\n","Skipping y_hat=695\n","Skipping y_hat=629\n","Skipping y_hat=608\n","Skipping y_hat=616\n","Skipping y_hat=615\n","Skipping y_hat=684\n","Skipping y_hat=775\n","Skipping y_hat=717\n","Skipping y_hat=751\n","Skipping y_hat=758\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=779\n","Skipping y_hat=749\n","Skipping y_hat=708\n","Skipping y_hat=709\n","Skipping y_hat=809\n","Skipping y_hat=811\n","Skipping y_hat=757\n","Skipping y_hat=895\n","Skipping y_hat=820\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=880\n","Skipping y_hat=911\n","Skipping y_hat=801\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=999\n","Skipping y_hat=996\n","Skipping y_hat=889\n","Skipping y_hat=899\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=932\n","Skipping y_hat=195\n","Skipping y_hat=1029\n","Skipping y_hat=1018\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=1021\n"," 19% 15/80 [00:00<00:02, 30.81it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1070\n","Skipping y_hat=900\n","Skipping y_hat=1067\n","Skipping y_hat=1082\n","Skipping y_hat=1001\n","100% 80/80 [00:02<00:00, 33.81it/s]\n","accuracy of 9900 examples: 9334/9900 (94.28282828282828%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 39.36it/s]\n","accuracy of 10000 examples: 9987/10000 (99.87%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=20\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=47\n","Skipping y_hat=49\n","Skipping y_hat=47\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=626\n","Skipping y_hat=976\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=704\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=841\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=475\n","Skipping y_hat=466\n","Skipping y_hat=420\n","Skipping y_hat=523\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=842\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=380\n","Skipping y_hat=481\n","Skipping y_hat=470\n","Skipping y_hat=525\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1001\n","Skipping y_hat=165\n","Skipping y_hat=167\n","Skipping y_hat=579\n","Skipping y_hat=977\n","Skipping y_hat=249\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=681\n","Skipping y_hat=684\n","Skipping y_hat=627\n","Skipping y_hat=622\n","Skipping y_hat=787\n","Skipping y_hat=1003\n","Skipping y_hat=935\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=557\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1002\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=589\n","Skipping y_hat=683\n","Skipping y_hat=796\n","Skipping y_hat=810\n","Skipping y_hat=131\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=248\n","Skipping y_hat=582\n","Skipping y_hat=526\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=729\n","Skipping y_hat=814\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=79\n","Skipping y_hat=81\n","Skipping y_hat=149\n","Skipping y_hat=120\n","Skipping y_hat=111\n","Skipping y_hat=149\n","Skipping y_hat=159\n","Skipping y_hat=109\n","Skipping y_hat=151\n","Skipping y_hat=153\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=122\n","Skipping y_hat=169\n","Skipping y_hat=163\n","Skipping y_hat=161\n","Skipping y_hat=208\n","Skipping y_hat=298\n","Skipping y_hat=263\n","Skipping y_hat=258\n","Skipping y_hat=281\n","Skipping y_hat=306\n","Skipping y_hat=319\n","Skipping y_hat=339\n","Skipping y_hat=463\n","Skipping y_hat=330\n","Skipping y_hat=361\n","Skipping y_hat=450\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=419\n","Skipping y_hat=451\n","Skipping y_hat=466\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=555\n","Skipping y_hat=569\n","Skipping y_hat=579\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=645\n","Skipping y_hat=655\n","Skipping y_hat=689\n","Skipping y_hat=689\n","Skipping y_hat=719\n","Skipping y_hat=721\n","Skipping y_hat=729\n","Skipping y_hat=755\n","Skipping y_hat=757\n","Skipping y_hat=789\n","Skipping y_hat=778\n","Skipping y_hat=870\n","Skipping y_hat=832\n","Skipping y_hat=858\n","Skipping y_hat=858\n","Skipping y_hat=881\n","Skipping y_hat=891\n","Skipping y_hat=100\n","Skipping y_hat=1001\n","  4% 3/80 [00:00<00:02, 27.24it/s]Skipping y_hat=608\n","  9% 7/80 [00:00<00:02, 30.96it/s]Skipping y_hat=1045\n","Skipping y_hat=506\n","Skipping y_hat=139\n","Skipping y_hat=162\n","Skipping y_hat=159\n","Skipping y_hat=209\n","Skipping y_hat=103\n","Skipping y_hat=140\n","Skipping y_hat=107\n","Skipping y_hat=219\n","Skipping y_hat=238\n","Skipping y_hat=291\n","Skipping y_hat=260\n","Skipping y_hat=295\n","Skipping y_hat=396\n","Skipping y_hat=299\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=288\n","Skipping y_hat=380\n","Skipping y_hat=300\n","Skipping y_hat=205\n","Skipping y_hat=328\n","Skipping y_hat=200\n","Skipping y_hat=306\n","Skipping y_hat=321\n","Skipping y_hat=407\n","Skipping y_hat=359\n","Skipping y_hat=362\n","Skipping y_hat=401\n","Skipping y_hat=357\n","Skipping y_hat=491\n","Skipping y_hat=301\n","Skipping y_hat=493\n","Skipping y_hat=441\n","Skipping y_hat=447\n","Skipping y_hat=409\n"," 14% 11/80 [00:00<00:02, 31.53it/s]Skipping y_hat=496\n","Skipping y_hat=401\n","Skipping y_hat=405\n","Skipping y_hat=449\n","Skipping y_hat=402\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=513\n","Skipping y_hat=404\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=549\n","Skipping y_hat=514\n","Skipping y_hat=511\n","Skipping y_hat=579\n","Skipping y_hat=695\n","Skipping y_hat=629\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=667\n","Skipping y_hat=609\n","Skipping y_hat=699\n","Skipping y_hat=684\n","Skipping y_hat=775\n","Skipping y_hat=713\n","Skipping y_hat=799\n","Skipping y_hat=793\n","Skipping y_hat=779\n","Skipping y_hat=796\n","Skipping y_hat=749\n","Skipping y_hat=708\n","Skipping y_hat=709\n","Skipping y_hat=781\n","Skipping y_hat=893\n","Skipping y_hat=809\n","Skipping y_hat=848\n","Skipping y_hat=811\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=880\n","Skipping y_hat=911\n","Skipping y_hat=869\n","Skipping y_hat=889\n","Skipping y_hat=801\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=999\n","Skipping y_hat=996\n","Skipping y_hat=889\n","Skipping y_hat=899\n","Skipping y_hat=988\n","Skipping y_hat=900\n","Skipping y_hat=195\n","Skipping y_hat=1029\n","Skipping y_hat=1022\n","Skipping y_hat=1018\n","Skipping y_hat=1031\n","Skipping y_hat=1012\n","Skipping y_hat=196\n","Skipping y_hat=1021\n"," 19% 15/80 [00:00<00:02, 31.17it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","Skipping y_hat=1065\n","100% 80/80 [00:02<00:00, 34.11it/s]\n","accuracy of 9900 examples: 9331/9900 (94.25252525252526%)\n","\n","Test Results:\n","test_10000_stripped: 94.25%\n","\n","iter 4900: train loss 0.6576, val loss 1.9338\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=20\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=47\n","Skipping y_hat=49\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=139\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=941\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=475\n","Skipping y_hat=466\n","Skipping y_hat=480\n","Skipping y_hat=618\n","Skipping y_hat=756\n","Skipping y_hat=802\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=360\n","Skipping y_hat=481\n","Skipping y_hat=470\n","Skipping y_hat=525\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1003\n","Skipping y_hat=165\n","Skipping y_hat=579\n","Skipping y_hat=947\n","Skipping y_hat=239\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=691\n","Skipping y_hat=684\n","Skipping y_hat=627\n","Skipping y_hat=602\n","Skipping y_hat=787\n","Skipping y_hat=1003\n","Skipping y_hat=985\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1002\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=673\n","Skipping y_hat=796\n","Skipping y_hat=810\n","Skipping y_hat=131\n","Skipping y_hat=240\n","Skipping y_hat=249\n","Skipping y_hat=348\n","Skipping y_hat=582\n","Skipping y_hat=586\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=729\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=81\n","Skipping y_hat=149\n","Skipping y_hat=120\n","Skipping y_hat=111\n","Skipping y_hat=119\n","Skipping y_hat=159\n","Skipping y_hat=101\n","Skipping y_hat=161\n","Skipping y_hat=113\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=124\n","Skipping y_hat=209\n","Skipping y_hat=102\n","Skipping y_hat=120\n","Skipping y_hat=208\n","Skipping y_hat=298\n","Skipping y_hat=258\n","Skipping y_hat=297\n","Skipping y_hat=381\n","Skipping y_hat=306\n","Skipping y_hat=339\n","Skipping y_hat=432\n","Skipping y_hat=330\n","Skipping y_hat=361\n","Skipping y_hat=450\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=419\n","Skipping y_hat=451\n","Skipping y_hat=466\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=518\n","Skipping y_hat=559\n","Skipping y_hat=569\n","Skipping y_hat=579\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=645\n","Skipping y_hat=655\n","Skipping y_hat=719\n","Skipping y_hat=711\n","Skipping y_hat=729\n","Skipping y_hat=706\n","Skipping y_hat=785\n","Skipping y_hat=757\n","Skipping y_hat=784\n","Skipping y_hat=788\n","Skipping y_hat=877\n","Skipping y_hat=812\n","Skipping y_hat=858\n","Skipping y_hat=958\n","Skipping y_hat=881\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=988\n","Skipping y_hat=1001\n","  4% 3/80 [00:00<00:02, 27.15it/s]Skipping y_hat=608\n","Skipping y_hat=603\n","  9% 7/80 [00:00<00:02, 31.28it/s]Skipping y_hat=506\n","Skipping y_hat=139\n","Skipping y_hat=162\n","Skipping y_hat=159\n","Skipping y_hat=108\n","Skipping y_hat=140\n","Skipping y_hat=107\n","Skipping y_hat=238\n","Skipping y_hat=291\n","Skipping y_hat=295\n","Skipping y_hat=396\n","Skipping y_hat=299\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=211\n","Skipping y_hat=380\n","Skipping y_hat=300\n","Skipping y_hat=348\n","Skipping y_hat=205\n","Skipping y_hat=328\n","Skipping y_hat=306\n","Skipping y_hat=329\n","Skipping y_hat=493\n","Skipping y_hat=447\n","Skipping y_hat=479\n","Skipping y_hat=447\n","Skipping y_hat=409\n"," 14% 11/80 [00:00<00:02, 32.17it/s]Skipping y_hat=499\n","Skipping y_hat=407\n","Skipping y_hat=449\n","Skipping y_hat=403\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=513\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=532\n","Skipping y_hat=514\n","Skipping y_hat=511\n","Skipping y_hat=579\n","Skipping y_hat=695\n","Skipping y_hat=629\n","Skipping y_hat=628\n","Skipping y_hat=608\n","Skipping y_hat=616\n","Skipping y_hat=627\n","Skipping y_hat=696\n","Skipping y_hat=615\n","Skipping y_hat=699\n","Skipping y_hat=719\n","Skipping y_hat=775\n","Skipping y_hat=600\n","Skipping y_hat=719\n","Skipping y_hat=717\n","Skipping y_hat=671\n","Skipping y_hat=751\n","Skipping y_hat=758\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=759\n","Skipping y_hat=796\n","Skipping y_hat=708\n","Skipping y_hat=818\n","Skipping y_hat=731\n","Skipping y_hat=709\n","Skipping y_hat=809\n","Skipping y_hat=849\n","Skipping y_hat=895\n","Skipping y_hat=820\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=859\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=898\n","Skipping y_hat=911\n","Skipping y_hat=870\n","Skipping y_hat=801\n","Skipping y_hat=909\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=999\n","Skipping y_hat=998\n","Skipping y_hat=899\n","Skipping y_hat=932\n","Skipping y_hat=195\n","Skipping y_hat=1029\n","Skipping y_hat=999\n","Skipping y_hat=1018\n","Skipping y_hat=900\n","Skipping y_hat=986\n","Skipping y_hat=1012\n","Skipping y_hat=196\n","Skipping y_hat=1021\n","Skipping y_hat=999\n","Skipping y_hat=196\n"," 19% 15/80 [00:00<00:02, 31.45it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=1093\n","100% 80/80 [00:02<00:00, 34.08it/s]\n","accuracy of 9900 examples: 9333/9900 (94.27272727272728%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 39.30it/s]\n","accuracy of 10000 examples: 9988/10000 (99.88%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=47\n","Skipping y_hat=49\n","Skipping y_hat=92\n","Skipping y_hat=380\n","Skipping y_hat=586\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=704\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=841\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=202\n","Skipping y_hat=336\n","Skipping y_hat=475\n","Skipping y_hat=476\n","Skipping y_hat=480\n","Skipping y_hat=618\n","Skipping y_hat=716\n","Skipping y_hat=802\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=360\n","Skipping y_hat=481\n","Skipping y_hat=470\n","Skipping y_hat=525\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=873\n","Skipping y_hat=1003\n","Skipping y_hat=165\n","Skipping y_hat=167\n","Skipping y_hat=579\n","Skipping y_hat=947\n","Skipping y_hat=249\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=691\n","Skipping y_hat=684\n","Skipping y_hat=627\n","Skipping y_hat=622\n","Skipping y_hat=717\n","Skipping y_hat=1003\n","Skipping y_hat=985\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1002\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=673\n","Skipping y_hat=746\n","Skipping y_hat=810\n","Skipping y_hat=131\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=248\n","Skipping y_hat=582\n","Skipping y_hat=626\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=779\n","Skipping y_hat=874\n","Skipping y_hat=1006\n","Skipping y_hat=81\n","Skipping y_hat=149\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=109\n","Skipping y_hat=162\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=124\n","Skipping y_hat=129\n","Skipping y_hat=162\n","Skipping y_hat=111\n","Skipping y_hat=228\n","Skipping y_hat=298\n","Skipping y_hat=258\n","Skipping y_hat=297\n","Skipping y_hat=281\n","Skipping y_hat=306\n","Skipping y_hat=319\n","Skipping y_hat=339\n","Skipping y_hat=454\n","Skipping y_hat=330\n","Skipping y_hat=361\n","Skipping y_hat=450\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=416\n","Skipping y_hat=466\n","Skipping y_hat=519\n","Skipping y_hat=531\n","Skipping y_hat=559\n","Skipping y_hat=569\n","Skipping y_hat=579\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=619\n","Skipping y_hat=655\n","Skipping y_hat=718\n","Skipping y_hat=711\n","Skipping y_hat=728\n","Skipping y_hat=706\n","Skipping y_hat=785\n","Skipping y_hat=758\n","Skipping y_hat=789\n","Skipping y_hat=776\n","Skipping y_hat=808\n","Skipping y_hat=812\n","Skipping y_hat=858\n","Skipping y_hat=958\n","Skipping y_hat=891\n","Skipping y_hat=1001\n","Skipping y_hat=988\n","Skipping y_hat=1001\n","Skipping y_hat=608\n"," 10% 8/80 [00:00<00:02, 33.17it/s]Skipping y_hat=1045\n","Skipping y_hat=506\n","Skipping y_hat=149\n","Skipping y_hat=159\n","Skipping y_hat=140\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=240\n","Skipping y_hat=238\n","Skipping y_hat=291\n","Skipping y_hat=295\n","Skipping y_hat=299\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=211\n","Skipping y_hat=213\n","Skipping y_hat=280\n","Skipping y_hat=300\n","Skipping y_hat=348\n","Skipping y_hat=347\n","Skipping y_hat=305\n","Skipping y_hat=328\n","Skipping y_hat=306\n","Skipping y_hat=407\n","Skipping y_hat=362\n","Skipping y_hat=493\n","Skipping y_hat=447\n","Skipping y_hat=310\n","Skipping y_hat=447\n","Skipping y_hat=407\n","Skipping y_hat=499\n","Skipping y_hat=406\n","Skipping y_hat=449\n","Skipping y_hat=403\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=488\n","Skipping y_hat=515\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=514\n","Skipping y_hat=511\n"," 15% 12/80 [00:00<00:02, 32.40it/s]Skipping y_hat=579\n","Skipping y_hat=629\n","Skipping y_hat=608\n","Skipping y_hat=626\n","Skipping y_hat=627\n","Skipping y_hat=696\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=684\n","Skipping y_hat=600\n","Skipping y_hat=795\n","Skipping y_hat=720\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=759\n","Skipping y_hat=796\n","Skipping y_hat=708\n","Skipping y_hat=731\n","Skipping y_hat=709\n","Skipping y_hat=809\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=859\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=870\n","Skipping y_hat=805\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=999\n","Skipping y_hat=998\n","Skipping y_hat=889\n","Skipping y_hat=899\n","Skipping y_hat=920\n","Skipping y_hat=988\n","Skipping y_hat=188\n","Skipping y_hat=189\n","Skipping y_hat=950\n","Skipping y_hat=1029\n","Skipping y_hat=1018\n","Skipping y_hat=900\n","Skipping y_hat=986\n","Skipping y_hat=1012\n","Skipping y_hat=196\n","Skipping y_hat=1021\n","Skipping y_hat=199\n","Skipping y_hat=198\n","Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1070\n","Skipping y_hat=900\n","Skipping y_hat=1092\n","Skipping y_hat=1015\n","Skipping y_hat=1078\n","100% 80/80 [00:02<00:00, 34.05it/s]\n","accuracy of 9900 examples: 9336/9900 (94.3030303030303%)\n","\n","Test Results:\n","test_10000_stripped: 94.30%\n","\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=22\n","Skipping y_hat=20\n","Skipping y_hat=38\n","Skipping y_hat=37\n","Skipping y_hat=47\n","Skipping y_hat=39\n","Skipping y_hat=380\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=781\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=784\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=991\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=465\n","Skipping y_hat=476\n","Skipping y_hat=400\n","Skipping y_hat=526\n","Skipping y_hat=618\n","Skipping y_hat=816\n","Skipping y_hat=892\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=380\n","Skipping y_hat=481\n","Skipping y_hat=480\n","Skipping y_hat=525\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=843\n","Skipping y_hat=103\n","Skipping y_hat=165\n","Skipping y_hat=167\n","Skipping y_hat=579\n","Skipping y_hat=947\n","Skipping y_hat=239\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=581\n","Skipping y_hat=684\n","Skipping y_hat=627\n","Skipping y_hat=622\n","Skipping y_hat=717\n","Skipping y_hat=1003\n","Skipping y_hat=975\n","Skipping y_hat=273\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=557\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1002\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=673\n","Skipping y_hat=796\n","Skipping y_hat=810\n","Skipping y_hat=131\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=348\n","Skipping y_hat=582\n","Skipping y_hat=626\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=779\n","Skipping y_hat=834\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=79\n","Skipping y_hat=149\n","Skipping y_hat=120\n","Skipping y_hat=116\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=101\n","Skipping y_hat=166\n","Skipping y_hat=128\n","Skipping y_hat=161\n","Skipping y_hat=160\n","Skipping y_hat=124\n","Skipping y_hat=109\n","Skipping y_hat=102\n","Skipping y_hat=161\n","Skipping y_hat=208\n","Skipping y_hat=297\n","Skipping y_hat=258\n","Skipping y_hat=291\n","Skipping y_hat=381\n","Skipping y_hat=306\n","Skipping y_hat=339\n","Skipping y_hat=464\n","Skipping y_hat=430\n","Skipping y_hat=361\n","Skipping y_hat=450\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=419\n","Skipping y_hat=451\n","Skipping y_hat=466\n","Skipping y_hat=517\n","Skipping y_hat=519\n","Skipping y_hat=518\n","Skipping y_hat=555\n","Skipping y_hat=551\n","Skipping y_hat=569\n","Skipping y_hat=579\n","Skipping y_hat=588\n","Skipping y_hat=576\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=655\n","Skipping y_hat=718\n","Skipping y_hat=711\n","Skipping y_hat=729\n","Skipping y_hat=705\n","Skipping y_hat=788\n","Skipping y_hat=757\n","Skipping y_hat=751\n","Skipping y_hat=788\n","Skipping y_hat=877\n","Skipping y_hat=812\n","Skipping y_hat=858\n","Skipping y_hat=948\n","Skipping y_hat=888\n","Skipping y_hat=881\n","Skipping y_hat=891\n","Skipping y_hat=100\n","Skipping y_hat=1001\n","  9% 7/80 [00:00<00:02, 31.19it/s]Skipping y_hat=506\n","Skipping y_hat=139\n","Skipping y_hat=209\n","Skipping y_hat=159\n","Skipping y_hat=209\n","Skipping y_hat=140\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=238\n","Skipping y_hat=291\n","Skipping y_hat=260\n","Skipping y_hat=396\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=349\n","Skipping y_hat=211\n","Skipping y_hat=389\n","Skipping y_hat=300\n","Skipping y_hat=328\n","Skipping y_hat=200\n","Skipping y_hat=306\n","Skipping y_hat=329\n","Skipping y_hat=417\n","Skipping y_hat=340\n","Skipping y_hat=362\n","Skipping y_hat=491\n","Skipping y_hat=493\n","Skipping y_hat=447\n","Skipping y_hat=497\n","Skipping y_hat=448\n","Skipping y_hat=309\n"," 14% 11/80 [00:00<00:02, 31.67it/s]Skipping y_hat=496\n","Skipping y_hat=401\n","Skipping y_hat=449\n","Skipping y_hat=593\n","Skipping y_hat=403\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=514\n","Skipping y_hat=403\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=559\n","Skipping y_hat=530\n","Skipping y_hat=549\n","Skipping y_hat=514\n","Skipping y_hat=511\n","Skipping y_hat=579\n","Skipping y_hat=695\n","Skipping y_hat=626\n","Skipping y_hat=696\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=699\n","Skipping y_hat=775\n","Skipping y_hat=719\n","Skipping y_hat=717\n","Skipping y_hat=751\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=779\n","Skipping y_hat=749\n","Skipping y_hat=708\n","Skipping y_hat=709\n","Skipping y_hat=781\n","Skipping y_hat=809\n","Skipping y_hat=819\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=869\n","Skipping y_hat=870\n","Skipping y_hat=801\n","Skipping y_hat=909\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=996\n","Skipping y_hat=889\n","Skipping y_hat=920\n","Skipping y_hat=870\n","Skipping y_hat=920\n","Skipping y_hat=806\n","Skipping y_hat=988\n","Skipping y_hat=1029\n","Skipping y_hat=1017\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=1021\n","Skipping y_hat=999\n"," 19% 15/80 [00:00<00:02, 31.61it/s]Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","Skipping y_hat=1093\n","Skipping y_hat=1099\n","100% 80/80 [00:02<00:00, 33.99it/s]\n","accuracy of 9900 examples: 9344/9900 (94.38383838383838%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 39.03it/s]\n","accuracy of 10000 examples: 9992/10000 (99.92%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=20\n","Skipping y_hat=37\n","Skipping y_hat=37\n","Skipping y_hat=47\n","Skipping y_hat=49\n","Skipping y_hat=380\n","Skipping y_hat=581\n","Skipping y_hat=686\n","Skipping y_hat=751\n","Skipping y_hat=127\n","Skipping y_hat=528\n","Skipping y_hat=946\n","Skipping y_hat=127\n","Skipping y_hat=129\n","Skipping y_hat=587\n","Skipping y_hat=672\n","Skipping y_hat=704\n","Skipping y_hat=809\n","Skipping y_hat=860\n","Skipping y_hat=841\n","Skipping y_hat=972\n","Skipping y_hat=201\n","Skipping y_hat=302\n","Skipping y_hat=336\n","Skipping y_hat=475\n","Skipping y_hat=476\n","Skipping y_hat=420\n","Skipping y_hat=523\n","Skipping y_hat=618\n","Skipping y_hat=716\n","Skipping y_hat=842\n","Skipping y_hat=238\n","Skipping y_hat=337\n","Skipping y_hat=360\n","Skipping y_hat=481\n","Skipping y_hat=480\n","Skipping y_hat=555\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=756\n","Skipping y_hat=751\n","Skipping y_hat=843\n","Skipping y_hat=103\n","Skipping y_hat=165\n","Skipping y_hat=137\n","Skipping y_hat=579\n","Skipping y_hat=937\n","Skipping y_hat=239\n","Skipping y_hat=337\n","Skipping y_hat=578\n","Skipping y_hat=681\n","Skipping y_hat=684\n","Skipping y_hat=627\n","Skipping y_hat=622\n","Skipping y_hat=817\n","Skipping y_hat=1003\n","Skipping y_hat=1005\n","Skipping y_hat=173\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=264\n","Skipping y_hat=486\n","Skipping y_hat=756\n","Skipping y_hat=846\n","Skipping y_hat=1002\n","Skipping y_hat=1004\n","Skipping y_hat=186\n","Skipping y_hat=486\n","Skipping y_hat=673\n","Skipping y_hat=796\n","Skipping y_hat=830\n","Skipping y_hat=131\n","Skipping y_hat=340\n","Skipping y_hat=249\n","Skipping y_hat=348\n","Skipping y_hat=582\n","Skipping y_hat=626\n","Skipping y_hat=579\n","Skipping y_hat=606\n","Skipping y_hat=779\n","Skipping y_hat=834\n","Skipping y_hat=1006\n","Skipping y_hat=50\n","Skipping y_hat=79\n","Skipping y_hat=81\n","Skipping y_hat=149\n","Skipping y_hat=116\n","Skipping y_hat=129\n","Skipping y_hat=159\n","Skipping y_hat=109\n","Skipping y_hat=166\n","Skipping y_hat=143\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=129\n","Skipping y_hat=103\n","Skipping y_hat=160\n","Skipping y_hat=208\n","Skipping y_hat=298\n","Skipping y_hat=258\n","Skipping y_hat=297\n","Skipping y_hat=381\n","Skipping y_hat=306\n","Skipping y_hat=339\n","Skipping y_hat=434\n","Skipping y_hat=330\n","Skipping y_hat=361\n","Skipping y_hat=460\n","Skipping y_hat=491\n","Skipping y_hat=493\n","Skipping y_hat=416\n","Skipping y_hat=466\n","Skipping y_hat=519\n","Skipping y_hat=517\n","Skipping y_hat=555\n","Skipping y_hat=550\n","Skipping y_hat=566\n","Skipping y_hat=568\n","Skipping y_hat=576\n","Skipping y_hat=599\n","Skipping y_hat=619\n","Skipping y_hat=634\n","Skipping y_hat=665\n","Skipping y_hat=689\n","Skipping y_hat=719\n","Skipping y_hat=711\n","Skipping y_hat=729\n","Skipping y_hat=705\n","Skipping y_hat=788\n","Skipping y_hat=757\n","Skipping y_hat=751\n","Skipping y_hat=784\n","Skipping y_hat=776\n","Skipping y_hat=807\n","Skipping y_hat=812\n","Skipping y_hat=850\n","Skipping y_hat=958\n","Skipping y_hat=888\n","Skipping y_hat=891\n","Skipping y_hat=100\n","Skipping y_hat=102\n","Skipping y_hat=1001\n","  4% 3/80 [00:00<00:02, 27.75it/s]Skipping y_hat=608\n","  9% 7/80 [00:00<00:02, 32.14it/s]Skipping y_hat=403\n","Skipping y_hat=139\n","Skipping y_hat=162\n","Skipping y_hat=159\n","Skipping y_hat=140\n","Skipping y_hat=220\n","Skipping y_hat=107\n","Skipping y_hat=238\n","Skipping y_hat=291\n","Skipping y_hat=396\n","Skipping y_hat=305\n","Skipping y_hat=209\n","Skipping y_hat=349\n","Skipping y_hat=211\n","Skipping y_hat=380\n","Skipping y_hat=300\n","Skipping y_hat=328\n","Skipping y_hat=200\n","Skipping y_hat=306\n","Skipping y_hat=329\n","Skipping y_hat=340\n","Skipping y_hat=362\n","Skipping y_hat=491\n","Skipping y_hat=493\n","Skipping y_hat=447\n","Skipping y_hat=309\n"," 14% 11/80 [00:00<00:02, 32.57it/s]Skipping y_hat=496\n","Skipping y_hat=491\n","Skipping y_hat=492\n","Skipping y_hat=449\n","Skipping y_hat=593\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=547\n","Skipping y_hat=404\n","Skipping y_hat=405\n","Skipping y_hat=403\n","Skipping y_hat=530\n","Skipping y_hat=514\n","Skipping y_hat=511\n","Skipping y_hat=695\n","Skipping y_hat=629\n","Skipping y_hat=626\n","Skipping y_hat=696\n","Skipping y_hat=615\n","Skipping y_hat=661\n","Skipping y_hat=719\n","Skipping y_hat=775\n","Skipping y_hat=600\n","Skipping y_hat=671\n","Skipping y_hat=795\n","Skipping y_hat=708\n","Skipping y_hat=793\n","Skipping y_hat=779\n","Skipping y_hat=708\n","Skipping y_hat=740\n","Skipping y_hat=709\n","Skipping y_hat=809\n","Skipping y_hat=895\n","Skipping y_hat=897\n","Skipping y_hat=869\n","Skipping y_hat=811\n","Skipping y_hat=840\n","Skipping y_hat=841\n","Skipping y_hat=916\n","Skipping y_hat=850\n","Skipping y_hat=870\n","Skipping y_hat=801\n","Skipping y_hat=809\n","Skipping y_hat=850\n","Skipping y_hat=809\n","Skipping y_hat=919\n","Skipping y_hat=996\n","Skipping y_hat=870\n","Skipping y_hat=806\n","Skipping y_hat=988\n","Skipping y_hat=935\n","Skipping y_hat=950\n","Skipping y_hat=900\n","Skipping y_hat=1029\n","Skipping y_hat=1018\n","Skipping y_hat=900\n","Skipping y_hat=1012\n","Skipping y_hat=196\n","Skipping y_hat=1021\n","Skipping y_hat=999\n","Skipping y_hat=196\n"," 19% 15/80 [00:00<00:02, 31.31it/s]Skipping y_hat=903\n","Skipping y_hat=1061\n","Skipping y_hat=1032\n","Skipping y_hat=1072\n","Skipping y_hat=900\n","Skipping y_hat=1092\n","100% 80/80 [00:02<00:00, 33.62it/s]\n","accuracy of 9900 examples: 9332/9900 (94.26262626262626%)\n","\n","Final Test Results:\n","test_10000_stripped: 94.26%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m2_operands_0_to_999_their_data_plain\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/b0bb020n\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m2_operands_0_to_999_their_data_plain/out/wandb/run-20250621_212033-b0bb020n/logs\u001b[0m\n"]}],"source":["!python train_end_padding_auto_val.py 2_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"g4wN-aDwbLZt"},"source":["### Reversed"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":680263,"status":"ok","timestamp":1750542538223,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"JpkehAqKbM54","outputId":"8c9724bb-c4e4-46eb-fc7f-7eabdf5cd122"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=139\n","Skipping y_hat=328\n","Skipping y_hat=449\n","Skipping y_hat=846\n","Skipping y_hat=769\n","Skipping y_hat=1044\n","Skipping y_hat=806\n","Skipping y_hat=321\n","Skipping y_hat=545\n","Skipping y_hat=265\n","Skipping y_hat=568\n","Skipping y_hat=773\n","Skipping y_hat=397\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=412\n","Skipping y_hat=415\n","Skipping y_hat=337\n","Skipping y_hat=359\n","Skipping y_hat=487\n","Skipping y_hat=1000\n","Skipping y_hat=605\n","Skipping y_hat=415\n","Skipping y_hat=533\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=450\n","Skipping y_hat=460\n","Skipping y_hat=624\n","Skipping y_hat=568\n","Skipping y_hat=306\n","Skipping y_hat=745\n","Skipping y_hat=461\n","Skipping y_hat=488\n","Skipping y_hat=451\n","Skipping y_hat=561\n","Skipping y_hat=581\n","Skipping y_hat=998\n","Skipping y_hat=815\n","Skipping y_hat=825\n","Skipping y_hat=541\n","Skipping y_hat=575\n","Skipping y_hat=677\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=728\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=467\n","Skipping y_hat=492\n","Skipping y_hat=804\n","Skipping y_hat=735\n","Skipping y_hat=876\n","Skipping y_hat=785\n","Skipping y_hat=883\n"," 60% 48/80 [00:01<00:00, 34.30it/s]Skipping y_hat=1476\n","100% 80/80 [00:02<00:00, 34.25it/s]\n","accuracy of 9900 examples: 9783/9900 (98.81818181818181%)\n","\n","Test Results:\n","test_10000_stripped: 98.82%\n","\n","iter 2600: train loss 1.0143, val loss 1.0616\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=63\n","Skipping y_hat=546\n","Skipping y_hat=841\n","Skipping y_hat=618\n","Skipping y_hat=776\n","Skipping y_hat=176\n","Skipping y_hat=217\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=1080\n","Skipping y_hat=1001\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=355\n","Skipping y_hat=450\n","Skipping y_hat=313\n","Skipping y_hat=479\n","Skipping y_hat=802\n","Skipping y_hat=227\n","Skipping y_hat=398\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=750\n","Skipping y_hat=658\n","Skipping y_hat=616\n","Skipping y_hat=781\n","Skipping y_hat=803\n","Skipping y_hat=1003\n","Skipping y_hat=479\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=628\n","Skipping y_hat=761\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=752\n","Skipping y_hat=863\n","Skipping y_hat=1005\n","Skipping y_hat=183\n","Skipping y_hat=178\n","Skipping y_hat=267\n","Skipping y_hat=384\n","Skipping y_hat=406\n","Skipping y_hat=547\n","Skipping y_hat=796\n","Skipping y_hat=782\n","Skipping y_hat=976\n","Skipping y_hat=1082\n","Skipping y_hat=1074\n","Skipping y_hat=1093\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=940\n","Skipping y_hat=181\n","Skipping y_hat=310\n","Skipping y_hat=239\n","Skipping y_hat=308\n","Skipping y_hat=452\n","Skipping y_hat=506\n","Skipping y_hat=479\n","Skipping y_hat=806\n","Skipping y_hat=769\n","Skipping y_hat=1044\n","Skipping y_hat=1096\n","Skipping y_hat=241\n","Skipping y_hat=397\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=112\n","Skipping y_hat=315\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=400\n","Skipping y_hat=405\n","Skipping y_hat=533\n","Skipping y_hat=243\n","Skipping y_hat=248\n","Skipping y_hat=450\n","Skipping y_hat=460\n","Skipping y_hat=261\n","Skipping y_hat=1000\n","Skipping y_hat=504\n","Skipping y_hat=724\n","Skipping y_hat=408\n","Skipping y_hat=626\n","Skipping y_hat=461\n","Skipping y_hat=471\n","Skipping y_hat=678\n","Skipping y_hat=492\n","Skipping y_hat=524\n","Skipping y_hat=532\n","Skipping y_hat=751\n","Skipping y_hat=761\n","Skipping y_hat=778\n","Skipping y_hat=922\n","Skipping y_hat=125\n","Skipping y_hat=247\n","Skipping y_hat=1001\n","Skipping y_hat=728\n","Skipping y_hat=751\n","Skipping y_hat=763\n","Skipping y_hat=767\n","Skipping y_hat=783\n","Skipping y_hat=992\n","Skipping y_hat=735\n","Skipping y_hat=785\n","100% 80/80 [00:02<00:00, 33.95it/s]\n","accuracy of 9900 examples: 9788/9900 (98.86868686868688%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.14it/s]\n","accuracy of 10000 examples: 9954/10000 (99.53999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=63\n","Skipping y_hat=20\n","Skipping y_hat=746\n","Skipping y_hat=618\n","Skipping y_hat=876\n","Skipping y_hat=76\n","Skipping y_hat=217\n","Skipping y_hat=119\n","Skipping y_hat=557\n","Skipping y_hat=662\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=991\n","Skipping y_hat=311\n","Skipping y_hat=312\n","Skipping y_hat=555\n","Skipping y_hat=556\n","Skipping y_hat=440\n","Skipping y_hat=779\n","Skipping y_hat=728\n","Skipping y_hat=626\n","Skipping y_hat=802\n","Skipping y_hat=327\n","Skipping y_hat=208\n","Skipping y_hat=347\n","Skipping y_hat=380\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=720\n","Skipping y_hat=758\n","Skipping y_hat=891\n","Skipping y_hat=703\n","Skipping y_hat=1003\n","Skipping y_hat=225\n","Skipping y_hat=197\n","Skipping y_hat=469\n","Skipping y_hat=817\n","Skipping y_hat=229\n","Skipping y_hat=628\n","Skipping y_hat=751\n","Skipping y_hat=704\n","Skipping y_hat=647\n","Skipping y_hat=752\n","Skipping y_hat=787\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=184\n","Skipping y_hat=306\n","Skipping y_hat=847\n","Skipping y_hat=796\n","Skipping y_hat=986\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=1096\n","Skipping y_hat=116\n","Skipping y_hat=396\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=876\n","Skipping y_hat=950\n","Skipping y_hat=181\n","Skipping y_hat=400\n","Skipping y_hat=239\n","Skipping y_hat=308\n","Skipping y_hat=706\n","Skipping y_hat=579\n","Skipping y_hat=796\n","Skipping y_hat=879\n","Skipping y_hat=844\n","Skipping y_hat=1086\n","Skipping y_hat=237\n","Skipping y_hat=397\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=315\n","Skipping y_hat=159\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=900\n","Skipping y_hat=100\n","Skipping y_hat=605\n","Skipping y_hat=415\n","Skipping y_hat=433\n","Skipping y_hat=243\n","Skipping y_hat=143\n","Skipping y_hat=250\n","Skipping y_hat=460\n","Skipping y_hat=461\n","Skipping y_hat=724\n","Skipping y_hat=158\n","Skipping y_hat=559\n","Skipping y_hat=568\n","Skipping y_hat=308\n","Skipping y_hat=306\n","Skipping y_hat=461\n","Skipping y_hat=471\n","Skipping y_hat=678\n","Skipping y_hat=488\n","Skipping y_hat=524\n","Skipping y_hat=732\n","Skipping y_hat=951\n","Skipping y_hat=761\n","Skipping y_hat=578\n","Skipping y_hat=877\n","Skipping y_hat=1001\n","Skipping y_hat=728\n","Skipping y_hat=751\n","Skipping y_hat=963\n","Skipping y_hat=767\n","Skipping y_hat=983\n","Skipping y_hat=1092\n","Skipping y_hat=1004\n","Skipping y_hat=835\n","Skipping y_hat=876\n","Skipping y_hat=883\n","100% 80/80 [00:02<00:00, 34.56it/s]\n","accuracy of 9900 examples: 9779/9900 (98.77777777777777%)\n","\n","Test Results:\n","test_10000_stripped: 98.78%\n","\n","iter 2700: train loss 0.9990, val loss 1.0635\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=48\n","Skipping y_hat=33\n","Skipping y_hat=63\n","Skipping y_hat=546\n","Skipping y_hat=641\n","Skipping y_hat=786\n","Skipping y_hat=1086\n","Skipping y_hat=557\n","Skipping y_hat=614\n","Skipping y_hat=1001\n","Skipping y_hat=221\n","Skipping y_hat=322\n","Skipping y_hat=226\n","Skipping y_hat=355\n","Skipping y_hat=479\n","Skipping y_hat=628\n","Skipping y_hat=227\n","Skipping y_hat=208\n","Skipping y_hat=347\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=658\n","Skipping y_hat=781\n","Skipping y_hat=803\n","Skipping y_hat=3\n","Skipping y_hat=187\n","Skipping y_hat=479\n","Skipping y_hat=807\n","Skipping y_hat=229\n","Skipping y_hat=217\n","Skipping y_hat=628\n","Skipping y_hat=504\n","Skipping y_hat=752\n","Skipping y_hat=877\n","Skipping y_hat=853\n","Skipping y_hat=1005\n","Skipping y_hat=3\n","Skipping y_hat=178\n","Skipping y_hat=157\n","Skipping y_hat=184\n","Skipping y_hat=406\n","Skipping y_hat=796\n","Skipping y_hat=782\n","Skipping y_hat=1072\n","Skipping y_hat=1004\n","Skipping y_hat=116\n","Skipping y_hat=486\n","Skipping y_hat=480\n","Skipping y_hat=653\n","Skipping y_hat=846\n","Skipping y_hat=840\n","Skipping y_hat=141\n","Skipping y_hat=110\n","Skipping y_hat=129\n","Skipping y_hat=806\n","Skipping y_hat=549\n","Skipping y_hat=706\n","Skipping y_hat=749\n","Skipping y_hat=906\n","Skipping y_hat=241\n","Skipping y_hat=261\n","Skipping y_hat=265\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=1001\n","Skipping y_hat=307\n","Skipping y_hat=412\n","Skipping y_hat=315\n","Skipping y_hat=380\n","Skipping y_hat=487\n","Skipping y_hat=200\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=543\n","Skipping y_hat=450\n","Skipping y_hat=460\n","Skipping y_hat=461\n","Skipping y_hat=517\n","Skipping y_hat=724\n","Skipping y_hat=559\n","Skipping y_hat=408\n","Skipping y_hat=726\n","Skipping y_hat=471\n","Skipping y_hat=678\n","Skipping y_hat=488\n","Skipping y_hat=524\n","Skipping y_hat=451\n","Skipping y_hat=578\n","Skipping y_hat=998\n","Skipping y_hat=647\n","Skipping y_hat=877\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=728\n","Skipping y_hat=951\n","Skipping y_hat=763\n","Skipping y_hat=767\n","Skipping y_hat=783\n","Skipping y_hat=992\n","Skipping y_hat=876\n","Skipping y_hat=883\n","100% 80/80 [00:02<00:00, 34.23it/s]\n","accuracy of 9900 examples: 9795/9900 (98.93939393939394%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 37.34it/s]\n","accuracy of 10000 examples: 9963/10000 (99.63%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=63\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=127\n","Skipping y_hat=776\n","Skipping y_hat=86\n","Skipping y_hat=189\n","Skipping y_hat=557\n","Skipping y_hat=614\n","Skipping y_hat=1001\n","Skipping y_hat=1002\n","Skipping y_hat=211\n","Skipping y_hat=322\n","Skipping y_hat=455\n","Skipping y_hat=450\n","Skipping y_hat=679\n","Skipping y_hat=628\n","Skipping y_hat=127\n","Skipping y_hat=308\n","Skipping y_hat=380\n","Skipping y_hat=341\n","Skipping y_hat=440\n","Skipping y_hat=830\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=781\n","Skipping y_hat=723\n","Skipping y_hat=93\n","Skipping y_hat=25\n","Skipping y_hat=817\n","Skipping y_hat=229\n","Skipping y_hat=628\n","Skipping y_hat=504\n","Skipping y_hat=752\n","Skipping y_hat=787\n","Skipping y_hat=853\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=157\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=782\n","Skipping y_hat=866\n","Skipping y_hat=1082\n","Skipping y_hat=1074\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=856\n","Skipping y_hat=860\n","Skipping y_hat=141\n","Skipping y_hat=300\n","Skipping y_hat=229\n","Skipping y_hat=338\n","Skipping y_hat=382\n","Skipping y_hat=706\n","Skipping y_hat=549\n","Skipping y_hat=746\n","Skipping y_hat=849\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=221\n","Skipping y_hat=945\n","Skipping y_hat=241\n","Skipping y_hat=997\n","Skipping y_hat=901\n","Skipping y_hat=412\n","Skipping y_hat=415\n","Skipping y_hat=337\n","Skipping y_hat=459\n","Skipping y_hat=180\n","Skipping y_hat=487\n","Skipping y_hat=200\n","Skipping y_hat=415\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=248\n","Skipping y_hat=250\n","Skipping y_hat=260\n","Skipping y_hat=512\n","Skipping y_hat=917\n","Skipping y_hat=724\n","Skipping y_hat=560\n","Skipping y_hat=826\n","Skipping y_hat=445\n","Skipping y_hat=461\n","Skipping y_hat=471\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=922\n","Skipping y_hat=925\n","Skipping y_hat=841\n","Skipping y_hat=547\n","Skipping y_hat=877\n","Skipping y_hat=1001\n","Skipping y_hat=728\n","Skipping y_hat=751\n","Skipping y_hat=763\n","Skipping y_hat=767\n","Skipping y_hat=835\n","Skipping y_hat=885\n","100% 80/80 [00:02<00:00, 34.02it/s]\n","accuracy of 9900 examples: 9792/9900 (98.9090909090909%)\n","\n","Test Results:\n","test_10000_stripped: 98.91%\n","\n","iter 2800: train loss 0.9937, val loss 1.0675\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=83\n","Skipping y_hat=30\n","Skipping y_hat=39\n","Skipping y_hat=641\n","Skipping y_hat=217\n","Skipping y_hat=618\n","Skipping y_hat=786\n","Skipping y_hat=186\n","Skipping y_hat=132\n","Skipping y_hat=762\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=801\n","Skipping y_hat=882\n","Skipping y_hat=221\n","Skipping y_hat=455\n","Skipping y_hat=450\n","Skipping y_hat=626\n","Skipping y_hat=702\n","Skipping y_hat=227\n","Skipping y_hat=198\n","Skipping y_hat=280\n","Skipping y_hat=351\n","Skipping y_hat=450\n","Skipping y_hat=630\n","Skipping y_hat=658\n","Skipping y_hat=891\n","Skipping y_hat=703\n","Skipping y_hat=13\n","Skipping y_hat=669\n","Skipping y_hat=817\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=451\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=752\n","Skipping y_hat=887\n","Skipping y_hat=853\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=406\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=882\n","Skipping y_hat=886\n","Skipping y_hat=1082\n","Skipping y_hat=884\n","Skipping y_hat=116\n","Skipping y_hat=486\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=856\n","Skipping y_hat=860\n","Skipping y_hat=200\n","Skipping y_hat=119\n","Skipping y_hat=108\n","Skipping y_hat=352\n","Skipping y_hat=816\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=997\n","Skipping y_hat=1001\n","Skipping y_hat=307\n","Skipping y_hat=412\n","Skipping y_hat=315\n","Skipping y_hat=180\n","Skipping y_hat=487\n","Skipping y_hat=405\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=543\n","Skipping y_hat=150\n","Skipping y_hat=460\n","Skipping y_hat=517\n","Skipping y_hat=524\n","Skipping y_hat=671\n","Skipping y_hat=678\n","Skipping y_hat=524\n","Skipping y_hat=551\n","Skipping y_hat=841\n","Skipping y_hat=877\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=763\n","Skipping y_hat=767\n","Skipping y_hat=1092\n","Skipping y_hat=835\n","Skipping y_hat=876\n","Skipping y_hat=785\n","  9% 7/80 [00:00<00:02, 32.26it/s]Skipping y_hat=411\n","100% 80/80 [00:02<00:00, 33.92it/s]\n","accuracy of 9900 examples: 9802/9900 (99.01010101010101%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 37.51it/s]\n","accuracy of 10000 examples: 9973/10000 (99.72999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=33\n","Skipping y_hat=63\n","Skipping y_hat=30\n","Skipping y_hat=841\n","Skipping y_hat=618\n","Skipping y_hat=586\n","Skipping y_hat=132\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=701\n","Skipping y_hat=882\n","Skipping y_hat=221\n","Skipping y_hat=222\n","Skipping y_hat=345\n","Skipping y_hat=556\n","Skipping y_hat=626\n","Skipping y_hat=752\n","Skipping y_hat=127\n","Skipping y_hat=380\n","Skipping y_hat=341\n","Skipping y_hat=450\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=781\n","Skipping y_hat=803\n","Skipping y_hat=3\n","Skipping y_hat=669\n","Skipping y_hat=129\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=451\n","Skipping y_hat=304\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=853\n","Skipping y_hat=1095\n","Skipping y_hat=178\n","Skipping y_hat=184\n","Skipping y_hat=496\n","Skipping y_hat=547\n","Skipping y_hat=796\n","Skipping y_hat=882\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=116\n","Skipping y_hat=486\n","Skipping y_hat=480\n","Skipping y_hat=853\n","Skipping y_hat=856\n","Skipping y_hat=860\n","Skipping y_hat=100\n","Skipping y_hat=119\n","Skipping y_hat=318\n","Skipping y_hat=342\n","Skipping y_hat=816\n","Skipping y_hat=649\n","Skipping y_hat=806\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=241\n","Skipping y_hat=296\n","Skipping y_hat=997\n","Skipping y_hat=1001\n","Skipping y_hat=307\n","Skipping y_hat=412\n","Skipping y_hat=315\n","Skipping y_hat=359\n","Skipping y_hat=180\n","Skipping y_hat=487\n","Skipping y_hat=405\n","Skipping y_hat=415\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=150\n","Skipping y_hat=260\n","Skipping y_hat=617\n","Skipping y_hat=624\n","Skipping y_hat=568\n","Skipping y_hat=461\n","Skipping y_hat=671\n","Skipping y_hat=678\n","Skipping y_hat=488\n","Skipping y_hat=551\n","Skipping y_hat=581\n","Skipping y_hat=825\n","Skipping y_hat=841\n","Skipping y_hat=660\n","Skipping y_hat=701\n","Skipping y_hat=728\n","Skipping y_hat=951\n","Skipping y_hat=763\n","Skipping y_hat=767\n","Skipping y_hat=492\n","Skipping y_hat=835\n","Skipping y_hat=876\n","Skipping y_hat=885\n","100% 80/80 [00:02<00:00, 34.47it/s]\n","accuracy of 9900 examples: 9799/9900 (98.97979797979798%)\n","\n","Test Results:\n","test_10000_stripped: 98.98%\n","\n","iter 2900: train loss 0.9946, val loss 1.0758\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=48\n","Skipping y_hat=28\n","Skipping y_hat=33\n","Skipping y_hat=83\n","Skipping y_hat=20\n","Skipping y_hat=22\n","Skipping y_hat=546\n","Skipping y_hat=641\n","Skipping y_hat=586\n","Skipping y_hat=1086\n","Skipping y_hat=557\n","Skipping y_hat=552\n","Skipping y_hat=614\n","Skipping y_hat=1001\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=479\n","Skipping y_hat=628\n","Skipping y_hat=626\n","Skipping y_hat=702\n","Skipping y_hat=227\n","Skipping y_hat=208\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=440\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=616\n","Skipping y_hat=703\n","Skipping y_hat=3\n","Skipping y_hat=187\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=461\n","Skipping y_hat=404\n","Skipping y_hat=547\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=496\n","Skipping y_hat=692\n","Skipping y_hat=1072\n","Skipping y_hat=1004\n","Skipping y_hat=1093\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=543\n","Skipping y_hat=866\n","Skipping y_hat=940\n","Skipping y_hat=200\n","Skipping y_hat=239\n","Skipping y_hat=328\n","Skipping y_hat=716\n","Skipping y_hat=806\n","Skipping y_hat=749\n","Skipping y_hat=1044\n","Skipping y_hat=1006\n","Skipping y_hat=273\n","Skipping y_hat=996\n","Skipping y_hat=997\n","Skipping y_hat=1001\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=400\n","Skipping y_hat=405\n","Skipping y_hat=415\n","Skipping y_hat=533\n","Skipping y_hat=243\n","Skipping y_hat=543\n","Skipping y_hat=450\n","Skipping y_hat=517\n","Skipping y_hat=524\n","Skipping y_hat=559\n","Skipping y_hat=568\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=426\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=451\n","Skipping y_hat=598\n","Skipping y_hat=625\n","Skipping y_hat=660\n","Skipping y_hat=877\n","Skipping y_hat=593\n","Skipping y_hat=1001\n","Skipping y_hat=928\n","Skipping y_hat=751\n","Skipping y_hat=963\n","Skipping y_hat=767\n","Skipping y_hat=983\n","Skipping y_hat=492\n","100% 80/80 [00:02<00:00, 33.54it/s]\n","accuracy of 9900 examples: 9796/9900 (98.94949494949495%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 39.08it/s]\n","accuracy of 10000 examples: 9968/10000 (99.68%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=33\n","Skipping y_hat=83\n","Skipping y_hat=14\n","Skipping y_hat=546\n","Skipping y_hat=841\n","Skipping y_hat=586\n","Skipping y_hat=116\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=1001\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=440\n","Skipping y_hat=489\n","Skipping y_hat=628\n","Skipping y_hat=626\n","Skipping y_hat=702\n","Skipping y_hat=127\n","Skipping y_hat=208\n","Skipping y_hat=347\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=440\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=516\n","Skipping y_hat=503\n","Skipping y_hat=3\n","Skipping y_hat=669\n","Skipping y_hat=229\n","Skipping y_hat=628\n","Skipping y_hat=551\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=787\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=157\n","Skipping y_hat=384\n","Skipping y_hat=406\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=782\n","Skipping y_hat=1072\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=1096\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=856\n","Skipping y_hat=950\n","Skipping y_hat=300\n","Skipping y_hat=139\n","Skipping y_hat=338\n","Skipping y_hat=402\n","Skipping y_hat=706\n","Skipping y_hat=549\n","Skipping y_hat=816\n","Skipping y_hat=749\n","Skipping y_hat=1044\n","Skipping y_hat=1006\n","Skipping y_hat=965\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=1001\n","Skipping y_hat=307\n","Skipping y_hat=412\n","Skipping y_hat=415\n","Skipping y_hat=359\n","Skipping y_hat=680\n","Skipping y_hat=487\n","Skipping y_hat=1000\n","Skipping y_hat=405\n","Skipping y_hat=415\n","Skipping y_hat=533\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=450\n","Skipping y_hat=260\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=461\n","Skipping y_hat=488\n","Skipping y_hat=492\n","Skipping y_hat=511\n","Skipping y_hat=532\n","Skipping y_hat=451\n","Skipping y_hat=561\n","Skipping y_hat=578\n","Skipping y_hat=598\n","Skipping y_hat=822\n","Skipping y_hat=647\n","Skipping y_hat=877\n","Skipping y_hat=1001\n","Skipping y_hat=928\n","Skipping y_hat=751\n","Skipping y_hat=763\n","Skipping y_hat=967\n","Skipping y_hat=992\n","Skipping y_hat=1035\n","Skipping y_hat=1040\n","Skipping y_hat=1085\n","100% 80/80 [00:02<00:00, 34.38it/s]\n","accuracy of 9900 examples: 9789/9900 (98.87878787878788%)\n","\n","Test Results:\n","test_10000_stripped: 98.88%\n","\n","iter 3000: train loss 0.9899, val loss 1.0716\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=83\n","Skipping y_hat=34\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=186\n","Skipping y_hat=217\n","Skipping y_hat=212\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=799\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=426\n","Skipping y_hat=545\n","Skipping y_hat=400\n","Skipping y_hat=679\n","Skipping y_hat=626\n","Skipping y_hat=752\n","Skipping y_hat=127\n","Skipping y_hat=408\n","Skipping y_hat=341\n","Skipping y_hat=450\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=781\n","Skipping y_hat=723\n","Skipping y_hat=903\n","Skipping y_hat=225\n","Skipping y_hat=187\n","Skipping y_hat=669\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=361\n","Skipping y_hat=304\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=887\n","Skipping y_hat=853\n","Skipping y_hat=278\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=772\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=906\n","Skipping y_hat=116\n","Skipping y_hat=486\n","Skipping y_hat=480\n","Skipping y_hat=843\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=300\n","Skipping y_hat=229\n","Skipping y_hat=338\n","Skipping y_hat=402\n","Skipping y_hat=706\n","Skipping y_hat=469\n","Skipping y_hat=856\n","Skipping y_hat=769\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=265\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=412\n","Skipping y_hat=115\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=405\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=248\n","Skipping y_hat=150\n","Skipping y_hat=260\n","Skipping y_hat=517\n","Skipping y_hat=724\n","Skipping y_hat=560\n","Skipping y_hat=568\n","Skipping y_hat=408\n","Skipping y_hat=626\n","Skipping y_hat=488\n","Skipping y_hat=724\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=578\n","Skipping y_hat=581\n","Skipping y_hat=825\n","Skipping y_hat=877\n","Skipping y_hat=893\n","Skipping y_hat=901\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=835\n","Skipping y_hat=885\n","100% 80/80 [00:02<00:00, 33.84it/s]\n","accuracy of 9900 examples: 9797/9900 (98.95959595959596%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.44it/s]\n","accuracy of 10000 examples: 9983/10000 (99.83%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=6\n","Skipping y_hat=618\n","Skipping y_hat=786\n","Skipping y_hat=657\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=882\n","Skipping y_hat=321\n","Skipping y_hat=222\n","Skipping y_hat=226\n","Skipping y_hat=455\n","Skipping y_hat=556\n","Skipping y_hat=440\n","Skipping y_hat=738\n","Skipping y_hat=227\n","Skipping y_hat=308\n","Skipping y_hat=441\n","Skipping y_hat=440\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=881\n","Skipping y_hat=823\n","Skipping y_hat=913\n","Skipping y_hat=225\n","Skipping y_hat=187\n","Skipping y_hat=689\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=528\n","Skipping y_hat=461\n","Skipping y_hat=704\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=853\n","Skipping y_hat=905\n","Skipping y_hat=3\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=896\n","Skipping y_hat=682\n","Skipping y_hat=1082\n","Skipping y_hat=1004\n","Skipping y_hat=893\n","Skipping y_hat=216\n","Skipping y_hat=680\n","Skipping y_hat=853\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=310\n","Skipping y_hat=239\n","Skipping y_hat=328\n","Skipping y_hat=402\n","Skipping y_hat=806\n","Skipping y_hat=569\n","Skipping y_hat=816\n","Skipping y_hat=869\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=261\n","Skipping y_hat=965\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=115\n","Skipping y_hat=337\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=143\n","Skipping y_hat=150\n","Skipping y_hat=260\n","Skipping y_hat=461\n","Skipping y_hat=300\n","Skipping y_hat=517\n","Skipping y_hat=524\n","Skipping y_hat=560\n","Skipping y_hat=568\n","Skipping y_hat=608\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=581\n","Skipping y_hat=598\n","Skipping y_hat=922\n","Skipping y_hat=877\n","Skipping y_hat=693\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=767\n","Skipping y_hat=492\n","Skipping y_hat=835\n","Skipping y_hat=876\n","Skipping y_hat=785\n","100% 80/80 [00:02<00:00, 34.04it/s]\n","accuracy of 9900 examples: 9794/9900 (98.92929292929293%)\n","\n","Test Results:\n","test_10000_stripped: 98.93%\n","\n","iter 3100: train loss 0.9926, val loss 1.0716\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=33\n","Skipping y_hat=83\n","Skipping y_hat=14\n","Skipping y_hat=20\n","Skipping y_hat=30\n","Skipping y_hat=756\n","Skipping y_hat=841\n","Skipping y_hat=618\n","Skipping y_hat=786\n","Skipping y_hat=222\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=426\n","Skipping y_hat=556\n","Skipping y_hat=450\n","Skipping y_hat=479\n","Skipping y_hat=816\n","Skipping y_hat=802\n","Skipping y_hat=127\n","Skipping y_hat=208\n","Skipping y_hat=441\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=891\n","Skipping y_hat=893\n","Skipping y_hat=3\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=628\n","Skipping y_hat=461\n","Skipping y_hat=1504\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1053\n","Skipping y_hat=905\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=357\n","Skipping y_hat=384\n","Skipping y_hat=406\n","Skipping y_hat=896\n","Skipping y_hat=782\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=843\n","Skipping y_hat=866\n","Skipping y_hat=840\n","Skipping y_hat=181\n","Skipping y_hat=200\n","Skipping y_hat=309\n","Skipping y_hat=308\n","Skipping y_hat=402\n","Skipping y_hat=706\n","Skipping y_hat=579\n","Skipping y_hat=806\n","Skipping y_hat=749\n","Skipping y_hat=944\n","Skipping y_hat=1006\n","Skipping y_hat=965\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=1001\n","Skipping y_hat=307\n","Skipping y_hat=412\n","Skipping y_hat=115\n","Skipping y_hat=337\n","Skipping y_hat=380\n","Skipping y_hat=487\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=543\n","Skipping y_hat=300\n","Skipping y_hat=617\n","Skipping y_hat=624\n","Skipping y_hat=408\n","Skipping y_hat=606\n","Skipping y_hat=826\n","Skipping y_hat=678\n","Skipping y_hat=551\n","Skipping y_hat=761\n","Skipping y_hat=815\n","Skipping y_hat=825\n","Skipping y_hat=841\n","Skipping y_hat=877\n","Skipping y_hat=901\n","Skipping y_hat=951\n","Skipping y_hat=763\n","Skipping y_hat=967\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 33.90it/s]\n","accuracy of 9900 examples: 9803/9900 (99.02020202020202%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.80it/s]\n","accuracy of 10000 examples: 9986/10000 (99.86%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=33\n","Skipping y_hat=22\n","Skipping y_hat=30\n","Skipping y_hat=756\n","Skipping y_hat=841\n","Skipping y_hat=608\n","Skipping y_hat=586\n","Skipping y_hat=906\n","Skipping y_hat=232\n","Skipping y_hat=109\n","Skipping y_hat=557\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=332\n","Skipping y_hat=455\n","Skipping y_hat=450\n","Skipping y_hat=613\n","Skipping y_hat=679\n","Skipping y_hat=628\n","Skipping y_hat=616\n","Skipping y_hat=702\n","Skipping y_hat=227\n","Skipping y_hat=208\n","Skipping y_hat=380\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=891\n","Skipping y_hat=803\n","Skipping y_hat=913\n","Skipping y_hat=679\n","Skipping y_hat=229\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=1504\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=887\n","Skipping y_hat=1053\n","Skipping y_hat=1095\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=357\n","Skipping y_hat=284\n","Skipping y_hat=406\n","Skipping y_hat=637\n","Skipping y_hat=796\n","Skipping y_hat=782\n","Skipping y_hat=1082\n","Skipping y_hat=884\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=843\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=200\n","Skipping y_hat=329\n","Skipping y_hat=308\n","Skipping y_hat=402\n","Skipping y_hat=706\n","Skipping y_hat=579\n","Skipping y_hat=896\n","Skipping y_hat=944\n","Skipping y_hat=241\n","Skipping y_hat=996\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=337\n","Skipping y_hat=159\n","Skipping y_hat=380\n","Skipping y_hat=387\n","Skipping y_hat=415\n","Skipping y_hat=243\n","Skipping y_hat=250\n","Skipping y_hat=624\n","Skipping y_hat=568\n","Skipping y_hat=606\n","Skipping y_hat=626\n","Skipping y_hat=626\n","Skipping y_hat=678\n","Skipping y_hat=815\n","Skipping y_hat=825\n","Skipping y_hat=841\n","Skipping y_hat=875\n","Skipping y_hat=877\n","Skipping y_hat=893\n","Skipping y_hat=901\n","Skipping y_hat=951\n","Skipping y_hat=967\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 34.20it/s]\n","accuracy of 9900 examples: 9801/9900 (99.0%)\n","\n","Test Results:\n","test_10000_stripped: 99.00%\n","\n","iter 3200: train loss 0.9842, val loss 1.0802\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=63\n","Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=786\n","Skipping y_hat=299\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=1001\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=226\n","Skipping y_hat=450\n","Skipping y_hat=479\n","Skipping y_hat=752\n","Skipping y_hat=127\n","Skipping y_hat=490\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=781\n","Skipping y_hat=823\n","Skipping y_hat=3\n","Skipping y_hat=669\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=628\n","Skipping y_hat=461\n","Skipping y_hat=494\n","Skipping y_hat=732\n","Skipping y_hat=787\n","Skipping y_hat=53\n","Skipping y_hat=1905\n","Skipping y_hat=178\n","Skipping y_hat=357\n","Skipping y_hat=384\n","Skipping y_hat=406\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=782\n","Skipping y_hat=1002\n","Skipping y_hat=1084\n","Skipping y_hat=996\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=200\n","Skipping y_hat=308\n","Skipping y_hat=706\n","Skipping y_hat=549\n","Skipping y_hat=806\n","Skipping y_hat=849\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=241\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=487\n","Skipping y_hat=200\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=250\n","Skipping y_hat=260\n","Skipping y_hat=300\n","Skipping y_hat=517\n","Skipping y_hat=524\n","Skipping y_hat=560\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=626\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=581\n","Skipping y_hat=598\n","Skipping y_hat=622\n","Skipping y_hat=841\n","Skipping y_hat=877\n","Skipping y_hat=1001\n","Skipping y_hat=751\n","Skipping y_hat=763\n","Skipping y_hat=992\n","Skipping y_hat=876\n","Skipping y_hat=885\n","  4% 3/80 [00:00<00:02, 29.84it/s]Skipping y_hat=950\n","100% 80/80 [00:02<00:00, 33.90it/s]\n","accuracy of 9900 examples: 9810/9900 (99.0909090909091%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 37.53it/s]\n","accuracy of 10000 examples: 9992/10000 (99.92%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=22\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=841\n","Skipping y_hat=618\n","Skipping y_hat=586\n","Skipping y_hat=537\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=619\n","Skipping y_hat=801\n","Skipping y_hat=221\n","Skipping y_hat=222\n","Skipping y_hat=450\n","Skipping y_hat=628\n","Skipping y_hat=626\n","Skipping y_hat=712\n","Skipping y_hat=127\n","Skipping y_hat=208\n","Skipping y_hat=347\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=440\n","Skipping y_hat=720\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=923\n","Skipping y_hat=903\n","Skipping y_hat=669\n","Skipping y_hat=229\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=953\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=394\n","Skipping y_hat=406\n","Skipping y_hat=727\n","Skipping y_hat=866\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=906\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=733\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=300\n","Skipping y_hat=339\n","Skipping y_hat=308\n","Skipping y_hat=449\n","Skipping y_hat=816\n","Skipping y_hat=749\n","Skipping y_hat=1096\n","Skipping y_hat=241\n","Skipping y_hat=273\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=315\n","Skipping y_hat=487\n","Skipping y_hat=200\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=250\n","Skipping y_hat=260\n","Skipping y_hat=461\n","Skipping y_hat=517\n","Skipping y_hat=124\n","Skipping y_hat=560\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=675\n","Skipping y_hat=678\n","Skipping y_hat=551\n","Skipping y_hat=361\n","Skipping y_hat=598\n","Skipping y_hat=841\n","Skipping y_hat=660\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=767\n","Skipping y_hat=983\n","Skipping y_hat=992\n","Skipping y_hat=1004\n","100% 80/80 [00:02<00:00, 34.08it/s]\n","accuracy of 9900 examples: 9805/9900 (99.04040404040404%)\n","\n","Test Results:\n","test_10000_stripped: 99.04%\n","\n","iter 3300: train loss 0.9649, val loss 1.0886\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=746\n","Skipping y_hat=586\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=426\n","Skipping y_hat=440\n","Skipping y_hat=613\n","Skipping y_hat=679\n","Skipping y_hat=712\n","Skipping y_hat=227\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=781\n","Skipping y_hat=813\n","Skipping y_hat=903\n","Skipping y_hat=187\n","Skipping y_hat=669\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=953\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=347\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=782\n","Skipping y_hat=1082\n","Skipping y_hat=1093\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=863\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=310\n","Skipping y_hat=329\n","Skipping y_hat=308\n","Skipping y_hat=806\n","Skipping y_hat=549\n","Skipping y_hat=816\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=773\n","Skipping y_hat=297\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=315\n","Skipping y_hat=359\n","Skipping y_hat=380\n","Skipping y_hat=487\n","Skipping y_hat=405\n","Skipping y_hat=415\n","Skipping y_hat=243\n","Skipping y_hat=260\n","Skipping y_hat=461\n","Skipping y_hat=304\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=626\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=561\n","Skipping y_hat=622\n","Skipping y_hat=841\n","Skipping y_hat=677\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=767\n","Skipping y_hat=992\n","Skipping y_hat=835\n","Skipping y_hat=876\n","100% 80/80 [00:02<00:00, 33.42it/s]\n","accuracy of 9900 examples: 9803/9900 (99.02020202020202%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.42it/s]\n","accuracy of 10000 examples: 9989/10000 (99.89%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=30\n","Skipping y_hat=556\n","Skipping y_hat=841\n","Skipping y_hat=217\n","Skipping y_hat=618\n","Skipping y_hat=786\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=426\n","Skipping y_hat=355\n","Skipping y_hat=613\n","Skipping y_hat=679\n","Skipping y_hat=826\n","Skipping y_hat=752\n","Skipping y_hat=227\n","Skipping y_hat=208\n","Skipping y_hat=247\n","Skipping y_hat=480\n","Skipping y_hat=450\n","Skipping y_hat=660\n","Skipping y_hat=658\n","Skipping y_hat=781\n","Skipping y_hat=893\n","Skipping y_hat=913\n","Skipping y_hat=669\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=752\n","Skipping y_hat=807\n","Skipping y_hat=953\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=596\n","Skipping y_hat=782\n","Skipping y_hat=1072\n","Skipping y_hat=1096\n","Skipping y_hat=116\n","Skipping y_hat=386\n","Skipping y_hat=753\n","Skipping y_hat=866\n","Skipping y_hat=760\n","Skipping y_hat=181\n","Skipping y_hat=200\n","Skipping y_hat=229\n","Skipping y_hat=308\n","Skipping y_hat=716\n","Skipping y_hat=549\n","Skipping y_hat=816\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=906\n","Skipping y_hat=273\n","Skipping y_hat=1001\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=315\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=200\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=543\n","Skipping y_hat=260\n","Skipping y_hat=517\n","Skipping y_hat=524\n","Skipping y_hat=568\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=626\n","Skipping y_hat=678\n","Skipping y_hat=551\n","Skipping y_hat=561\n","Skipping y_hat=778\n","Skipping y_hat=841\n","Skipping y_hat=877\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 34.13it/s]\n","accuracy of 9900 examples: 9806/9900 (99.05050505050505%)\n","\n","Test Results:\n","test_10000_stripped: 99.05%\n","\n","iter 3400: train loss 0.9609, val loss 1.0971\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=34\n","Skipping y_hat=746\n","Skipping y_hat=586\n","Skipping y_hat=217\n","Skipping y_hat=232\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=780\n","Skipping y_hat=801\n","Skipping y_hat=331\n","Skipping y_hat=322\n","Skipping y_hat=455\n","Skipping y_hat=470\n","Skipping y_hat=679\n","Skipping y_hat=616\n","Skipping y_hat=752\n","Skipping y_hat=227\n","Skipping y_hat=490\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=630\n","Skipping y_hat=668\n","Skipping y_hat=823\n","Skipping y_hat=3\n","Skipping y_hat=669\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=618\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=547\n","Skipping y_hat=742\n","Skipping y_hat=707\n","Skipping y_hat=953\n","Skipping y_hat=905\n","Skipping y_hat=103\n","Skipping y_hat=278\n","Skipping y_hat=357\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=627\n","Skipping y_hat=796\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=960\n","Skipping y_hat=181\n","Skipping y_hat=300\n","Skipping y_hat=329\n","Skipping y_hat=308\n","Skipping y_hat=392\n","Skipping y_hat=506\n","Skipping y_hat=549\n","Skipping y_hat=806\n","Skipping y_hat=779\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=241\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=112\n","Skipping y_hat=315\n","Skipping y_hat=359\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=415\n","Skipping y_hat=243\n","Skipping y_hat=450\n","Skipping y_hat=260\n","Skipping y_hat=461\n","Skipping y_hat=517\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=471\n","Skipping y_hat=678\n","Skipping y_hat=551\n","Skipping y_hat=561\n","Skipping y_hat=581\n","Skipping y_hat=825\n","Skipping y_hat=1001\n","Skipping y_hat=751\n","Skipping y_hat=963\n","Skipping y_hat=767\n","Skipping y_hat=992\n","Skipping y_hat=835\n","100% 80/80 [00:02<00:00, 33.63it/s]\n","accuracy of 9900 examples: 9805/9900 (99.04040404040404%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.63it/s]\n","accuracy of 10000 examples: 9991/10000 (99.91%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=83\n","Skipping y_hat=546\n","Skipping y_hat=586\n","Skipping y_hat=1086\n","Skipping y_hat=222\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=662\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=801\n","Skipping y_hat=882\n","Skipping y_hat=321\n","Skipping y_hat=222\n","Skipping y_hat=450\n","Skipping y_hat=613\n","Skipping y_hat=479\n","Skipping y_hat=626\n","Skipping y_hat=752\n","Skipping y_hat=227\n","Skipping y_hat=318\n","Skipping y_hat=490\n","Skipping y_hat=440\n","Skipping y_hat=630\n","Skipping y_hat=616\n","Skipping y_hat=823\n","Skipping y_hat=903\n","Skipping y_hat=187\n","Skipping y_hat=669\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=761\n","Skipping y_hat=647\n","Skipping y_hat=887\n","Skipping y_hat=953\n","Skipping y_hat=905\n","Skipping y_hat=213\n","Skipping y_hat=178\n","Skipping y_hat=357\n","Skipping y_hat=384\n","Skipping y_hat=406\n","Skipping y_hat=537\n","Skipping y_hat=406\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=316\n","Skipping y_hat=486\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=328\n","Skipping y_hat=506\n","Skipping y_hat=549\n","Skipping y_hat=716\n","Skipping y_hat=779\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=1001\n","Skipping y_hat=107\n","Skipping y_hat=315\n","Skipping y_hat=337\n","Skipping y_hat=487\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=250\n","Skipping y_hat=304\n","Skipping y_hat=517\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=626\n","Skipping y_hat=645\n","Skipping y_hat=461\n","Skipping y_hat=471\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=581\n","Skipping y_hat=1001\n","Skipping y_hat=728\n","Skipping y_hat=951\n","Skipping y_hat=983\n","Skipping y_hat=492\n","100% 80/80 [00:02<00:00, 33.97it/s]\n","accuracy of 9900 examples: 9805/9900 (99.04040404040404%)\n","\n","Test Results:\n","test_10000_stripped: 99.04%\n","\n","iter 3500: train loss 0.9558, val loss 1.0966\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=33\n","Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=618\n","Skipping y_hat=586\n","Skipping y_hat=186\n","Skipping y_hat=557\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=332\n","Skipping y_hat=226\n","Skipping y_hat=613\n","Skipping y_hat=738\n","Skipping y_hat=802\n","Skipping y_hat=227\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=801\n","Skipping y_hat=803\n","Skipping y_hat=3\n","Skipping y_hat=669\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=618\n","Skipping y_hat=404\n","Skipping y_hat=747\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=267\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=637\n","Skipping y_hat=796\n","Skipping y_hat=702\n","Skipping y_hat=866\n","Skipping y_hat=82\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=310\n","Skipping y_hat=319\n","Skipping y_hat=308\n","Skipping y_hat=579\n","Skipping y_hat=816\n","Skipping y_hat=869\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=315\n","Skipping y_hat=159\n","Skipping y_hat=480\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=250\n","Skipping y_hat=260\n","Skipping y_hat=300\n","Skipping y_hat=524\n","Skipping y_hat=568\n","Skipping y_hat=406\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=561\n","Skipping y_hat=581\n","Skipping y_hat=825\n","Skipping y_hat=877\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=767\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 33.71it/s]\n","accuracy of 9900 examples: 9813/9900 (99.12121212121212%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.57it/s]\n","accuracy of 10000 examples: 9994/10000 (99.94%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=33\n","Skipping y_hat=83\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=841\n","Skipping y_hat=786\n","Skipping y_hat=186\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=226\n","Skipping y_hat=355\n","Skipping y_hat=679\n","Skipping y_hat=802\n","Skipping y_hat=227\n","Skipping y_hat=208\n","Skipping y_hat=300\n","Skipping y_hat=450\n","Skipping y_hat=750\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=823\n","Skipping y_hat=3\n","Skipping y_hat=187\n","Skipping y_hat=669\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=618\n","Skipping y_hat=304\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=213\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=702\n","Skipping y_hat=886\n","Skipping y_hat=82\n","Skipping y_hat=1084\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=763\n","Skipping y_hat=856\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=310\n","Skipping y_hat=319\n","Skipping y_hat=308\n","Skipping y_hat=402\n","Skipping y_hat=569\n","Skipping y_hat=896\n","Skipping y_hat=879\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=337\n","Skipping y_hat=380\n","Skipping y_hat=387\n","Skipping y_hat=200\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=250\n","Skipping y_hat=304\n","Skipping y_hat=317\n","Skipping y_hat=568\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=471\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=578\n","Skipping y_hat=581\n","Skipping y_hat=598\n","Skipping y_hat=841\n","Skipping y_hat=677\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=992\n","Skipping y_hat=835\n","Skipping y_hat=585\n","100% 80/80 [00:02<00:00, 34.47it/s]\n","accuracy of 9900 examples: 9802/9900 (99.01010101010101%)\n","\n","Test Results:\n","test_10000_stripped: 99.01%\n","\n","iter 3600: train loss 0.9473, val loss 1.1066\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=33\n","Skipping y_hat=83\n","Skipping y_hat=30\n","Skipping y_hat=260\n","Skipping y_hat=546\n","Skipping y_hat=841\n","Skipping y_hat=196\n","Skipping y_hat=217\n","Skipping y_hat=299\n","Skipping y_hat=614\n","Skipping y_hat=701\n","Skipping y_hat=221\n","Skipping y_hat=222\n","Skipping y_hat=455\n","Skipping y_hat=356\n","Skipping y_hat=450\n","Skipping y_hat=479\n","Skipping y_hat=626\n","Skipping y_hat=802\n","Skipping y_hat=227\n","Skipping y_hat=208\n","Skipping y_hat=380\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=803\n","Skipping y_hat=913\n","Skipping y_hat=669\n","Skipping y_hat=329\n","Skipping y_hat=628\n","Skipping y_hat=761\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=278\n","Skipping y_hat=384\n","Skipping y_hat=406\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=1002\n","Skipping y_hat=884\n","Skipping y_hat=906\n","Skipping y_hat=116\n","Skipping y_hat=490\n","Skipping y_hat=753\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=100\n","Skipping y_hat=119\n","Skipping y_hat=108\n","Skipping y_hat=569\n","Skipping y_hat=816\n","Skipping y_hat=869\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=965\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=1001\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=337\n","Skipping y_hat=159\n","Skipping y_hat=480\n","Skipping y_hat=387\n","Skipping y_hat=200\n","Skipping y_hat=405\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=443\n","Skipping y_hat=243\n","Skipping y_hat=450\n","Skipping y_hat=460\n","Skipping y_hat=300\n","Skipping y_hat=504\n","Skipping y_hat=524\n","Skipping y_hat=606\n","Skipping y_hat=678\n","Skipping y_hat=551\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=751\n","Skipping y_hat=963\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 33.79it/s]\n","accuracy of 9900 examples: 9810/9900 (99.0909090909091%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.74it/s]\n","accuracy of 10000 examples: 9995/10000 (99.95%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=83\n","Skipping y_hat=30\n","Skipping y_hat=618\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=800\n","Skipping y_hat=801\n","Skipping y_hat=992\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=556\n","Skipping y_hat=450\n","Skipping y_hat=613\n","Skipping y_hat=802\n","Skipping y_hat=227\n","Skipping y_hat=208\n","Skipping y_hat=380\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=701\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=547\n","Skipping y_hat=742\n","Skipping y_hat=953\n","Skipping y_hat=1005\n","Skipping y_hat=213\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=406\n","Skipping y_hat=537\n","Skipping y_hat=886\n","Skipping y_hat=1002\n","Skipping y_hat=1004\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=856\n","Skipping y_hat=860\n","Skipping y_hat=200\n","Skipping y_hat=108\n","Skipping y_hat=506\n","Skipping y_hat=579\n","Skipping y_hat=816\n","Skipping y_hat=879\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=965\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=180\n","Skipping y_hat=487\n","Skipping y_hat=200\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=543\n","Skipping y_hat=250\n","Skipping y_hat=260\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=524\n","Skipping y_hat=626\n","Skipping y_hat=532\n","Skipping y_hat=561\n","Skipping y_hat=581\n","Skipping y_hat=598\n","Skipping y_hat=660\n","Skipping y_hat=877\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=963\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 34.18it/s]\n","accuracy of 9900 examples: 9816/9900 (99.15151515151514%)\n","\n","Test Results:\n","test_10000_stripped: 99.15%\n","\n","iter 3700: train loss 0.9449, val loss 1.1112\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=786\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=426\n","Skipping y_hat=455\n","Skipping y_hat=450\n","Skipping y_hat=679\n","Skipping y_hat=752\n","Skipping y_hat=327\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=504\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=1002\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=200\n","Skipping y_hat=209\n","Skipping y_hat=308\n","Skipping y_hat=402\n","Skipping y_hat=506\n","Skipping y_hat=549\n","Skipping y_hat=816\n","Skipping y_hat=869\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=159\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=200\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=260\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=317\n","Skipping y_hat=524\n","Skipping y_hat=568\n","Skipping y_hat=408\n","Skipping y_hat=678\n","Skipping y_hat=561\n","Skipping y_hat=581\n","Skipping y_hat=660\n","Skipping y_hat=893\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","100% 80/80 [00:02<00:00, 34.20it/s]\n","accuracy of 9900 examples: 9814/9900 (99.13131313131314%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.38it/s]\n","accuracy of 10000 examples: 9998/10000 (99.98%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=546\n","Skipping y_hat=618\n","Skipping y_hat=586\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=450\n","Skipping y_hat=738\n","Skipping y_hat=626\n","Skipping y_hat=842\n","Skipping y_hat=227\n","Skipping y_hat=208\n","Skipping y_hat=490\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=658\n","Skipping y_hat=891\n","Skipping y_hat=803\n","Skipping y_hat=3\n","Skipping y_hat=229\n","Skipping y_hat=618\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=905\n","Skipping y_hat=103\n","Skipping y_hat=278\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=1082\n","Skipping y_hat=884\n","Skipping y_hat=116\n","Skipping y_hat=486\n","Skipping y_hat=680\n","Skipping y_hat=753\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=200\n","Skipping y_hat=308\n","Skipping y_hat=482\n","Skipping y_hat=416\n","Skipping y_hat=816\n","Skipping y_hat=779\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=965\n","Skipping y_hat=273\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=112\n","Skipping y_hat=115\n","Skipping y_hat=159\n","Skipping y_hat=380\n","Skipping y_hat=487\n","Skipping y_hat=200\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=260\n","Skipping y_hat=300\n","Skipping y_hat=317\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=578\n","Skipping y_hat=581\n","Skipping y_hat=877\n","Skipping y_hat=893\n","Skipping y_hat=901\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=792\n","Skipping y_hat=835\n","100% 80/80 [00:02<00:00, 34.27it/s]\n","accuracy of 9900 examples: 9814/9900 (99.13131313131314%)\n","\n","Test Results:\n","test_10000_stripped: 99.13%\n","\n","iter 3800: train loss 0.9361, val loss 1.1189\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=20\n","Skipping y_hat=22\n","Skipping y_hat=546\n","Skipping y_hat=618\n","Skipping y_hat=586\n","Skipping y_hat=186\n","Skipping y_hat=217\n","Skipping y_hat=557\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=455\n","Skipping y_hat=450\n","Skipping y_hat=613\n","Skipping y_hat=679\n","Skipping y_hat=227\n","Skipping y_hat=398\n","Skipping y_hat=480\n","Skipping y_hat=450\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=893\n","Skipping y_hat=903\n","Skipping y_hat=669\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=953\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=866\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=856\n","Skipping y_hat=960\n","Skipping y_hat=181\n","Skipping y_hat=300\n","Skipping y_hat=319\n","Skipping y_hat=308\n","Skipping y_hat=816\n","Skipping y_hat=769\n","Skipping y_hat=844\n","Skipping y_hat=1086\n","Skipping y_hat=773\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=315\n","Skipping y_hat=359\n","Skipping y_hat=487\n","Skipping y_hat=415\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=250\n","Skipping y_hat=460\n","Skipping y_hat=300\n","Skipping y_hat=724\n","Skipping y_hat=568\n","Skipping y_hat=406\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=581\n","Skipping y_hat=877\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=992\n","Skipping y_hat=840\n","Skipping y_hat=885\n","100% 80/80 [00:02<00:00, 33.74it/s]\n","accuracy of 9900 examples: 9814/9900 (99.13131313131314%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 39.01it/s]\n","accuracy of 10000 examples: 9994/10000 (99.94%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=14\n","Skipping y_hat=546\n","Skipping y_hat=886\n","Skipping y_hat=132\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=701\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=426\n","Skipping y_hat=455\n","Skipping y_hat=440\n","Skipping y_hat=679\n","Skipping y_hat=626\n","Skipping y_hat=227\n","Skipping y_hat=308\n","Skipping y_hat=480\n","Skipping y_hat=450\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=881\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=229\n","Skipping y_hat=618\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=953\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=782\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=993\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=843\n","Skipping y_hat=866\n","Skipping y_hat=200\n","Skipping y_hat=318\n","Skipping y_hat=482\n","Skipping y_hat=569\n","Skipping y_hat=816\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=965\n","Skipping y_hat=773\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=315\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=405\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=460\n","Skipping y_hat=304\n","Skipping y_hat=917\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=488\n","Skipping y_hat=551\n","Skipping y_hat=561\n","Skipping y_hat=622\n","Skipping y_hat=951\n","Skipping y_hat=967\n","Skipping y_hat=792\n","Skipping y_hat=835\n","Skipping y_hat=885\n","100% 80/80 [00:02<00:00, 34.12it/s]\n","accuracy of 9900 examples: 9813/9900 (99.12121212121212%)\n","\n","Test Results:\n","test_10000_stripped: 99.12%\n","\n","iter 3900: train loss 0.9307, val loss 1.1249\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=586\n","Skipping y_hat=109\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=332\n","Skipping y_hat=545\n","Skipping y_hat=450\n","Skipping y_hat=613\n","Skipping y_hat=716\n","Skipping y_hat=812\n","Skipping y_hat=227\n","Skipping y_hat=208\n","Skipping y_hat=480\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=469\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=618\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=752\n","Skipping y_hat=1053\n","Skipping y_hat=1015\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=1092\n","Skipping y_hat=1084\n","Skipping y_hat=1096\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=100\n","Skipping y_hat=308\n","Skipping y_hat=416\n","Skipping y_hat=449\n","Skipping y_hat=856\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=1086\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=359\n","Skipping y_hat=487\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=460\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=624\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=561\n","Skipping y_hat=581\n","Skipping y_hat=841\n","Skipping y_hat=877\n","Skipping y_hat=1001\n","Skipping y_hat=728\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=992\n","Skipping y_hat=835\n","100% 80/80 [00:02<00:00, 34.24it/s]\n","accuracy of 9900 examples: 9819/9900 (99.18181818181819%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.32it/s]\n","accuracy of 10000 examples: 9995/10000 (99.95%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=63\n","Skipping y_hat=546\n","Skipping y_hat=586\n","Skipping y_hat=186\n","Skipping y_hat=217\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=221\n","Skipping y_hat=222\n","Skipping y_hat=450\n","Skipping y_hat=679\n","Skipping y_hat=538\n","Skipping y_hat=716\n","Skipping y_hat=812\n","Skipping y_hat=227\n","Skipping y_hat=208\n","Skipping y_hat=480\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=891\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=817\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=707\n","Skipping y_hat=953\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=267\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=886\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=300\n","Skipping y_hat=108\n","Skipping y_hat=406\n","Skipping y_hat=549\n","Skipping y_hat=816\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=1086\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=159\n","Skipping y_hat=180\n","Skipping y_hat=387\n","Skipping y_hat=200\n","Skipping y_hat=405\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=450\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=317\n","Skipping y_hat=624\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=581\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 34.27it/s]\n","accuracy of 9900 examples: 9809/9900 (99.08080808080808%)\n","\n","Test Results:\n","test_10000_stripped: 99.08%\n","\n","iter 4000: train loss 0.9208, val loss 1.1462\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=546\n","Skipping y_hat=586\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=801\n","Skipping y_hat=221\n","Skipping y_hat=322\n","Skipping y_hat=450\n","Skipping y_hat=716\n","Skipping y_hat=52\n","Skipping y_hat=327\n","Skipping y_hat=308\n","Skipping y_hat=380\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=720\n","Skipping y_hat=658\n","Skipping y_hat=891\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=469\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=618\n","Skipping y_hat=461\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=382\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=119\n","Skipping y_hat=318\n","Skipping y_hat=506\n","Skipping y_hat=549\n","Skipping y_hat=816\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=965\n","Skipping y_hat=773\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=405\n","Skipping y_hat=415\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=450\n","Skipping y_hat=460\n","Skipping y_hat=304\n","Skipping y_hat=624\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=551\n","Skipping y_hat=615\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 34.14it/s]\n","accuracy of 9900 examples: 9818/9900 (99.17171717171716%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 37.61it/s]\n","accuracy of 10000 examples: 9997/10000 (99.97%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=546\n","Skipping y_hat=841\n","Skipping y_hat=586\n","Skipping y_hat=186\n","Skipping y_hat=217\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=762\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=222\n","Skipping y_hat=336\n","Skipping y_hat=455\n","Skipping y_hat=450\n","Skipping y_hat=613\n","Skipping y_hat=679\n","Skipping y_hat=752\n","Skipping y_hat=227\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=720\n","Skipping y_hat=891\n","Skipping y_hat=803\n","Skipping y_hat=913\n","Skipping y_hat=669\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=618\n","Skipping y_hat=761\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=752\n","Skipping y_hat=787\n","Skipping y_hat=1063\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=886\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=1096\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=856\n","Skipping y_hat=840\n","Skipping y_hat=181\n","Skipping y_hat=318\n","Skipping y_hat=569\n","Skipping y_hat=716\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=1086\n","Skipping y_hat=261\n","Skipping y_hat=965\n","Skipping y_hat=273\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=315\n","Skipping y_hat=487\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=250\n","Skipping y_hat=460\n","Skipping y_hat=304\n","Skipping y_hat=624\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=622\n","Skipping y_hat=625\n","Skipping y_hat=647\n","Skipping y_hat=1001\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=492\n","Skipping y_hat=885\n","100% 80/80 [00:02<00:00, 34.29it/s]\n","accuracy of 9900 examples: 9813/9900 (99.12121212121212%)\n","\n","Test Results:\n","test_10000_stripped: 99.12%\n","\n","iter 4100: train loss 0.9252, val loss 1.1516\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=618\n","Skipping y_hat=586\n","Skipping y_hat=186\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=455\n","Skipping y_hat=450\n","Skipping y_hat=613\n","Skipping y_hat=679\n","Skipping y_hat=716\n","Skipping y_hat=227\n","Skipping y_hat=347\n","Skipping y_hat=380\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=781\n","Skipping y_hat=803\n","Skipping y_hat=3\n","Skipping y_hat=669\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=618\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=707\n","Skipping y_hat=1063\n","Skipping y_hat=1005\n","Skipping y_hat=13\n","Skipping y_hat=178\n","Skipping y_hat=394\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=886\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=816\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=866\n","Skipping y_hat=840\n","Skipping y_hat=181\n","Skipping y_hat=319\n","Skipping y_hat=108\n","Skipping y_hat=482\n","Skipping y_hat=506\n","Skipping y_hat=579\n","Skipping y_hat=716\n","Skipping y_hat=779\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=261\n","Skipping y_hat=965\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=400\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=460\n","Skipping y_hat=300\n","Skipping y_hat=406\n","Skipping y_hat=532\n","Skipping y_hat=581\n","Skipping y_hat=893\n","Skipping y_hat=701\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=492\n","Skipping y_hat=585\n","100% 80/80 [00:02<00:00, 34.09it/s]\n","accuracy of 9900 examples: 9815/9900 (99.14141414141415%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.70it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=83\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=618\n","Skipping y_hat=586\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=1001\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=455\n","Skipping y_hat=450\n","Skipping y_hat=613\n","Skipping y_hat=626\n","Skipping y_hat=742\n","Skipping y_hat=227\n","Skipping y_hat=380\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=816\n","Skipping y_hat=803\n","Skipping y_hat=13\n","Skipping y_hat=669\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=628\n","Skipping y_hat=761\n","Skipping y_hat=504\n","Skipping y_hat=547\n","Skipping y_hat=742\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=13\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=216\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=308\n","Skipping y_hat=402\n","Skipping y_hat=506\n","Skipping y_hat=579\n","Skipping y_hat=716\n","Skipping y_hat=779\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=261\n","Skipping y_hat=273\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=359\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=243\n","Skipping y_hat=250\n","Skipping y_hat=460\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=406\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=581\n","Skipping y_hat=660\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=992\n","Skipping y_hat=835\n","Skipping y_hat=485\n","100% 80/80 [00:02<00:00, 34.46it/s]\n","accuracy of 9900 examples: 9814/9900 (99.13131313131314%)\n","\n","Test Results:\n","test_10000_stripped: 99.13%\n","\n","iter 4200: train loss 0.9121, val loss 1.1619\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=14\n","Skipping y_hat=30\n","Skipping y_hat=786\n","Skipping y_hat=186\n","Skipping y_hat=217\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=613\n","Skipping y_hat=826\n","Skipping y_hat=742\n","Skipping y_hat=227\n","Skipping y_hat=347\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=720\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=803\n","Skipping y_hat=3\n","Skipping y_hat=669\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=618\n","Skipping y_hat=751\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1063\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=200\n","Skipping y_hat=119\n","Skipping y_hat=328\n","Skipping y_hat=579\n","Skipping y_hat=816\n","Skipping y_hat=779\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=359\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=405\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=250\n","Skipping y_hat=460\n","Skipping y_hat=304\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=524\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=893\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 34.29it/s]\n","accuracy of 9900 examples: 9816/9900 (99.15151515151514%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.04it/s]\n","accuracy of 10000 examples: 9996/10000 (99.96000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=28\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=841\n","Skipping y_hat=586\n","Skipping y_hat=886\n","Skipping y_hat=217\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=801\n","Skipping y_hat=221\n","Skipping y_hat=322\n","Skipping y_hat=450\n","Skipping y_hat=679\n","Skipping y_hat=826\n","Skipping y_hat=842\n","Skipping y_hat=227\n","Skipping y_hat=388\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=630\n","Skipping y_hat=803\n","Skipping y_hat=3\n","Skipping y_hat=669\n","Skipping y_hat=917\n","Skipping y_hat=329\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=742\n","Skipping y_hat=887\n","Skipping y_hat=1063\n","Skipping y_hat=1095\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=406\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=886\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=816\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=181\n","Skipping y_hat=300\n","Skipping y_hat=109\n","Skipping y_hat=308\n","Skipping y_hat=416\n","Skipping y_hat=579\n","Skipping y_hat=716\n","Skipping y_hat=879\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=261\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=315\n","Skipping y_hat=359\n","Skipping y_hat=487\n","Skipping y_hat=405\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=250\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=626\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=992\n","Skipping y_hat=585\n","100% 80/80 [00:02<00:00, 34.24it/s]\n","accuracy of 9900 examples: 9816/9900 (99.15151515151514%)\n","\n","Test Results:\n","test_10000_stripped: 99.15%\n","\n","iter 4300: train loss 0.9054, val loss 1.1615\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=546\n","Skipping y_hat=586\n","Skipping y_hat=222\n","Skipping y_hat=299\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=221\n","Skipping y_hat=322\n","Skipping y_hat=545\n","Skipping y_hat=440\n","Skipping y_hat=679\n","Skipping y_hat=626\n","Skipping y_hat=832\n","Skipping y_hat=327\n","Skipping y_hat=347\n","Skipping y_hat=380\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=803\n","Skipping y_hat=3\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=394\n","Skipping y_hat=406\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1096\n","Skipping y_hat=816\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=856\n","Skipping y_hat=830\n","Skipping y_hat=181\n","Skipping y_hat=310\n","Skipping y_hat=119\n","Skipping y_hat=318\n","Skipping y_hat=579\n","Skipping y_hat=856\n","Skipping y_hat=869\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=965\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=1001\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=159\n","Skipping y_hat=480\n","Skipping y_hat=387\n","Skipping y_hat=200\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=450\n","Skipping y_hat=460\n","Skipping y_hat=300\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=551\n","Skipping y_hat=561\n","Skipping y_hat=581\n","Skipping y_hat=622\n","Skipping y_hat=901\n","Skipping y_hat=951\n","Skipping y_hat=992\n","Skipping y_hat=835\n","100% 80/80 [00:02<00:00, 34.03it/s]\n","accuracy of 9900 examples: 9816/9900 (99.15151515151514%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.63it/s]\n","accuracy of 10000 examples: 9998/10000 (99.98%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=33\n","Skipping y_hat=656\n","Skipping y_hat=618\n","Skipping y_hat=586\n","Skipping y_hat=217\n","Skipping y_hat=222\n","Skipping y_hat=299\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=332\n","Skipping y_hat=555\n","Skipping y_hat=450\n","Skipping y_hat=679\n","Skipping y_hat=626\n","Skipping y_hat=732\n","Skipping y_hat=227\n","Skipping y_hat=347\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=816\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=1063\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=247\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=866\n","Skipping y_hat=1082\n","Skipping y_hat=884\n","Skipping y_hat=1096\n","Skipping y_hat=816\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=856\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=128\n","Skipping y_hat=402\n","Skipping y_hat=416\n","Skipping y_hat=549\n","Skipping y_hat=716\n","Skipping y_hat=779\n","Skipping y_hat=844\n","Skipping y_hat=1006\n","Skipping y_hat=773\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=480\n","Skipping y_hat=200\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=450\n","Skipping y_hat=300\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=626\n","Skipping y_hat=678\n","Skipping y_hat=692\n","Skipping y_hat=901\n","Skipping y_hat=951\n","Skipping y_hat=967\n","Skipping y_hat=792\n","Skipping y_hat=885\n","100% 80/80 [00:02<00:00, 33.89it/s]\n","accuracy of 9900 examples: 9817/9900 (99.16161616161617%)\n","\n","Test Results:\n","test_10000_stripped: 99.16%\n","\n","iter 4400: train loss 0.8894, val loss 1.1581\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=618\n","Skipping y_hat=586\n","Skipping y_hat=299\n","Skipping y_hat=801\n","Skipping y_hat=221\n","Skipping y_hat=322\n","Skipping y_hat=450\n","Skipping y_hat=613\n","Skipping y_hat=679\n","Skipping y_hat=538\n","Skipping y_hat=227\n","Skipping y_hat=308\n","Skipping y_hat=380\n","Skipping y_hat=450\n","Skipping y_hat=720\n","Skipping y_hat=816\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=742\n","Skipping y_hat=1053\n","Skipping y_hat=1095\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=886\n","Skipping y_hat=1002\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=816\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=840\n","Skipping y_hat=181\n","Skipping y_hat=108\n","Skipping y_hat=506\n","Skipping y_hat=579\n","Skipping y_hat=816\n","Skipping y_hat=849\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=359\n","Skipping y_hat=380\n","Skipping y_hat=487\n","Skipping y_hat=200\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=248\n","Skipping y_hat=450\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=578\n","Skipping y_hat=581\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 34.10it/s]\n","accuracy of 9900 examples: 9820/9900 (99.19191919191918%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 37.93it/s]\n","accuracy of 10000 examples: 9997/10000 (99.97%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=546\n","Skipping y_hat=586\n","Skipping y_hat=299\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=450\n","Skipping y_hat=679\n","Skipping y_hat=826\n","Skipping y_hat=842\n","Skipping y_hat=227\n","Skipping y_hat=398\n","Skipping y_hat=490\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=630\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=528\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1053\n","Skipping y_hat=1095\n","Skipping y_hat=178\n","Skipping y_hat=394\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=866\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=816\n","Skipping y_hat=480\n","Skipping y_hat=733\n","Skipping y_hat=866\n","Skipping y_hat=840\n","Skipping y_hat=181\n","Skipping y_hat=310\n","Skipping y_hat=119\n","Skipping y_hat=108\n","Skipping y_hat=406\n","Skipping y_hat=716\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=261\n","Skipping y_hat=965\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=159\n","Skipping y_hat=380\n","Skipping y_hat=487\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=450\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=581\n","Skipping y_hat=598\n","Skipping y_hat=625\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","100% 80/80 [00:02<00:00, 34.16it/s]\n","accuracy of 9900 examples: 9819/9900 (99.18181818181819%)\n","\n","Test Results:\n","test_10000_stripped: 99.18%\n","\n","iter 4500: train loss 0.8895, val loss 1.1778\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=546\n","Skipping y_hat=841\n","Skipping y_hat=186\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=222\n","Skipping y_hat=440\n","Skipping y_hat=613\n","Skipping y_hat=679\n","Skipping y_hat=842\n","Skipping y_hat=227\n","Skipping y_hat=208\n","Skipping y_hat=347\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=781\n","Skipping y_hat=803\n","Skipping y_hat=3\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=618\n","Skipping y_hat=404\n","Skipping y_hat=762\n","Skipping y_hat=953\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=896\n","Skipping y_hat=1002\n","Skipping y_hat=1084\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=856\n","Skipping y_hat=181\n","Skipping y_hat=100\n","Skipping y_hat=209\n","Skipping y_hat=318\n","Skipping y_hat=406\n","Skipping y_hat=549\n","Skipping y_hat=716\n","Skipping y_hat=879\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=1001\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=159\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=250\n","Skipping y_hat=260\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=561\n","Skipping y_hat=622\n","Skipping y_hat=877\n","Skipping y_hat=893\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=992\n","Skipping y_hat=835\n","Skipping y_hat=485\n","100% 80/80 [00:02<00:00, 34.08it/s]\n","accuracy of 9900 examples: 9816/9900 (99.15151515151514%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.05it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=546\n","Skipping y_hat=586\n","Skipping y_hat=186\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=226\n","Skipping y_hat=450\n","Skipping y_hat=679\n","Skipping y_hat=826\n","Skipping y_hat=812\n","Skipping y_hat=227\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=650\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=897\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=796\n","Skipping y_hat=1092\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=1096\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=866\n","Skipping y_hat=840\n","Skipping y_hat=181\n","Skipping y_hat=110\n","Skipping y_hat=308\n","Skipping y_hat=416\n","Skipping y_hat=549\n","Skipping y_hat=716\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=112\n","Skipping y_hat=115\n","Skipping y_hat=480\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=248\n","Skipping y_hat=450\n","Skipping y_hat=304\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=561\n","Skipping y_hat=581\n","Skipping y_hat=798\n","Skipping y_hat=893\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 34.30it/s]\n","accuracy of 9900 examples: 9821/9900 (99.20202020202021%)\n","\n","Test Results:\n","test_10000_stripped: 99.20%\n","\n","iter 4600: train loss 0.8888, val loss 1.1822\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=83\n","Skipping y_hat=34\n","Skipping y_hat=546\n","Skipping y_hat=618\n","Skipping y_hat=586\n","Skipping y_hat=186\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=719\n","Skipping y_hat=321\n","Skipping y_hat=222\n","Skipping y_hat=450\n","Skipping y_hat=679\n","Skipping y_hat=626\n","Skipping y_hat=712\n","Skipping y_hat=227\n","Skipping y_hat=380\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=893\n","Skipping y_hat=13\n","Skipping y_hat=187\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=504\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1063\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=1096\n","Skipping y_hat=816\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=119\n","Skipping y_hat=108\n","Skipping y_hat=716\n","Skipping y_hat=549\n","Skipping y_hat=716\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=221\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=380\n","Skipping y_hat=487\n","Skipping y_hat=200\n","Skipping y_hat=415\n","Skipping y_hat=243\n","Skipping y_hat=250\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=561\n","Skipping y_hat=901\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=792\n","Skipping y_hat=876\n","Skipping y_hat=885\n","100% 80/80 [00:02<00:00, 34.25it/s]\n","accuracy of 9900 examples: 9819/9900 (99.18181818181819%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 39.10it/s]\n","accuracy of 10000 examples: 9996/10000 (99.96000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=546\n","Skipping y_hat=886\n","Skipping y_hat=299\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=455\n","Skipping y_hat=440\n","Skipping y_hat=613\n","Skipping y_hat=679\n","Skipping y_hat=626\n","Skipping y_hat=752\n","Skipping y_hat=227\n","Skipping y_hat=347\n","Skipping y_hat=380\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=803\n","Skipping y_hat=3\n","Skipping y_hat=669\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=387\n","Skipping y_hat=628\n","Skipping y_hat=461\n","Skipping y_hat=404\n","Skipping y_hat=787\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=394\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=866\n","Skipping y_hat=840\n","Skipping y_hat=181\n","Skipping y_hat=110\n","Skipping y_hat=119\n","Skipping y_hat=308\n","Skipping y_hat=506\n","Skipping y_hat=449\n","Skipping y_hat=716\n","Skipping y_hat=849\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=359\n","Skipping y_hat=480\n","Skipping y_hat=387\n","Skipping y_hat=200\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=248\n","Skipping y_hat=450\n","Skipping y_hat=460\n","Skipping y_hat=304\n","Skipping y_hat=524\n","Skipping y_hat=678\n","Skipping y_hat=551\n","Skipping y_hat=581\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=792\n","Skipping y_hat=785\n","100% 80/80 [00:02<00:00, 34.40it/s]\n","accuracy of 9900 examples: 9820/9900 (99.19191919191918%)\n","\n","Test Results:\n","test_10000_stripped: 99.19%\n","\n","iter 4700: train loss 0.8809, val loss 1.1826\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=33\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=586\n","Skipping y_hat=299\n","Skipping y_hat=752\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=222\n","Skipping y_hat=426\n","Skipping y_hat=345\n","Skipping y_hat=440\n","Skipping y_hat=679\n","Skipping y_hat=712\n","Skipping y_hat=227\n","Skipping y_hat=347\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=651\n","Skipping y_hat=404\n","Skipping y_hat=742\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=213\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=796\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=733\n","Skipping y_hat=856\n","Skipping y_hat=840\n","Skipping y_hat=200\n","Skipping y_hat=318\n","Skipping y_hat=416\n","Skipping y_hat=549\n","Skipping y_hat=716\n","Skipping y_hat=749\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=359\n","Skipping y_hat=380\n","Skipping y_hat=387\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=450\n","Skipping y_hat=460\n","Skipping y_hat=300\n","Skipping y_hat=917\n","Skipping y_hat=124\n","Skipping y_hat=406\n","Skipping y_hat=678\n","Skipping y_hat=660\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=492\n","100% 80/80 [00:02<00:00, 34.71it/s]\n","accuracy of 9900 examples: 9822/9900 (99.2121212121212%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.20it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=586\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=992\n","Skipping y_hat=321\n","Skipping y_hat=222\n","Skipping y_hat=450\n","Skipping y_hat=479\n","Skipping y_hat=732\n","Skipping y_hat=227\n","Skipping y_hat=347\n","Skipping y_hat=480\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=823\n","Skipping y_hat=903\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=404\n","Skipping y_hat=647\n","Skipping y_hat=742\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=213\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=1002\n","Skipping y_hat=1084\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=856\n","Skipping y_hat=840\n","Skipping y_hat=181\n","Skipping y_hat=100\n","Skipping y_hat=319\n","Skipping y_hat=318\n","Skipping y_hat=416\n","Skipping y_hat=549\n","Skipping y_hat=716\n","Skipping y_hat=849\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=450\n","Skipping y_hat=260\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=524\n","Skipping y_hat=406\n","Skipping y_hat=726\n","Skipping y_hat=626\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=647\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=792\n","Skipping y_hat=835\n","100% 80/80 [00:02<00:00, 33.94it/s]\n","accuracy of 9900 examples: 9815/9900 (99.14141414141415%)\n","\n","Test Results:\n","test_10000_stripped: 99.14%\n","\n","iter 4800: train loss 0.8735, val loss 1.1976\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=33\n","Skipping y_hat=546\n","Skipping y_hat=786\n","Skipping y_hat=186\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=752\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=222\n","Skipping y_hat=226\n","Skipping y_hat=440\n","Skipping y_hat=613\n","Skipping y_hat=679\n","Skipping y_hat=626\n","Skipping y_hat=32\n","Skipping y_hat=227\n","Skipping y_hat=218\n","Skipping y_hat=347\n","Skipping y_hat=490\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=701\n","Skipping y_hat=803\n","Skipping y_hat=3\n","Skipping y_hat=669\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=387\n","Skipping y_hat=618\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1063\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=1002\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=216\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=840\n","Skipping y_hat=119\n","Skipping y_hat=308\n","Skipping y_hat=406\n","Skipping y_hat=569\n","Skipping y_hat=816\n","Skipping y_hat=779\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=359\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=1000\n","Skipping y_hat=405\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=450\n","Skipping y_hat=460\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=678\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=561\n","Skipping y_hat=581\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 33.64it/s]\n","accuracy of 9900 examples: 9810/9900 (99.0909090909091%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.10it/s]\n","accuracy of 10000 examples: 9998/10000 (99.98%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=546\n","Skipping y_hat=786\n","Skipping y_hat=886\n","Skipping y_hat=299\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=440\n","Skipping y_hat=613\n","Skipping y_hat=479\n","Skipping y_hat=616\n","Skipping y_hat=742\n","Skipping y_hat=227\n","Skipping y_hat=347\n","Skipping y_hat=490\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=620\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=781\n","Skipping y_hat=803\n","Skipping y_hat=13\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=618\n","Skipping y_hat=661\n","Skipping y_hat=504\n","Skipping y_hat=732\n","Skipping y_hat=897\n","Skipping y_hat=1063\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=896\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=216\n","Skipping y_hat=480\n","Skipping y_hat=753\n","Skipping y_hat=856\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=119\n","Skipping y_hat=318\n","Skipping y_hat=816\n","Skipping y_hat=816\n","Skipping y_hat=769\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=359\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=450\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=406\n","Skipping y_hat=661\n","Skipping y_hat=678\n","Skipping y_hat=551\n","Skipping y_hat=622\n","Skipping y_hat=625\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","100% 80/80 [00:02<00:00, 33.96it/s]\n","accuracy of 9900 examples: 9820/9900 (99.19191919191918%)\n","\n","Test Results:\n","test_10000_stripped: 99.19%\n","\n","iter 4900: train loss 0.8628, val loss 1.1972\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=33\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=586\n","Skipping y_hat=132\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=321\n","Skipping y_hat=222\n","Skipping y_hat=440\n","Skipping y_hat=479\n","Skipping y_hat=42\n","Skipping y_hat=227\n","Skipping y_hat=380\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=720\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=803\n","Skipping y_hat=903\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=742\n","Skipping y_hat=787\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=796\n","Skipping y_hat=886\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1093\n","Skipping y_hat=816\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=110\n","Skipping y_hat=119\n","Skipping y_hat=308\n","Skipping y_hat=806\n","Skipping y_hat=549\n","Skipping y_hat=816\n","Skipping y_hat=879\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=112\n","Skipping y_hat=115\n","Skipping y_hat=359\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=450\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=917\n","Skipping y_hat=678\n","Skipping y_hat=581\n","Skipping y_hat=598\n","Skipping y_hat=893\n","Skipping y_hat=1001\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=492\n","100% 80/80 [00:02<00:00, 34.21it/s]\n","accuracy of 9900 examples: 9822/9900 (99.2121212121212%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.89it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=20\n","Skipping y_hat=30\n","Skipping y_hat=546\n","Skipping y_hat=586\n","Skipping y_hat=217\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=614\n","Skipping y_hat=801\n","Skipping y_hat=221\n","Skipping y_hat=322\n","Skipping y_hat=450\n","Skipping y_hat=679\n","Skipping y_hat=716\n","Skipping y_hat=732\n","Skipping y_hat=227\n","Skipping y_hat=308\n","Skipping y_hat=380\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=720\n","Skipping y_hat=816\n","Skipping y_hat=803\n","Skipping y_hat=913\n","Skipping y_hat=669\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=397\n","Skipping y_hat=618\n","Skipping y_hat=661\n","Skipping y_hat=404\n","Skipping y_hat=742\n","Skipping y_hat=1063\n","Skipping y_hat=1005\n","Skipping y_hat=103\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=796\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=816\n","Skipping y_hat=480\n","Skipping y_hat=733\n","Skipping y_hat=856\n","Skipping y_hat=840\n","Skipping y_hat=181\n","Skipping y_hat=110\n","Skipping y_hat=318\n","Skipping y_hat=406\n","Skipping y_hat=549\n","Skipping y_hat=716\n","Skipping y_hat=879\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=273\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=359\n","Skipping y_hat=387\n","Skipping y_hat=415\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=443\n","Skipping y_hat=450\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=408\n","Skipping y_hat=626\n","Skipping y_hat=532\n","Skipping y_hat=951\n","Skipping y_hat=967\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 34.57it/s]\n","accuracy of 9900 examples: 9825/9900 (99.24242424242425%)\n","\n","Test Results:\n","test_10000_stripped: 99.24%\n","\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=546\n","Skipping y_hat=586\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=614\n","Skipping y_hat=719\n","Skipping y_hat=801\n","Skipping y_hat=992\n","Skipping y_hat=321\n","Skipping y_hat=222\n","Skipping y_hat=556\n","Skipping y_hat=450\n","Skipping y_hat=626\n","Skipping y_hat=732\n","Skipping y_hat=227\n","Skipping y_hat=380\n","Skipping y_hat=441\n","Skipping y_hat=450\n","Skipping y_hat=720\n","Skipping y_hat=658\n","Skipping y_hat=816\n","Skipping y_hat=823\n","Skipping y_hat=913\n","Skipping y_hat=817\n","Skipping y_hat=229\n","Skipping y_hat=317\n","Skipping y_hat=628\n","Skipping y_hat=661\n","Skipping y_hat=1004\n","Skipping y_hat=742\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=537\n","Skipping y_hat=796\n","Skipping y_hat=702\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=1096\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=743\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=209\n","Skipping y_hat=118\n","Skipping y_hat=506\n","Skipping y_hat=549\n","Skipping y_hat=716\n","Skipping y_hat=769\n","Skipping y_hat=844\n","Skipping y_hat=1096\n","Skipping y_hat=861\n","Skipping y_hat=273\n","Skipping y_hat=997\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=359\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=450\n","Skipping y_hat=260\n","Skipping y_hat=304\n","Skipping y_hat=524\n","Skipping y_hat=408\n","Skipping y_hat=532\n","Skipping y_hat=551\n","Skipping y_hat=581\n","Skipping y_hat=622\n","Skipping y_hat=841\n","Skipping y_hat=901\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=967\n","Skipping y_hat=992\n","Skipping y_hat=885\n","100% 80/80 [00:02<00:00, 34.25it/s]\n","accuracy of 9900 examples: 9815/9900 (99.14141414141415%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.40it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/80 [00:00<?, ?it/s]Skipping y_hat=28\n","Skipping y_hat=546\n","Skipping y_hat=186\n","Skipping y_hat=299\n","Skipping y_hat=557\n","Skipping y_hat=614\n","Skipping y_hat=870\n","Skipping y_hat=1001\n","Skipping y_hat=321\n","Skipping y_hat=322\n","Skipping y_hat=545\n","Skipping y_hat=440\n","Skipping y_hat=742\n","Skipping y_hat=227\n","Skipping y_hat=288\n","Skipping y_hat=380\n","Skipping y_hat=541\n","Skipping y_hat=450\n","Skipping y_hat=730\n","Skipping y_hat=658\n","Skipping y_hat=781\n","Skipping y_hat=803\n","Skipping y_hat=13\n","Skipping y_hat=187\n","Skipping y_hat=917\n","Skipping y_hat=229\n","Skipping y_hat=528\n","Skipping y_hat=461\n","Skipping y_hat=404\n","Skipping y_hat=742\n","Skipping y_hat=1053\n","Skipping y_hat=1005\n","Skipping y_hat=178\n","Skipping y_hat=384\n","Skipping y_hat=496\n","Skipping y_hat=796\n","Skipping y_hat=1082\n","Skipping y_hat=1084\n","Skipping y_hat=116\n","Skipping y_hat=480\n","Skipping y_hat=643\n","Skipping y_hat=866\n","Skipping y_hat=860\n","Skipping y_hat=181\n","Skipping y_hat=119\n","Skipping y_hat=318\n","Skipping y_hat=506\n","Skipping y_hat=549\n","Skipping y_hat=816\n","Skipping y_hat=849\n","Skipping y_hat=844\n","Skipping y_hat=906\n","Skipping y_hat=273\n","Skipping y_hat=996\n","Skipping y_hat=101\n","Skipping y_hat=307\n","Skipping y_hat=312\n","Skipping y_hat=115\n","Skipping y_hat=480\n","Skipping y_hat=487\n","Skipping y_hat=233\n","Skipping y_hat=243\n","Skipping y_hat=450\n","Skipping y_hat=260\n","Skipping y_hat=300\n","Skipping y_hat=304\n","Skipping y_hat=524\n","Skipping y_hat=678\n","Skipping y_hat=524\n","Skipping y_hat=532\n","Skipping y_hat=561\n","Skipping y_hat=581\n","Skipping y_hat=951\n","Skipping y_hat=963\n","Skipping y_hat=992\n","100% 80/80 [00:02<00:00, 34.13it/s]\n","accuracy of 9900 examples: 9825/9900 (99.24242424242425%)\n","\n","Final Test Results:\n","test_10000_stripped: 99.24%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m2_operands_0_to_999_their_data_reversed\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/igsryu84\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m2_operands_0_to_999_their_data/out_reversed/wandb/run-20250621_213745-igsryu84/logs\u001b[0m\n"]}],"source":["!python train_end_padding_auto_val.py 2_operands_addition_reversed.txt"]},{"cell_type":"markdown","metadata":{"id":"DKkP6l27iZIl"},"source":["## Reproduce 2 Operands 0-999 Addition (Our Code, Their Training Data Only)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"66k2O-FridzP"},"source":["### Plain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":745,"status":"ok","timestamp":1750734080375,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"c3anm3_Uikuw","outputId":"eb23e63f-1198-4fa1-8da6-003e80b864bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 100\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'our_model_their_training_data_only_plain_more_early_eval'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'algo_reasoning'\n","data_format='plain'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_plain.pt'\n","\n","# to edit: whether the result is reversed\n","reverse_c = False\n","eval_addition = True\n","\n","analysis = False\n","\n","# to edit: the number of addition operations to perform, which equals to (#operands -1)\n","num_addition = 1\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","# whether need zero‐padding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory\n","out_dir = '/content/drive/MyDrive/addition/2_operands_0_to_999_their_training_data_only_more_eval/plain_out'\n","\n","# to edit: training data\n","train_data_path = '/content/drive/MyDrive/addition/data/2_operands_0_to_999_balanced_digit_their_training_data/train_3digit_10000_dollar.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","start_train = \"FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_balanced_digit_their_training_data/train_3digit_10000_dollar_eval.txt\"\n","\n","# to edit: valuation data\n","val_data_path = '/content/drive/MyDrive/addition/data/2_operands_0_to_999_balanced_digit_their_training_data/val.txt'\n","\n","# to edit: test data (start is just the test file)\n","start = 'FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_balanced_digit_their_training_data/test/test.txt'\n","\n","# (optional param) to_edit: whether to enable detailed metric recording at each eval_interval\n","# test_dir: the directory storing test files\n","test_dir = '/content/drive/MyDrive/addition/data/2_operands_0_to_999_balanced_digit_their_training_data/test'\n","eval_additional_test = True  # whether to evaluate on additional test files \n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval = True\n","early_eval_interval = 25\n","early_eval_iters = 1000"]}],"source":["%cat 2_operands_addition_plain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":977919,"status":"ok","timestamp":1750793535136,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"eB2j2swjie_i","outputId":"c9589d2e-ed61-4ab2-9796-96238bd66b71"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=899\n","Skipping y_hat=687\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=725\n","Skipping y_hat=626\n","Skipping y_hat=486\n","Skipping y_hat=43\n","Skipping y_hat=109\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=839\n","Skipping y_hat=522\n","Skipping y_hat=627\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=140\n","Skipping y_hat=633\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=337\n","Skipping y_hat=863\n","Skipping y_hat=594\n","Skipping y_hat=413\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=571\n","Skipping y_hat=848\n","Skipping y_hat=426\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=238\n","Skipping y_hat=206\n","Skipping y_hat=268\n","Skipping y_hat=682\n","Skipping y_hat=333\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=861\n","Skipping y_hat=915\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=100\n","Skipping y_hat=628\n","Skipping y_hat=535\n","Skipping y_hat=506\n","Skipping y_hat=658\n","Skipping y_hat=894\n","Skipping y_hat=100\n","Skipping y_hat=701\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=668\n","Skipping y_hat=752\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=638\n","Skipping y_hat=830\n","Skipping y_hat=459\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=985\n","Skipping y_hat=902\n","Skipping y_hat=366\n","Skipping y_hat=639\n","Skipping y_hat=416\n","Skipping y_hat=637\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=880\n","Skipping y_hat=829\n","Skipping y_hat=712\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=867\n","Skipping y_hat=756\n","Skipping y_hat=168\n","Skipping y_hat=812\n","Skipping y_hat=85\n","Skipping y_hat=63\n","Skipping y_hat=85\n","Skipping y_hat=35\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.57it/s]\n","accuracy of 10000 examples: 9317/10000 (93.17%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.76it/s]\n","accuracy of 10000 examples: 9987/10000 (99.87%)\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 33.80it/s]Skipping y_hat=170\n","Skipping y_hat=748\n","Skipping y_hat=799\n","Skipping y_hat=882\n","Skipping y_hat=479\n","Skipping y_hat=180\n","Skipping y_hat=852\n","Skipping y_hat=170\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=700\n","Skipping y_hat=820\n","Skipping y_hat=808\n","Skipping y_hat=130\n","Skipping y_hat=554\n","Skipping y_hat=287\n","Skipping y_hat=809\n","Skipping y_hat=591\n","Skipping y_hat=969\n","Skipping y_hat=936\n","Skipping y_hat=409\n","Skipping y_hat=403\n","Skipping y_hat=476\n","Skipping y_hat=999\n","Skipping y_hat=461\n","Skipping y_hat=981\n","Skipping y_hat=190\n","Skipping y_hat=512\n","Skipping y_hat=742\n","Skipping y_hat=500\n","Skipping y_hat=708\n","Skipping y_hat=607\n","Skipping y_hat=811\n","Skipping y_hat=742\n","Skipping y_hat=520\n","Skipping y_hat=116\n","Skipping y_hat=421\n","Skipping y_hat=609\n","Skipping y_hat=629\n","Skipping y_hat=808\n","Skipping y_hat=782\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=209\n","Skipping y_hat=607\n"," 85% 68/80 [00:02<00:00, 32.85it/s]Skipping y_hat=230\n","Skipping y_hat=331\n","Skipping y_hat=1028\n","Skipping y_hat=709\n","Skipping y_hat=1028\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=269\n","Skipping y_hat=351\n","Skipping y_hat=569\n","Skipping y_hat=357\n","Skipping y_hat=137\n","Skipping y_hat=891\n","Skipping y_hat=759\n","Skipping y_hat=853\n","Skipping y_hat=406\n","Skipping y_hat=359\n","Skipping y_hat=911\n","Skipping y_hat=189\n","Skipping y_hat=704\n","Skipping y_hat=159\n","Skipping y_hat=800\n","Skipping y_hat=758\n","Skipping y_hat=900\n","Skipping y_hat=273\n","Skipping y_hat=513\n","Skipping y_hat=831\n","Skipping y_hat=196\n","Skipping y_hat=900\n","Skipping y_hat=936\n","Skipping y_hat=1007\n","Skipping y_hat=909\n","Skipping y_hat=415\n","Skipping y_hat=461\n","Skipping y_hat=920\n","Skipping y_hat=691\n","Skipping y_hat=505\n"," 90% 72/80 [00:02<00:00, 32.16it/s]Skipping y_hat=398\n","Skipping y_hat=1072\n","Skipping y_hat=609\n","Skipping y_hat=1017\n","Skipping y_hat=1039\n","Skipping y_hat=901\n","Skipping y_hat=301\n","Skipping y_hat=719\n","Skipping y_hat=405\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=290\n","Skipping y_hat=357\n","Skipping y_hat=799\n","Skipping y_hat=109\n","Skipping y_hat=339\n","Skipping y_hat=127\n","Skipping y_hat=780\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=781\n","Skipping y_hat=640\n","Skipping y_hat=552\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=402\n","Skipping y_hat=592\n","Skipping y_hat=1005\n","Skipping y_hat=809\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=799\n","Skipping y_hat=772\n","Skipping y_hat=1042\n","Skipping y_hat=743\n","Skipping y_hat=405\n","Skipping y_hat=292\n","Skipping y_hat=999\n","Skipping y_hat=347\n"," 95% 76/80 [00:02<00:00, 32.07it/s]Skipping y_hat=600\n","Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=181\n","Skipping y_hat=727\n","Skipping y_hat=1000\n","Skipping y_hat=230\n","Skipping y_hat=179\n","Skipping y_hat=570\n","Skipping y_hat=391\n","Skipping y_hat=958\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=687\n","Skipping y_hat=905\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=626\n","Skipping y_hat=486\n","Skipping y_hat=433\n","Skipping y_hat=109\n","Skipping y_hat=891\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=522\n","Skipping y_hat=617\n","Skipping y_hat=774\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=801\n","Skipping y_hat=258\n","Skipping y_hat=613\n","Skipping y_hat=886\n","Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=335\n","Skipping y_hat=863\n","Skipping y_hat=582\n","Skipping y_hat=594\n","Skipping y_hat=415\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=571\n","Skipping y_hat=818\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=126\n","Skipping y_hat=206\n","Skipping y_hat=268\n","Skipping y_hat=582\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=250\n","Skipping y_hat=456\n","Skipping y_hat=840\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=586\n","Skipping y_hat=658\n","Skipping y_hat=814\n","Skipping y_hat=436\n","Skipping y_hat=100\n","Skipping y_hat=701\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=771\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=762\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=638\n","Skipping y_hat=459\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=80\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=985\n","Skipping y_hat=903\n","Skipping y_hat=366\n","Skipping y_hat=639\n","Skipping y_hat=416\n","Skipping y_hat=637\n","Skipping y_hat=678\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=870\n","Skipping y_hat=839\n","Skipping y_hat=712\n","Skipping y_hat=739\n","Skipping y_hat=756\n","Skipping y_hat=812\n","Skipping y_hat=87\n","Skipping y_hat=63\n","Skipping y_hat=85\n","Skipping y_hat=35\n","Skipping y_hat=12\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.25it/s]\n","accuracy of 10000 examples: 9331/10000 (93.31%)\n","/content/drive/MyDrive/addition/evaluation.py:318: DtypeWarning: Columns (34,35,39,45,49,50,53,58,61,63,69,70,71,77,79,81,82,83,84,85,86,88,89,90,94,99,100,102,103,104,106,107,108,109,110,111,112,113,115,116,119,120,121,122,123,127,129,130,132,133,134,135,136,137,138,140,142,143,144,145,147,149,150,152,154,155,156,157,162,163,182,190) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Test Results:\n","test: 93.31%\n","\n","iter 4100: train loss 0.6876, val loss 1.7176\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 34.76it/s]Skipping y_hat=794\n","Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=477\n","Skipping y_hat=470\n","Skipping y_hat=180\n","Skipping y_hat=311\n","Skipping y_hat=200\n","Skipping y_hat=405\n","Skipping y_hat=700\n","Skipping y_hat=820\n","Skipping y_hat=613\n","Skipping y_hat=808\n","Skipping y_hat=618\n","Skipping y_hat=554\n","Skipping y_hat=287\n","Skipping y_hat=110\n","Skipping y_hat=809\n","Skipping y_hat=505\n","Skipping y_hat=936\n","Skipping y_hat=409\n","Skipping y_hat=401\n","Skipping y_hat=999\n","Skipping y_hat=461\n","Skipping y_hat=319\n","Skipping y_hat=759\n","Skipping y_hat=500\n","Skipping y_hat=959\n","Skipping y_hat=633\n","Skipping y_hat=707\n","Skipping y_hat=607\n","Skipping y_hat=813\n","Skipping y_hat=742\n","Skipping y_hat=111\n","Skipping y_hat=420\n","Skipping y_hat=609\n","Skipping y_hat=629\n","Skipping y_hat=210\n","Skipping y_hat=808\n","Skipping y_hat=782\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=102\n","Skipping y_hat=607\n"," 85% 68/80 [00:01<00:00, 34.00it/s]Skipping y_hat=230\n","Skipping y_hat=981\n","Skipping y_hat=331\n","Skipping y_hat=1084\n","Skipping y_hat=1027\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=219\n","Skipping y_hat=269\n","Skipping y_hat=1019\n","Skipping y_hat=569\n","Skipping y_hat=755\n","Skipping y_hat=853\n","Skipping y_hat=406\n","Skipping y_hat=171\n","Skipping y_hat=800\n","Skipping y_hat=900\n","Skipping y_hat=689\n","Skipping y_hat=273\n","Skipping y_hat=607\n","Skipping y_hat=513\n","Skipping y_hat=798\n","Skipping y_hat=180\n","Skipping y_hat=831\n","Skipping y_hat=812\n","Skipping y_hat=939\n","Skipping y_hat=794\n","Skipping y_hat=462\n","Skipping y_hat=181\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=505\n"," 90% 72/80 [00:02<00:00, 33.80it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=703\n","Skipping y_hat=719\n","Skipping y_hat=405\n","Skipping y_hat=196\n","Skipping y_hat=901\n","Skipping y_hat=681\n","Skipping y_hat=621\n","Skipping y_hat=339\n","Skipping y_hat=228\n","Skipping y_hat=457\n","Skipping y_hat=890\n","Skipping y_hat=980\n","Skipping y_hat=293\n","Skipping y_hat=639\n","Skipping y_hat=552\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=979\n","Skipping y_hat=592\n","Skipping y_hat=1005\n","Skipping y_hat=807\n","Skipping y_hat=430\n","Skipping y_hat=359\n","Skipping y_hat=197\n","Skipping y_hat=1151\n","Skipping y_hat=798\n","Skipping y_hat=1042\n","Skipping y_hat=742\n","Skipping y_hat=188\n","Skipping y_hat=405\n","Skipping y_hat=294\n"," 95% 76/80 [00:02<00:00, 33.59it/s]Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=665\n","Skipping y_hat=181\n","Skipping y_hat=558\n","Skipping y_hat=1000\n","Skipping y_hat=361\n","Skipping y_hat=399\n","Skipping y_hat=958\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=899\n","Skipping y_hat=607\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=626\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=109\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=522\n","Skipping y_hat=617\n","Skipping y_hat=744\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=801\n","Skipping y_hat=460\n","Skipping y_hat=258\n","Skipping y_hat=673\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=337\n","Skipping y_hat=863\n","Skipping y_hat=594\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=581\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=238\n","Skipping y_hat=217\n","Skipping y_hat=512\n","Skipping y_hat=333\n","Skipping y_hat=938\n","Skipping y_hat=485\n","Skipping y_hat=309\n","Skipping y_hat=351\n","Skipping y_hat=456\n","Skipping y_hat=840\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=1002\n","Skipping y_hat=618\n","Skipping y_hat=533\n","Skipping y_hat=686\n","Skipping y_hat=658\n","Skipping y_hat=814\n","Skipping y_hat=100\n","Skipping y_hat=751\n","Skipping y_hat=455\n","Skipping y_hat=337\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=662\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=631\n","Skipping y_hat=830\n","Skipping y_hat=459\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=735\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=935\n","Skipping y_hat=932\n","Skipping y_hat=480\n","Skipping y_hat=629\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=881\n","Skipping y_hat=829\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=703\n","Skipping y_hat=756\n","Skipping y_hat=168\n","Skipping y_hat=892\n","Skipping y_hat=788\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=84\n","Skipping y_hat=34\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.96it/s]\n","accuracy of 10000 examples: 9322/10000 (93.22%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.06it/s]\n","accuracy of 10000 examples: 9979/10000 (99.79%)\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 34.13it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=889\n","Skipping y_hat=477\n","Skipping y_hat=470\n","Skipping y_hat=851\n","Skipping y_hat=170\n","Skipping y_hat=311\n","Skipping y_hat=200\n","Skipping y_hat=405\n","Skipping y_hat=694\n","Skipping y_hat=700\n","Skipping y_hat=820\n","Skipping y_hat=319\n","Skipping y_hat=612\n","Skipping y_hat=808\n","Skipping y_hat=618\n","Skipping y_hat=554\n","Skipping y_hat=287\n","Skipping y_hat=110\n","Skipping y_hat=909\n","Skipping y_hat=505\n","Skipping y_hat=936\n","Skipping y_hat=505\n","Skipping y_hat=403\n","Skipping y_hat=999\n","Skipping y_hat=461\n","Skipping y_hat=981\n","Skipping y_hat=319\n","Skipping y_hat=742\n","Skipping y_hat=500\n","Skipping y_hat=509\n","Skipping y_hat=959\n","Skipping y_hat=708\n","Skipping y_hat=919\n","Skipping y_hat=811\n","Skipping y_hat=742\n","Skipping y_hat=520\n","Skipping y_hat=111\n","Skipping y_hat=629\n","Skipping y_hat=808\n","Skipping y_hat=782\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=102\n","Skipping y_hat=607\n"," 85% 68/80 [00:02<00:00, 33.31it/s]Skipping y_hat=230\n","Skipping y_hat=991\n","Skipping y_hat=403\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=353\n","Skipping y_hat=1019\n","Skipping y_hat=569\n","Skipping y_hat=755\n","Skipping y_hat=853\n","Skipping y_hat=406\n","Skipping y_hat=359\n","Skipping y_hat=780\n","Skipping y_hat=171\n","Skipping y_hat=809\n","Skipping y_hat=900\n","Skipping y_hat=688\n","Skipping y_hat=273\n","Skipping y_hat=513\n","Skipping y_hat=798\n","Skipping y_hat=831\n","Skipping y_hat=812\n","Skipping y_hat=939\n","Skipping y_hat=415\n","Skipping y_hat=794\n","Skipping y_hat=462\n","Skipping y_hat=351\n","Skipping y_hat=981\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=506\n"," 90% 72/80 [00:02<00:00, 32.97it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=609\n","Skipping y_hat=1017\n","Skipping y_hat=301\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=680\n","Skipping y_hat=621\n","Skipping y_hat=339\n","Skipping y_hat=228\n","Skipping y_hat=102\n","Skipping y_hat=457\n","Skipping y_hat=890\n","Skipping y_hat=641\n","Skipping y_hat=520\n","Skipping y_hat=293\n","Skipping y_hat=781\n","Skipping y_hat=649\n","Skipping y_hat=889\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=661\n","Skipping y_hat=592\n","Skipping y_hat=1005\n","Skipping y_hat=807\n","Skipping y_hat=430\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=797\n","Skipping y_hat=141\n","Skipping y_hat=1042\n","Skipping y_hat=743\n","Skipping y_hat=292\n","Skipping y_hat=405\n","Skipping y_hat=292\n","Skipping y_hat=909\n"," 95% 76/80 [00:02<00:00, 32.82it/s]Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=181\n","Skipping y_hat=806\n","Skipping y_hat=1000\n","Skipping y_hat=399\n","Skipping y_hat=948\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=899\n","Skipping y_hat=607\n","Skipping y_hat=447\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=486\n","Skipping y_hat=437\n","Skipping y_hat=109\n","Skipping y_hat=811\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=522\n","Skipping y_hat=617\n","Skipping y_hat=744\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=633\n","Skipping y_hat=806\n","Skipping y_hat=231\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=337\n","Skipping y_hat=843\n","Skipping y_hat=594\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=104\n","Skipping y_hat=571\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=529\n","Skipping y_hat=238\n","Skipping y_hat=206\n","Skipping y_hat=208\n","Skipping y_hat=612\n","Skipping y_hat=333\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=309\n","Skipping y_hat=351\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=1002\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=616\n","Skipping y_hat=658\n","Skipping y_hat=814\n","Skipping y_hat=416\n","Skipping y_hat=751\n","Skipping y_hat=455\n","Skipping y_hat=337\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=662\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=631\n","Skipping y_hat=459\n","Skipping y_hat=185\n","Skipping y_hat=98\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=915\n","Skipping y_hat=932\n","Skipping y_hat=480\n","Skipping y_hat=629\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=880\n","Skipping y_hat=839\n","Skipping y_hat=712\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=847\n","Skipping y_hat=756\n","Skipping y_hat=802\n","Skipping y_hat=88\n","Skipping y_hat=42\n","Skipping y_hat=63\n","Skipping y_hat=84\n","Skipping y_hat=34\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.18it/s]\n","accuracy of 10000 examples: 9325/10000 (93.25%)\n","/content/drive/MyDrive/addition/evaluation.py:318: DtypeWarning: Columns (34,35,39,45,49,50,53,58,61,63,69,70,71,77,79,81,82,83,84,85,86,88,89,90,94,99,100,102,103,104,106,107,108,109,110,111,112,113,115,116,119,120,121,122,123,127,129,130,132,133,134,135,136,137,138,140,142,143,144,145,147,149,150,152,154,155,156,157,162,163,182,190) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Test Results:\n","test: 93.25%\n","\n","iter 4200: train loss 0.6774, val loss 1.7386\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 34.03it/s]Skipping y_hat=794\n","Skipping y_hat=170\n","Skipping y_hat=748\n","Skipping y_hat=881\n","Skipping y_hat=1072\n","Skipping y_hat=479\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=700\n","Skipping y_hat=879\n","Skipping y_hat=319\n","Skipping y_hat=808\n","Skipping y_hat=109\n","Skipping y_hat=287\n","Skipping y_hat=809\n","Skipping y_hat=505\n","Skipping y_hat=969\n","Skipping y_hat=936\n","Skipping y_hat=409\n","Skipping y_hat=403\n","Skipping y_hat=476\n","Skipping y_hat=990\n","Skipping y_hat=461\n","Skipping y_hat=191\n","Skipping y_hat=319\n","Skipping y_hat=294\n","Skipping y_hat=500\n","Skipping y_hat=991\n","Skipping y_hat=708\n","Skipping y_hat=811\n","Skipping y_hat=742\n","Skipping y_hat=112\n","Skipping y_hat=421\n","Skipping y_hat=629\n","Skipping y_hat=803\n","Skipping y_hat=132\n","Skipping y_hat=782\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=209\n","Skipping y_hat=608\n"," 85% 68/80 [00:02<00:00, 32.91it/s]Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=989\n","Skipping y_hat=331\n","Skipping y_hat=403\n","Skipping y_hat=1029\n","Skipping y_hat=312\n","Skipping y_hat=707\n","Skipping y_hat=1028\n","Skipping y_hat=961\n","Skipping y_hat=358\n","Skipping y_hat=901\n","Skipping y_hat=119\n","Skipping y_hat=353\n","Skipping y_hat=569\n","Skipping y_hat=357\n","Skipping y_hat=853\n","Skipping y_hat=406\n","Skipping y_hat=359\n","Skipping y_hat=780\n","Skipping y_hat=911\n","Skipping y_hat=171\n","Skipping y_hat=159\n","Skipping y_hat=900\n","Skipping y_hat=273\n","Skipping y_hat=513\n","Skipping y_hat=798\n","Skipping y_hat=819\n","Skipping y_hat=831\n","Skipping y_hat=900\n","Skipping y_hat=1007\n","Skipping y_hat=909\n","Skipping y_hat=415\n","Skipping y_hat=794\n","Skipping y_hat=462\n","Skipping y_hat=351\n","Skipping y_hat=920\n","Skipping y_hat=181\n","Skipping y_hat=1040\n","Skipping y_hat=410\n","Skipping y_hat=900\n","Skipping y_hat=506\n"," 90% 72/80 [00:02<00:00, 31.29it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=760\n","Skipping y_hat=1017\n","Skipping y_hat=703\n","Skipping y_hat=301\n","Skipping y_hat=719\n","Skipping y_hat=405\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=290\n","Skipping y_hat=799\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=621\n","Skipping y_hat=402\n","Skipping y_hat=128\n","Skipping y_hat=999\n","Skipping y_hat=293\n","Skipping y_hat=640\n","Skipping y_hat=552\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=402\n","Skipping y_hat=1005\n","Skipping y_hat=809\n","Skipping y_hat=197\n","Skipping y_hat=762\n","Skipping y_hat=797\n","Skipping y_hat=789\n","Skipping y_hat=660\n","Skipping y_hat=1042\n","Skipping y_hat=742\n","Skipping y_hat=519\n","Skipping y_hat=298\n","Skipping y_hat=405\n","Skipping y_hat=294\n","Skipping y_hat=999\n"," 95% 76/80 [00:02<00:00, 30.58it/s]Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=181\n","Skipping y_hat=806\n","Skipping y_hat=900\n","Skipping y_hat=399\n","Skipping y_hat=958\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=899\n","Skipping y_hat=687\n","Skipping y_hat=451\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=861\n","Skipping y_hat=20\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=522\n","Skipping y_hat=88\n","Skipping y_hat=627\n","Skipping y_hat=754\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=633\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=271\n","Skipping y_hat=337\n","Skipping y_hat=863\n","Skipping y_hat=594\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=435\n","Skipping y_hat=264\n","Skipping y_hat=581\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=549\n","Skipping y_hat=238\n","Skipping y_hat=206\n","Skipping y_hat=552\n","Skipping y_hat=313\n","Skipping y_hat=938\n","Skipping y_hat=485\n","Skipping y_hat=309\n","Skipping y_hat=351\n","Skipping y_hat=456\n","Skipping y_hat=840\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=1002\n","Skipping y_hat=628\n","Skipping y_hat=533\n","Skipping y_hat=646\n","Skipping y_hat=658\n","Skipping y_hat=894\n","Skipping y_hat=436\n","Skipping y_hat=100\n","Skipping y_hat=701\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=742\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=651\n","Skipping y_hat=831\n","Skipping y_hat=459\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=80\n","Skipping y_hat=149\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=935\n","Skipping y_hat=903\n","Skipping y_hat=386\n","Skipping y_hat=629\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=712\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=703\n","Skipping y_hat=857\n","Skipping y_hat=756\n","Skipping y_hat=168\n","Skipping y_hat=892\n","Skipping y_hat=88\n","Skipping y_hat=42\n","Skipping y_hat=63\n","Skipping y_hat=35\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 32.84it/s]\n","accuracy of 10000 examples: 9332/10000 (93.32000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.47it/s]\n","accuracy of 10000 examples: 9989/10000 (99.89%)\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 33.96it/s]Skipping y_hat=794\n","Skipping y_hat=171\n","Skipping y_hat=881\n","Skipping y_hat=851\n","Skipping y_hat=170\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=709\n","Skipping y_hat=863\n","Skipping y_hat=988\n","Skipping y_hat=613\n","Skipping y_hat=808\n","Skipping y_hat=287\n","Skipping y_hat=110\n","Skipping y_hat=809\n","Skipping y_hat=505\n","Skipping y_hat=969\n","Skipping y_hat=936\n","Skipping y_hat=409\n","Skipping y_hat=411\n","Skipping y_hat=476\n","Skipping y_hat=996\n","Skipping y_hat=461\n","Skipping y_hat=319\n","Skipping y_hat=294\n","Skipping y_hat=570\n","Skipping y_hat=489\n","Skipping y_hat=500\n","Skipping y_hat=991\n","Skipping y_hat=959\n","Skipping y_hat=705\n","Skipping y_hat=605\n","Skipping y_hat=811\n","Skipping y_hat=742\n","Skipping y_hat=901\n","Skipping y_hat=111\n","Skipping y_hat=421\n","Skipping y_hat=629\n","Skipping y_hat=806\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=209\n","Skipping y_hat=607\n"," 85% 68/80 [00:02<00:00, 33.36it/s]Skipping y_hat=991\n","Skipping y_hat=403\n","Skipping y_hat=1028\n","Skipping y_hat=1084\n","Skipping y_hat=312\n","Skipping y_hat=707\n","Skipping y_hat=1028\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=119\n","Skipping y_hat=353\n","Skipping y_hat=1010\n","Skipping y_hat=569\n","Skipping y_hat=853\n","Skipping y_hat=406\n","Skipping y_hat=911\n","Skipping y_hat=159\n","Skipping y_hat=758\n","Skipping y_hat=900\n","Skipping y_hat=689\n","Skipping y_hat=273\n","Skipping y_hat=513\n","Skipping y_hat=831\n","Skipping y_hat=800\n","Skipping y_hat=1007\n","Skipping y_hat=800\n","Skipping y_hat=415\n","Skipping y_hat=461\n","Skipping y_hat=351\n","Skipping y_hat=920\n","Skipping y_hat=1000\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=507\n"," 90% 72/80 [00:02<00:00, 32.93it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=609\n","Skipping y_hat=1017\n","Skipping y_hat=703\n","Skipping y_hat=301\n","Skipping y_hat=805\n","Skipping y_hat=480\n","Skipping y_hat=309\n","Skipping y_hat=199\n","Skipping y_hat=290\n","Skipping y_hat=128\n","Skipping y_hat=999\n","Skipping y_hat=641\n","Skipping y_hat=293\n","Skipping y_hat=781\n","Skipping y_hat=639\n","Skipping y_hat=552\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=402\n","Skipping y_hat=592\n","Skipping y_hat=1005\n","Skipping y_hat=809\n","Skipping y_hat=359\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=1017\n","Skipping y_hat=797\n","Skipping y_hat=140\n","Skipping y_hat=679\n","Skipping y_hat=1042\n","Skipping y_hat=292\n","Skipping y_hat=405\n","Skipping y_hat=295\n","Skipping y_hat=999\n"," 95% 76/80 [00:02<00:00, 32.36it/s]Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=1000\n","Skipping y_hat=160\n","Skipping y_hat=391\n","Skipping y_hat=950\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=607\n","Skipping y_hat=905\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=486\n","Skipping y_hat=433\n","Skipping y_hat=109\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=829\n","Skipping y_hat=522\n","Skipping y_hat=617\n","Skipping y_hat=754\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=673\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=337\n","Skipping y_hat=863\n","Skipping y_hat=115\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=435\n","Skipping y_hat=264\n","Skipping y_hat=571\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=549\n","Skipping y_hat=238\n","Skipping y_hat=206\n","Skipping y_hat=209\n","Skipping y_hat=552\n","Skipping y_hat=331\n","Skipping y_hat=908\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=456\n","Skipping y_hat=840\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=1002\n","Skipping y_hat=618\n","Skipping y_hat=533\n","Skipping y_hat=646\n","Skipping y_hat=658\n","Skipping y_hat=994\n","Skipping y_hat=100\n","Skipping y_hat=701\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=742\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=62\n","Skipping y_hat=831\n","Skipping y_hat=459\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=98\n","Skipping y_hat=149\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=935\n","Skipping y_hat=903\n","Skipping y_hat=389\n","Skipping y_hat=629\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=829\n","Skipping y_hat=716\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=857\n","Skipping y_hat=756\n","Skipping y_hat=812\n","Skipping y_hat=781\n","Skipping y_hat=88\n","Skipping y_hat=42\n","Skipping y_hat=63\n","Skipping y_hat=35\n","Skipping y_hat=104\n","Skipping y_hat=73\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.13it/s]\n","accuracy of 10000 examples: 9331/10000 (93.31%)\n","/content/drive/MyDrive/addition/evaluation.py:318: DtypeWarning: Columns (34,35,39,45,49,50,53,58,61,63,69,70,71,77,79,81,82,83,84,85,86,88,89,90,94,99,100,102,103,104,106,107,108,109,110,111,112,113,115,116,119,120,121,122,123,127,129,130,132,133,134,135,136,137,138,140,142,143,144,145,147,149,150,152,154,155,156,157,162,163,182,190) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Test Results:\n","test: 93.31%\n","\n","iter 4300: train loss 0.6741, val loss 1.7826\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 33.89it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=889\n","Skipping y_hat=477\n","Skipping y_hat=1072\n","Skipping y_hat=852\n","Skipping y_hat=170\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=690\n","Skipping y_hat=319\n","Skipping y_hat=612\n","Skipping y_hat=808\n","Skipping y_hat=618\n","Skipping y_hat=645\n","Skipping y_hat=275\n","Skipping y_hat=287\n","Skipping y_hat=110\n","Skipping y_hat=809\n","Skipping y_hat=505\n","Skipping y_hat=401\n","Skipping y_hat=996\n","Skipping y_hat=461\n","Skipping y_hat=981\n","Skipping y_hat=290\n","Skipping y_hat=500\n","Skipping y_hat=399\n","Skipping y_hat=959\n","Skipping y_hat=707\n","Skipping y_hat=607\n","Skipping y_hat=813\n","Skipping y_hat=742\n","Skipping y_hat=901\n","Skipping y_hat=111\n","Skipping y_hat=421\n","Skipping y_hat=609\n","Skipping y_hat=808\n","Skipping y_hat=782\n","Skipping y_hat=209\n","Skipping y_hat=619\n","Skipping y_hat=607\n"," 85% 68/80 [00:02<00:00, 32.74it/s]Skipping y_hat=991\n","Skipping y_hat=909\n","Skipping y_hat=1028\n","Skipping y_hat=963\n","Skipping y_hat=515\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=569\n","Skipping y_hat=853\n","Skipping y_hat=407\n","Skipping y_hat=159\n","Skipping y_hat=758\n","Skipping y_hat=500\n","Skipping y_hat=900\n","Skipping y_hat=689\n","Skipping y_hat=273\n","Skipping y_hat=690\n","Skipping y_hat=529\n","Skipping y_hat=816\n","Skipping y_hat=798\n","Skipping y_hat=500\n","Skipping y_hat=819\n","Skipping y_hat=505\n","Skipping y_hat=831\n","Skipping y_hat=900\n","Skipping y_hat=939\n","Skipping y_hat=1007\n","Skipping y_hat=909\n","Skipping y_hat=920\n","Skipping y_hat=181\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=506\n","Skipping y_hat=511\n"," 90% 72/80 [00:02<00:00, 31.96it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=760\n","Skipping y_hat=1017\n","Skipping y_hat=301\n","Skipping y_hat=719\n","Skipping y_hat=805\n","Skipping y_hat=309\n","Skipping y_hat=420\n","Skipping y_hat=196\n","Skipping y_hat=290\n","Skipping y_hat=780\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=339\n","Skipping y_hat=228\n","Skipping y_hat=890\n","Skipping y_hat=999\n","Skipping y_hat=520\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=799\n","Skipping y_hat=639\n","Skipping y_hat=710\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=592\n","Skipping y_hat=196\n","Skipping y_hat=807\n","Skipping y_hat=430\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=797\n","Skipping y_hat=678\n","Skipping y_hat=1042\n","Skipping y_hat=801\n","Skipping y_hat=519\n","Skipping y_hat=292\n","Skipping y_hat=405\n","Skipping y_hat=296\n"," 95% 76/80 [00:02<00:00, 30.91it/s]Skipping y_hat=600\n","Skipping y_hat=190\n","Skipping y_hat=818\n","Skipping y_hat=727\n","Skipping y_hat=260\n","Skipping y_hat=806\n","Skipping y_hat=1000\n","Skipping y_hat=230\n","Skipping y_hat=391\n","Skipping y_hat=958\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=839\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=486\n","Skipping y_hat=437\n","Skipping y_hat=90\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=522\n","Skipping y_hat=88\n","Skipping y_hat=627\n","Skipping y_hat=744\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=569\n","Skipping y_hat=258\n","Skipping y_hat=633\n","Skipping y_hat=856\n","Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=271\n","Skipping y_hat=893\n","Skipping y_hat=598\n","Skipping y_hat=594\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=571\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=549\n","Skipping y_hat=238\n","Skipping y_hat=206\n","Skipping y_hat=652\n","Skipping y_hat=331\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=757\n","Skipping y_hat=456\n","Skipping y_hat=840\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=646\n","Skipping y_hat=658\n","Skipping y_hat=894\n","Skipping y_hat=436\n","Skipping y_hat=701\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=119\n","Skipping y_hat=742\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=638\n","Skipping y_hat=830\n","Skipping y_hat=459\n","Skipping y_hat=185\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=945\n","Skipping y_hat=932\n","Skipping y_hat=356\n","Skipping y_hat=639\n","Skipping y_hat=416\n","Skipping y_hat=637\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=712\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=857\n","Skipping y_hat=756\n","Skipping y_hat=168\n","Skipping y_hat=812\n","Skipping y_hat=796\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=34\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 32.58it/s]\n","accuracy of 10000 examples: 9325/10000 (93.25%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.25it/s]\n","accuracy of 10000 examples: 9985/10000 (99.85000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 33.11it/s]Skipping y_hat=794\n","Skipping y_hat=171\n","Skipping y_hat=672\n","Skipping y_hat=748\n","Skipping y_hat=1072\n","Skipping y_hat=479\n","Skipping y_hat=852\n","Skipping y_hat=311\n","Skipping y_hat=200\n","Skipping y_hat=405\n","Skipping y_hat=700\n","Skipping y_hat=988\n","Skipping y_hat=759\n","Skipping y_hat=612\n","Skipping y_hat=808\n","Skipping y_hat=220\n","Skipping y_hat=109\n","Skipping y_hat=287\n","Skipping y_hat=119\n","Skipping y_hat=809\n","Skipping y_hat=505\n","Skipping y_hat=936\n","Skipping y_hat=409\n","Skipping y_hat=401\n","Skipping y_hat=476\n","Skipping y_hat=999\n","Skipping y_hat=460\n","Skipping y_hat=191\n","Skipping y_hat=290\n","Skipping y_hat=512\n","Skipping y_hat=589\n","Skipping y_hat=742\n","Skipping y_hat=500\n","Skipping y_hat=399\n","Skipping y_hat=509\n","Skipping y_hat=959\n","Skipping y_hat=229\n","Skipping y_hat=705\n","Skipping y_hat=604\n","Skipping y_hat=811\n","Skipping y_hat=742\n","Skipping y_hat=112\n","Skipping y_hat=609\n","Skipping y_hat=808\n","Skipping y_hat=782\n","Skipping y_hat=189\n","Skipping y_hat=319\n","Skipping y_hat=209\n","Skipping y_hat=608\n"," 85% 68/80 [00:02<00:00, 31.27it/s]Skipping y_hat=230\n","Skipping y_hat=981\n","Skipping y_hat=331\n","Skipping y_hat=1028\n","Skipping y_hat=312\n","Skipping y_hat=1027\n","Skipping y_hat=963\n","Skipping y_hat=900\n","Skipping y_hat=219\n","Skipping y_hat=569\n","Skipping y_hat=357\n","Skipping y_hat=406\n","Skipping y_hat=359\n","Skipping y_hat=780\n","Skipping y_hat=809\n","Skipping y_hat=159\n","Skipping y_hat=758\n","Skipping y_hat=900\n","Skipping y_hat=688\n","Skipping y_hat=273\n","Skipping y_hat=797\n","Skipping y_hat=816\n","Skipping y_hat=500\n","Skipping y_hat=819\n","Skipping y_hat=505\n","Skipping y_hat=831\n","Skipping y_hat=809\n","Skipping y_hat=939\n","Skipping y_hat=1007\n","Skipping y_hat=809\n","Skipping y_hat=461\n","Skipping y_hat=920\n","Skipping y_hat=191\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=506\n"," 90% 72/80 [00:02<00:00, 30.19it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=760\n","Skipping y_hat=1017\n","Skipping y_hat=809\n","Skipping y_hat=301\n","Skipping y_hat=719\n","Skipping y_hat=805\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=290\n","Skipping y_hat=799\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=621\n","Skipping y_hat=228\n","Skipping y_hat=332\n","Skipping y_hat=520\n","Skipping y_hat=293\n","Skipping y_hat=781\n","Skipping y_hat=640\n","Skipping y_hat=552\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=592\n","Skipping y_hat=1031\n","Skipping y_hat=1005\n","Skipping y_hat=807\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=900\n","Skipping y_hat=797\n","Skipping y_hat=144\n","Skipping y_hat=678\n","Skipping y_hat=1032\n","Skipping y_hat=801\n","Skipping y_hat=292\n","Skipping y_hat=296\n","Skipping y_hat=999\n"," 95% 76/80 [00:02<00:00, 30.06it/s]Skipping y_hat=190\n","Skipping y_hat=818\n","Skipping y_hat=727\n","Skipping y_hat=558\n","Skipping y_hat=230\n","Skipping y_hat=179\n","Skipping y_hat=399\n","Skipping y_hat=958\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=687\n","Skipping y_hat=955\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=451\n","Skipping y_hat=486\n","Skipping y_hat=437\n","Skipping y_hat=109\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=522\n","Skipping y_hat=88\n","Skipping y_hat=627\n","Skipping y_hat=385\n","Skipping y_hat=754\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=569\n","Skipping y_hat=258\n","Skipping y_hat=633\n","Skipping y_hat=846\n","Skipping y_hat=230\n","Skipping y_hat=986\n","Skipping y_hat=752\n","Skipping y_hat=863\n","Skipping y_hat=594\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=571\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=549\n","Skipping y_hat=126\n","Skipping y_hat=238\n","Skipping y_hat=206\n","Skipping y_hat=209\n","Skipping y_hat=652\n","Skipping y_hat=313\n","Skipping y_hat=938\n","Skipping y_hat=485\n","Skipping y_hat=309\n","Skipping y_hat=251\n","Skipping y_hat=717\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=646\n","Skipping y_hat=658\n","Skipping y_hat=894\n","Skipping y_hat=436\n","Skipping y_hat=100\n","Skipping y_hat=751\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=119\n","Skipping y_hat=658\n","Skipping y_hat=742\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=658\n","Skipping y_hat=459\n","Skipping y_hat=185\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=935\n","Skipping y_hat=932\n","Skipping y_hat=386\n","Skipping y_hat=639\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=703\n","Skipping y_hat=847\n","Skipping y_hat=756\n","Skipping y_hat=892\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=34\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 32.10it/s]\n","accuracy of 10000 examples: 9324/10000 (93.24%)\n","/content/drive/MyDrive/addition/evaluation.py:318: DtypeWarning: Columns (34,35,39,45,49,50,53,58,61,63,69,70,71,77,79,81,82,83,84,85,86,88,89,90,94,99,100,102,103,104,106,107,108,109,110,111,112,113,115,116,119,120,121,122,123,127,129,130,132,133,134,135,136,137,138,140,142,143,144,145,147,149,150,152,154,155,156,157,162,163,182,190) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Test Results:\n","test: 93.24%\n","\n","iter 4400: train loss 0.6703, val loss 1.8110\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 33.89it/s]Skipping y_hat=171\n","Skipping y_hat=882\n","Skipping y_hat=589\n","Skipping y_hat=853\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=709\n","Skipping y_hat=873\n","Skipping y_hat=319\n","Skipping y_hat=614\n","Skipping y_hat=808\n","Skipping y_hat=618\n","Skipping y_hat=554\n","Skipping y_hat=287\n","Skipping y_hat=501\n","Skipping y_hat=409\n","Skipping y_hat=476\n","Skipping y_hat=999\n","Skipping y_hat=1045\n","Skipping y_hat=319\n","Skipping y_hat=512\n","Skipping y_hat=589\n","Skipping y_hat=742\n","Skipping y_hat=500\n","Skipping y_hat=247\n","Skipping y_hat=257\n","Skipping y_hat=708\n","Skipping y_hat=605\n","Skipping y_hat=111\n","Skipping y_hat=439\n","Skipping y_hat=210\n","Skipping y_hat=808\n","Skipping y_hat=1021\n","Skipping y_hat=608\n"," 85% 68/80 [00:02<00:00, 33.21it/s]Skipping y_hat=991\n","Skipping y_hat=331\n","Skipping y_hat=870\n","Skipping y_hat=312\n","Skipping y_hat=1028\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=489\n","Skipping y_hat=901\n","Skipping y_hat=119\n","Skipping y_hat=357\n","Skipping y_hat=853\n","Skipping y_hat=408\n","Skipping y_hat=359\n","Skipping y_hat=171\n","Skipping y_hat=159\n","Skipping y_hat=900\n","Skipping y_hat=689\n","Skipping y_hat=273\n","Skipping y_hat=709\n","Skipping y_hat=513\n","Skipping y_hat=829\n","Skipping y_hat=798\n","Skipping y_hat=831\n","Skipping y_hat=901\n","Skipping y_hat=939\n","Skipping y_hat=1007\n","Skipping y_hat=909\n","Skipping y_hat=414\n","Skipping y_hat=794\n","Skipping y_hat=351\n","Skipping y_hat=181\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=505\n"," 90% 72/80 [00:02<00:00, 32.65it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=760\n","Skipping y_hat=1017\n","Skipping y_hat=301\n","Skipping y_hat=719\n","Skipping y_hat=309\n","Skipping y_hat=439\n","Skipping y_hat=196\n","Skipping y_hat=290\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=680\n","Skipping y_hat=621\n","Skipping y_hat=228\n","Skipping y_hat=890\n","Skipping y_hat=980\n","Skipping y_hat=897\n","Skipping y_hat=293\n","Skipping y_hat=781\n","Skipping y_hat=640\n","Skipping y_hat=171\n","Skipping y_hat=960\n","Skipping y_hat=692\n","Skipping y_hat=1005\n","Skipping y_hat=807\n","Skipping y_hat=447\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=1001\n","Skipping y_hat=797\n","Skipping y_hat=1042\n","Skipping y_hat=519\n","Skipping y_hat=292\n","Skipping y_hat=405\n","Skipping y_hat=295\n","Skipping y_hat=999\n"," 95% 76/80 [00:02<00:00, 32.45it/s]Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=181\n","Skipping y_hat=728\n","Skipping y_hat=1000\n","Skipping y_hat=361\n","Skipping y_hat=399\n","Skipping y_hat=958\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=617\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=451\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=109\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=829\n","Skipping y_hat=522\n","Skipping y_hat=88\n","Skipping y_hat=617\n","Skipping y_hat=755\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=673\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=863\n","Skipping y_hat=598\n","Skipping y_hat=694\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=571\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=238\n","Skipping y_hat=206\n","Skipping y_hat=612\n","Skipping y_hat=313\n","Skipping y_hat=938\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=456\n","Skipping y_hat=840\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=658\n","Skipping y_hat=814\n","Skipping y_hat=436\n","Skipping y_hat=100\n","Skipping y_hat=701\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=113\n","Skipping y_hat=658\n","Skipping y_hat=752\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=631\n","Skipping y_hat=830\n","Skipping y_hat=459\n","Skipping y_hat=185\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=945\n","Skipping y_hat=366\n","Skipping y_hat=629\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=881\n","Skipping y_hat=829\n","Skipping y_hat=712\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=847\n","Skipping y_hat=756\n","Skipping y_hat=812\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=35\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 32.94it/s]\n","accuracy of 10000 examples: 9339/10000 (93.39%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.12it/s]\n","accuracy of 10000 examples: 9986/10000 (99.86%)\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 34.80it/s]Skipping y_hat=794\n","Skipping y_hat=171\n","Skipping y_hat=477\n","Skipping y_hat=1072\n","Skipping y_hat=470\n","Skipping y_hat=852\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=873\n","Skipping y_hat=768\n","Skipping y_hat=319\n","Skipping y_hat=482\n","Skipping y_hat=612\n","Skipping y_hat=808\n","Skipping y_hat=554\n","Skipping y_hat=287\n","Skipping y_hat=200\n","Skipping y_hat=501\n","Skipping y_hat=936\n","Skipping y_hat=900\n","Skipping y_hat=181\n","Skipping y_hat=512\n","Skipping y_hat=589\n","Skipping y_hat=742\n","Skipping y_hat=500\n","Skipping y_hat=247\n","Skipping y_hat=399\n","Skipping y_hat=257\n","Skipping y_hat=1028\n","Skipping y_hat=708\n","Skipping y_hat=742\n","Skipping y_hat=901\n","Skipping y_hat=520\n","Skipping y_hat=111\n","Skipping y_hat=782\n","Skipping y_hat=209\n","Skipping y_hat=607\n"," 85% 68/80 [00:01<00:00, 33.82it/s]Skipping y_hat=999\n","Skipping y_hat=331\n","Skipping y_hat=1028\n","Skipping y_hat=1027\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=541\n","Skipping y_hat=219\n","Skipping y_hat=357\n","Skipping y_hat=853\n","Skipping y_hat=408\n","Skipping y_hat=359\n","Skipping y_hat=171\n","Skipping y_hat=159\n","Skipping y_hat=900\n","Skipping y_hat=688\n","Skipping y_hat=273\n","Skipping y_hat=790\n","Skipping y_hat=529\n","Skipping y_hat=816\n","Skipping y_hat=798\n","Skipping y_hat=831\n","Skipping y_hat=901\n","Skipping y_hat=939\n","Skipping y_hat=1007\n","Skipping y_hat=909\n","Skipping y_hat=415\n","Skipping y_hat=794\n","Skipping y_hat=544\n","Skipping y_hat=181\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=506\n"," 90% 72/80 [00:02<00:00, 33.17it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=760\n","Skipping y_hat=1038\n","Skipping y_hat=186\n","Skipping y_hat=719\n","Skipping y_hat=309\n","Skipping y_hat=439\n","Skipping y_hat=196\n","Skipping y_hat=290\n","Skipping y_hat=901\n","Skipping y_hat=680\n","Skipping y_hat=621\n","Skipping y_hat=228\n","Skipping y_hat=880\n","Skipping y_hat=980\n","Skipping y_hat=641\n","Skipping y_hat=897\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=640\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=979\n","Skipping y_hat=591\n","Skipping y_hat=1005\n","Skipping y_hat=809\n","Skipping y_hat=447\n","Skipping y_hat=820\n","Skipping y_hat=1007\n","Skipping y_hat=799\n","Skipping y_hat=1042\n","Skipping y_hat=292\n","Skipping y_hat=292\n","Skipping y_hat=999\n"," 95% 76/80 [00:02<00:00, 32.64it/s]Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=181\n","Skipping y_hat=727\n","Skipping y_hat=1000\n","Skipping y_hat=361\n","Skipping y_hat=399\n","Skipping y_hat=950\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=687\n","Skipping y_hat=905\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=451\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=109\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=532\n","Skipping y_hat=88\n","Skipping y_hat=617\n","Skipping y_hat=385\n","Skipping y_hat=754\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=633\n","Skipping y_hat=230\n","Skipping y_hat=986\n","Skipping y_hat=752\n","Skipping y_hat=843\n","Skipping y_hat=115\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=571\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=529\n","Skipping y_hat=238\n","Skipping y_hat=206\n","Skipping y_hat=612\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=717\n","Skipping y_hat=456\n","Skipping y_hat=840\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=917\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=616\n","Skipping y_hat=658\n","Skipping y_hat=814\n","Skipping y_hat=416\n","Skipping y_hat=100\n","Skipping y_hat=751\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=656\n","Skipping y_hat=113\n","Skipping y_hat=658\n","Skipping y_hat=742\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=831\n","Skipping y_hat=459\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=945\n","Skipping y_hat=902\n","Skipping y_hat=365\n","Skipping y_hat=639\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=712\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=857\n","Skipping y_hat=756\n","Skipping y_hat=168\n","Skipping y_hat=812\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=35\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.63it/s]\n","accuracy of 10000 examples: 9331/10000 (93.31%)\n","/content/drive/MyDrive/addition/evaluation.py:318: DtypeWarning: Columns (34,35,39,45,49,50,53,58,61,63,69,70,71,77,79,81,82,83,84,85,86,88,89,90,94,99,100,102,103,104,106,107,108,109,110,111,112,113,115,116,119,120,121,122,123,127,129,130,132,133,134,135,136,137,138,140,142,143,144,145,147,149,150,152,154,155,156,157,162,163,182,190) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Test Results:\n","test: 93.31%\n","\n","iter 4500: train loss 0.6801, val loss 1.8138\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 33.00it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=470\n","Skipping y_hat=180\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=482\n","Skipping y_hat=612\n","Skipping y_hat=808\n","Skipping y_hat=601\n","Skipping y_hat=287\n","Skipping y_hat=501\n","Skipping y_hat=898\n","Skipping y_hat=936\n","Skipping y_hat=409\n","Skipping y_hat=403\n","Skipping y_hat=476\n","Skipping y_hat=999\n","Skipping y_hat=461\n","Skipping y_hat=589\n","Skipping y_hat=742\n","Skipping y_hat=489\n","Skipping y_hat=500\n","Skipping y_hat=901\n","Skipping y_hat=257\n","Skipping y_hat=1028\n","Skipping y_hat=799\n","Skipping y_hat=607\n","Skipping y_hat=811\n","Skipping y_hat=742\n","Skipping y_hat=520\n","Skipping y_hat=421\n","Skipping y_hat=965\n","Skipping y_hat=300\n","Skipping y_hat=808\n","Skipping y_hat=782\n","Skipping y_hat=171\n","Skipping y_hat=607\n"," 85% 68/80 [00:02<00:00, 31.25it/s]Skipping y_hat=331\n","Skipping y_hat=312\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=900\n","Skipping y_hat=109\n","Skipping y_hat=354\n","Skipping y_hat=853\n","Skipping y_hat=407\n","Skipping y_hat=359\n","Skipping y_hat=911\n","Skipping y_hat=159\n","Skipping y_hat=801\n","Skipping y_hat=900\n","Skipping y_hat=273\n","Skipping y_hat=790\n","Skipping y_hat=529\n","Skipping y_hat=828\n","Skipping y_hat=798\n","Skipping y_hat=831\n","Skipping y_hat=758\n","Skipping y_hat=901\n","Skipping y_hat=1007\n","Skipping y_hat=544\n","Skipping y_hat=351\n","Skipping y_hat=981\n","Skipping y_hat=1040\n","Skipping y_hat=410\n","Skipping y_hat=691\n","Skipping y_hat=505\n"," 90% 72/80 [00:02<00:00, 31.24it/s]Skipping y_hat=398\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=1017\n","Skipping y_hat=1038\n","Skipping y_hat=719\n","Skipping y_hat=484\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=616\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=228\n","Skipping y_hat=780\n","Skipping y_hat=641\n","Skipping y_hat=897\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=640\n","Skipping y_hat=438\n","Skipping y_hat=171\n","Skipping y_hat=692\n","Skipping y_hat=1005\n","Skipping y_hat=809\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=797\n","Skipping y_hat=770\n","Skipping y_hat=1042\n","Skipping y_hat=801\n","Skipping y_hat=292\n","Skipping y_hat=297\n","Skipping y_hat=999\n"," 95% 76/80 [00:02<00:00, 30.00it/s]Skipping y_hat=600\n","Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=803\n","Skipping y_hat=900\n","Skipping y_hat=247\n","Skipping y_hat=811\n","Skipping y_hat=574\n","Skipping y_hat=360\n","Skipping y_hat=399\n","Skipping y_hat=948\n","Skipping y_hat=863\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=687\n","Skipping y_hat=955\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=725\n","Skipping y_hat=646\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=109\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=829\n","Skipping y_hat=522\n","Skipping y_hat=88\n","Skipping y_hat=627\n","Skipping y_hat=754\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=673\n","Skipping y_hat=886\n","Skipping y_hat=230\n","Skipping y_hat=981\n","Skipping y_hat=752\n","Skipping y_hat=271\n","Skipping y_hat=853\n","Skipping y_hat=594\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=581\n","Skipping y_hat=848\n","Skipping y_hat=426\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=238\n","Skipping y_hat=206\n","Skipping y_hat=582\n","Skipping y_hat=333\n","Skipping y_hat=908\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=757\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=688\n","Skipping y_hat=535\n","Skipping y_hat=686\n","Skipping y_hat=658\n","Skipping y_hat=894\n","Skipping y_hat=436\n","Skipping y_hat=160\n","Skipping y_hat=751\n","Skipping y_hat=455\n","Skipping y_hat=337\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=762\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=661\n","Skipping y_hat=459\n","Skipping y_hat=185\n","Skipping y_hat=736\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=935\n","Skipping y_hat=902\n","Skipping y_hat=466\n","Skipping y_hat=639\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=716\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=857\n","Skipping y_hat=756\n","Skipping y_hat=812\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=34\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 32.29it/s]\n","accuracy of 10000 examples: 9321/10000 (93.21000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 37.53it/s]\n","accuracy of 10000 examples: 9992/10000 (99.92%)\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 33.54it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=211\n","Skipping y_hat=470\n","Skipping y_hat=180\n","Skipping y_hat=852\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=701\n","Skipping y_hat=319\n","Skipping y_hat=612\n","Skipping y_hat=808\n","Skipping y_hat=601\n","Skipping y_hat=109\n","Skipping y_hat=287\n","Skipping y_hat=200\n","Skipping y_hat=591\n","Skipping y_hat=936\n","Skipping y_hat=409\n","Skipping y_hat=401\n","Skipping y_hat=476\n","Skipping y_hat=999\n","Skipping y_hat=460\n","Skipping y_hat=589\n","Skipping y_hat=500\n","Skipping y_hat=247\n","Skipping y_hat=901\n","Skipping y_hat=257\n","Skipping y_hat=1028\n","Skipping y_hat=959\n","Skipping y_hat=708\n","Skipping y_hat=609\n","Skipping y_hat=901\n","Skipping y_hat=742\n","Skipping y_hat=111\n","Skipping y_hat=439\n","Skipping y_hat=965\n","Skipping y_hat=300\n","Skipping y_hat=808\n","Skipping y_hat=782\n","Skipping y_hat=100\n","Skipping y_hat=607\n","Skipping y_hat=642\n"," 85% 68/80 [00:02<00:00, 31.52it/s]Skipping y_hat=331\n","Skipping y_hat=1028\n","Skipping y_hat=312\n","Skipping y_hat=1028\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=353\n","Skipping y_hat=569\n","Skipping y_hat=853\n","Skipping y_hat=408\n","Skipping y_hat=359\n","Skipping y_hat=911\n","Skipping y_hat=159\n","Skipping y_hat=809\n","Skipping y_hat=900\n","Skipping y_hat=273\n","Skipping y_hat=790\n","Skipping y_hat=513\n","Skipping y_hat=831\n","Skipping y_hat=758\n","Skipping y_hat=901\n","Skipping y_hat=936\n","Skipping y_hat=1007\n","Skipping y_hat=809\n","Skipping y_hat=544\n","Skipping y_hat=351\n","Skipping y_hat=186\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=505\n"," 90% 72/80 [00:02<00:00, 30.62it/s]Skipping y_hat=1072\n","Skipping y_hat=609\n","Skipping y_hat=1017\n","Skipping y_hat=809\n","Skipping y_hat=719\n","Skipping y_hat=484\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=290\n","Skipping y_hat=799\n","Skipping y_hat=816\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=228\n","Skipping y_hat=780\n","Skipping y_hat=999\n","Skipping y_hat=641\n","Skipping y_hat=897\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=640\n","Skipping y_hat=438\n","Skipping y_hat=171\n","Skipping y_hat=692\n","Skipping y_hat=279\n","Skipping y_hat=1005\n","Skipping y_hat=807\n","Skipping y_hat=197\n","Skipping y_hat=1017\n","Skipping y_hat=799\n","Skipping y_hat=789\n","Skipping y_hat=1042\n","Skipping y_hat=801\n","Skipping y_hat=744\n","Skipping y_hat=292\n","Skipping y_hat=296\n","Skipping y_hat=999\n"," 95% 76/80 [00:02<00:00, 30.87it/s]Skipping y_hat=600\n","Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=583\n","Skipping y_hat=806\n","Skipping y_hat=900\n","Skipping y_hat=399\n","Skipping y_hat=930\n","Skipping y_hat=873\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=687\n","Skipping y_hat=955\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=451\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=109\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=522\n","Skipping y_hat=88\n","Skipping y_hat=627\n","Skipping y_hat=754\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=673\n","Skipping y_hat=856\n","Skipping y_hat=230\n","Skipping y_hat=981\n","Skipping y_hat=752\n","Skipping y_hat=863\n","Skipping y_hat=115\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=435\n","Skipping y_hat=264\n","Skipping y_hat=581\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=238\n","Skipping y_hat=582\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=757\n","Skipping y_hat=456\n","Skipping y_hat=840\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=912\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=686\n","Skipping y_hat=658\n","Skipping y_hat=814\n","Skipping y_hat=436\n","Skipping y_hat=100\n","Skipping y_hat=751\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=123\n","Skipping y_hat=658\n","Skipping y_hat=752\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=661\n","Skipping y_hat=830\n","Skipping y_hat=459\n","Skipping y_hat=857\n","Skipping y_hat=189\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=935\n","Skipping y_hat=933\n","Skipping y_hat=466\n","Skipping y_hat=619\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=829\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=847\n","Skipping y_hat=756\n","Skipping y_hat=812\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=35\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 32.52it/s]\n","accuracy of 10000 examples: 9337/10000 (93.37%)\n","/content/drive/MyDrive/addition/evaluation.py:318: DtypeWarning: Columns (34,35,39,45,49,50,53,58,61,63,69,70,71,77,79,81,82,83,84,85,86,88,89,90,94,99,100,102,103,104,106,107,108,109,110,111,112,113,115,116,119,120,121,122,123,127,129,130,132,133,134,135,136,137,138,140,142,143,144,145,147,149,150,152,154,155,156,157,162,163,182,190) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Test Results:\n","test: 93.37%\n","\n","iter 4600: train loss 0.6719, val loss 1.8732\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 34.57it/s]Skipping y_hat=794\n","Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=881\n","Skipping y_hat=477\n","Skipping y_hat=1072\n","Skipping y_hat=470\n","Skipping y_hat=852\n","Skipping y_hat=189\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=694\n","Skipping y_hat=863\n","Skipping y_hat=873\n","Skipping y_hat=319\n","Skipping y_hat=482\n","Skipping y_hat=612\n","Skipping y_hat=808\n","Skipping y_hat=287\n","Skipping y_hat=219\n","Skipping y_hat=599\n","Skipping y_hat=505\n","Skipping y_hat=403\n","Skipping y_hat=999\n","Skipping y_hat=461\n","Skipping y_hat=981\n","Skipping y_hat=294\n","Skipping y_hat=512\n","Skipping y_hat=500\n","Skipping y_hat=247\n","Skipping y_hat=708\n","Skipping y_hat=742\n","Skipping y_hat=113\n","Skipping y_hat=629\n","Skipping y_hat=808\n","Skipping y_hat=782\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=607\n"," 85% 68/80 [00:01<00:00, 33.69it/s]Skipping y_hat=331\n","Skipping y_hat=1049\n","Skipping y_hat=312\n","Skipping y_hat=1028\n","Skipping y_hat=963\n","Skipping y_hat=861\n","Skipping y_hat=900\n","Skipping y_hat=109\n","Skipping y_hat=354\n","Skipping y_hat=1010\n","Skipping y_hat=569\n","Skipping y_hat=357\n","Skipping y_hat=853\n","Skipping y_hat=406\n","Skipping y_hat=359\n","Skipping y_hat=404\n","Skipping y_hat=212\n","Skipping y_hat=159\n","Skipping y_hat=809\n","Skipping y_hat=516\n","Skipping y_hat=900\n","Skipping y_hat=689\n","Skipping y_hat=273\n","Skipping y_hat=790\n","Skipping y_hat=513\n","Skipping y_hat=798\n","Skipping y_hat=500\n","Skipping y_hat=505\n","Skipping y_hat=831\n","Skipping y_hat=758\n","Skipping y_hat=801\n","Skipping y_hat=1007\n","Skipping y_hat=809\n","Skipping y_hat=462\n","Skipping y_hat=351\n","Skipping y_hat=181\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=505\n"," 90% 72/80 [00:02<00:00, 32.86it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=1017\n","Skipping y_hat=1038\n","Skipping y_hat=719\n","Skipping y_hat=405\n","Skipping y_hat=309\n","Skipping y_hat=439\n","Skipping y_hat=196\n","Skipping y_hat=290\n","Skipping y_hat=780\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=680\n","Skipping y_hat=229\n","Skipping y_hat=880\n","Skipping y_hat=520\n","Skipping y_hat=897\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=649\n","Skipping y_hat=552\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=592\n","Skipping y_hat=1031\n","Skipping y_hat=1005\n","Skipping y_hat=807\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=1017\n","Skipping y_hat=799\n","Skipping y_hat=405\n","Skipping y_hat=770\n","Skipping y_hat=1042\n","Skipping y_hat=742\n","Skipping y_hat=192\n","Skipping y_hat=292\n"," 95% 76/80 [00:02<00:00, 32.30it/s]Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=623\n","Skipping y_hat=181\n","Skipping y_hat=728\n","Skipping y_hat=806\n","Skipping y_hat=1000\n","Skipping y_hat=391\n","Skipping y_hat=958\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=617\n","Skipping y_hat=965\n","Skipping y_hat=452\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=891\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=829\n","Skipping y_hat=522\n","Skipping y_hat=88\n","Skipping y_hat=617\n","Skipping y_hat=385\n","Skipping y_hat=744\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=803\n","Skipping y_hat=258\n","Skipping y_hat=613\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=986\n","Skipping y_hat=752\n","Skipping y_hat=863\n","Skipping y_hat=595\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=435\n","Skipping y_hat=264\n","Skipping y_hat=581\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=772\n","Skipping y_hat=238\n","Skipping y_hat=612\n","Skipping y_hat=313\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=688\n","Skipping y_hat=535\n","Skipping y_hat=646\n","Skipping y_hat=658\n","Skipping y_hat=814\n","Skipping y_hat=436\n","Skipping y_hat=100\n","Skipping y_hat=751\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=113\n","Skipping y_hat=658\n","Skipping y_hat=752\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=651\n","Skipping y_hat=831\n","Skipping y_hat=459\n","Skipping y_hat=185\n","Skipping y_hat=149\n","Skipping y_hat=735\n","Skipping y_hat=354\n","Skipping y_hat=146\n","Skipping y_hat=985\n","Skipping y_hat=933\n","Skipping y_hat=486\n","Skipping y_hat=629\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=712\n","Skipping y_hat=739\n","Skipping y_hat=847\n","Skipping y_hat=756\n","Skipping y_hat=168\n","Skipping y_hat=892\n","Skipping y_hat=88\n","Skipping y_hat=34\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.60it/s]\n","accuracy of 10000 examples: 9323/10000 (93.23%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.34it/s]\n","accuracy of 10000 examples: 9988/10000 (99.88%)\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 34.26it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=881\n","Skipping y_hat=189\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=694\n","Skipping y_hat=709\n","Skipping y_hat=861\n","Skipping y_hat=690\n","Skipping y_hat=612\n","Skipping y_hat=882\n","Skipping y_hat=808\n","Skipping y_hat=618\n","Skipping y_hat=109\n","Skipping y_hat=554\n","Skipping y_hat=287\n","Skipping y_hat=809\n","Skipping y_hat=591\n","Skipping y_hat=897\n","Skipping y_hat=936\n","Skipping y_hat=401\n","Skipping y_hat=476\n","Skipping y_hat=999\n","Skipping y_hat=461\n","Skipping y_hat=294\n","Skipping y_hat=512\n","Skipping y_hat=742\n","Skipping y_hat=488\n","Skipping y_hat=500\n","Skipping y_hat=709\n","Skipping y_hat=742\n","Skipping y_hat=112\n","Skipping y_hat=808\n","Skipping y_hat=782\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=607\n"," 85% 68/80 [00:02<00:00, 32.14it/s]Skipping y_hat=981\n","Skipping y_hat=312\n","Skipping y_hat=1028\n","Skipping y_hat=900\n","Skipping y_hat=961\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=119\n","Skipping y_hat=971\n","Skipping y_hat=353\n","Skipping y_hat=569\n","Skipping y_hat=357\n","Skipping y_hat=759\n","Skipping y_hat=853\n","Skipping y_hat=406\n","Skipping y_hat=359\n","Skipping y_hat=159\n","Skipping y_hat=809\n","Skipping y_hat=900\n","Skipping y_hat=711\n","Skipping y_hat=273\n","Skipping y_hat=790\n","Skipping y_hat=513\n","Skipping y_hat=798\n","Skipping y_hat=505\n","Skipping y_hat=831\n","Skipping y_hat=901\n","Skipping y_hat=939\n","Skipping y_hat=1007\n","Skipping y_hat=809\n","Skipping y_hat=412\n","Skipping y_hat=794\n","Skipping y_hat=351\n","Skipping y_hat=191\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=506\n"," 90% 72/80 [00:02<00:00, 32.30it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=609\n","Skipping y_hat=1017\n","Skipping y_hat=809\n","Skipping y_hat=1038\n","Skipping y_hat=719\n","Skipping y_hat=805\n","Skipping y_hat=309\n","Skipping y_hat=439\n","Skipping y_hat=196\n","Skipping y_hat=290\n","Skipping y_hat=780\n","Skipping y_hat=109\n","Skipping y_hat=228\n","Skipping y_hat=980\n","Skipping y_hat=520\n","Skipping y_hat=897\n","Skipping y_hat=293\n","Skipping y_hat=784\n","Skipping y_hat=639\n","Skipping y_hat=552\n","Skipping y_hat=438\n","Skipping y_hat=171\n","Skipping y_hat=639\n","Skipping y_hat=592\n","Skipping y_hat=1031\n","Skipping y_hat=1005\n","Skipping y_hat=809\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=797\n","Skipping y_hat=405\n","Skipping y_hat=953\n","Skipping y_hat=801\n","Skipping y_hat=519\n","Skipping y_hat=192\n","Skipping y_hat=292\n","Skipping y_hat=999\n"," 95% 76/80 [00:02<00:00, 32.35it/s]Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=623\n","Skipping y_hat=121\n","Skipping y_hat=181\n","Skipping y_hat=727\n","Skipping y_hat=806\n","Skipping y_hat=1000\n","Skipping y_hat=230\n","Skipping y_hat=391\n","Skipping y_hat=950\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=607\n","Skipping y_hat=955\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=522\n","Skipping y_hat=617\n","Skipping y_hat=385\n","Skipping y_hat=747\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=613\n","Skipping y_hat=230\n","Skipping y_hat=986\n","Skipping y_hat=752\n","Skipping y_hat=843\n","Skipping y_hat=694\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=581\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=772\n","Skipping y_hat=238\n","Skipping y_hat=512\n","Skipping y_hat=332\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=861\n","Skipping y_hat=100\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=686\n","Skipping y_hat=658\n","Skipping y_hat=814\n","Skipping y_hat=436\n","Skipping y_hat=100\n","Skipping y_hat=751\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=752\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=830\n","Skipping y_hat=459\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=98\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=935\n","Skipping y_hat=942\n","Skipping y_hat=389\n","Skipping y_hat=629\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=712\n","Skipping y_hat=739\n","Skipping y_hat=847\n","Skipping y_hat=756\n","Skipping y_hat=812\n","Skipping y_hat=88\n","Skipping y_hat=34\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.30it/s]\n","accuracy of 10000 examples: 9327/10000 (93.27%)\n","/content/drive/MyDrive/addition/evaluation.py:318: DtypeWarning: Columns (34,35,39,45,49,50,53,58,61,63,69,70,71,77,79,81,82,83,84,85,86,88,89,90,94,99,100,102,103,104,106,107,108,109,110,111,112,113,115,116,119,120,121,122,123,127,129,130,132,133,134,135,136,137,138,140,142,143,144,145,147,149,150,152,154,155,156,157,162,163,182,190) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Test Results:\n","test: 93.27%\n","\n","iter 4700: train loss 0.6763, val loss 1.8448\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 34.07it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=882\n","Skipping y_hat=470\n","Skipping y_hat=851\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=701\n","Skipping y_hat=319\n","Skipping y_hat=482\n","Skipping y_hat=612\n","Skipping y_hat=882\n","Skipping y_hat=808\n","Skipping y_hat=601\n","Skipping y_hat=109\n","Skipping y_hat=554\n","Skipping y_hat=287\n","Skipping y_hat=119\n","Skipping y_hat=809\n","Skipping y_hat=591\n","Skipping y_hat=936\n","Skipping y_hat=411\n","Skipping y_hat=900\n","Skipping y_hat=461\n","Skipping y_hat=512\n","Skipping y_hat=742\n","Skipping y_hat=500\n","Skipping y_hat=509\n","Skipping y_hat=1028\n","Skipping y_hat=959\n","Skipping y_hat=708\n","Skipping y_hat=813\n","Skipping y_hat=111\n","Skipping y_hat=311\n","Skipping y_hat=629\n","Skipping y_hat=808\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=1013\n","Skipping y_hat=607\n"," 85% 68/80 [00:02<00:00, 33.25it/s]Skipping y_hat=1028\n","Skipping y_hat=312\n","Skipping y_hat=1028\n","Skipping y_hat=900\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=119\n","Skipping y_hat=353\n","Skipping y_hat=1019\n","Skipping y_hat=569\n","Skipping y_hat=340\n","Skipping y_hat=854\n","Skipping y_hat=406\n","Skipping y_hat=359\n","Skipping y_hat=780\n","Skipping y_hat=159\n","Skipping y_hat=800\n","Skipping y_hat=758\n","Skipping y_hat=900\n","Skipping y_hat=689\n","Skipping y_hat=273\n","Skipping y_hat=600\n","Skipping y_hat=513\n","Skipping y_hat=180\n","Skipping y_hat=819\n","Skipping y_hat=505\n","Skipping y_hat=831\n","Skipping y_hat=803\n","Skipping y_hat=1007\n","Skipping y_hat=809\n","Skipping y_hat=414\n","Skipping y_hat=794\n","Skipping y_hat=461\n","Skipping y_hat=351\n","Skipping y_hat=920\n","Skipping y_hat=191\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=505\n"," 90% 72/80 [00:02<00:00, 32.89it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=760\n","Skipping y_hat=1017\n","Skipping y_hat=1038\n","Skipping y_hat=719\n","Skipping y_hat=806\n","Skipping y_hat=480\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=799\n","Skipping y_hat=901\n","Skipping y_hat=339\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=552\n","Skipping y_hat=438\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=818\n","Skipping y_hat=592\n","Skipping y_hat=1031\n","Skipping y_hat=1005\n","Skipping y_hat=807\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=905\n","Skipping y_hat=798\n","Skipping y_hat=405\n","Skipping y_hat=953\n","Skipping y_hat=1042\n","Skipping y_hat=889\n","Skipping y_hat=192\n","Skipping y_hat=405\n","Skipping y_hat=290\n"," 95% 76/80 [00:02<00:00, 32.65it/s]Skipping y_hat=600\n","Skipping y_hat=190\n","Skipping y_hat=818\n","Skipping y_hat=727\n","Skipping y_hat=1000\n","Skipping y_hat=570\n","Skipping y_hat=399\n","Skipping y_hat=950\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=607\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=451\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=891\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=522\n","Skipping y_hat=617\n","Skipping y_hat=774\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=613\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=986\n","Skipping y_hat=752\n","Skipping y_hat=863\n","Skipping y_hat=594\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=435\n","Skipping y_hat=264\n","Skipping y_hat=571\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=238\n","Skipping y_hat=136\n","Skipping y_hat=612\n","Skipping y_hat=331\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=717\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=494\n","Skipping y_hat=861\n","Skipping y_hat=915\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=533\n","Skipping y_hat=646\n","Skipping y_hat=658\n","Skipping y_hat=894\n","Skipping y_hat=416\n","Skipping y_hat=100\n","Skipping y_hat=701\n","Skipping y_hat=453\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=113\n","Skipping y_hat=658\n","Skipping y_hat=762\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=631\n","Skipping y_hat=459\n","Skipping y_hat=185\n","Skipping y_hat=80\n","Skipping y_hat=394\n","Skipping y_hat=146\n","Skipping y_hat=985\n","Skipping y_hat=933\n","Skipping y_hat=386\n","Skipping y_hat=629\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=712\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=857\n","Skipping y_hat=756\n","Skipping y_hat=812\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=38\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.31it/s]\n","accuracy of 10000 examples: 9342/10000 (93.42%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.41it/s]\n","accuracy of 10000 examples: 9993/10000 (99.92999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 34.68it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=470\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=701\n","Skipping y_hat=873\n","Skipping y_hat=319\n","Skipping y_hat=482\n","Skipping y_hat=612\n","Skipping y_hat=808\n","Skipping y_hat=618\n","Skipping y_hat=109\n","Skipping y_hat=554\n","Skipping y_hat=287\n","Skipping y_hat=219\n","Skipping y_hat=809\n","Skipping y_hat=591\n","Skipping y_hat=936\n","Skipping y_hat=409\n","Skipping y_hat=401\n","Skipping y_hat=476\n","Skipping y_hat=900\n","Skipping y_hat=561\n","Skipping y_hat=512\n","Skipping y_hat=500\n","Skipping y_hat=1028\n","Skipping y_hat=959\n","Skipping y_hat=708\n","Skipping y_hat=815\n","Skipping y_hat=740\n","Skipping y_hat=111\n","Skipping y_hat=629\n","Skipping y_hat=210\n","Skipping y_hat=809\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=607\n"," 85% 68/80 [00:02<00:00, 33.93it/s]Skipping y_hat=999\n","Skipping y_hat=331\n","Skipping y_hat=1028\n","Skipping y_hat=312\n","Skipping y_hat=900\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=900\n","Skipping y_hat=219\n","Skipping y_hat=354\n","Skipping y_hat=1019\n","Skipping y_hat=569\n","Skipping y_hat=357\n","Skipping y_hat=891\n","Skipping y_hat=853\n","Skipping y_hat=408\n","Skipping y_hat=359\n","Skipping y_hat=159\n","Skipping y_hat=800\n","Skipping y_hat=758\n","Skipping y_hat=999\n","Skipping y_hat=900\n","Skipping y_hat=689\n","Skipping y_hat=273\n","Skipping y_hat=690\n","Skipping y_hat=513\n","Skipping y_hat=180\n","Skipping y_hat=819\n","Skipping y_hat=505\n","Skipping y_hat=831\n","Skipping y_hat=801\n","Skipping y_hat=939\n","Skipping y_hat=1007\n","Skipping y_hat=800\n","Skipping y_hat=462\n","Skipping y_hat=351\n","Skipping y_hat=691\n","Skipping y_hat=506\n","Skipping y_hat=619\n"," 90% 72/80 [00:02<00:00, 32.93it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=1017\n","Skipping y_hat=186\n","Skipping y_hat=719\n","Skipping y_hat=806\n","Skipping y_hat=615\n","Skipping y_hat=480\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=780\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=680\n","Skipping y_hat=229\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=630\n","Skipping y_hat=552\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=402\n","Skipping y_hat=592\n","Skipping y_hat=1005\n","Skipping y_hat=750\n","Skipping y_hat=807\n","Skipping y_hat=820\n","Skipping y_hat=196\n","Skipping y_hat=799\n","Skipping y_hat=405\n","Skipping y_hat=953\n","Skipping y_hat=678\n","Skipping y_hat=430\n","Skipping y_hat=1042\n","Skipping y_hat=291\n","Skipping y_hat=290\n","Skipping y_hat=347\n"," 95% 76/80 [00:02<00:00, 32.39it/s]Skipping y_hat=190\n","Skipping y_hat=818\n","Skipping y_hat=121\n","Skipping y_hat=181\n","Skipping y_hat=728\n","Skipping y_hat=1000\n","Skipping y_hat=230\n","Skipping y_hat=187\n","Skipping y_hat=179\n","Skipping y_hat=399\n","Skipping y_hat=950\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=607\n","Skipping y_hat=955\n","Skipping y_hat=458\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=486\n","Skipping y_hat=438\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=522\n","Skipping y_hat=617\n","Skipping y_hat=385\n","Skipping y_hat=744\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=613\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=981\n","Skipping y_hat=752\n","Skipping y_hat=863\n","Skipping y_hat=594\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=581\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=238\n","Skipping y_hat=682\n","Skipping y_hat=334\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=494\n","Skipping y_hat=861\n","Skipping y_hat=915\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=1002\n","Skipping y_hat=618\n","Skipping y_hat=533\n","Skipping y_hat=646\n","Skipping y_hat=658\n","Skipping y_hat=814\n","Skipping y_hat=416\n","Skipping y_hat=100\n","Skipping y_hat=711\n","Skipping y_hat=455\n","Skipping y_hat=337\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=113\n","Skipping y_hat=658\n","Skipping y_hat=762\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=631\n","Skipping y_hat=830\n","Skipping y_hat=459\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=935\n","Skipping y_hat=902\n","Skipping y_hat=389\n","Skipping y_hat=619\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=829\n","Skipping y_hat=712\n","Skipping y_hat=499\n","Skipping y_hat=739\n","Skipping y_hat=857\n","Skipping y_hat=716\n","Skipping y_hat=168\n","Skipping y_hat=802\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=35\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.34it/s]\n","accuracy of 10000 examples: 9329/10000 (93.28999999999999%)\n","/content/drive/MyDrive/addition/evaluation.py:318: DtypeWarning: Columns (34,35,39,45,49,50,53,58,61,63,69,70,71,77,79,81,82,83,84,85,86,88,89,90,94,99,100,102,103,104,106,107,108,109,110,111,112,113,115,116,119,120,121,122,123,127,129,130,132,133,134,135,136,137,138,140,142,143,144,145,147,149,150,152,154,155,156,157,162,163,182,190) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Test Results:\n","test: 93.29%\n","\n","iter 4800: train loss 0.6715, val loss 1.8130\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 34.03it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=470\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=701\n","Skipping y_hat=614\n","Skipping y_hat=808\n","Skipping y_hat=618\n","Skipping y_hat=554\n","Skipping y_hat=297\n","Skipping y_hat=809\n","Skipping y_hat=591\n","Skipping y_hat=969\n","Skipping y_hat=409\n","Skipping y_hat=401\n","Skipping y_hat=900\n","Skipping y_hat=579\n","Skipping y_hat=461\n","Skipping y_hat=1045\n","Skipping y_hat=512\n","Skipping y_hat=589\n","Skipping y_hat=500\n","Skipping y_hat=509\n","Skipping y_hat=959\n","Skipping y_hat=708\n","Skipping y_hat=919\n","Skipping y_hat=811\n","Skipping y_hat=742\n","Skipping y_hat=520\n","Skipping y_hat=111\n","Skipping y_hat=629\n","Skipping y_hat=808\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=209\n","Skipping y_hat=608\n"," 85% 68/80 [00:02<00:00, 32.92it/s]Skipping y_hat=991\n","Skipping y_hat=210\n","Skipping y_hat=331\n","Skipping y_hat=1028\n","Skipping y_hat=1028\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=354\n","Skipping y_hat=1019\n","Skipping y_hat=759\n","Skipping y_hat=853\n","Skipping y_hat=408\n","Skipping y_hat=359\n","Skipping y_hat=780\n","Skipping y_hat=212\n","Skipping y_hat=159\n","Skipping y_hat=809\n","Skipping y_hat=758\n","Skipping y_hat=900\n","Skipping y_hat=688\n","Skipping y_hat=273\n","Skipping y_hat=690\n","Skipping y_hat=578\n","Skipping y_hat=513\n","Skipping y_hat=798\n","Skipping y_hat=819\n","Skipping y_hat=505\n","Skipping y_hat=831\n","Skipping y_hat=809\n","Skipping y_hat=1007\n","Skipping y_hat=809\n","Skipping y_hat=415\n","Skipping y_hat=351\n","Skipping y_hat=1040\n","Skipping y_hat=410\n","Skipping y_hat=691\n","Skipping y_hat=506\n"," 90% 72/80 [00:02<00:00, 32.62it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=1017\n","Skipping y_hat=809\n","Skipping y_hat=719\n","Skipping y_hat=805\n","Skipping y_hat=457\n","Skipping y_hat=405\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=533\n","Skipping y_hat=901\n","Skipping y_hat=680\n","Skipping y_hat=340\n","Skipping y_hat=228\n","Skipping y_hat=780\n","Skipping y_hat=520\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=649\n","Skipping y_hat=552\n","Skipping y_hat=171\n","Skipping y_hat=592\n","Skipping y_hat=1031\n","Skipping y_hat=1005\n","Skipping y_hat=807\n","Skipping y_hat=430\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=1141\n","Skipping y_hat=799\n","Skipping y_hat=405\n","Skipping y_hat=141\n","Skipping y_hat=678\n","Skipping y_hat=1042\n","Skipping y_hat=519\n","Skipping y_hat=192\n","Skipping y_hat=405\n","Skipping y_hat=290\n","Skipping y_hat=347\n"," 95% 76/80 [00:02<00:00, 32.31it/s]Skipping y_hat=600\n","Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=180\n","Skipping y_hat=727\n","Skipping y_hat=1000\n","Skipping y_hat=230\n","Skipping y_hat=399\n","Skipping y_hat=950\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=607\n","Skipping y_hat=458\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=486\n","Skipping y_hat=439\n","Skipping y_hat=109\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=829\n","Skipping y_hat=522\n","Skipping y_hat=617\n","Skipping y_hat=385\n","Skipping y_hat=744\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=613\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=863\n","Skipping y_hat=595\n","Skipping y_hat=415\n","Skipping y_hat=482\n","Skipping y_hat=264\n","Skipping y_hat=581\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=579\n","Skipping y_hat=106\n","Skipping y_hat=269\n","Skipping y_hat=582\n","Skipping y_hat=334\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=309\n","Skipping y_hat=251\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=915\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=686\n","Skipping y_hat=658\n","Skipping y_hat=814\n","Skipping y_hat=436\n","Skipping y_hat=100\n","Skipping y_hat=751\n","Skipping y_hat=455\n","Skipping y_hat=337\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=762\n","Skipping y_hat=615\n","Skipping y_hat=780\n","Skipping y_hat=638\n","Skipping y_hat=83\n","Skipping y_hat=459\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=735\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=985\n","Skipping y_hat=932\n","Skipping y_hat=466\n","Skipping y_hat=619\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=59\n","Skipping y_hat=712\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=847\n","Skipping y_hat=756\n","Skipping y_hat=892\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=34\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.24it/s]\n","accuracy of 10000 examples: 9331/10000 (93.31%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 34.79it/s]\n","accuracy of 10000 examples: 9993/10000 (99.92999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n"," 79% 63/80 [00:01<00:00, 32.94it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=200\n","Skipping y_hat=405\n","Skipping y_hat=700\n","Skipping y_hat=873\n","Skipping y_hat=612\n","Skipping y_hat=808\n","Skipping y_hat=618\n","Skipping y_hat=554\n","Skipping y_hat=287\n","Skipping y_hat=200\n","Skipping y_hat=809\n","Skipping y_hat=501\n","Skipping y_hat=936\n","Skipping y_hat=409\n","Skipping y_hat=401\n","Skipping y_hat=999\n","Skipping y_hat=461\n","Skipping y_hat=1045\n","Skipping y_hat=512\n","Skipping y_hat=500\n","Skipping y_hat=509\n","Skipping y_hat=257\n","Skipping y_hat=1028\n","Skipping y_hat=959\n","Skipping y_hat=708\n","Skipping y_hat=605\n"," 84% 67/80 [00:02<00:00, 32.13it/s]Skipping y_hat=811\n","Skipping y_hat=742\n","Skipping y_hat=520\n","Skipping y_hat=111\n","Skipping y_hat=629\n","Skipping y_hat=808\n","Skipping y_hat=319\n","Skipping y_hat=607\n","Skipping y_hat=981\n","Skipping y_hat=403\n","Skipping y_hat=1028\n","Skipping y_hat=1027\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=354\n","Skipping y_hat=551\n","Skipping y_hat=391\n","Skipping y_hat=759\n","Skipping y_hat=853\n","Skipping y_hat=406\n","Skipping y_hat=780\n","Skipping y_hat=159\n","Skipping y_hat=800\n","Skipping y_hat=758\n","Skipping y_hat=900\n","Skipping y_hat=689\n","Skipping y_hat=273\n","Skipping y_hat=600\n","Skipping y_hat=578\n","Skipping y_hat=798\n","Skipping y_hat=819\n","Skipping y_hat=505\n","Skipping y_hat=831\n"," 89% 71/80 [00:02<00:00, 30.81it/s]Skipping y_hat=900\n","Skipping y_hat=936\n","Skipping y_hat=1007\n","Skipping y_hat=809\n","Skipping y_hat=412\n","Skipping y_hat=461\n","Skipping y_hat=351\n","Skipping y_hat=181\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=507\n","Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=760\n","Skipping y_hat=1017\n","Skipping y_hat=809\n","Skipping y_hat=719\n","Skipping y_hat=805\n","Skipping y_hat=405\n","Skipping y_hat=199\n","Skipping y_hat=799\n","Skipping y_hat=901\n","Skipping y_hat=349\n","Skipping y_hat=228\n","Skipping y_hat=520\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=640\n","Skipping y_hat=552\n","Skipping y_hat=438\n","Skipping y_hat=171\n","Skipping y_hat=797\n","Skipping y_hat=592\n","Skipping y_hat=1005\n","Skipping y_hat=809\n"," 94% 75/80 [00:02<00:00, 30.08it/s]Skipping y_hat=197\n","Skipping y_hat=799\n","Skipping y_hat=1042\n","Skipping y_hat=801\n","Skipping y_hat=519\n","Skipping y_hat=292\n","Skipping y_hat=405\n","Skipping y_hat=290\n","Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=181\n","Skipping y_hat=727\n","Skipping y_hat=1000\n","Skipping y_hat=230\n","Skipping y_hat=391\n","Skipping y_hat=958\n","Skipping y_hat=873\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=607\n","Skipping y_hat=955\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=109\n","Skipping y_hat=861\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=522\n","Skipping y_hat=617\n","Skipping y_hat=774\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=613\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=863\n","Skipping y_hat=594\n","Skipping y_hat=415\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=571\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=579\n","Skipping y_hat=106\n","Skipping y_hat=269\n","Skipping y_hat=582\n","Skipping y_hat=333\n","Skipping y_hat=938\n","Skipping y_hat=110\n","Skipping y_hat=485\n","Skipping y_hat=309\n","Skipping y_hat=251\n","Skipping y_hat=757\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=646\n","Skipping y_hat=658\n","Skipping y_hat=894\n","Skipping y_hat=436\n","Skipping y_hat=100\n","Skipping y_hat=711\n","Skipping y_hat=455\n","Skipping y_hat=337\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=762\n","Skipping y_hat=615\n","Skipping y_hat=621\n","Skipping y_hat=830\n","Skipping y_hat=459\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=149\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=985\n","Skipping y_hat=933\n","Skipping y_hat=486\n","Skipping y_hat=619\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=712\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=847\n","Skipping y_hat=756\n","Skipping y_hat=892\n"," 99% 79/80 [00:02<00:00, 29.77it/s]Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=36\n","Skipping y_hat=104\n","Skipping y_hat=63\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 31.62it/s]\n","accuracy of 10000 examples: 9335/10000 (93.35%)\n","/content/drive/MyDrive/addition/evaluation.py:318: DtypeWarning: Columns (34,35,39,45,49,50,53,58,61,63,69,70,71,77,79,81,82,83,84,85,86,88,89,90,94,99,100,102,103,104,106,107,108,109,110,111,112,113,115,116,119,120,121,122,123,127,129,130,132,133,134,135,136,137,138,140,142,143,144,145,147,149,150,152,154,155,156,157,162,163,182,190) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Test Results:\n","test: 93.35%\n","\n","iter 4900: train loss 0.6697, val loss 1.8594\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 33.31it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=882\n","Skipping y_hat=851\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=701\n","Skipping y_hat=612\n","Skipping y_hat=881\n","Skipping y_hat=808\n","Skipping y_hat=554\n","Skipping y_hat=287\n","Skipping y_hat=591\n","Skipping y_hat=897\n","Skipping y_hat=409\n","Skipping y_hat=403\n","Skipping y_hat=476\n","Skipping y_hat=900\n","Skipping y_hat=579\n","Skipping y_hat=461\n","Skipping y_hat=294\n","Skipping y_hat=512\n","Skipping y_hat=589\n","Skipping y_hat=489\n","Skipping y_hat=500\n","Skipping y_hat=509\n","Skipping y_hat=1028\n","Skipping y_hat=959\n","Skipping y_hat=708\n","Skipping y_hat=811\n","Skipping y_hat=311\n","Skipping y_hat=629\n","Skipping y_hat=808\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=209\n","Skipping y_hat=607\n"," 85% 68/80 [00:02<00:00, 32.57it/s]Skipping y_hat=999\n","Skipping y_hat=331\n","Skipping y_hat=1028\n","Skipping y_hat=312\n","Skipping y_hat=1028\n","Skipping y_hat=900\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=541\n","Skipping y_hat=219\n","Skipping y_hat=353\n","Skipping y_hat=1019\n","Skipping y_hat=569\n","Skipping y_hat=340\n","Skipping y_hat=759\n","Skipping y_hat=853\n","Skipping y_hat=406\n","Skipping y_hat=359\n","Skipping y_hat=638\n","Skipping y_hat=159\n","Skipping y_hat=800\n","Skipping y_hat=758\n","Skipping y_hat=500\n","Skipping y_hat=900\n","Skipping y_hat=689\n","Skipping y_hat=273\n","Skipping y_hat=691\n","Skipping y_hat=578\n","Skipping y_hat=513\n","Skipping y_hat=798\n","Skipping y_hat=500\n","Skipping y_hat=505\n","Skipping y_hat=831\n","Skipping y_hat=900\n","Skipping y_hat=1007\n","Skipping y_hat=415\n","Skipping y_hat=544\n","Skipping y_hat=351\n","Skipping y_hat=191\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=509\n"," 90% 72/80 [00:02<00:00, 31.82it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=760\n","Skipping y_hat=1017\n","Skipping y_hat=719\n","Skipping y_hat=405\n","Skipping y_hat=480\n","Skipping y_hat=196\n","Skipping y_hat=342\n","Skipping y_hat=799\n","Skipping y_hat=901\n","Skipping y_hat=228\n","Skipping y_hat=520\n","Skipping y_hat=897\n","Skipping y_hat=293\n","Skipping y_hat=649\n","Skipping y_hat=171\n","Skipping y_hat=592\n","Skipping y_hat=1005\n","Skipping y_hat=807\n","Skipping y_hat=197\n","Skipping y_hat=798\n","Skipping y_hat=678\n","Skipping y_hat=1042\n","Skipping y_hat=292\n","Skipping y_hat=295\n","Skipping y_hat=999\n"," 95% 76/80 [00:02<00:00, 31.94it/s]Skipping y_hat=190\n","Skipping y_hat=818\n","Skipping y_hat=181\n","Skipping y_hat=396\n","Skipping y_hat=1000\n","Skipping y_hat=230\n","Skipping y_hat=574\n","Skipping y_hat=399\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=687\n","Skipping y_hat=955\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=451\n","Skipping y_hat=486\n","Skipping y_hat=433\n","Skipping y_hat=891\n","Skipping y_hat=838\n","Skipping y_hat=512\n","Skipping y_hat=617\n","Skipping y_hat=385\n","Skipping y_hat=774\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=807\n","Skipping y_hat=258\n","Skipping y_hat=613\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=986\n","Skipping y_hat=752\n","Skipping y_hat=893\n","Skipping y_hat=594\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=437\n","Skipping y_hat=264\n","Skipping y_hat=571\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=238\n","Skipping y_hat=269\n","Skipping y_hat=582\n","Skipping y_hat=331\n","Skipping y_hat=938\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=358\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=686\n","Skipping y_hat=658\n","Skipping y_hat=894\n","Skipping y_hat=416\n","Skipping y_hat=100\n","Skipping y_hat=701\n","Skipping y_hat=454\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=762\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=660\n","Skipping y_hat=831\n","Skipping y_hat=459\n","Skipping y_hat=189\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=735\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=935\n","Skipping y_hat=933\n","Skipping y_hat=386\n","Skipping y_hat=639\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=716\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=703\n","Skipping y_hat=847\n","Skipping y_hat=756\n","Skipping y_hat=168\n","Skipping y_hat=812\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=85\n","Skipping y_hat=33\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.05it/s]\n","accuracy of 10000 examples: 9339/10000 (93.39%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.62it/s]\n","accuracy of 10000 examples: 9992/10000 (99.92%)\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 34.17it/s]Skipping y_hat=748\n","Skipping y_hat=882\n","Skipping y_hat=470\n","Skipping y_hat=498\n","Skipping y_hat=311\n","Skipping y_hat=405\n","Skipping y_hat=701\n","Skipping y_hat=870\n","Skipping y_hat=769\n","Skipping y_hat=612\n","Skipping y_hat=808\n","Skipping y_hat=618\n","Skipping y_hat=297\n","Skipping y_hat=591\n","Skipping y_hat=948\n","Skipping y_hat=409\n","Skipping y_hat=403\n","Skipping y_hat=476\n","Skipping y_hat=900\n","Skipping y_hat=561\n","Skipping y_hat=461\n","Skipping y_hat=294\n","Skipping y_hat=589\n","Skipping y_hat=489\n","Skipping y_hat=500\n","Skipping y_hat=509\n","Skipping y_hat=1028\n","Skipping y_hat=959\n","Skipping y_hat=708\n","Skipping y_hat=919\n","Skipping y_hat=111\n","Skipping y_hat=421\n","Skipping y_hat=629\n","Skipping y_hat=808\n","Skipping y_hat=782\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=608\n"," 85% 68/80 [00:02<00:00, 33.25it/s]Skipping y_hat=980\n","Skipping y_hat=331\n","Skipping y_hat=312\n","Skipping y_hat=1028\n","Skipping y_hat=900\n","Skipping y_hat=963\n","Skipping y_hat=358\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=353\n","Skipping y_hat=1019\n","Skipping y_hat=659\n","Skipping y_hat=391\n","Skipping y_hat=759\n","Skipping y_hat=853\n","Skipping y_hat=408\n","Skipping y_hat=359\n","Skipping y_hat=780\n","Skipping y_hat=212\n","Skipping y_hat=159\n","Skipping y_hat=800\n","Skipping y_hat=515\n","Skipping y_hat=756\n","Skipping y_hat=900\n","Skipping y_hat=689\n","Skipping y_hat=273\n","Skipping y_hat=691\n","Skipping y_hat=513\n","Skipping y_hat=798\n","Skipping y_hat=500\n","Skipping y_hat=505\n","Skipping y_hat=831\n","Skipping y_hat=901\n","Skipping y_hat=1007\n","Skipping y_hat=909\n","Skipping y_hat=415\n","Skipping y_hat=794\n","Skipping y_hat=544\n","Skipping y_hat=462\n","Skipping y_hat=351\n","Skipping y_hat=981\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=506\n"," 90% 72/80 [00:02<00:00, 32.32it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=760\n","Skipping y_hat=1017\n","Skipping y_hat=809\n","Skipping y_hat=719\n","Skipping y_hat=405\n","Skipping y_hat=481\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=901\n","Skipping y_hat=228\n","Skipping y_hat=880\n","Skipping y_hat=980\n","Skipping y_hat=520\n","Skipping y_hat=897\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=639\n","Skipping y_hat=552\n","Skipping y_hat=171\n","Skipping y_hat=638\n","Skipping y_hat=592\n","Skipping y_hat=1005\n","Skipping y_hat=807\n","Skipping y_hat=340\n","Skipping y_hat=197\n","Skipping y_hat=797\n","Skipping y_hat=660\n","Skipping y_hat=1042\n","Skipping y_hat=517\n","Skipping y_hat=292\n","Skipping y_hat=405\n","Skipping y_hat=296\n"," 95% 76/80 [00:02<00:00, 32.27it/s]Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=181\n","Skipping y_hat=1000\n","Skipping y_hat=247\n","Skipping y_hat=574\n","Skipping y_hat=399\n","Skipping y_hat=948\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=687\n","Skipping y_hat=955\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=451\n","Skipping y_hat=486\n","Skipping y_hat=439\n","Skipping y_hat=891\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=829\n","Skipping y_hat=522\n","Skipping y_hat=617\n","Skipping y_hat=385\n","Skipping y_hat=744\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=801\n","Skipping y_hat=258\n","Skipping y_hat=613\n","Skipping y_hat=846\n","Skipping y_hat=230\n","Skipping y_hat=986\n","Skipping y_hat=752\n","Skipping y_hat=893\n","Skipping y_hat=594\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=435\n","Skipping y_hat=264\n","Skipping y_hat=581\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=529\n","Skipping y_hat=238\n","Skipping y_hat=269\n","Skipping y_hat=682\n","Skipping y_hat=333\n","Skipping y_hat=908\n","Skipping y_hat=485\n","Skipping y_hat=309\n","Skipping y_hat=358\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=646\n","Skipping y_hat=658\n","Skipping y_hat=894\n","Skipping y_hat=416\n","Skipping y_hat=100\n","Skipping y_hat=751\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=762\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=831\n","Skipping y_hat=459\n","Skipping y_hat=185\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=985\n","Skipping y_hat=932\n","Skipping y_hat=386\n","Skipping y_hat=629\n","Skipping y_hat=416\n","Skipping y_hat=678\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=839\n","Skipping y_hat=716\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=847\n","Skipping y_hat=756\n","Skipping y_hat=168\n","Skipping y_hat=812\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=85\n","Skipping y_hat=38\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 33.31it/s]\n","accuracy of 10000 examples: 9333/10000 (93.33%)\n","/content/drive/MyDrive/addition/evaluation.py:318: DtypeWarning: Columns (34,35,39,45,49,50,53,58,61,63,69,70,71,77,79,81,82,83,84,85,86,88,89,90,94,99,100,102,103,104,106,107,108,109,110,111,112,113,115,116,119,120,121,122,123,127,129,130,132,133,134,135,136,137,138,140,142,143,144,145,147,149,150,152,154,155,156,157,162,163,182,190) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Test Results:\n","test: 93.33%\n","\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 33.93it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=470\n","Skipping y_hat=933\n","Skipping y_hat=311\n","Skipping y_hat=405\n","Skipping y_hat=701\n","Skipping y_hat=863\n","Skipping y_hat=612\n","Skipping y_hat=882\n","Skipping y_hat=808\n","Skipping y_hat=554\n","Skipping y_hat=297\n","Skipping y_hat=119\n","Skipping y_hat=809\n","Skipping y_hat=501\n","Skipping y_hat=409\n","Skipping y_hat=401\n","Skipping y_hat=476\n","Skipping y_hat=900\n","Skipping y_hat=561\n","Skipping y_hat=461\n","Skipping y_hat=319\n","Skipping y_hat=512\n","Skipping y_hat=759\n","Skipping y_hat=489\n","Skipping y_hat=500\n","Skipping y_hat=1028\n","Skipping y_hat=991\n","Skipping y_hat=959\n","Skipping y_hat=708\n","Skipping y_hat=811\n","Skipping y_hat=740\n","Skipping y_hat=112\n","Skipping y_hat=609\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=209\n","Skipping y_hat=608\n"," 85% 68/80 [00:02<00:00, 33.61it/s]Skipping y_hat=994\n","Skipping y_hat=1028\n","Skipping y_hat=709\n","Skipping y_hat=870\n","Skipping y_hat=1028\n","Skipping y_hat=963\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=559\n","Skipping y_hat=119\n","Skipping y_hat=569\n","Skipping y_hat=359\n","Skipping y_hat=891\n","Skipping y_hat=853\n","Skipping y_hat=406\n","Skipping y_hat=359\n","Skipping y_hat=911\n","Skipping y_hat=159\n","Skipping y_hat=801\n","Skipping y_hat=758\n","Skipping y_hat=900\n","Skipping y_hat=273\n","Skipping y_hat=697\n","Skipping y_hat=513\n","Skipping y_hat=798\n","Skipping y_hat=819\n","Skipping y_hat=505\n","Skipping y_hat=831\n","Skipping y_hat=809\n","Skipping y_hat=936\n","Skipping y_hat=1007\n","Skipping y_hat=800\n","Skipping y_hat=412\n","Skipping y_hat=794\n","Skipping y_hat=462\n","Skipping y_hat=351\n","Skipping y_hat=981\n","Skipping y_hat=309\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=506\n"," 90% 72/80 [00:02<00:00, 32.56it/s]Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=1017\n","Skipping y_hat=809\n","Skipping y_hat=719\n","Skipping y_hat=457\n","Skipping y_hat=615\n","Skipping y_hat=405\n","Skipping y_hat=480\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=901\n","Skipping y_hat=402\n","Skipping y_hat=339\n","Skipping y_hat=228\n","Skipping y_hat=520\n","Skipping y_hat=897\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=796\n","Skipping y_hat=430\n","Skipping y_hat=640\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=592\n","Skipping y_hat=196\n","Skipping y_hat=807\n","Skipping y_hat=430\n","Skipping y_hat=340\n","Skipping y_hat=820\n","Skipping y_hat=196\n","Skipping y_hat=797\n","Skipping y_hat=770\n","Skipping y_hat=1042\n","Skipping y_hat=801\n","Skipping y_hat=292\n","Skipping y_hat=405\n","Skipping y_hat=296\n","Skipping y_hat=999\n"," 95% 76/80 [00:02<00:00, 31.21it/s]Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=122\n","Skipping y_hat=181\n","Skipping y_hat=1000\n","Skipping y_hat=230\n","Skipping y_hat=179\n","Skipping y_hat=399\n","Skipping y_hat=950\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=607\n","Skipping y_hat=965\n","Skipping y_hat=457\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=451\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=891\n","Skipping y_hat=38\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=829\n","Skipping y_hat=512\n","Skipping y_hat=617\n","Skipping y_hat=395\n","Skipping y_hat=755\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=460\n","Skipping y_hat=258\n","Skipping y_hat=613\n","Skipping y_hat=806\n","Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=893\n","Skipping y_hat=594\n","Skipping y_hat=416\n","Skipping y_hat=482\n","Skipping y_hat=435\n","Skipping y_hat=164\n","Skipping y_hat=581\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=549\n","Skipping y_hat=238\n","Skipping y_hat=306\n","Skipping y_hat=269\n","Skipping y_hat=582\n","Skipping y_hat=334\n","Skipping y_hat=938\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=456\n","Skipping y_hat=880\n","Skipping y_hat=474\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=103\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=686\n","Skipping y_hat=658\n","Skipping y_hat=894\n","Skipping y_hat=436\n","Skipping y_hat=100\n","Skipping y_hat=751\n","Skipping y_hat=454\n","Skipping y_hat=337\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=762\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=661\n","Skipping y_hat=830\n","Skipping y_hat=459\n","Skipping y_hat=857\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=935\n","Skipping y_hat=932\n","Skipping y_hat=386\n","Skipping y_hat=619\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=829\n","Skipping y_hat=716\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=703\n","Skipping y_hat=857\n","Skipping y_hat=756\n","Skipping y_hat=168\n","Skipping y_hat=892\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=38\n","Skipping y_hat=104\n","Skipping y_hat=36\n","100% 80/80 [00:02<00:00, 32.69it/s]\n","accuracy of 10000 examples: 9310/10000 (93.10000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 80/80 [00:02<00:00, 38.01it/s]\n","accuracy of 10000 examples: 9991/10000 (99.91%)\n","Using precomputed batches\n","Max number of tokens 5.\n"," 80% 64/80 [00:01<00:00, 33.67it/s]Skipping y_hat=171\n","Skipping y_hat=748\n","Skipping y_hat=470\n","Skipping y_hat=311\n","Skipping y_hat=201\n","Skipping y_hat=405\n","Skipping y_hat=701\n","Skipping y_hat=863\n","Skipping y_hat=614\n","Skipping y_hat=882\n","Skipping y_hat=808\n","Skipping y_hat=618\n","Skipping y_hat=109\n","Skipping y_hat=554\n","Skipping y_hat=287\n","Skipping y_hat=1001\n","Skipping y_hat=809\n","Skipping y_hat=501\n","Skipping y_hat=409\n","Skipping y_hat=403\n","Skipping y_hat=476\n","Skipping y_hat=900\n","Skipping y_hat=461\n","Skipping y_hat=294\n","Skipping y_hat=512\n","Skipping y_hat=759\n","Skipping y_hat=489\n","Skipping y_hat=500\n","Skipping y_hat=901\n","Skipping y_hat=509\n","Skipping y_hat=1028\n","Skipping y_hat=716\n","Skipping y_hat=915\n","Skipping y_hat=742\n","Skipping y_hat=539\n","Skipping y_hat=111\n","Skipping y_hat=609\n","Skipping y_hat=629\n","Skipping y_hat=782\n","Skipping y_hat=171\n","Skipping y_hat=319\n","Skipping y_hat=209\n","Skipping y_hat=607\n"," 85% 68/80 [00:02<00:00, 31.83it/s]Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=1028\n","Skipping y_hat=870\n","Skipping y_hat=1028\n","Skipping y_hat=963\n","Skipping y_hat=861\n","Skipping y_hat=901\n","Skipping y_hat=119\n","Skipping y_hat=515\n","Skipping y_hat=353\n","Skipping y_hat=891\n","Skipping y_hat=853\n","Skipping y_hat=406\n","Skipping y_hat=359\n","Skipping y_hat=780\n","Skipping y_hat=701\n","Skipping y_hat=159\n","Skipping y_hat=800\n","Skipping y_hat=758\n","Skipping y_hat=900\n","Skipping y_hat=273\n","Skipping y_hat=791\n","Skipping y_hat=513\n","Skipping y_hat=180\n","Skipping y_hat=505\n","Skipping y_hat=831\n","Skipping y_hat=900\n","Skipping y_hat=936\n","Skipping y_hat=1007\n","Skipping y_hat=794\n","Skipping y_hat=544\n","Skipping y_hat=461\n","Skipping y_hat=351\n","Skipping y_hat=981\n","Skipping y_hat=1040\n","Skipping y_hat=691\n","Skipping y_hat=506\n"," 90% 72/80 [00:02<00:00, 30.73it/s]Skipping y_hat=399\n","Skipping y_hat=1072\n","Skipping y_hat=840\n","Skipping y_hat=609\n","Skipping y_hat=760\n","Skipping y_hat=719\n","Skipping y_hat=481\n","Skipping y_hat=309\n","Skipping y_hat=196\n","Skipping y_hat=616\n","Skipping y_hat=901\n","Skipping y_hat=109\n","Skipping y_hat=681\n","Skipping y_hat=995\n","Skipping y_hat=402\n","Skipping y_hat=339\n","Skipping y_hat=228\n","Skipping y_hat=669\n","Skipping y_hat=520\n","Skipping y_hat=293\n","Skipping y_hat=780\n","Skipping y_hat=781\n","Skipping y_hat=640\n","Skipping y_hat=552\n","Skipping y_hat=969\n","Skipping y_hat=171\n","Skipping y_hat=407\n","Skipping y_hat=592\n","Skipping y_hat=1031\n","Skipping y_hat=1005\n","Skipping y_hat=750\n","Skipping y_hat=809\n","Skipping y_hat=430\n","Skipping y_hat=820\n","Skipping y_hat=197\n","Skipping y_hat=797\n","Skipping y_hat=963\n","Skipping y_hat=1042\n","Skipping y_hat=292\n","Skipping y_hat=405\n","Skipping y_hat=290\n","Skipping y_hat=999\n"," 95% 76/80 [00:02<00:00, 30.75it/s]Skipping y_hat=190\n","Skipping y_hat=492\n","Skipping y_hat=818\n","Skipping y_hat=558\n","Skipping y_hat=1000\n","Skipping y_hat=179\n","Skipping y_hat=570\n","Skipping y_hat=399\n","Skipping y_hat=958\n","Skipping y_hat=893\n","Skipping y_hat=454\n","Skipping y_hat=879\n","Skipping y_hat=607\n","Skipping y_hat=965\n","Skipping y_hat=458\n","Skipping y_hat=1008\n","Skipping y_hat=646\n","Skipping y_hat=486\n","Skipping y_hat=436\n","Skipping y_hat=109\n","Skipping y_hat=891\n","Skipping y_hat=846\n","Skipping y_hat=838\n","Skipping y_hat=829\n","Skipping y_hat=522\n","Skipping y_hat=617\n","Skipping y_hat=385\n","Skipping y_hat=754\n","Skipping y_hat=381\n","Skipping y_hat=486\n","Skipping y_hat=258\n","Skipping y_hat=673\n","Skipping y_hat=886\n","Skipping y_hat=230\n","Skipping y_hat=980\n","Skipping y_hat=752\n","Skipping y_hat=327\n","Skipping y_hat=863\n","Skipping y_hat=594\n","Skipping y_hat=415\n","Skipping y_hat=482\n","Skipping y_hat=430\n","Skipping y_hat=264\n","Skipping y_hat=571\n","Skipping y_hat=848\n","Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=772\n","Skipping y_hat=306\n","Skipping y_hat=582\n","Skipping y_hat=333\n","Skipping y_hat=938\n","Skipping y_hat=485\n","Skipping y_hat=329\n","Skipping y_hat=251\n","Skipping y_hat=757\n","Skipping y_hat=456\n","Skipping y_hat=840\n","Skipping y_hat=861\n","Skipping y_hat=1002\n","Skipping y_hat=601\n","Skipping y_hat=107\n","Skipping y_hat=110\n","Skipping y_hat=618\n","Skipping y_hat=535\n","Skipping y_hat=646\n","Skipping y_hat=658\n","Skipping y_hat=894\n","Skipping y_hat=416\n","Skipping y_hat=100\n","Skipping y_hat=751\n","Skipping y_hat=455\n","Skipping y_hat=331\n","Skipping y_hat=801\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=658\n","Skipping y_hat=752\n","Skipping y_hat=615\n","Skipping y_hat=700\n","Skipping y_hat=661\n","Skipping y_hat=831\n","Skipping y_hat=459\n","Skipping y_hat=165\n","Skipping y_hat=185\n","Skipping y_hat=735\n","Skipping y_hat=314\n","Skipping y_hat=146\n","Skipping y_hat=985\n","Skipping y_hat=933\n","Skipping y_hat=386\n","Skipping y_hat=629\n","Skipping y_hat=416\n","Skipping y_hat=688\n","Skipping y_hat=129\n","Skipping y_hat=358\n","Skipping y_hat=829\n","Skipping y_hat=40\n","Skipping y_hat=716\n","Skipping y_hat=498\n","Skipping y_hat=739\n","Skipping y_hat=703\n","Skipping y_hat=857\n","Skipping y_hat=756\n","Skipping y_hat=892\n","Skipping y_hat=88\n","Skipping y_hat=63\n","Skipping y_hat=104\n","Skipping y_hat=26\n","100% 80/80 [00:02<00:00, 32.83it/s]\n","accuracy of 10000 examples: 9324/10000 (93.24%)\n","/content/drive/MyDrive/addition/evaluation.py:318: DtypeWarning: Columns (34,35,39,45,49,50,53,58,61,63,69,70,71,77,79,81,82,83,84,85,86,88,89,90,94,99,100,102,103,104,106,107,108,109,110,111,112,113,115,116,119,120,121,122,123,127,129,130,132,133,134,135,136,137,138,140,142,143,144,145,147,149,150,152,154,155,156,157,162,163,182,190) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Final Test Results:\n","test: 93.24%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mour_model_their_training_data_only_plain_more_early_eval\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/z3dfdjw9\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m2_operands_0_to_999_their_training_data_only_more_eval/plain_out/wandb/run-20250624_185259-z3dfdjw9/logs\u001b[0m\n"]}],"source":["!python train_end_padding_more_early_eval.py 2_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"qxc4ZaLSaeHp"},"source":["## 4 Operands 0-999 Addition (Our Generated Balanced Digit Data)"]},{"cell_type":"markdown","metadata":{"id":"ECsn2_dNbAsr"},"source":["### Plain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":409,"status":"ok","timestamp":1750709718000,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"gDEzDcAdbiFq","outputId":"ca2fc0ce-c485-40dc-c8cf-1f7664ca6afa"},"outputs":[{"name":"stdout","output_type":"stream","text":["# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_0_to_999_balanced_digit_plain_v4'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'algo_reasoning'\n","data_format='plain'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: whether the result is reversed\n","reverse_c = False\n","eval_addition = True\n","\n","analysis = False\n","\n","# to edit: the number of addition operations to perform, which equals to (#operands -1)\n","num_addition = 3\n","\n","# to edit: num of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","# whether need zero‐padding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 300000\n","lr_decay_iters = 300000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory\n","out_dir = '/content/drive/MyDrive/addition/4_operands_0_to_999_balanced_digit/plain_out_v4'\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","\n","# to edit: training data\n","train_data_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_balanced_digit/train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","start_train = \"FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_balanced_digit/train_eval.txt\"\n","\n","# to edit: valuation data\n","val_data_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_balanced_digit/val.txt'\n","\n","# to edit: test data (start is just the test file)\n","start = 'FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_balanced_digit/test/test.txt'\n","\n","# (optional param) to_edit: whether to enable detailed metric recording at each eval_interval\n","# test_dir: the directory storing test files\n","test_dir = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_balanced_digit/test'\n","eval_additional_test = True  # whether to evaluate on additional test files \n"]}],"source":["%cat 4_operands_addition_plain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":3507732,"status":"ok","timestamp":1750713233690,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"5k_DydpRadgd","outputId":"c1466343-26c3-4e9d-c14a-60ed72f8b514"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n"," 53% 44/83 [00:01<00:01, 32.77it/s]Skipping y_hat=499\n","Skipping y_hat=1301\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=1098\n","Skipping y_hat=1000\n","Skipping y_hat=1369\n"," 58% 48/83 [00:01<00:01, 32.23it/s]Skipping y_hat=797\n","Skipping y_hat=1099\n","Skipping y_hat=666\n","Skipping y_hat=498\n","Skipping y_hat=1001\n","Skipping y_hat=999\n","Skipping y_hat=940\n","Skipping y_hat=1080\n","Skipping y_hat=1499\n"," 63% 52/83 [00:01<00:00, 31.77it/s]Skipping y_hat=1006\n"," 72% 60/83 [00:01<00:00, 32.70it/s]Skipping y_hat=700\n","Skipping y_hat=1099\n"," 77% 64/83 [00:01<00:00, 32.75it/s]Skipping y_hat=298\n","Skipping y_hat=490\n","Skipping y_hat=699\n"," 82% 68/83 [00:02<00:00, 32.55it/s]Skipping y_hat=1903\n","Skipping y_hat=1010\n","Skipping y_hat=1460\n","Skipping y_hat=938\n","Skipping y_hat=1084\n","Skipping y_hat=1797\n"," 92% 76/83 [00:02<00:00, 33.93it/s]Skipping y_hat=2304\n","Skipping y_hat=2198\n","Skipping y_hat=2792\n","Skipping y_hat=1497\n","Skipping y_hat=1690\n","Skipping y_hat=1462\n","Skipping y_hat=1119\n","Skipping y_hat=906\n","Skipping y_hat=2177\n","Skipping y_hat=1900\n","Skipping y_hat=2305\n","Skipping y_hat=2202\n","Skipping y_hat=600\n","Skipping y_hat=2246\n","Skipping y_hat=2106\n","Skipping y_hat=1849\n"," 96% 80/83 [00:02<00:00, 33.01it/s]Skipping y_hat=1973\n","Skipping y_hat=2002\n","Skipping y_hat=697\n","Skipping y_hat=1296\n","100% 83/83 [00:02<00:00, 32.65it/s]\n","accuracy of 10000 examples: 9861/10000 (98.61%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.76it/s]\n","accuracy of 10000 examples: 9849/10000 (98.49%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=904\n","Skipping y_hat=935\n","Skipping y_hat=500\n","Skipping y_hat=1001\n","Skipping y_hat=1599\n","Skipping y_hat=699\n","Skipping y_hat=670\n","Skipping y_hat=1309\n","Skipping y_hat=548\n","  5% 4/83 [00:00<00:02, 33.71it/s]Skipping y_hat=1200\n","Skipping y_hat=904\n","Skipping y_hat=1220\n","Skipping y_hat=905\n","Skipping y_hat=1299\n","Skipping y_hat=1499\n","Skipping y_hat=639\n","Skipping y_hat=501\n","Skipping y_hat=1140\n"," 10% 8/83 [00:00<00:02, 33.85it/s]Skipping y_hat=1210\n","Skipping y_hat=1100\n","Skipping y_hat=999\n","Skipping y_hat=898\n","Skipping y_hat=1000\n","Skipping y_hat=390\n","Skipping y_hat=499\n","Skipping y_hat=1730\n"," 14% 12/83 [00:00<00:02, 34.02it/s]Skipping y_hat=1390\n","Skipping y_hat=1090\n","Skipping y_hat=1480\n","Skipping y_hat=1007\n","Skipping y_hat=1171\n","Skipping y_hat=301\n","Skipping y_hat=1009\n","Skipping y_hat=1301\n","Skipping y_hat=308\n","Skipping y_hat=1300\n","Skipping y_hat=762\n"," 19% 16/83 [00:00<00:01, 33.72it/s]Skipping y_hat=1400\n","Skipping y_hat=430\n","Skipping y_hat=1205\n","Skipping y_hat=1005\n","Skipping y_hat=1003\n","Skipping y_hat=1549\n","Skipping y_hat=1707\n","Skipping y_hat=1203\n","Skipping y_hat=1294\n","Skipping y_hat=1076\n"," 24% 20/83 [00:00<00:01, 33.58it/s]Skipping y_hat=1373\n","Skipping y_hat=1172\n","Skipping y_hat=1089\n","Skipping y_hat=1198\n","Skipping y_hat=1399\n","Skipping y_hat=2306\n","Skipping y_hat=1635\n","Skipping y_hat=1800\n","Skipping y_hat=1599\n","Skipping y_hat=1800\n","Skipping y_hat=1493\n","Skipping y_hat=2098\n","Skipping y_hat=1240\n","Skipping y_hat=1290\n","Skipping y_hat=809\n","Skipping y_hat=2299\n","Skipping y_hat=1489\n","Skipping y_hat=1300\n","Skipping y_hat=1299\n"," 29% 24/83 [00:00<00:01, 33.11it/s]Skipping y_hat=1301\n","Skipping y_hat=1600\n","Skipping y_hat=708\n","Skipping y_hat=1099\n","Skipping y_hat=1600\n","Skipping y_hat=129\n"," 34% 28/83 [00:00<00:01, 33.17it/s]Skipping y_hat=59\n"," 39% 32/83 [00:00<00:01, 33.49it/s]Skipping y_hat=630\n","Skipping y_hat=40\n"," 43% 36/83 [00:01<00:01, 33.77it/s]Skipping y_hat=104\n","Skipping y_hat=995\n"," 48% 40/83 [00:01<00:01, 33.91it/s]Skipping y_hat=397\n","Skipping y_hat=1289\n","Skipping y_hat=118\n","Skipping y_hat=167\n"," 53% 44/83 [00:01<00:01, 34.01it/s]Skipping y_hat=499\n","Skipping y_hat=999\n","Skipping y_hat=1098\n"," 58% 48/83 [00:01<00:01, 34.20it/s]Skipping y_hat=677\n","Skipping y_hat=666\n","Skipping y_hat=498\n","Skipping y_hat=940\n","Skipping y_hat=1080\n"," 63% 52/83 [00:01<00:00, 34.32it/s]Skipping y_hat=640\n","Skipping y_hat=460\n"," 67% 56/83 [00:01<00:00, 34.44it/s]Skipping y_hat=322\n","Skipping y_hat=107\n"," 72% 60/83 [00:01<00:00, 34.38it/s]Skipping y_hat=1099\n"," 77% 64/83 [00:01<00:00, 34.35it/s]Skipping y_hat=298\n","Skipping y_hat=490\n","Skipping y_hat=803\n"," 82% 68/83 [00:02<00:00, 34.40it/s]Skipping y_hat=539\n","Skipping y_hat=2019\n","Skipping y_hat=2491\n","Skipping y_hat=1335\n","Skipping y_hat=2295\n","Skipping y_hat=1932\n","Skipping y_hat=2059\n","Skipping y_hat=938\n","Skipping y_hat=1084\n","Skipping y_hat=2063\n","Skipping y_hat=2199\n","Skipping y_hat=1644\n"," 87% 72/83 [00:02<00:00, 34.66it/s]Skipping y_hat=80\n"," 92% 76/83 [00:02<00:00, 35.15it/s]Skipping y_hat=2304\n","Skipping y_hat=1497\n","Skipping y_hat=2403\n","Skipping y_hat=1790\n","Skipping y_hat=1558\n","Skipping y_hat=1462\n","Skipping y_hat=1219\n","Skipping y_hat=1597\n","Skipping y_hat=2300\n","Skipping y_hat=905\n","Skipping y_hat=2177\n","Skipping y_hat=2305\n","Skipping y_hat=2403\n","Skipping y_hat=1601\n","Skipping y_hat=2732\n","Skipping y_hat=2246\n","Skipping y_hat=1269\n","Skipping y_hat=2106\n","Skipping y_hat=2299\n"," 96% 80/83 [00:02<00:00, 35.14it/s]Skipping y_hat=2300\n","Skipping y_hat=1494\n","Skipping y_hat=697\n","Skipping y_hat=1296\n","100% 83/83 [00:02<00:00, 34.36it/s]\n","accuracy of 10000 examples: 9857/10000 (98.57000000000001%)\n","\n","Test Results:\n","test: 98.57%\n","\n","iter 31000: train loss 1.3343, val loss 1.3356\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=590\n","Skipping y_hat=999\n","Skipping y_hat=990\n","Skipping y_hat=1006\n","Skipping y_hat=1270\n","Skipping y_hat=1104\n","Skipping y_hat=1080\n","Skipping y_hat=670\n","Skipping y_hat=494\n","  5% 4/83 [00:00<00:02, 32.21it/s]Skipping y_hat=1290\n","Skipping y_hat=814\n","Skipping y_hat=600\n","Skipping y_hat=590\n","Skipping y_hat=501\n","Skipping y_hat=258\n","Skipping y_hat=1300\n","Skipping y_hat=520\n"," 10% 8/83 [00:00<00:02, 32.67it/s]Skipping y_hat=594\n","Skipping y_hat=1590\n","Skipping y_hat=506\n","Skipping y_hat=456\n","Skipping y_hat=500\n","Skipping y_hat=1600\n","Skipping y_hat=397\n","Skipping y_hat=600\n","Skipping y_hat=1356\n","Skipping y_hat=1800\n","Skipping y_hat=741\n","Skipping y_hat=1102\n","Skipping y_hat=1460\n"," 14% 12/83 [00:00<00:02, 33.02it/s]Skipping y_hat=1300\n","Skipping y_hat=1099\n","Skipping y_hat=1520\n","Skipping y_hat=1200\n","Skipping y_hat=1004\n","Skipping y_hat=1301\n","Skipping y_hat=1490\n","Skipping y_hat=800\n","Skipping y_hat=656\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:02, 32.51it/s]Skipping y_hat=706\n","Skipping y_hat=600\n","Skipping y_hat=808\n","Skipping y_hat=1900\n","Skipping y_hat=1231\n","Skipping y_hat=2350\n","Skipping y_hat=1306\n","Skipping y_hat=1199\n","Skipping y_hat=810\n","Skipping y_hat=1552\n","Skipping y_hat=1397\n"," 24% 20/83 [00:00<00:02, 31.18it/s]Skipping y_hat=1355\n","Skipping y_hat=599\n","Skipping y_hat=1907\n","Skipping y_hat=1797\n","Skipping y_hat=1089\n","Skipping y_hat=705\n","Skipping y_hat=1798\n","Skipping y_hat=1050\n","Skipping y_hat=1287\n","Skipping y_hat=1380\n","Skipping y_hat=1800\n","Skipping y_hat=1303\n","Skipping y_hat=706\n","Skipping y_hat=1240\n","Skipping y_hat=1180\n","Skipping y_hat=1557\n","Skipping y_hat=1370\n","Skipping y_hat=401\n","Skipping y_hat=1299\n","Skipping y_hat=1099\n"," 29% 24/83 [00:00<00:01, 30.16it/s]Skipping y_hat=1260\n","Skipping y_hat=1195\n","Skipping y_hat=1670\n","Skipping y_hat=401\n","Skipping y_hat=1300\n","Skipping y_hat=1765\n","Skipping y_hat=801\n","Skipping y_hat=1299\n","Skipping y_hat=1600\n","Skipping y_hat=1296\n","Skipping y_hat=1862\n"," 34% 28/83 [00:00<00:01, 30.35it/s]Skipping y_hat=90\n","Skipping y_hat=299\n"," 39% 32/83 [00:01<00:01, 31.51it/s]Skipping y_hat=80\n","Skipping y_hat=120\n","Skipping y_hat=299\n","Skipping y_hat=500\n"," 43% 36/83 [00:01<00:01, 32.13it/s]Skipping y_hat=900\n","Skipping y_hat=294\n","Skipping y_hat=1080\n","Skipping y_hat=799\n","Skipping y_hat=1570\n","Skipping y_hat=830\n"," 48% 40/83 [00:01<00:01, 32.34it/s]Skipping y_hat=902\n","Skipping y_hat=899\n","Skipping y_hat=1340\n"," 53% 44/83 [00:01<00:01, 32.37it/s]Skipping y_hat=999\n","Skipping y_hat=908\n","Skipping y_hat=1570\n"," 58% 48/83 [00:01<00:01, 32.38it/s]Skipping y_hat=999\n"," 63% 52/83 [00:01<00:00, 32.86it/s]Skipping y_hat=1000\n"," 67% 56/83 [00:01<00:00, 33.35it/s]Skipping y_hat=410\n","Skipping y_hat=461\n"," 72% 60/83 [00:01<00:00, 33.63it/s]Skipping y_hat=899\n","Skipping y_hat=799\n"," 77% 64/83 [00:01<00:00, 33.81it/s]Skipping y_hat=1040\n"," 82% 68/83 [00:02<00:00, 34.03it/s]Skipping y_hat=150\n","Skipping y_hat=1918\n","Skipping y_hat=3029\n","Skipping y_hat=3447\n","Skipping y_hat=2193\n","Skipping y_hat=2295\n","Skipping y_hat=1949\n","Skipping y_hat=2199\n"," 87% 72/83 [00:02<00:00, 34.20it/s]Skipping y_hat=50\n"," 92% 76/83 [00:02<00:00, 34.62it/s]Skipping y_hat=1605\n","Skipping y_hat=1501\n","Skipping y_hat=1600\n","Skipping y_hat=1100\n","Skipping y_hat=1802\n","Skipping y_hat=1202\n","Skipping y_hat=603\n","Skipping y_hat=1900\n","Skipping y_hat=1401\n","Skipping y_hat=3445\n","Skipping y_hat=1803\n","Skipping y_hat=2580\n","Skipping y_hat=2189\n","Skipping y_hat=1401\n"," 96% 80/83 [00:02<00:00, 34.37it/s]Skipping y_hat=2402\n","100% 83/83 [00:02<00:00, 33.01it/s]\n","accuracy of 10000 examples: 9842/10000 (98.42%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.33it/s]\n","accuracy of 10000 examples: 9828/10000 (98.28%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=905\n","Skipping y_hat=400\n","Skipping y_hat=500\n","Skipping y_hat=999\n","Skipping y_hat=900\n","Skipping y_hat=1500\n","Skipping y_hat=1006\n","Skipping y_hat=1270\n","Skipping y_hat=1080\n","  5% 4/83 [00:00<00:02, 31.77it/s]Skipping y_hat=814\n","Skipping y_hat=858\n","Skipping y_hat=600\n","Skipping y_hat=1110\n","Skipping y_hat=1300\n"," 10% 8/83 [00:00<00:02, 33.06it/s]Skipping y_hat=594\n","Skipping y_hat=740\n","Skipping y_hat=506\n","Skipping y_hat=500\n","Skipping y_hat=397\n","Skipping y_hat=850\n","Skipping y_hat=600\n","Skipping y_hat=1800\n","Skipping y_hat=741\n","Skipping y_hat=702\n"," 14% 12/83 [00:00<00:02, 33.61it/s]Skipping y_hat=1300\n","Skipping y_hat=1693\n","Skipping y_hat=1200\n","Skipping y_hat=1009\n","Skipping y_hat=1301\n","Skipping y_hat=1490\n","Skipping y_hat=800\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:01, 33.59it/s]Skipping y_hat=1300\n","Skipping y_hat=499\n","Skipping y_hat=600\n","Skipping y_hat=2007\n","Skipping y_hat=808\n","Skipping y_hat=2350\n","Skipping y_hat=1599\n","Skipping y_hat=1100\n","Skipping y_hat=2500\n","Skipping y_hat=1208\n","Skipping y_hat=1306\n","Skipping y_hat=1104\n"," 24% 20/83 [00:00<00:01, 33.40it/s]Skipping y_hat=1355\n","Skipping y_hat=2200\n","Skipping y_hat=1907\n","Skipping y_hat=1797\n","Skipping y_hat=1102\n","Skipping y_hat=807\n","Skipping y_hat=705\n","Skipping y_hat=944\n","Skipping y_hat=1798\n","Skipping y_hat=1380\n","Skipping y_hat=1800\n","Skipping y_hat=706\n","Skipping y_hat=1240\n","Skipping y_hat=401\n"," 29% 24/83 [00:00<00:01, 32.81it/s]Skipping y_hat=801\n","Skipping y_hat=1299\n","Skipping y_hat=1600\n","Skipping y_hat=1840\n"," 34% 28/83 [00:00<00:01, 32.84it/s]Skipping y_hat=200\n","Skipping y_hat=400\n","Skipping y_hat=122\n"," 39% 32/83 [00:00<00:01, 32.80it/s]Skipping y_hat=120\n","Skipping y_hat=500\n"," 43% 36/83 [00:01<00:01, 33.16it/s]Skipping y_hat=900\n","Skipping y_hat=294\n","Skipping y_hat=1080\n","Skipping y_hat=1000\n","Skipping y_hat=1080\n","Skipping y_hat=799\n"," 48% 40/83 [00:01<00:01, 32.60it/s]Skipping y_hat=200\n","Skipping y_hat=930\n","Skipping y_hat=899\n","Skipping y_hat=903\n"," 53% 44/83 [00:01<00:01, 33.11it/s]Skipping y_hat=360\n","Skipping y_hat=999\n","Skipping y_hat=908\n","Skipping y_hat=630\n","Skipping y_hat=185\n","Skipping y_hat=1570\n","Skipping y_hat=1250\n"," 58% 48/83 [00:01<00:01, 33.06it/s]Skipping y_hat=999\n"," 63% 52/83 [00:01<00:00, 33.45it/s]Skipping y_hat=1000\n"," 67% 56/83 [00:01<00:00, 33.96it/s]Skipping y_hat=439\n","Skipping y_hat=410\n","Skipping y_hat=461\n"," 72% 60/83 [00:01<00:00, 34.03it/s]Skipping y_hat=470\n"," 77% 64/83 [00:01<00:00, 33.82it/s]Skipping y_hat=150\n","Skipping y_hat=300\n"," 82% 68/83 [00:02<00:00, 33.97it/s]Skipping y_hat=580\n","Skipping y_hat=2981\n","Skipping y_hat=1273\n","Skipping y_hat=1810\n","Skipping y_hat=2491\n","Skipping y_hat=1503\n","Skipping y_hat=2295\n","Skipping y_hat=2042\n","Skipping y_hat=1949\n","Skipping y_hat=3487\n","Skipping y_hat=2517\n","Skipping y_hat=2924\n","Skipping y_hat=1891\n"," 87% 72/83 [00:02<00:00, 34.26it/s]Skipping y_hat=50\n","Skipping y_hat=60\n","Skipping y_hat=30\n"," 92% 76/83 [00:02<00:00, 34.59it/s]Skipping y_hat=1497\n","Skipping y_hat=1563\n","Skipping y_hat=2280\n","Skipping y_hat=1600\n","Skipping y_hat=1100\n","Skipping y_hat=1802\n","Skipping y_hat=1202\n","Skipping y_hat=1900\n","Skipping y_hat=1401\n","Skipping y_hat=3254\n","Skipping y_hat=600\n","Skipping y_hat=1400\n","Skipping y_hat=1803\n","Skipping y_hat=2140\n","Skipping y_hat=1520\n","Skipping y_hat=2189\n"," 96% 80/83 [00:02<00:00, 34.58it/s]Skipping y_hat=1130\n","Skipping y_hat=2402\n","Skipping y_hat=2460\n","Skipping y_hat=1158\n","Skipping y_hat=1794\n","Skipping y_hat=1502\n","100% 83/83 [00:02<00:00, 33.74it/s]\n","accuracy of 10000 examples: 9846/10000 (98.46000000000001%)\n","\n","Test Results:\n","test: 98.46%\n","\n","iter 32000: train loss 1.3312, val loss 1.3309\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=935\n","Skipping y_hat=898\n","Skipping y_hat=1591\n","Skipping y_hat=1006\n","Skipping y_hat=1599\n","Skipping y_hat=1493\n","Skipping y_hat=639\n","Skipping y_hat=801\n","Skipping y_hat=1159\n","Skipping y_hat=837\n","Skipping y_hat=1190\n","Skipping y_hat=998\n","  5% 4/83 [00:00<00:02, 32.48it/s]Skipping y_hat=1298\n","Skipping y_hat=1699\n","Skipping y_hat=1499\n","Skipping y_hat=1370\n","Skipping y_hat=303\n","Skipping y_hat=1459\n","Skipping y_hat=1300\n","Skipping y_hat=1375\n","Skipping y_hat=969\n"," 10% 8/83 [00:00<00:02, 32.65it/s]Skipping y_hat=304\n","Skipping y_hat=1597\n","Skipping y_hat=1490\n","Skipping y_hat=438\n","Skipping y_hat=399\n","Skipping y_hat=899\n","Skipping y_hat=1800\n","Skipping y_hat=703\n","Skipping y_hat=1679\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 32.80it/s]Skipping y_hat=1300\n","Skipping y_hat=1099\n","Skipping y_hat=679\n","Skipping y_hat=1179\n","Skipping y_hat=895\n","Skipping y_hat=1429\n","Skipping y_hat=1007\n","Skipping y_hat=301\n","Skipping y_hat=905\n","Skipping y_hat=1301\n","Skipping y_hat=1099\n","Skipping y_hat=1799\n","Skipping y_hat=403\n"," 19% 16/83 [00:00<00:02, 32.62it/s]Skipping y_hat=1898\n","Skipping y_hat=887\n","Skipping y_hat=499\n","Skipping y_hat=808\n","Skipping y_hat=1400\n","Skipping y_hat=2369\n","Skipping y_hat=1704\n","Skipping y_hat=1829\n","Skipping y_hat=1100\n","Skipping y_hat=1506\n","Skipping y_hat=2005\n"," 24% 20/83 [00:00<00:01, 32.66it/s]Skipping y_hat=2200\n","Skipping y_hat=1797\n","Skipping y_hat=1798\n","Skipping y_hat=1635\n","Skipping y_hat=706\n","Skipping y_hat=1259\n","Skipping y_hat=2209\n","Skipping y_hat=1586\n","Skipping y_hat=1489\n","Skipping y_hat=1902\n"," 29% 24/83 [00:00<00:01, 32.87it/s]Skipping y_hat=1441\n","Skipping y_hat=1608\n","Skipping y_hat=802\n"," 34% 28/83 [00:00<00:01, 33.20it/s]Skipping y_hat=620\n"," 39% 32/83 [00:00<00:01, 33.62it/s]Skipping y_hat=630\n","Skipping y_hat=79\n"," 43% 36/83 [00:01<00:01, 33.75it/s]Skipping y_hat=497\n","Skipping y_hat=500\n","Skipping y_hat=799\n","Skipping y_hat=741\n"," 48% 40/83 [00:01<00:01, 33.66it/s]Skipping y_hat=903\n"," 53% 44/83 [00:01<00:01, 33.87it/s]Skipping y_hat=999\n","Skipping y_hat=908\n","Skipping y_hat=499\n","Skipping y_hat=295\n","Skipping y_hat=790\n"," 58% 48/83 [00:01<00:01, 32.85it/s]Skipping y_hat=1091\n","Skipping y_hat=498\n","Skipping y_hat=1038\n","Skipping y_hat=400\n","Skipping y_hat=299\n","Skipping y_hat=999\n","Skipping y_hat=922\n"," 63% 52/83 [00:01<00:00, 32.48it/s]Skipping y_hat=769\n","Skipping y_hat=1299\n","Skipping y_hat=1029\n","Skipping y_hat=269\n","Skipping y_hat=500\n","Skipping y_hat=190\n","Skipping y_hat=700\n"," 72% 60/83 [00:01<00:00, 32.90it/s]Skipping y_hat=401\n","Skipping y_hat=892\n","Skipping y_hat=899\n","Skipping y_hat=759\n","Skipping y_hat=599\n","Skipping y_hat=1099\n"," 77% 64/83 [00:01<00:00, 32.19it/s]Skipping y_hat=299\n"," 82% 68/83 [00:02<00:00, 32.40it/s]Skipping y_hat=2919\n","Skipping y_hat=2449\n","Skipping y_hat=2442\n","Skipping y_hat=1525\n","Skipping y_hat=3956\n","Skipping y_hat=1508\n","Skipping y_hat=2491\n","Skipping y_hat=2955\n","Skipping y_hat=2573\n","Skipping y_hat=2404\n","Skipping y_hat=3287\n","Skipping y_hat=2045\n","Skipping y_hat=2025\n","Skipping y_hat=2072\n","Skipping y_hat=2559\n","Skipping y_hat=3009\n","Skipping y_hat=1312\n","Skipping y_hat=1805\n","Skipping y_hat=2101\n"," 92% 76/83 [00:02<00:00, 32.91it/s]Skipping y_hat=919\n","Skipping y_hat=1497\n","Skipping y_hat=2304\n","Skipping y_hat=1639\n","Skipping y_hat=2045\n","Skipping y_hat=1600\n","Skipping y_hat=1100\n","Skipping y_hat=905\n","Skipping y_hat=1202\n","Skipping y_hat=1900\n","Skipping y_hat=1702\n","Skipping y_hat=1401\n","Skipping y_hat=1803\n","Skipping y_hat=600\n","Skipping y_hat=1803\n","Skipping y_hat=2048\n","Skipping y_hat=1442\n"," 96% 80/83 [00:02<00:00, 32.62it/s]Skipping y_hat=706\n","Skipping y_hat=1296\n","100% 83/83 [00:02<00:00, 32.96it/s]\n","accuracy of 10000 examples: 9798/10000 (97.98%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 37.92it/s]\n","accuracy of 10000 examples: 9818/10000 (98.18%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=904\n","Skipping y_hat=1398\n","Skipping y_hat=1292\n","Skipping y_hat=1599\n","Skipping y_hat=837\n","  5% 4/83 [00:00<00:02, 32.49it/s]Skipping y_hat=1699\n","Skipping y_hat=697\n","Skipping y_hat=303\n","Skipping y_hat=795\n","Skipping y_hat=503\n","Skipping y_hat=1300\n","Skipping y_hat=969\n"," 10% 8/83 [00:00<00:02, 32.18it/s]Skipping y_hat=1597\n","Skipping y_hat=370\n","Skipping y_hat=399\n","Skipping y_hat=1800\n"," 14% 12/83 [00:00<00:02, 31.74it/s]Skipping y_hat=903\n","Skipping y_hat=1300\n","Skipping y_hat=1099\n","Skipping y_hat=679\n","Skipping y_hat=700\n","Skipping y_hat=403\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:02, 31.72it/s]Skipping y_hat=1898\n","Skipping y_hat=2611\n","Skipping y_hat=887\n","Skipping y_hat=499\n","Skipping y_hat=1400\n","Skipping y_hat=1703\n","Skipping y_hat=1829\n","Skipping y_hat=1100\n","Skipping y_hat=1506\n","Skipping y_hat=2301\n","Skipping y_hat=2005\n"," 24% 20/83 [00:00<00:01, 31.57it/s]Skipping y_hat=941\n","Skipping y_hat=1907\n","Skipping y_hat=1797\n","Skipping y_hat=1307\n","Skipping y_hat=1183\n","Skipping y_hat=815\n","Skipping y_hat=1798\n","Skipping y_hat=1372\n","Skipping y_hat=706\n","Skipping y_hat=1202\n","Skipping y_hat=2299\n"," 29% 24/83 [00:00<00:01, 31.38it/s]Skipping y_hat=1804\n","Skipping y_hat=1301\n","Skipping y_hat=1195\n","Skipping y_hat=401\n","Skipping y_hat=1608\n","Skipping y_hat=905\n"," 34% 28/83 [00:00<00:01, 31.63it/s]Skipping y_hat=859\n"," 39% 32/83 [00:01<00:01, 31.91it/s]Skipping y_hat=90\n","Skipping y_hat=649\n","Skipping y_hat=799\n","Skipping y_hat=79\n"," 43% 36/83 [00:01<00:01, 32.28it/s]Skipping y_hat=497\n","Skipping y_hat=969\n","Skipping y_hat=390\n","Skipping y_hat=500\n","Skipping y_hat=1299\n","Skipping y_hat=799\n","Skipping y_hat=300\n","Skipping y_hat=741\n"," 48% 40/83 [00:01<00:01, 31.96it/s]Skipping y_hat=969\n","Skipping y_hat=1289\n","Skipping y_hat=999\n","Skipping y_hat=389\n","Skipping y_hat=399\n","Skipping y_hat=159\n","Skipping y_hat=939\n","Skipping y_hat=1364\n"," 53% 44/83 [00:01<00:01, 31.48it/s]Skipping y_hat=205\n","Skipping y_hat=908\n"," 58% 48/83 [00:01<00:01, 31.40it/s]Skipping y_hat=1091\n","Skipping y_hat=192\n","Skipping y_hat=498\n","Skipping y_hat=299\n","Skipping y_hat=999\n","Skipping y_hat=1389\n","Skipping y_hat=1093\n"," 63% 52/83 [00:01<00:00, 31.12it/s]Skipping y_hat=769\n","Skipping y_hat=1029\n","Skipping y_hat=500\n","Skipping y_hat=700\n"," 72% 60/83 [00:01<00:00, 31.60it/s]Skipping y_hat=402\n","Skipping y_hat=892\n","Skipping y_hat=899\n","Skipping y_hat=759\n","Skipping y_hat=599\n","Skipping y_hat=1099\n"," 77% 64/83 [00:02<00:00, 31.64it/s]Skipping y_hat=150\n"," 82% 68/83 [00:02<00:00, 31.72it/s]Skipping y_hat=1718\n","Skipping y_hat=2919\n","Skipping y_hat=2449\n","Skipping y_hat=1473\n","Skipping y_hat=2442\n","Skipping y_hat=1508\n","Skipping y_hat=2491\n","Skipping y_hat=3955\n","Skipping y_hat=2573\n","Skipping y_hat=2404\n","Skipping y_hat=2045\n","Skipping y_hat=3929\n","Skipping y_hat=2559\n","Skipping y_hat=3009\n","Skipping y_hat=1312\n","Skipping y_hat=1805\n"," 92% 76/83 [00:02<00:00, 32.55it/s]Skipping y_hat=1639\n","Skipping y_hat=2045\n","Skipping y_hat=1600\n","Skipping y_hat=2300\n","Skipping y_hat=1802\n","Skipping y_hat=905\n","Skipping y_hat=1900\n","Skipping y_hat=1705\n","Skipping y_hat=1702\n","Skipping y_hat=1401\n","Skipping y_hat=1803\n","Skipping y_hat=600\n","Skipping y_hat=2201\n","Skipping y_hat=1803\n","Skipping y_hat=2026\n"," 96% 80/83 [00:02<00:00, 32.28it/s]Skipping y_hat=2402\n","Skipping y_hat=2506\n","Skipping y_hat=706\n","Skipping y_hat=1666\n","Skipping y_hat=1296\n","100% 83/83 [00:02<00:00, 31.92it/s]\n","accuracy of 10000 examples: 9814/10000 (98.14%)\n","\n","Test Results:\n","test: 98.14%\n","\n","iter 33000: train loss 1.3335, val loss 1.3303\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=905\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=230\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","  5% 4/83 [00:00<00:02, 32.90it/s]Skipping y_hat=697\n","Skipping y_hat=600\n","Skipping y_hat=501\n","Skipping y_hat=1730\n"," 10% 8/83 [00:00<00:02, 33.11it/s]Skipping y_hat=1400\n","Skipping y_hat=802\n","Skipping y_hat=1840\n","Skipping y_hat=399\n","Skipping y_hat=1800\n","Skipping y_hat=595\n","Skipping y_hat=300\n","Skipping y_hat=1730\n"," 14% 12/83 [00:00<00:02, 33.08it/s]Skipping y_hat=1300\n","Skipping y_hat=849\n","Skipping y_hat=1099\n","Skipping y_hat=610\n"," 19% 16/83 [00:00<00:02, 33.50it/s]Skipping y_hat=1320\n","Skipping y_hat=499\n","Skipping y_hat=600\n","Skipping y_hat=1703\n","Skipping y_hat=1100\n","Skipping y_hat=1008\n"," 24% 20/83 [00:00<00:01, 33.12it/s]Skipping y_hat=1527\n","Skipping y_hat=1409\n","Skipping y_hat=1399\n","Skipping y_hat=1599\n","Skipping y_hat=1719\n","Skipping y_hat=1800\n","Skipping y_hat=706\n","Skipping y_hat=1701\n","Skipping y_hat=1202\n","Skipping y_hat=1255\n","Skipping y_hat=1029\n","Skipping y_hat=1489\n"," 29% 24/83 [00:00<00:01, 33.07it/s]Skipping y_hat=895\n","Skipping y_hat=1404\n","Skipping y_hat=1300\n","Skipping y_hat=802\n","Skipping y_hat=1193\n","Skipping y_hat=1750\n","Skipping y_hat=109\n"," 34% 28/83 [00:00<00:01, 33.17it/s]Skipping y_hat=140\n","Skipping y_hat=639\n"," 39% 32/83 [00:00<00:01, 33.32it/s]Skipping y_hat=90\n"," 43% 36/83 [00:01<00:01, 33.59it/s]Skipping y_hat=520\n","Skipping y_hat=630\n","Skipping y_hat=800\n","Skipping y_hat=1080\n","Skipping y_hat=799\n","Skipping y_hat=1292\n","Skipping y_hat=830\n"," 48% 40/83 [00:01<00:01, 33.14it/s]Skipping y_hat=899\n","Skipping y_hat=600\n","Skipping y_hat=200\n"," 53% 44/83 [00:01<00:01, 32.80it/s]Skipping y_hat=449\n"," 58% 48/83 [00:01<00:01, 33.01it/s]Skipping y_hat=1199\n","Skipping y_hat=294\n","Skipping y_hat=1001\n","Skipping y_hat=999\n","Skipping y_hat=996\n","Skipping y_hat=306\n"," 63% 52/83 [00:01<00:00, 32.47it/s]Skipping y_hat=679\n","Skipping y_hat=300\n","Skipping y_hat=400\n","Skipping y_hat=999\n"," 67% 56/83 [00:01<00:00, 32.19it/s]Skipping y_hat=410\n","Skipping y_hat=150\n"," 72% 60/83 [00:01<00:00, 32.48it/s]Skipping y_hat=200\n","Skipping y_hat=599\n"," 77% 64/83 [00:01<00:00, 32.73it/s]Skipping y_hat=227\n","Skipping y_hat=150\n","Skipping y_hat=209\n","Skipping y_hat=299\n"," 82% 68/83 [00:02<00:00, 32.57it/s]Skipping y_hat=769\n","Skipping y_hat=810\n","Skipping y_hat=2491\n","Skipping y_hat=1503\n","Skipping y_hat=1460\n","Skipping y_hat=1467\n","Skipping y_hat=1838\n","Skipping y_hat=2283\n","Skipping y_hat=1422\n","Skipping y_hat=2101\n"," 87% 72/83 [00:02<00:00, 32.71it/s]Skipping y_hat=99\n","Skipping y_hat=89\n"," 92% 76/83 [00:02<00:00, 33.21it/s]Skipping y_hat=1497\n","Skipping y_hat=1639\n","Skipping y_hat=1600\n","Skipping y_hat=2300\n","Skipping y_hat=1802\n","Skipping y_hat=2204\n","Skipping y_hat=905\n","Skipping y_hat=2989\n","Skipping y_hat=2305\n","Skipping y_hat=986\n","Skipping y_hat=1640\n","Skipping y_hat=2106\n"," 96% 80/83 [00:02<00:00, 33.33it/s]Skipping y_hat=2099\n","Skipping y_hat=701\n","Skipping y_hat=1701\n","Skipping y_hat=1296\n","100% 83/83 [00:02<00:00, 33.11it/s]\n","accuracy of 10000 examples: 9867/10000 (98.67%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 37.35it/s]\n","accuracy of 10000 examples: 9888/10000 (98.88%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=590\n","Skipping y_hat=600\n","Skipping y_hat=230\n","Skipping y_hat=1000\n","Skipping y_hat=499\n","  5% 4/83 [00:00<00:02, 30.35it/s]Skipping y_hat=1197\n","Skipping y_hat=630\n","Skipping y_hat=697\n","Skipping y_hat=543\n","Skipping y_hat=501\n","Skipping y_hat=1730\n","Skipping y_hat=520\n"," 10% 8/83 [00:00<00:02, 29.88it/s]Skipping y_hat=370\n","Skipping y_hat=1490\n","Skipping y_hat=898\n","Skipping y_hat=1840\n","Skipping y_hat=470\n","Skipping y_hat=803\n","Skipping y_hat=193\n","Skipping y_hat=1800\n"," 13% 11/83 [00:00<00:02, 29.80it/s]Skipping y_hat=300\n","Skipping y_hat=1799\n","Skipping y_hat=1730\n","Skipping y_hat=1300\n","Skipping y_hat=1099\n","Skipping y_hat=600\n","Skipping y_hat=1301\n"," 18% 15/83 [00:00<00:02, 31.04it/s]Skipping y_hat=1065\n","Skipping y_hat=1320\n","Skipping y_hat=600\n","Skipping y_hat=1299\n","Skipping y_hat=1083\n","Skipping y_hat=1008\n"," 23% 19/83 [00:00<00:02, 31.86it/s]Skipping y_hat=1309\n","Skipping y_hat=987\n","Skipping y_hat=1399\n","Skipping y_hat=1599\n","Skipping y_hat=1296\n","Skipping y_hat=706\n","Skipping y_hat=1240\n"," 28% 23/83 [00:00<00:01, 32.00it/s]Skipping y_hat=905\n","Skipping y_hat=1202\n","Skipping y_hat=1300\n","Skipping y_hat=1404\n","Skipping y_hat=995\n","Skipping y_hat=802\n"," 33% 27/83 [00:00<00:01, 32.36it/s]Skipping y_hat=149\n","Skipping y_hat=140\n"," 37% 31/83 [00:00<00:01, 32.80it/s]Skipping y_hat=639\n","Skipping y_hat=259\n"," 42% 35/83 [00:01<00:01, 33.28it/s]Skipping y_hat=520\n","Skipping y_hat=630\n","Skipping y_hat=279\n","Skipping y_hat=1080\n"," 47% 39/83 [00:01<00:01, 33.32it/s]Skipping y_hat=1099\n","Skipping y_hat=600\n","Skipping y_hat=700\n","Skipping y_hat=299\n","Skipping y_hat=839\n"," 52% 43/83 [00:01<00:01, 33.64it/s]Skipping y_hat=695\n","Skipping y_hat=200\n","Skipping y_hat=849\n"," 61% 51/83 [00:01<00:00, 33.93it/s]Skipping y_hat=500\n","Skipping y_hat=300\n","Skipping y_hat=400\n"," 66% 55/83 [00:01<00:00, 33.80it/s]Skipping y_hat=700\n","Skipping y_hat=410\n","Skipping y_hat=150\n"," 71% 59/83 [00:01<00:00, 33.54it/s]Skipping y_hat=170\n","Skipping y_hat=899\n"," 76% 63/83 [00:01<00:00, 33.16it/s]Skipping y_hat=398\n","Skipping y_hat=299\n","Skipping y_hat=299\n","Skipping y_hat=849\n"," 81% 67/83 [00:02<00:00, 32.86it/s]Skipping y_hat=699\n","Skipping y_hat=199\n"," 86% 71/83 [00:02<00:00, 32.82it/s]Skipping y_hat=810\n","Skipping y_hat=1508\n","Skipping y_hat=1460\n","Skipping y_hat=1467\n","Skipping y_hat=2101\n","Skipping y_hat=119\n"," 90% 75/83 [00:02<00:00, 33.58it/s]Skipping y_hat=1497\n","Skipping y_hat=1501\n","Skipping y_hat=2304\n","Skipping y_hat=1414\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=901\n","Skipping y_hat=2300\n","Skipping y_hat=1802\n","Skipping y_hat=1505\n","Skipping y_hat=1401\n","Skipping y_hat=2305\n"," 95% 79/83 [00:02<00:00, 33.59it/s]Skipping y_hat=1803\n","Skipping y_hat=986\n","Skipping y_hat=2403\n","Skipping y_hat=992\n","Skipping y_hat=2099\n","Skipping y_hat=2205\n","Skipping y_hat=1502\n","Skipping y_hat=701\n","Skipping y_hat=1701\n","Skipping y_hat=1296\n","100% 83/83 [00:02<00:00, 32.97it/s]\n","accuracy of 10000 examples: 9871/10000 (98.71%)\n","\n","Test Results:\n","test: 98.71%\n","\n","iter 34000: train loss 1.3284, val loss 1.3301\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=994\n","Skipping y_hat=919\n","Skipping y_hat=590\n","Skipping y_hat=400\n","Skipping y_hat=1001\n","Skipping y_hat=500\n","Skipping y_hat=906\n","Skipping y_hat=902\n","Skipping y_hat=1605\n","Skipping y_hat=625\n","Skipping y_hat=670\n","Skipping y_hat=808\n","  5% 4/83 [00:00<00:02, 30.86it/s]Skipping y_hat=330\n","Skipping y_hat=697\n","Skipping y_hat=600\n","Skipping y_hat=590\n","Skipping y_hat=702\n","Skipping y_hat=1102\n","Skipping y_hat=849\n","Skipping y_hat=1300\n"," 10% 8/83 [00:00<00:02, 31.45it/s]Skipping y_hat=400\n","Skipping y_hat=370\n","Skipping y_hat=1370\n","Skipping y_hat=470\n","Skipping y_hat=500\n","Skipping y_hat=803\n","Skipping y_hat=1600\n","Skipping y_hat=545\n","Skipping y_hat=1200\n","Skipping y_hat=1447\n","Skipping y_hat=791\n","Skipping y_hat=1196\n"," 14% 12/83 [00:00<00:02, 30.96it/s]Skipping y_hat=903\n","Skipping y_hat=1300\n","Skipping y_hat=920\n","Skipping y_hat=700\n","Skipping y_hat=1670\n","Skipping y_hat=570\n","Skipping y_hat=1147\n","Skipping y_hat=1300\n","Skipping y_hat=1709\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:02, 31.15it/s]Skipping y_hat=1818\n","Skipping y_hat=1300\n","Skipping y_hat=2007\n","Skipping y_hat=1105\n","Skipping y_hat=2199\n","Skipping y_hat=708\n","Skipping y_hat=1203\n","Skipping y_hat=1679\n","Skipping y_hat=1808\n","Skipping y_hat=1097\n","Skipping y_hat=1636\n","Skipping y_hat=707\n","Skipping y_hat=1199\n","Skipping y_hat=2301\n","Skipping y_hat=1008\n"," 24% 20/83 [00:00<00:01, 31.66it/s]Skipping y_hat=1797\n","Skipping y_hat=2402\n","Skipping y_hat=1602\n","Skipping y_hat=705\n","Skipping y_hat=1399\n","Skipping y_hat=1492\n","Skipping y_hat=1800\n","Skipping y_hat=1599\n","Skipping y_hat=1719\n","Skipping y_hat=1800\n","Skipping y_hat=803\n","Skipping y_hat=642\n","Skipping y_hat=1240\n","Skipping y_hat=1488\n","Skipping y_hat=845\n","Skipping y_hat=1489\n","Skipping y_hat=1299\n"," 29% 24/83 [00:00<00:01, 31.43it/s]Skipping y_hat=1699\n","Skipping y_hat=1750\n","Skipping y_hat=1064\n","Skipping y_hat=1508\n","Skipping y_hat=1608\n","Skipping y_hat=1890\n","Skipping y_hat=1296\n","Skipping y_hat=200\n"," 34% 28/83 [00:00<00:01, 31.71it/s]Skipping y_hat=139\n"," 39% 32/83 [00:01<00:01, 31.98it/s]Skipping y_hat=79\n","Skipping y_hat=142\n"," 43% 36/83 [00:01<00:01, 32.44it/s]Skipping y_hat=709\n","Skipping y_hat=950\n","Skipping y_hat=900\n","Skipping y_hat=104\n","Skipping y_hat=805\n","Skipping y_hat=799\n","Skipping y_hat=274\n","Skipping y_hat=741\n"," 48% 40/83 [00:01<00:01, 32.29it/s]Skipping y_hat=109\n","Skipping y_hat=701\n","Skipping y_hat=500\n","Skipping y_hat=306\n","Skipping y_hat=600\n","Skipping y_hat=1061\n","Skipping y_hat=700\n","Skipping y_hat=1061\n","Skipping y_hat=603\n","Skipping y_hat=200\n"," 53% 44/83 [00:01<00:01, 32.42it/s]Skipping y_hat=809\n","Skipping y_hat=101\n","Skipping y_hat=167\n","Skipping y_hat=561\n","Skipping y_hat=549\n","Skipping y_hat=295\n","Skipping y_hat=202\n"," 58% 48/83 [00:01<00:01, 32.65it/s]Skipping y_hat=1002\n","Skipping y_hat=600\n","Skipping y_hat=1058\n"," 63% 52/83 [00:01<00:00, 32.38it/s]Skipping y_hat=502\n","Skipping y_hat=1000\n","Skipping y_hat=590\n","Skipping y_hat=300\n","Skipping y_hat=1069\n","Skipping y_hat=448\n","Skipping y_hat=601\n","Skipping y_hat=1000\n","Skipping y_hat=700\n","Skipping y_hat=801\n"," 67% 56/83 [00:01<00:00, 32.35it/s]Skipping y_hat=1010\n"," 72% 60/83 [00:01<00:00, 32.56it/s]Skipping y_hat=730\n","Skipping y_hat=170\n","Skipping y_hat=718\n","Skipping y_hat=609\n","Skipping y_hat=201\n"," 77% 64/83 [00:01<00:00, 32.49it/s]Skipping y_hat=400\n","Skipping y_hat=803\n","Skipping y_hat=860\n"," 82% 68/83 [00:02<00:00, 32.70it/s]Skipping y_hat=700\n","Skipping y_hat=539\n","Skipping y_hat=1098\n","Skipping y_hat=2919\n","Skipping y_hat=1912\n","Skipping y_hat=2308\n","Skipping y_hat=1903\n","Skipping y_hat=2384\n","Skipping y_hat=800\n","Skipping y_hat=2044\n","Skipping y_hat=1970\n","Skipping y_hat=2323\n","Skipping y_hat=2225\n","Skipping y_hat=2955\n","Skipping y_hat=1503\n","Skipping y_hat=2403\n","Skipping y_hat=2938\n","Skipping y_hat=2929\n","Skipping y_hat=2283\n","Skipping y_hat=1907\n","Skipping y_hat=2323\n","Skipping y_hat=2916\n","Skipping y_hat=2557\n","Skipping y_hat=2199\n"," 92% 76/83 [00:02<00:00, 33.36it/s]Skipping y_hat=1605\n","Skipping y_hat=1497\n","Skipping y_hat=1306\n","Skipping y_hat=1497\n","Skipping y_hat=2280\n","Skipping y_hat=1600\n","Skipping y_hat=1100\n","Skipping y_hat=2300\n","Skipping y_hat=1666\n","Skipping y_hat=1639\n","Skipping y_hat=1702\n","Skipping y_hat=1401\n","Skipping y_hat=2305\n","Skipping y_hat=600\n","Skipping y_hat=2447\n","Skipping y_hat=1076\n","Skipping y_hat=2403\n","Skipping y_hat=1601\n","Skipping y_hat=2299\n","Skipping y_hat=1625\n","Skipping y_hat=1487\n"," 96% 80/83 [00:02<00:00, 33.36it/s]Skipping y_hat=1747\n","Skipping y_hat=2099\n","Skipping y_hat=2070\n","Skipping y_hat=2402\n","Skipping y_hat=1606\n","Skipping y_hat=701\n","Skipping y_hat=1494\n","Skipping y_hat=1701\n","Skipping y_hat=1978\n","Skipping y_hat=1296\n","Skipping y_hat=29\n","Skipping y_hat=29\n","100% 83/83 [00:02<00:00, 32.52it/s]\n","accuracy of 10000 examples: 9777/10000 (97.77%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.41it/s]\n","accuracy of 10000 examples: 9789/10000 (97.89%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1096\n","Skipping y_hat=590\n","Skipping y_hat=400\n","Skipping y_hat=1001\n","Skipping y_hat=500\n","Skipping y_hat=906\n","Skipping y_hat=1500\n","Skipping y_hat=361\n","Skipping y_hat=1000\n","Skipping y_hat=801\n","  4% 3/83 [00:00<00:02, 30.00it/s]Skipping y_hat=1090\n","Skipping y_hat=644\n","Skipping y_hat=499\n","Skipping y_hat=1278\n","Skipping y_hat=704\n","Skipping y_hat=600\n","Skipping y_hat=702\n","Skipping y_hat=849\n","  8% 7/83 [00:00<00:02, 31.34it/s]Skipping y_hat=1300\n","Skipping y_hat=520\n","Skipping y_hat=400\n","Skipping y_hat=370\n","Skipping y_hat=410\n","Skipping y_hat=500\n","Skipping y_hat=1600\n","Skipping y_hat=600\n","Skipping y_hat=741\n"," 13% 11/83 [00:00<00:02, 31.78it/s]Skipping y_hat=1200\n","Skipping y_hat=791\n","Skipping y_hat=903\n","Skipping y_hat=1300\n","Skipping y_hat=205\n","Skipping y_hat=1520\n","Skipping y_hat=700\n","Skipping y_hat=805\n","Skipping y_hat=1670\n","Skipping y_hat=570\n","Skipping y_hat=1501\n","Skipping y_hat=1147\n","Skipping y_hat=1300\n","Skipping y_hat=1799\n","Skipping y_hat=700\n","Skipping y_hat=1209\n"," 18% 15/83 [00:00<00:02, 32.02it/s]Skipping y_hat=862\n","Skipping y_hat=1000\n","Skipping y_hat=1818\n","Skipping y_hat=808\n","Skipping y_hat=1105\n","Skipping y_hat=1400\n","Skipping y_hat=2355\n","Skipping y_hat=2430\n","Skipping y_hat=2350\n","Skipping y_hat=1679\n","Skipping y_hat=1602\n","Skipping y_hat=1484\n","Skipping y_hat=2200\n","Skipping y_hat=1603\n","Skipping y_hat=1230\n","Skipping y_hat=1636\n","Skipping y_hat=2301\n","Skipping y_hat=1008\n"," 23% 19/83 [00:00<00:01, 32.08it/s]Skipping y_hat=1829\n","Skipping y_hat=1894\n","Skipping y_hat=1797\n","Skipping y_hat=2402\n","Skipping y_hat=1602\n","Skipping y_hat=795\n","Skipping y_hat=1399\n","Skipping y_hat=1748\n","Skipping y_hat=1604\n","Skipping y_hat=1800\n","Skipping y_hat=1719\n","Skipping y_hat=1800\n","Skipping y_hat=803\n","Skipping y_hat=400\n","Skipping y_hat=1701\n","Skipping y_hat=1240\n","Skipping y_hat=1488\n"," 28% 23/83 [00:00<00:01, 32.12it/s]Skipping y_hat=920\n","Skipping y_hat=674\n","Skipping y_hat=1489\n","Skipping y_hat=401\n","Skipping y_hat=1299\n","Skipping y_hat=1901\n","Skipping y_hat=1890\n"," 33% 27/83 [00:00<00:01, 32.49it/s]Skipping y_hat=159\n"," 37% 31/83 [00:00<00:01, 32.94it/s]Skipping y_hat=690\n","Skipping y_hat=600\n"," 42% 35/83 [00:01<00:01, 32.95it/s]Skipping y_hat=709\n","Skipping y_hat=950\n","Skipping y_hat=900\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=799\n"," 47% 39/83 [00:01<00:01, 33.20it/s]Skipping y_hat=741\n","Skipping y_hat=109\n","Skipping y_hat=702\n","Skipping y_hat=600\n","Skipping y_hat=1061\n","Skipping y_hat=700\n","Skipping y_hat=1061\n","Skipping y_hat=201\n"," 52% 43/83 [00:01<00:01, 33.18it/s]Skipping y_hat=800\n","Skipping y_hat=301\n","Skipping y_hat=970\n","Skipping y_hat=295\n"," 57% 47/83 [00:01<00:01, 33.13it/s]Skipping y_hat=807\n","Skipping y_hat=600\n","Skipping y_hat=404\n","Skipping y_hat=248\n","Skipping y_hat=400\n"," 61% 51/83 [00:01<00:00, 32.93it/s]Skipping y_hat=461\n","Skipping y_hat=500\n","Skipping y_hat=300\n","Skipping y_hat=1520\n","Skipping y_hat=304\n","Skipping y_hat=1069\n","Skipping y_hat=448\n","Skipping y_hat=460\n","Skipping y_hat=447\n","Skipping y_hat=601\n","Skipping y_hat=1063\n"," 66% 55/83 [00:01<00:00, 32.92it/s]Skipping y_hat=190\n","Skipping y_hat=700\n","Skipping y_hat=801\n","Skipping y_hat=1010\n","Skipping y_hat=313\n"," 71% 59/83 [00:01<00:00, 32.90it/s]Skipping y_hat=244\n","Skipping y_hat=200\n","Skipping y_hat=170\n"," 76% 63/83 [00:01<00:00, 33.07it/s]Skipping y_hat=400\n","Skipping y_hat=803\n"," 81% 67/83 [00:02<00:00, 33.18it/s]Skipping y_hat=860\n","Skipping y_hat=166\n","Skipping y_hat=167\n","Skipping y_hat=801\n"," 86% 71/83 [00:02<00:00, 33.39it/s]Skipping y_hat=1098\n","Skipping y_hat=2919\n","Skipping y_hat=1912\n","Skipping y_hat=2308\n","Skipping y_hat=1721\n","Skipping y_hat=810\n","Skipping y_hat=2044\n","Skipping y_hat=1810\n","Skipping y_hat=1970\n","Skipping y_hat=2323\n","Skipping y_hat=2225\n","Skipping y_hat=2955\n","Skipping y_hat=2403\n","Skipping y_hat=2246\n","Skipping y_hat=2029\n","Skipping y_hat=2283\n","Skipping y_hat=1907\n","Skipping y_hat=2457\n","Skipping y_hat=2199\n"," 90% 75/83 [00:02<00:00, 33.61it/s]Skipping y_hat=60\n","Skipping y_hat=109\n","Skipping y_hat=1497\n","Skipping y_hat=2234\n","Skipping y_hat=2792\n","Skipping y_hat=2280\n","Skipping y_hat=1601\n","Skipping y_hat=2300\n","Skipping y_hat=1802\n","Skipping y_hat=2204\n","Skipping y_hat=1940\n","Skipping y_hat=1069\n","Skipping y_hat=905\n","Skipping y_hat=1309\n","Skipping y_hat=1085\n","Skipping y_hat=1702\n","Skipping y_hat=1401\n","Skipping y_hat=2202\n"," 95% 79/83 [00:02<00:00, 33.55it/s]Skipping y_hat=600\n","Skipping y_hat=2403\n","Skipping y_hat=1200\n","Skipping y_hat=1601\n","Skipping y_hat=1640\n","Skipping y_hat=1883\n","Skipping y_hat=1010\n","Skipping y_hat=2002\n","Skipping y_hat=2402\n","Skipping y_hat=1502\n","Skipping y_hat=701\n","Skipping y_hat=2208\n","Skipping y_hat=1348\n","Skipping y_hat=1978\n","Skipping y_hat=29\n","Skipping y_hat=29\n","100% 83/83 [00:02<00:00, 33.03it/s]\n","accuracy of 10000 examples: 9775/10000 (97.75%)\n","\n","Test Results:\n","test: 97.75%\n","\n","iter 35000: train loss 1.3284, val loss 1.3343\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1594\n","Skipping y_hat=1096\n","Skipping y_hat=909\n","Skipping y_hat=911\n","Skipping y_hat=300\n","  5% 4/83 [00:00<00:02, 33.36it/s]Skipping y_hat=267\n","Skipping y_hat=207\n","Skipping y_hat=1059\n","Skipping y_hat=460\n","Skipping y_hat=501\n","Skipping y_hat=1300\n"," 10% 8/83 [00:00<00:02, 33.08it/s]Skipping y_hat=259\n","Skipping y_hat=414\n","Skipping y_hat=1100\n","Skipping y_hat=1600\n","Skipping y_hat=399\n","Skipping y_hat=300\n","Skipping y_hat=1730\n"," 14% 12/83 [00:00<00:02, 32.29it/s]Skipping y_hat=1300\n","Skipping y_hat=301\n","Skipping y_hat=1119\n"," 19% 16/83 [00:00<00:02, 32.42it/s]Skipping y_hat=1105\n","Skipping y_hat=299\n","Skipping y_hat=759\n","Skipping y_hat=1100\n","Skipping y_hat=1636\n","Skipping y_hat=707\n","Skipping y_hat=1540\n","Skipping y_hat=1260\n","Skipping y_hat=908\n"," 24% 20/83 [00:00<00:01, 32.12it/s]Skipping y_hat=2284\n","Skipping y_hat=1397\n","Skipping y_hat=1201\n","Skipping y_hat=2306\n","Skipping y_hat=1371\n","Skipping y_hat=310\n","Skipping y_hat=1240\n","Skipping y_hat=2299\n"," 29% 24/83 [00:00<00:01, 31.91it/s]Skipping y_hat=306\n","Skipping y_hat=700\n","Skipping y_hat=401\n","Skipping y_hat=1608\n","Skipping y_hat=99\n"," 34% 28/83 [00:00<00:01, 32.12it/s]Skipping y_hat=318\n"," 39% 32/83 [00:00<00:01, 32.24it/s]Skipping y_hat=90\n","Skipping y_hat=40\n","Skipping y_hat=50\n","Skipping y_hat=80\n"," 48% 40/83 [00:01<00:01, 32.45it/s]Skipping y_hat=198\n","Skipping y_hat=780\n","Skipping y_hat=600\n","Skipping y_hat=700\n","Skipping y_hat=299\n"," 53% 44/83 [00:01<00:01, 32.46it/s]Skipping y_hat=498\n","Skipping y_hat=205\n","Skipping y_hat=1060\n"," 58% 48/83 [00:01<00:01, 32.63it/s]Skipping y_hat=498\n","Skipping y_hat=209\n"," 63% 52/83 [00:01<00:00, 32.95it/s]Skipping y_hat=630\n","Skipping y_hat=697\n","Skipping y_hat=500\n","Skipping y_hat=999\n"," 67% 56/83 [00:01<00:00, 32.16it/s]Skipping y_hat=1099\n"," 72% 60/83 [00:01<00:00, 32.01it/s]Skipping y_hat=599\n"," 77% 64/83 [00:01<00:00, 31.85it/s]Skipping y_hat=209\n","Skipping y_hat=699\n"," 82% 68/83 [00:02<00:00, 32.14it/s]Skipping y_hat=2494\n","Skipping y_hat=2044\n","Skipping y_hat=1473\n","Skipping y_hat=2115\n","Skipping y_hat=2520\n","Skipping y_hat=2075\n","Skipping y_hat=1312\n"," 92% 76/83 [00:02<00:00, 33.40it/s]Skipping y_hat=1764\n","Skipping y_hat=2120\n","Skipping y_hat=1529\n","Skipping y_hat=1219\n","Skipping y_hat=1100\n","Skipping y_hat=2305\n","Skipping y_hat=2202\n","Skipping y_hat=2403\n","Skipping y_hat=930\n"," 96% 80/83 [00:02<00:00, 33.35it/s]Skipping y_hat=0\n","100% 83/83 [00:02<00:00, 32.66it/s]\n","accuracy of 10000 examples: 9881/10000 (98.81%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 37.85it/s]\n","accuracy of 10000 examples: 9842/10000 (98.42%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1096\n","Skipping y_hat=905\n","Skipping y_hat=500\n","Skipping y_hat=400\n","  5% 4/83 [00:00<00:02, 33.56it/s]Skipping y_hat=1120\n","Skipping y_hat=1220\n","Skipping y_hat=697\n","Skipping y_hat=267\n","Skipping y_hat=460\n","Skipping y_hat=1702\n","Skipping y_hat=955\n","Skipping y_hat=1300\n","Skipping y_hat=730\n"," 10% 8/83 [00:00<00:02, 32.42it/s]Skipping y_hat=698\n","Skipping y_hat=1100\n","Skipping y_hat=903\n","Skipping y_hat=1059\n","Skipping y_hat=300\n","Skipping y_hat=1730\n"," 14% 12/83 [00:00<00:02, 32.47it/s]Skipping y_hat=1300\n","Skipping y_hat=1119\n","Skipping y_hat=1504\n"," 19% 16/83 [00:00<00:02, 32.93it/s]Skipping y_hat=1599\n","Skipping y_hat=1703\n","Skipping y_hat=1506\n","Skipping y_hat=1540\n","Skipping y_hat=1260\n","Skipping y_hat=2019\n"," 24% 20/83 [00:00<00:01, 32.20it/s]Skipping y_hat=2200\n","Skipping y_hat=1979\n","Skipping y_hat=2284\n","Skipping y_hat=1397\n","Skipping y_hat=1601\n","Skipping y_hat=987\n","Skipping y_hat=1602\n","Skipping y_hat=2306\n","Skipping y_hat=654\n","Skipping y_hat=1719\n","Skipping y_hat=1502\n","Skipping y_hat=701\n","Skipping y_hat=905\n","Skipping y_hat=401\n","Skipping y_hat=1099\n"," 29% 24/83 [00:00<00:01, 30.46it/s]Skipping y_hat=306\n","Skipping y_hat=1300\n","Skipping y_hat=1994\n","Skipping y_hat=1608\n"," 34% 28/83 [00:00<00:01, 30.35it/s]Skipping y_hat=150\n"," 39% 32/83 [00:01<00:01, 31.00it/s]Skipping y_hat=699\n","Skipping y_hat=90\n","Skipping y_hat=40\n","Skipping y_hat=90\n","Skipping y_hat=299\n"," 48% 40/83 [00:01<00:01, 31.44it/s]Skipping y_hat=198\n","Skipping y_hat=1060\n","Skipping y_hat=780\n","Skipping y_hat=299\n","Skipping y_hat=939\n"," 53% 44/83 [00:01<00:01, 31.01it/s]Skipping y_hat=205\n","Skipping y_hat=911\n","Skipping y_hat=1060\n"," 58% 48/83 [00:01<00:01, 31.33it/s]Skipping y_hat=797\n","Skipping y_hat=494\n"," 63% 52/83 [00:01<00:00, 31.52it/s]Skipping y_hat=630\n"," 67% 56/83 [00:01<00:00, 32.15it/s]Skipping y_hat=1099\n"," 72% 60/83 [00:01<00:00, 32.54it/s]Skipping y_hat=599\n","Skipping y_hat=1099\n"," 77% 64/83 [00:02<00:00, 32.87it/s]Skipping y_hat=209\n"," 82% 68/83 [00:02<00:00, 33.14it/s]Skipping y_hat=3919\n","Skipping y_hat=2251\n","Skipping y_hat=2944\n","Skipping y_hat=1473\n","Skipping y_hat=1997\n","Skipping y_hat=1335\n","Skipping y_hat=1503\n","Skipping y_hat=1660\n","Skipping y_hat=2773\n","Skipping y_hat=1932\n","Skipping y_hat=858\n","Skipping y_hat=2045\n","Skipping y_hat=2520\n","Skipping y_hat=2929\n","Skipping y_hat=2283\n","Skipping y_hat=2075\n","Skipping y_hat=1554\n"," 92% 76/83 [00:02<00:00, 33.97it/s]Skipping y_hat=2120\n","Skipping y_hat=1693\n","Skipping y_hat=2002\n","Skipping y_hat=1999\n","Skipping y_hat=2300\n","Skipping y_hat=2305\n","Skipping y_hat=2403\n","Skipping y_hat=2299\n"," 96% 80/83 [00:02<00:00, 33.49it/s]Skipping y_hat=701\n","Skipping y_hat=1701\n","100% 83/83 [00:02<00:00, 32.34it/s]\n","accuracy of 10000 examples: 9867/10000 (98.67%)\n","\n","Test Results:\n","test: 98.67%\n","\n","iter 36000: train loss 1.3347, val loss 1.3339\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=904\n","Skipping y_hat=955\n","Skipping y_hat=500\n","Skipping y_hat=694\n","Skipping y_hat=500\n","Skipping y_hat=699\n","Skipping y_hat=1000\n","Skipping y_hat=301\n","Skipping y_hat=499\n","Skipping y_hat=1000\n","  5% 4/83 [00:00<00:02, 33.00it/s]Skipping y_hat=697\n","Skipping y_hat=501\n"," 10% 8/83 [00:00<00:02, 33.57it/s]Skipping y_hat=1210\n","Skipping y_hat=1600\n","Skipping y_hat=1307\n","Skipping y_hat=1200\n","Skipping y_hat=1000\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 33.31it/s]Skipping y_hat=1300\n","Skipping y_hat=700\n","Skipping y_hat=649\n","Skipping y_hat=1370\n","Skipping y_hat=1300\n","Skipping y_hat=700\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:01, 33.59it/s]Skipping y_hat=1300\n","Skipping y_hat=1898\n","Skipping y_hat=2007\n","Skipping y_hat=1930\n","Skipping y_hat=1289\n","Skipping y_hat=1590\n","Skipping y_hat=1005\n","Skipping y_hat=1595\n","Skipping y_hat=2500\n","Skipping y_hat=1203\n","Skipping y_hat=1397\n"," 24% 20/83 [00:00<00:01, 33.62it/s]Skipping y_hat=599\n","Skipping y_hat=2200\n","Skipping y_hat=1797\n","Skipping y_hat=1711\n","Skipping y_hat=709\n","Skipping y_hat=1602\n","Skipping y_hat=705\n","Skipping y_hat=1798\n","Skipping y_hat=1604\n","Skipping y_hat=1655\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1701\n","Skipping y_hat=955\n","Skipping y_hat=1300\n","Skipping y_hat=1299\n"," 29% 24/83 [00:00<00:01, 33.40it/s]Skipping y_hat=1508\n","Skipping y_hat=1299\n","Skipping y_hat=1600\n","Skipping y_hat=780\n"," 39% 32/83 [00:00<00:01, 33.90it/s]Skipping y_hat=600\n"," 48% 40/83 [00:01<00:01, 34.58it/s]Skipping y_hat=939\n"," 53% 44/83 [00:01<00:01, 34.66it/s]Skipping y_hat=295\n"," 58% 48/83 [00:01<00:01, 34.45it/s]Skipping y_hat=1199\n","Skipping y_hat=1585\n"," 63% 52/83 [00:01<00:00, 34.21it/s]Skipping y_hat=500\n","Skipping y_hat=400\n","Skipping y_hat=202\n","Skipping y_hat=214\n","Skipping y_hat=100\n"," 72% 60/83 [00:01<00:00, 34.37it/s]Skipping y_hat=599\n"," 77% 64/83 [00:01<00:00, 34.49it/s]Skipping y_hat=400\n"," 82% 68/83 [00:01<00:00, 34.45it/s]Skipping y_hat=701\n","Skipping y_hat=2491\n","Skipping y_hat=1503\n","Skipping y_hat=2059\n","Skipping y_hat=2559\n","Skipping y_hat=1402\n","Skipping y_hat=2101\n","Skipping y_hat=2109\n"," 87% 72/83 [00:02<00:00, 34.62it/s]Skipping y_hat=89\n"," 92% 76/83 [00:02<00:00, 35.30it/s]Skipping y_hat=2389\n","Skipping y_hat=1712\n","Skipping y_hat=1219\n","Skipping y_hat=2300\n","Skipping y_hat=905\n","Skipping y_hat=1793\n","Skipping y_hat=2305\n","Skipping y_hat=2403\n","Skipping y_hat=1112\n"," 96% 80/83 [00:02<00:00, 35.16it/s]Skipping y_hat=2100\n","Skipping y_hat=992\n","Skipping y_hat=2002\n","Skipping y_hat=701\n","Skipping y_hat=1794\n","Skipping y_hat=2379\n","Skipping y_hat=1296\n","100% 83/83 [00:02<00:00, 34.37it/s]\n","accuracy of 10000 examples: 9897/10000 (98.97%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.80it/s]\n","accuracy of 10000 examples: 9905/10000 (99.05000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=904\n","Skipping y_hat=955\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=699\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","  5% 4/83 [00:00<00:02, 34.42it/s]Skipping y_hat=904\n","Skipping y_hat=501\n","Skipping y_hat=520\n"," 10% 8/83 [00:00<00:02, 34.58it/s]Skipping y_hat=1302\n","Skipping y_hat=1600\n","Skipping y_hat=1200\n","Skipping y_hat=300\n","Skipping y_hat=589\n"," 14% 12/83 [00:00<00:02, 34.70it/s]Skipping y_hat=1300\n","Skipping y_hat=479\n","Skipping y_hat=401\n","Skipping y_hat=301\n","Skipping y_hat=649\n","Skipping y_hat=1300\n","Skipping y_hat=1799\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:01, 34.73it/s]Skipping y_hat=1300\n","Skipping y_hat=1898\n","Skipping y_hat=997\n","Skipping y_hat=2007\n","Skipping y_hat=1289\n","Skipping y_hat=1590\n","Skipping y_hat=1599\n","Skipping y_hat=1100\n","Skipping y_hat=1595\n","Skipping y_hat=2500\n","Skipping y_hat=1203\n","Skipping y_hat=1173\n","Skipping y_hat=1203\n","Skipping y_hat=1397\n","Skipping y_hat=908\n"," 24% 20/83 [00:00<00:01, 34.01it/s]Skipping y_hat=599\n","Skipping y_hat=2200\n","Skipping y_hat=1797\n","Skipping y_hat=998\n","Skipping y_hat=709\n","Skipping y_hat=1602\n","Skipping y_hat=706\n","Skipping y_hat=705\n","Skipping y_hat=1798\n","Skipping y_hat=1800\n","Skipping y_hat=1204\n","Skipping y_hat=1502\n","Skipping y_hat=1800\n","Skipping y_hat=1701\n","Skipping y_hat=1300\n","Skipping y_hat=1299\n"," 29% 24/83 [00:00<00:01, 33.88it/s]Skipping y_hat=1299\n","Skipping y_hat=1600\n"," 48% 40/83 [00:01<00:01, 35.15it/s]Skipping y_hat=939\n"," 53% 44/83 [00:01<00:01, 35.16it/s]Skipping y_hat=999\n","Skipping y_hat=898\n"," 58% 48/83 [00:01<00:00, 35.18it/s]Skipping y_hat=1199\n","Skipping y_hat=999\n"," 63% 52/83 [00:01<00:00, 35.17it/s]Skipping y_hat=205\n","Skipping y_hat=269\n","Skipping y_hat=400\n","Skipping y_hat=202\n","Skipping y_hat=214\n","Skipping y_hat=100\n"," 67% 56/83 [00:01<00:00, 34.74it/s]Skipping y_hat=560\n","Skipping y_hat=180\n"," 72% 60/83 [00:01<00:00, 34.67it/s]Skipping y_hat=100\n","Skipping y_hat=599\n"," 77% 64/83 [00:01<00:00, 34.39it/s]Skipping y_hat=803\n","Skipping y_hat=571\n"," 82% 68/83 [00:01<00:00, 34.49it/s]Skipping y_hat=2198\n","Skipping y_hat=1503\n","Skipping y_hat=1660\n","Skipping y_hat=2059\n","Skipping y_hat=3383\n","Skipping y_hat=2559\n","Skipping y_hat=2860\n","Skipping y_hat=1402\n","Skipping y_hat=2101\n","Skipping y_hat=2009\n","Skipping y_hat=1554\n"," 92% 76/83 [00:02<00:00, 34.98it/s]Skipping y_hat=2036\n","Skipping y_hat=1219\n","Skipping y_hat=1100\n","Skipping y_hat=2300\n","Skipping y_hat=995\n","Skipping y_hat=1793\n","Skipping y_hat=2202\n","Skipping y_hat=2403\n","Skipping y_hat=2079\n","Skipping y_hat=1112\n","Skipping y_hat=2299\n"," 96% 80/83 [00:02<00:00, 34.42it/s]Skipping y_hat=2100\n","Skipping y_hat=1794\n","Skipping y_hat=1539\n","Skipping y_hat=2379\n","Skipping y_hat=1296\n","100% 83/83 [00:02<00:00, 34.66it/s]\n","accuracy of 10000 examples: 9890/10000 (98.9%)\n","\n","Test Results:\n","test: 98.90%\n","\n","iter 37000: train loss 1.3382, val loss 1.3330\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=542\n","  5% 4/83 [00:00<00:02, 33.04it/s]Skipping y_hat=820\n","Skipping y_hat=489\n","Skipping y_hat=697\n","Skipping y_hat=728\n","Skipping y_hat=1086\n","Skipping y_hat=1459\n","Skipping y_hat=520\n"," 10% 8/83 [00:00<00:02, 32.85it/s]Skipping y_hat=600\n"," 14% 12/83 [00:00<00:02, 33.35it/s]Skipping y_hat=1300\n","Skipping y_hat=909\n","Skipping y_hat=1007\n"," 19% 16/83 [00:00<00:01, 33.55it/s]Skipping y_hat=1696\n","Skipping y_hat=1299\n","Skipping y_hat=1595\n","Skipping y_hat=1209\n"," 24% 20/83 [00:00<00:01, 33.25it/s]Skipping y_hat=1207\n","Skipping y_hat=1798\n","Skipping y_hat=1599\n","Skipping y_hat=1192\n","Skipping y_hat=997\n","Skipping y_hat=2209\n","Skipping y_hat=905\n","Skipping y_hat=401\n","Skipping y_hat=1299\n","Skipping y_hat=1266\n"," 29% 24/83 [00:00<00:01, 33.00it/s]Skipping y_hat=1979\n","Skipping y_hat=1795\n","Skipping y_hat=1296\n"," 34% 28/83 [00:00<00:01, 32.80it/s]Skipping y_hat=39\n"," 39% 32/83 [00:00<00:01, 33.02it/s]Skipping y_hat=80\n","Skipping y_hat=179\n","Skipping y_hat=209\n"," 43% 36/83 [00:01<00:01, 33.31it/s]Skipping y_hat=497\n"," 48% 40/83 [00:01<00:01, 33.40it/s]Skipping y_hat=700\n"," 53% 44/83 [00:01<00:01, 33.64it/s]Skipping y_hat=499\n"," 58% 48/83 [00:01<00:01, 34.02it/s]Skipping y_hat=600\n","Skipping y_hat=909\n","Skipping y_hat=209\n","Skipping y_hat=101\n"," 63% 52/83 [00:01<00:00, 33.94it/s]Skipping y_hat=679\n","Skipping y_hat=297\n","Skipping y_hat=691\n","Skipping y_hat=293\n","Skipping y_hat=700\n","Skipping y_hat=900\n"," 72% 60/83 [00:01<00:00, 33.67it/s]Skipping y_hat=120\n"," 82% 68/83 [00:02<00:00, 34.02it/s]Skipping y_hat=79\n","Skipping y_hat=2508\n","Skipping y_hat=2494\n","Skipping y_hat=1010\n","Skipping y_hat=2444\n","Skipping y_hat=2080\n","Skipping y_hat=2225\n","Skipping y_hat=2006\n","Skipping y_hat=2055\n","Skipping y_hat=2042\n","Skipping y_hat=938\n","Skipping y_hat=1084\n","Skipping y_hat=2559\n","Skipping y_hat=2523\n","Skipping y_hat=2905\n","Skipping y_hat=2577\n","Skipping y_hat=2101\n","Skipping y_hat=2100\n"," 92% 76/83 [00:02<00:00, 34.64it/s]Skipping y_hat=1497\n","Skipping y_hat=1207\n","Skipping y_hat=1100\n","Skipping y_hat=2669\n","Skipping y_hat=600\n","Skipping y_hat=1055\n","Skipping y_hat=2032\n"," 96% 80/83 [00:02<00:00, 33.64it/s]Skipping y_hat=992\n","Skipping y_hat=2099\n","Skipping y_hat=2131\n","Skipping y_hat=1494\n","100% 83/83 [00:02<00:00, 33.68it/s]\n","accuracy of 10000 examples: 9891/10000 (98.91%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 37.82it/s]\n","accuracy of 10000 examples: 9883/10000 (98.83%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=532\n","Skipping y_hat=900\n","  5% 4/83 [00:00<00:02, 32.68it/s]Skipping y_hat=489\n","Skipping y_hat=993\n","Skipping y_hat=697\n","Skipping y_hat=728\n","Skipping y_hat=520\n"," 10% 8/83 [00:00<00:02, 30.78it/s]Skipping y_hat=414\n","Skipping y_hat=1308\n","Skipping y_hat=776\n","Skipping y_hat=1059\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 30.17it/s]Skipping y_hat=1300\n","Skipping y_hat=1099\n"," 19% 16/83 [00:00<00:02, 31.01it/s]Skipping y_hat=1696\n","Skipping y_hat=1299\n","Skipping y_hat=1003\n","Skipping y_hat=1595\n","Skipping y_hat=1209\n","Skipping y_hat=1698\n"," 24% 20/83 [00:00<00:02, 30.64it/s]Skipping y_hat=1492\n","Skipping y_hat=1797\n","Skipping y_hat=1192\n","Skipping y_hat=2209\n","Skipping y_hat=1490\n","Skipping y_hat=401\n","Skipping y_hat=1299\n"," 29% 24/83 [00:00<00:01, 30.24it/s]Skipping y_hat=1300\n","Skipping y_hat=1795\n","Skipping y_hat=1469\n","Skipping y_hat=2104\n","Skipping y_hat=1296\n"," 34% 28/83 [00:00<00:01, 30.33it/s]Skipping y_hat=39\n"," 39% 32/83 [00:01<00:01, 31.08it/s]Skipping y_hat=90\n","Skipping y_hat=109\n"," 43% 36/83 [00:01<00:01, 31.42it/s]Skipping y_hat=800\n","Skipping y_hat=420\n"," 48% 40/83 [00:01<00:01, 31.91it/s]Skipping y_hat=1099\n","Skipping y_hat=217\n","Skipping y_hat=703\n"," 53% 44/83 [00:01<00:01, 32.07it/s]Skipping y_hat=409\n","Skipping y_hat=207\n"," 58% 48/83 [00:01<00:01, 32.23it/s]Skipping y_hat=999\n"," 63% 52/83 [00:01<00:00, 32.55it/s]Skipping y_hat=679\n","Skipping y_hat=691\n","Skipping y_hat=300\n","Skipping y_hat=1069\n","Skipping y_hat=293\n","Skipping y_hat=900\n"," 67% 56/83 [00:01<00:00, 32.15it/s]Skipping y_hat=1089\n"," 72% 60/83 [00:01<00:00, 32.07it/s]Skipping y_hat=120\n"," 77% 64/83 [00:02<00:00, 32.58it/s]Skipping y_hat=229\n","Skipping y_hat=699\n"," 82% 68/83 [00:02<00:00, 32.98it/s]Skipping y_hat=2145\n","Skipping y_hat=2198\n","Skipping y_hat=1010\n","Skipping y_hat=2044\n","Skipping y_hat=2080\n","Skipping y_hat=2055\n","Skipping y_hat=2042\n","Skipping y_hat=948\n","Skipping y_hat=1084\n","Skipping y_hat=2387\n","Skipping y_hat=2483\n","Skipping y_hat=2569\n","Skipping y_hat=2405\n","Skipping y_hat=2577\n","Skipping y_hat=2063\n","Skipping y_hat=2101\n","Skipping y_hat=2100\n","Skipping y_hat=2199\n"," 92% 76/83 [00:02<00:00, 33.57it/s]Skipping y_hat=1497\n","Skipping y_hat=1100\n","Skipping y_hat=1793\n","Skipping y_hat=600\n","Skipping y_hat=2032\n","Skipping y_hat=930\n"," 96% 80/83 [00:02<00:00, 33.96it/s]Skipping y_hat=2100\n","Skipping y_hat=992\n","Skipping y_hat=2099\n","Skipping y_hat=2205\n","Skipping y_hat=1006\n","Skipping y_hat=1794\n","Skipping y_hat=1304\n","100% 83/83 [00:02<00:00, 32.30it/s]\n","accuracy of 10000 examples: 9879/10000 (98.79%)\n","\n","Test Results:\n","test: 98.79%\n","\n","iter 38000: train loss 1.3365, val loss 1.3344\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=542\n","Skipping y_hat=592\n","Skipping y_hat=607\n","Skipping y_hat=1000\n","Skipping y_hat=664\n","  5% 4/83 [00:00<00:02, 32.87it/s]Skipping y_hat=273\n","Skipping y_hat=1197\n","Skipping y_hat=501\n","Skipping y_hat=969\n"," 10% 8/83 [00:00<00:02, 33.08it/s]Skipping y_hat=1449\n","Skipping y_hat=304\n","Skipping y_hat=399\n","Skipping y_hat=397\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 33.46it/s]Skipping y_hat=1300\n","Skipping y_hat=1099\n","Skipping y_hat=399\n","Skipping y_hat=179\n","Skipping y_hat=888\n","Skipping y_hat=610\n","Skipping y_hat=996\n","Skipping y_hat=930\n"," 19% 16/83 [00:00<00:01, 33.97it/s]Skipping y_hat=2098\n","Skipping y_hat=499\n","Skipping y_hat=706\n","Skipping y_hat=600\n","Skipping y_hat=2007\n","Skipping y_hat=1299\n","Skipping y_hat=1100\n","Skipping y_hat=1102\n","Skipping y_hat=1008\n","Skipping y_hat=611\n"," 24% 20/83 [00:00<00:01, 33.35it/s]Skipping y_hat=601\n","Skipping y_hat=1309\n","Skipping y_hat=815\n","Skipping y_hat=1399\n","Skipping y_hat=1192\n","Skipping y_hat=1350\n","Skipping y_hat=2299\n","Skipping y_hat=962\n"," 29% 24/83 [00:00<00:01, 33.12it/s]Skipping y_hat=1098\n","Skipping y_hat=802\n","Skipping y_hat=1840\n"," 43% 36/83 [00:01<00:01, 34.05it/s]Skipping y_hat=798\n","Skipping y_hat=201\n","Skipping y_hat=500\n","Skipping y_hat=899\n","Skipping y_hat=630\n","Skipping y_hat=801\n"," 48% 40/83 [00:01<00:01, 34.14it/s]Skipping y_hat=1099\n","Skipping y_hat=696\n"," 53% 44/83 [00:01<00:01, 33.81it/s]Skipping y_hat=499\n"," 58% 48/83 [00:01<00:01, 33.91it/s]Skipping y_hat=797\n","Skipping y_hat=599\n","Skipping y_hat=1099\n","Skipping y_hat=498\n","Skipping y_hat=999\n"," 63% 52/83 [00:01<00:00, 33.70it/s]Skipping y_hat=749\n","Skipping y_hat=679\n","Skipping y_hat=297\n","Skipping y_hat=293\n"," 67% 56/83 [00:01<00:00, 33.94it/s]Skipping y_hat=410\n","Skipping y_hat=179\n","Skipping y_hat=999\n"," 72% 60/83 [00:01<00:00, 34.21it/s]Skipping y_hat=70\n"," 82% 68/83 [00:02<00:00, 34.57it/s]Skipping y_hat=179\n","Skipping y_hat=2198\n","Skipping y_hat=2352\n","Skipping y_hat=1998\n","Skipping y_hat=1832\n","Skipping y_hat=1805\n","Skipping y_hat=2101\n","Skipping y_hat=1797\n"," 92% 76/83 [00:02<00:00, 35.30it/s]Skipping y_hat=2304\n","Skipping y_hat=1578\n","Skipping y_hat=2300\n","Skipping y_hat=2204\n","Skipping y_hat=2305\n"," 96% 80/83 [00:02<00:00, 35.39it/s]Skipping y_hat=1099\n","Skipping y_hat=1699\n","Skipping y_hat=1701\n","100% 83/83 [00:02<00:00, 34.32it/s]\n","accuracy of 10000 examples: 9906/10000 (99.06%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 39.44it/s]\n","accuracy of 10000 examples: 9885/10000 (98.85000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=542\n","Skipping y_hat=592\n","Skipping y_hat=905\n","Skipping y_hat=700\n","Skipping y_hat=607\n","Skipping y_hat=994\n","Skipping y_hat=300\n","Skipping y_hat=1000\n","Skipping y_hat=664\n","  5% 4/83 [00:00<00:02, 34.15it/s]Skipping y_hat=273\n","Skipping y_hat=1197\n","Skipping y_hat=501\n","Skipping y_hat=1500\n"," 10% 8/83 [00:00<00:02, 34.30it/s]Skipping y_hat=698\n","Skipping y_hat=1400\n","Skipping y_hat=1600\n","Skipping y_hat=399\n","Skipping y_hat=600\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 34.24it/s]Skipping y_hat=1099\n","Skipping y_hat=179\n","Skipping y_hat=1198\n","Skipping y_hat=930\n"," 19% 16/83 [00:00<00:01, 34.33it/s]Skipping y_hat=2008\n","Skipping y_hat=499\n","Skipping y_hat=1696\n","Skipping y_hat=600\n","Skipping y_hat=1599\n","Skipping y_hat=1005\n","Skipping y_hat=1299\n","Skipping y_hat=1100\n","Skipping y_hat=1008\n"," 24% 20/83 [00:00<00:01, 34.30it/s]Skipping y_hat=2200\n","Skipping y_hat=601\n","Skipping y_hat=1397\n","Skipping y_hat=1589\n","Skipping y_hat=1309\n","Skipping y_hat=1399\n","Skipping y_hat=1399\n","Skipping y_hat=1192\n","Skipping y_hat=1350\n","Skipping y_hat=647\n","Skipping y_hat=1130\n","Skipping y_hat=2299\n","Skipping y_hat=962\n","Skipping y_hat=1300\n","Skipping y_hat=1099\n"," 29% 24/83 [00:00<00:01, 33.90it/s]Skipping y_hat=1700\n","Skipping y_hat=1098\n","Skipping y_hat=401\n","Skipping y_hat=1304\n","Skipping y_hat=802\n","Skipping y_hat=1840\n","Skipping y_hat=49\n"," 39% 32/83 [00:00<00:01, 33.77it/s]Skipping y_hat=61\n"," 43% 36/83 [00:01<00:01, 34.28it/s]Skipping y_hat=798\n","Skipping y_hat=995\n","Skipping y_hat=500\n","Skipping y_hat=1000\n","Skipping y_hat=1299\n","Skipping y_hat=801\n"," 48% 40/83 [00:01<00:01, 34.40it/s]Skipping y_hat=1095\n","Skipping y_hat=1099\n","Skipping y_hat=696\n"," 53% 44/83 [00:01<00:01, 34.62it/s]Skipping y_hat=1095\n","Skipping y_hat=499\n","Skipping y_hat=998\n","Skipping y_hat=499\n"," 58% 48/83 [00:01<00:01, 34.62it/s]Skipping y_hat=797\n","Skipping y_hat=599\n","Skipping y_hat=1099\n","Skipping y_hat=498\n","Skipping y_hat=169\n","Skipping y_hat=494\n","Skipping y_hat=299\n","Skipping y_hat=999\n"," 63% 52/83 [00:01<00:00, 33.74it/s]Skipping y_hat=297\n","Skipping y_hat=293\n"," 67% 56/83 [00:01<00:00, 33.59it/s]Skipping y_hat=169\n"," 72% 60/83 [00:01<00:00, 33.50it/s]Skipping y_hat=810\n"," 77% 64/83 [00:01<00:00, 33.53it/s]Skipping y_hat=639\n"," 82% 68/83 [00:02<00:00, 33.73it/s]Skipping y_hat=179\n","Skipping y_hat=2198\n","Skipping y_hat=2002\n","Skipping y_hat=2352\n","Skipping y_hat=1998\n","Skipping y_hat=1842\n"," 92% 76/83 [00:02<00:00, 33.98it/s]Skipping y_hat=1960\n","Skipping y_hat=1639\n","Skipping y_hat=1190\n","Skipping y_hat=1000\n","Skipping y_hat=2201\n","Skipping y_hat=1297\n"," 96% 80/83 [00:02<00:00, 33.70it/s]Skipping y_hat=1099\n","Skipping y_hat=1701\n","100% 83/83 [00:02<00:00, 33.94it/s]\n","accuracy of 10000 examples: 9890/10000 (98.9%)\n","\n","Test Results:\n","test: 98.90%\n","\n","iter 39000: train loss 1.3317, val loss 1.3363\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1582\n","Skipping y_hat=1599\n","Skipping y_hat=1408\n","Skipping y_hat=664\n","  5% 4/83 [00:00<00:02, 33.19it/s]Skipping y_hat=1782\n","Skipping y_hat=697\n","Skipping y_hat=1499\n"," 10% 8/83 [00:00<00:02, 31.89it/s]Skipping y_hat=1590\n","Skipping y_hat=665\n","Skipping y_hat=399\n","Skipping y_hat=300\n","Skipping y_hat=791\n"," 14% 12/83 [00:00<00:02, 32.46it/s]Skipping y_hat=1300\n","Skipping y_hat=500\n","Skipping y_hat=1200\n","Skipping y_hat=1099\n","Skipping y_hat=610\n","Skipping y_hat=762\n","Skipping y_hat=1100\n"," 19% 16/83 [00:00<00:02, 33.02it/s]Skipping y_hat=600\n","Skipping y_hat=1515\n","Skipping y_hat=1566\n","Skipping y_hat=1199\n","Skipping y_hat=1008\n","Skipping y_hat=973\n"," 24% 20/83 [00:00<00:01, 33.06it/s]Skipping y_hat=1397\n","Skipping y_hat=1832\n","Skipping y_hat=1097\n","Skipping y_hat=2083\n","Skipping y_hat=1595\n","Skipping y_hat=997\n","Skipping y_hat=1446\n","Skipping y_hat=2061\n","Skipping y_hat=2299\n","Skipping y_hat=1299\n","Skipping y_hat=1099\n"," 29% 24/83 [00:00<00:01, 33.31it/s]Skipping y_hat=895\n","Skipping y_hat=401\n","Skipping y_hat=1795\n","Skipping y_hat=1299\n","Skipping y_hat=1296\n","Skipping y_hat=2298\n"," 39% 32/83 [00:00<00:01, 33.94it/s]Skipping y_hat=109\n"," 43% 36/83 [00:01<00:01, 34.04it/s]Skipping y_hat=446\n","Skipping y_hat=1193\n","Skipping y_hat=422\n","Skipping y_hat=266\n"," 48% 40/83 [00:01<00:01, 34.20it/s]Skipping y_hat=500\n","Skipping y_hat=201\n","Skipping y_hat=1192\n"," 53% 44/83 [00:01<00:01, 34.21it/s]Skipping y_hat=207\n"," 58% 48/83 [00:01<00:01, 34.19it/s]Skipping y_hat=1099\n","Skipping y_hat=959\n","Skipping y_hat=101\n","Skipping y_hat=999\n","Skipping y_hat=699\n"," 63% 52/83 [00:01<00:00, 34.20it/s]Skipping y_hat=1299\n","Skipping y_hat=1492\n","Skipping y_hat=300\n","Skipping y_hat=700\n"," 72% 60/83 [00:01<00:00, 34.54it/s]Skipping y_hat=200\n","Skipping y_hat=719\n"," 77% 64/83 [00:01<00:00, 34.69it/s]Skipping y_hat=188\n","Skipping y_hat=830\n"," 82% 68/83 [00:02<00:00, 34.79it/s]Skipping y_hat=1903\n","Skipping y_hat=2491\n","Skipping y_hat=2955\n","Skipping y_hat=1503\n","Skipping y_hat=1160\n","Skipping y_hat=2538\n","Skipping y_hat=1312\n","Skipping y_hat=1805\n","Skipping y_hat=2210\n"," 92% 76/83 [00:02<00:00, 35.52it/s]Skipping y_hat=1202\n","Skipping y_hat=603\n","Skipping y_hat=1000\n","Skipping y_hat=2294\n","Skipping y_hat=600\n","Skipping y_hat=2403\n","Skipping y_hat=2209\n","Skipping y_hat=2061\n"," 96% 80/83 [00:02<00:00, 35.66it/s]Skipping y_hat=2099\n","Skipping y_hat=2223\n","Skipping y_hat=0\n","100% 83/83 [00:02<00:00, 34.41it/s]\n","accuracy of 10000 examples: 9910/10000 (99.1%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.96it/s]\n","accuracy of 10000 examples: 9912/10000 (99.11999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1105\n","Skipping y_hat=400\n","Skipping y_hat=664\n","Skipping y_hat=1283\n","  5% 4/83 [00:00<00:02, 33.97it/s]Skipping y_hat=697\n","Skipping y_hat=1499\n","Skipping y_hat=1064\n","Skipping y_hat=502\n"," 10% 8/83 [00:00<00:02, 34.09it/s]Skipping y_hat=1490\n","Skipping y_hat=665\n","Skipping y_hat=500\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 34.09it/s]Skipping y_hat=1300\n","Skipping y_hat=500\n","Skipping y_hat=602\n","Skipping y_hat=1200\n","Skipping y_hat=1099\n","Skipping y_hat=1164\n","Skipping y_hat=1799\n","Skipping y_hat=1660\n","Skipping y_hat=762\n","Skipping y_hat=1501\n"," 19% 16/83 [00:00<00:01, 34.23it/s]Skipping y_hat=2199\n","Skipping y_hat=1199\n","Skipping y_hat=1699\n"," 24% 20/83 [00:00<00:01, 33.95it/s]Skipping y_hat=599\n","Skipping y_hat=2091\n","Skipping y_hat=1832\n","Skipping y_hat=1097\n","Skipping y_hat=2083\n","Skipping y_hat=1599\n","Skipping y_hat=1212\n","Skipping y_hat=997\n","Skipping y_hat=647\n","Skipping y_hat=1446\n","Skipping y_hat=2161\n","Skipping y_hat=1592\n","Skipping y_hat=2299\n","Skipping y_hat=1099\n"," 29% 24/83 [00:00<00:01, 33.61it/s]Skipping y_hat=1795\n","Skipping y_hat=1299\n","Skipping y_hat=1193\n","Skipping y_hat=1786\n","Skipping y_hat=2298\n","Skipping y_hat=100\n"," 39% 32/83 [00:00<00:01, 33.82it/s]Skipping y_hat=699\n","Skipping y_hat=109\n","Skipping y_hat=169\n"," 43% 36/83 [00:01<00:01, 33.96it/s]Skipping y_hat=1193\n","Skipping y_hat=500\n","Skipping y_hat=266\n"," 48% 40/83 [00:01<00:01, 33.90it/s]Skipping y_hat=500\n","Skipping y_hat=1192\n"," 53% 44/83 [00:01<00:01, 34.07it/s]Skipping y_hat=207\n"," 58% 48/83 [00:01<00:01, 34.09it/s]Skipping y_hat=1099\n","Skipping y_hat=959\n","Skipping y_hat=999\n","Skipping y_hat=699\n"," 63% 52/83 [00:01<00:00, 34.00it/s]Skipping y_hat=1299\n","Skipping y_hat=100\n"," 72% 60/83 [00:01<00:00, 33.98it/s]Skipping y_hat=719\n"," 77% 64/83 [00:01<00:00, 33.97it/s]Skipping y_hat=188\n"," 82% 68/83 [00:01<00:00, 34.25it/s]Skipping y_hat=700\n","Skipping y_hat=1903\n","Skipping y_hat=2491\n","Skipping y_hat=2955\n","Skipping y_hat=1160\n","Skipping y_hat=2538\n","Skipping y_hat=3504\n","Skipping y_hat=2403\n","Skipping y_hat=2101\n","Skipping y_hat=2210\n"," 87% 72/83 [00:02<00:00, 34.42it/s]Skipping y_hat=59\n"," 92% 76/83 [00:02<00:00, 35.02it/s]Skipping y_hat=1202\n","Skipping y_hat=2230\n","Skipping y_hat=2305\n","Skipping y_hat=2294\n","Skipping y_hat=600\n","Skipping y_hat=2403\n","Skipping y_hat=1601\n","Skipping y_hat=2356\n","Skipping y_hat=2209\n"," 96% 80/83 [00:02<00:00, 35.10it/s]Skipping y_hat=1082\n","Skipping y_hat=2099\n","Skipping y_hat=1817\n","Skipping y_hat=0\n","100% 83/83 [00:02<00:00, 34.34it/s]\n","accuracy of 10000 examples: 9909/10000 (99.09%)\n","\n","Test Results:\n","test: 99.09%\n","\n","iter 40000: train loss 1.3336, val loss 1.3356\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=994\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=300\n","  5% 4/83 [00:00<00:02, 33.47it/s]Skipping y_hat=590\n","Skipping y_hat=501\n","Skipping y_hat=1129\n","Skipping y_hat=1461\n"," 10% 8/83 [00:00<00:02, 33.62it/s]Skipping y_hat=594\n","Skipping y_hat=898\n","Skipping y_hat=1200\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 33.15it/s]Skipping y_hat=1300\n","Skipping y_hat=1099\n","Skipping y_hat=205\n","Skipping y_hat=1520\n"," 19% 16/83 [00:00<00:01, 33.69it/s]Skipping y_hat=1400\n","Skipping y_hat=1083\n","Skipping y_hat=1008\n"," 24% 20/83 [00:00<00:01, 33.38it/s]Skipping y_hat=1596\n","Skipping y_hat=1494\n","Skipping y_hat=1798\n","Skipping y_hat=1599\n","Skipping y_hat=1701\n","Skipping y_hat=701\n","Skipping y_hat=1293\n","Skipping y_hat=1275\n"," 29% 24/83 [00:00<00:01, 32.99it/s]Skipping y_hat=1899\n","Skipping y_hat=374\n","Skipping y_hat=1528\n","Skipping y_hat=1299\n","Skipping y_hat=954\n","Skipping y_hat=1296\n","Skipping y_hat=50\n"," 34% 28/83 [00:00<00:01, 32.98it/s]Skipping y_hat=103\n"," 39% 32/83 [00:00<00:01, 33.13it/s]Skipping y_hat=79\n","Skipping y_hat=299\n"," 43% 36/83 [00:01<00:01, 33.78it/s]Skipping y_hat=214\n","Skipping y_hat=899\n","Skipping y_hat=799\n","Skipping y_hat=1099\n"," 48% 40/83 [00:01<00:01, 33.48it/s]Skipping y_hat=1099\n","Skipping y_hat=500\n","Skipping y_hat=101\n","Skipping y_hat=760\n"," 53% 44/83 [00:01<00:01, 33.35it/s]Skipping y_hat=499\n","Skipping y_hat=499\n"," 58% 48/83 [00:01<00:01, 33.42it/s]Skipping y_hat=299\n","Skipping y_hat=999\n"," 63% 52/83 [00:01<00:00, 33.55it/s]Skipping y_hat=500\n","Skipping y_hat=400\n","Skipping y_hat=999\n","Skipping y_hat=100\n"," 72% 60/83 [00:01<00:00, 33.63it/s]Skipping y_hat=310\n"," 77% 64/83 [00:01<00:00, 34.02it/s]Skipping y_hat=830\n"," 82% 68/83 [00:02<00:00, 34.25it/s]Skipping y_hat=539\n","Skipping y_hat=1718\n","Skipping y_hat=3919\n","Skipping y_hat=810\n","Skipping y_hat=2944\n","Skipping y_hat=2929\n","Skipping y_hat=2107\n"," 92% 76/83 [00:02<00:00, 34.92it/s]Skipping y_hat=1497\n","Skipping y_hat=1219\n","Skipping y_hat=1693\n","Skipping y_hat=2196\n","Skipping y_hat=1200\n","Skipping y_hat=2109\n","Skipping y_hat=1401\n"," 96% 80/83 [00:02<00:00, 34.61it/s]Skipping y_hat=2279\n","Skipping y_hat=2099\n","Skipping y_hat=1412\n","Skipping y_hat=701\n","Skipping y_hat=697\n","100% 83/83 [00:02<00:00, 33.94it/s]\n","accuracy of 10000 examples: 9908/10000 (99.08%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 39.13it/s]\n","accuracy of 10000 examples: 9901/10000 (99.00999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=592\n","Skipping y_hat=1001\n","Skipping y_hat=300\n","  5% 4/83 [00:00<00:02, 34.03it/s]Skipping y_hat=590\n","Skipping y_hat=639\n"," 10% 8/83 [00:00<00:02, 34.22it/s]Skipping y_hat=898\n","Skipping y_hat=1268\n","Skipping y_hat=300\n","Skipping y_hat=999\n"," 14% 12/83 [00:00<00:02, 33.70it/s]Skipping y_hat=1300\n","Skipping y_hat=1009\n"," 19% 16/83 [00:00<00:01, 34.31it/s]Skipping y_hat=1898\n","Skipping y_hat=1248\n","Skipping y_hat=1482\n","Skipping y_hat=1072\n","Skipping y_hat=1008\n"," 24% 20/83 [00:00<00:01, 34.06it/s]Skipping y_hat=1596\n","Skipping y_hat=2154\n","Skipping y_hat=1798\n","Skipping y_hat=938\n","Skipping y_hat=1599\n","Skipping y_hat=701\n","Skipping y_hat=1275\n","Skipping y_hat=2299\n"," 29% 24/83 [00:00<00:01, 33.65it/s]Skipping y_hat=1301\n","Skipping y_hat=782\n","Skipping y_hat=374\n","Skipping y_hat=1528\n","Skipping y_hat=1299\n","Skipping y_hat=1296\n","Skipping y_hat=2298\n"," 43% 36/83 [00:01<00:01, 34.49it/s]Skipping y_hat=214\n","Skipping y_hat=899\n","Skipping y_hat=799\n"," 48% 40/83 [00:01<00:01, 34.45it/s]Skipping y_hat=1099\n"," 53% 44/83 [00:01<00:01, 34.50it/s]Skipping y_hat=499\n","Skipping y_hat=499\n"," 58% 48/83 [00:01<00:01, 34.69it/s]Skipping y_hat=991\n","Skipping y_hat=999\n","Skipping y_hat=276\n"," 63% 52/83 [00:01<00:00, 34.72it/s]Skipping y_hat=502\n","Skipping y_hat=500\n","Skipping y_hat=400\n","Skipping y_hat=202\n","Skipping y_hat=999\n","Skipping y_hat=100\n"," 67% 56/83 [00:01<00:00, 34.58it/s]Skipping y_hat=499\n"," 77% 64/83 [00:01<00:00, 34.72it/s]Skipping y_hat=803\n","Skipping y_hat=830\n"," 82% 68/83 [00:01<00:00, 34.71it/s]Skipping y_hat=679\n","Skipping y_hat=539\n","Skipping y_hat=2919\n","Skipping y_hat=2494\n","Skipping y_hat=2272\n","Skipping y_hat=2074\n","Skipping y_hat=3929\n","Skipping y_hat=2264\n","Skipping y_hat=2199\n"," 92% 76/83 [00:02<00:00, 35.62it/s]Skipping y_hat=1219\n","Skipping y_hat=1999\n","Skipping y_hat=2300\n","Skipping y_hat=2196\n"," 96% 80/83 [00:02<00:00, 35.84it/s]Skipping y_hat=2290\n","Skipping y_hat=697\n","Skipping y_hat=1260\n","100% 83/83 [00:02<00:00, 34.82it/s]\n","accuracy of 10000 examples: 9920/10000 (99.2%)\n","\n","Test Results:\n","test: 99.20%\n","\n","iter 41000: train loss 1.3283, val loss 1.3352\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=935\n","Skipping y_hat=1105\n","Skipping y_hat=860\n","  5% 4/83 [00:00<00:02, 33.79it/s]Skipping y_hat=1152\n","Skipping y_hat=501\n","Skipping y_hat=1688\n"," 10% 8/83 [00:00<00:02, 33.88it/s]Skipping y_hat=1401\n","Skipping y_hat=1400\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 33.88it/s]Skipping y_hat=1300\n","Skipping y_hat=1200\n","Skipping y_hat=1300\n","Skipping y_hat=1660\n"," 19% 16/83 [00:00<00:01, 33.99it/s]Skipping y_hat=2007\n","Skipping y_hat=2199\n","Skipping y_hat=1703\n","Skipping y_hat=1203\n","Skipping y_hat=1718\n","Skipping y_hat=1230\n","Skipping y_hat=1707\n","Skipping y_hat=2301\n"," 24% 20/83 [00:00<00:01, 33.71it/s]Skipping y_hat=1040\n","Skipping y_hat=2402\n","Skipping y_hat=912\n","Skipping y_hat=1635\n","Skipping y_hat=1204\n","Skipping y_hat=1517\n","Skipping y_hat=1800\n","Skipping y_hat=1724\n","Skipping y_hat=1489\n","Skipping y_hat=1300\n"," 29% 24/83 [00:00<00:01, 33.18it/s]Skipping y_hat=1600\n","Skipping y_hat=1501\n","Skipping y_hat=1300\n","Skipping y_hat=1605\n"," 34% 28/83 [00:00<00:01, 33.31it/s]Skipping y_hat=170\n"," 39% 32/83 [00:00<00:01, 33.28it/s]Skipping y_hat=700\n"," 43% 36/83 [00:01<00:01, 33.74it/s]Skipping y_hat=520\n"," 48% 40/83 [00:01<00:01, 34.04it/s]Skipping y_hat=929\n","Skipping y_hat=994\n","Skipping y_hat=500\n","Skipping y_hat=600\n"," 53% 44/83 [00:01<00:01, 34.03it/s]Skipping y_hat=1032\n","Skipping y_hat=702\n"," 58% 48/83 [00:01<00:01, 33.57it/s]Skipping y_hat=1199\n","Skipping y_hat=102\n","Skipping y_hat=1040\n","Skipping y_hat=999\n","Skipping y_hat=943\n","Skipping y_hat=1000\n"," 63% 52/83 [00:01<00:00, 33.55it/s]Skipping y_hat=679\n","Skipping y_hat=295\n","Skipping y_hat=400\n","Skipping y_hat=999\n","Skipping y_hat=738\n"," 67% 56/83 [00:01<00:00, 33.70it/s]Skipping y_hat=269\n"," 72% 60/83 [00:01<00:00, 34.07it/s]Skipping y_hat=600\n"," 82% 68/83 [00:02<00:00, 34.33it/s]Skipping y_hat=2198\n","Skipping y_hat=3919\n","Skipping y_hat=2753\n","Skipping y_hat=3960\n","Skipping y_hat=3247\n","Skipping y_hat=2577\n","Skipping y_hat=2101\n"," 92% 76/83 [00:02<00:00, 35.30it/s]Skipping y_hat=1811\n","Skipping y_hat=2304\n","Skipping y_hat=1619\n","Skipping y_hat=1600\n","Skipping y_hat=1100\n","Skipping y_hat=2300\n","Skipping y_hat=1802\n","Skipping y_hat=2204\n","Skipping y_hat=603\n","Skipping y_hat=1401\n","Skipping y_hat=2201\n","Skipping y_hat=1803\n","Skipping y_hat=2246\n","Skipping y_hat=2192\n","Skipping y_hat=2106\n"," 96% 80/83 [00:02<00:00, 35.12it/s]Skipping y_hat=1817\n","Skipping y_hat=1701\n","100% 83/83 [00:02<00:00, 34.23it/s]\n","accuracy of 10000 examples: 9907/10000 (99.07000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 37.69it/s]\n","accuracy of 10000 examples: 9902/10000 (99.02%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=562\n","Skipping y_hat=935\n","Skipping y_hat=1105\n","Skipping y_hat=850\n","Skipping y_hat=1601\n","  4% 3/83 [00:00<00:02, 29.76it/s]Skipping y_hat=1506\n","Skipping y_hat=300\n","Skipping y_hat=697\n","Skipping y_hat=1152\n","Skipping y_hat=501\n","  8% 7/83 [00:00<00:02, 30.31it/s]Skipping y_hat=1575\n","Skipping y_hat=1400\n"," 13% 11/83 [00:00<00:02, 31.27it/s]Skipping y_hat=300\n","Skipping y_hat=1300\n","Skipping y_hat=1200\n","Skipping y_hat=1301\n","Skipping y_hat=1300\n","Skipping y_hat=1660\n"," 18% 15/83 [00:00<00:02, 31.14it/s]Skipping y_hat=1728\n","Skipping y_hat=499\n","Skipping y_hat=600\n","Skipping y_hat=2007\n","Skipping y_hat=1203\n","Skipping y_hat=1703\n","Skipping y_hat=1203\n","Skipping y_hat=1718\n","Skipping y_hat=1602\n","Skipping y_hat=1707\n"," 23% 19/83 [00:00<00:02, 31.01it/s]Skipping y_hat=1699\n","Skipping y_hat=1492\n","Skipping y_hat=2402\n","Skipping y_hat=1608\n","Skipping y_hat=912\n","Skipping y_hat=1635\n","Skipping y_hat=1204\n","Skipping y_hat=1517\n","Skipping y_hat=1800\n","Skipping y_hat=1383\n","Skipping y_hat=1240\n","Skipping y_hat=1724\n"," 28% 23/83 [00:00<00:01, 30.87it/s]Skipping y_hat=1255\n","Skipping y_hat=1300\n","Skipping y_hat=1098\n","Skipping y_hat=1600\n","Skipping y_hat=1300\n","Skipping y_hat=1304\n","Skipping y_hat=696\n","Skipping y_hat=1605\n","Skipping y_hat=100\n"," 37% 31/83 [00:00<00:01, 32.34it/s]Skipping y_hat=39\n","Skipping y_hat=749\n"," 42% 35/83 [00:01<00:01, 32.97it/s]Skipping y_hat=520\n"," 47% 39/83 [00:01<00:01, 33.65it/s]Skipping y_hat=111\n","Skipping y_hat=994\n","Skipping y_hat=500\n","Skipping y_hat=600\n"," 57% 47/83 [00:01<00:01, 34.16it/s]Skipping y_hat=797\n","Skipping y_hat=936\n","Skipping y_hat=999\n","Skipping y_hat=943\n"," 61% 51/83 [00:01<00:00, 34.30it/s]Skipping y_hat=106\n","Skipping y_hat=1000\n","Skipping y_hat=679\n","Skipping y_hat=1132\n","Skipping y_hat=300\n","Skipping y_hat=400\n","Skipping y_hat=222\n"," 71% 59/83 [00:01<00:00, 34.15it/s]Skipping y_hat=200\n"," 86% 71/83 [00:02<00:00, 34.62it/s]Skipping y_hat=2198\n","Skipping y_hat=2753\n","Skipping y_hat=3247\n","Skipping y_hat=2149\n","Skipping y_hat=2393\n","Skipping y_hat=1907\n","Skipping y_hat=2101\n"," 90% 75/83 [00:02<00:00, 34.84it/s]Skipping y_hat=90\n","Skipping y_hat=2304\n","Skipping y_hat=1619\n","Skipping y_hat=1903\n","Skipping y_hat=1600\n","Skipping y_hat=1462\n","Skipping y_hat=1100\n","Skipping y_hat=1802\n","Skipping y_hat=2204\n","Skipping y_hat=603\n"," 95% 79/83 [00:02<00:00, 34.82it/s]Skipping y_hat=600\n","Skipping y_hat=2104\n","Skipping y_hat=1803\n","Skipping y_hat=2246\n","Skipping y_hat=2106\n","Skipping y_hat=2100\n","Skipping y_hat=1977\n","Skipping y_hat=1817\n","100% 83/83 [00:02<00:00, 33.48it/s]\n","accuracy of 10000 examples: 9894/10000 (98.94%)\n","\n","Test Results:\n","test: 98.94%\n","\n","iter 42000: train loss 1.3267, val loss 1.3369\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=904\n","Skipping y_hat=1096\n","Skipping y_hat=1050\n","Skipping y_hat=1500\n","Skipping y_hat=500\n","Skipping y_hat=1050\n","Skipping y_hat=292\n","Skipping y_hat=1000\n","Skipping y_hat=1101\n","Skipping y_hat=1000\n","  5% 4/83 [00:00<00:02, 33.71it/s]Skipping y_hat=704\n","Skipping y_hat=880\n","Skipping y_hat=630\n","Skipping y_hat=760\n","Skipping y_hat=580\n","Skipping y_hat=330\n","Skipping y_hat=520\n"," 10% 8/83 [00:00<00:02, 31.83it/s]Skipping y_hat=379\n","Skipping y_hat=370\n","Skipping y_hat=788\n","Skipping y_hat=898\n","Skipping y_hat=1600\n","Skipping y_hat=600\n","Skipping y_hat=1230\n","Skipping y_hat=1108\n","Skipping y_hat=491\n"," 14% 12/83 [00:00<00:02, 30.55it/s]Skipping y_hat=1390\n","Skipping y_hat=1200\n","Skipping y_hat=1490\n"," 19% 16/83 [00:00<00:02, 31.08it/s]Skipping y_hat=1092\n","Skipping y_hat=2098\n","Skipping y_hat=1898\n","Skipping y_hat=1960\n","Skipping y_hat=499\n","Skipping y_hat=808\n","Skipping y_hat=790\n","Skipping y_hat=1560\n","Skipping y_hat=1085\n","Skipping y_hat=2199\n","Skipping y_hat=1703\n","Skipping y_hat=1003\n","Skipping y_hat=1210\n","Skipping y_hat=1595\n","Skipping y_hat=730\n","Skipping y_hat=1109\n","Skipping y_hat=932\n","Skipping y_hat=1395\n","Skipping y_hat=1854\n","Skipping y_hat=1179\n"," 24% 20/83 [00:00<00:01, 31.67it/s]Skipping y_hat=1380\n","Skipping y_hat=1797\n","Skipping y_hat=1601\n","Skipping y_hat=1399\n","Skipping y_hat=1798\n","Skipping y_hat=1609\n","Skipping y_hat=1303\n","Skipping y_hat=1293\n","Skipping y_hat=1299\n"," 29% 24/83 [00:00<00:01, 32.42it/s]Skipping y_hat=1603\n","Skipping y_hat=818\n","Skipping y_hat=1608\n","Skipping y_hat=801\n","Skipping y_hat=1896\n","Skipping y_hat=70\n"," 34% 28/83 [00:00<00:01, 32.82it/s]Skipping y_hat=90\n","Skipping y_hat=399\n"," 39% 32/83 [00:00<00:01, 33.29it/s]Skipping y_hat=630\n"," 43% 36/83 [00:01<00:01, 33.94it/s]Skipping y_hat=990\n","Skipping y_hat=1000\n","Skipping y_hat=214\n","Skipping y_hat=630\n","Skipping y_hat=991\n","Skipping y_hat=111\n"," 48% 40/83 [00:01<00:01, 34.03it/s]Skipping y_hat=899\n","Skipping y_hat=500\n","Skipping y_hat=1340\n","Skipping y_hat=540\n","Skipping y_hat=340\n","Skipping y_hat=399\n","Skipping y_hat=660\n","Skipping y_hat=1072\n"," 53% 44/83 [00:01<00:01, 34.15it/s]Skipping y_hat=660\n","Skipping y_hat=999\n","Skipping y_hat=760\n"," 58% 48/83 [00:01<00:01, 33.88it/s]Skipping y_hat=1000\n"," 63% 52/83 [00:01<00:00, 34.18it/s]Skipping y_hat=295\n","Skipping y_hat=500\n","Skipping y_hat=820\n","Skipping y_hat=299\n","Skipping y_hat=999\n","Skipping y_hat=700\n","Skipping y_hat=900\n","Skipping y_hat=930\n"," 67% 56/83 [00:01<00:00, 34.27it/s]Skipping y_hat=1070\n"," 72% 60/83 [00:01<00:00, 34.40it/s]Skipping y_hat=170\n","Skipping y_hat=630\n"," 82% 68/83 [00:02<00:00, 34.71it/s]Skipping y_hat=140\n","Skipping y_hat=2198\n","Skipping y_hat=900\n","Skipping y_hat=2044\n","Skipping y_hat=1473\n","Skipping y_hat=1428\n","Skipping y_hat=2942\n","Skipping y_hat=2945\n","Skipping y_hat=1422\n","Skipping y_hat=2100\n","Skipping y_hat=2199\n"," 87% 72/83 [00:02<00:00, 34.83it/s]Skipping y_hat=10\n","Skipping y_hat=60\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=30\n","Skipping y_hat=10\n","Skipping y_hat=70\n"," 92% 76/83 [00:02<00:00, 35.08it/s]Skipping y_hat=1407\n","Skipping y_hat=1693\n","Skipping y_hat=2500\n","Skipping y_hat=1702\n","Skipping y_hat=1601\n","Skipping y_hat=950\n","Skipping y_hat=2109\n"," 96% 80/83 [00:02<00:00, 35.13it/s]Skipping y_hat=1794\n","Skipping y_hat=1594\n","100% 83/83 [00:02<00:00, 33.95it/s]\n","accuracy of 10000 examples: 9826/10000 (98.26%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.55it/s]\n","accuracy of 10000 examples: 9843/10000 (98.42999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1050\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=1050\n","Skipping y_hat=625\n","Skipping y_hat=292\n","Skipping y_hat=1101\n","Skipping y_hat=1000\n","  5% 4/83 [00:00<00:02, 32.36it/s]Skipping y_hat=880\n","Skipping y_hat=630\n","Skipping y_hat=760\n","Skipping y_hat=888\n","Skipping y_hat=580\n","Skipping y_hat=330\n","Skipping y_hat=520\n"," 10% 8/83 [00:00<00:02, 30.90it/s]Skipping y_hat=370\n","Skipping y_hat=1400\n","Skipping y_hat=600\n","Skipping y_hat=703\n","Skipping y_hat=1230\n","Skipping y_hat=499\n"," 14% 12/83 [00:00<00:02, 31.41it/s]Skipping y_hat=1390\n","Skipping y_hat=980\n","Skipping y_hat=1208\n","Skipping y_hat=1390\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:02, 31.56it/s]Skipping y_hat=2098\n","Skipping y_hat=499\n","Skipping y_hat=808\n","Skipping y_hat=2199\n","Skipping y_hat=1595\n","Skipping y_hat=1707\n","Skipping y_hat=1854\n","Skipping y_hat=1104\n","Skipping y_hat=1343\n"," 24% 20/83 [00:00<00:01, 31.59it/s]Skipping y_hat=1380\n","Skipping y_hat=1797\n","Skipping y_hat=1601\n","Skipping y_hat=1399\n","Skipping y_hat=1798\n","Skipping y_hat=1599\n","Skipping y_hat=1609\n","Skipping y_hat=1180\n","Skipping y_hat=809\n","Skipping y_hat=1299\n"," 29% 24/83 [00:00<00:01, 31.94it/s]Skipping y_hat=1603\n","Skipping y_hat=1608\n","Skipping y_hat=801\n","Skipping y_hat=1360\n","Skipping y_hat=70\n"," 34% 28/83 [00:00<00:01, 32.61it/s]Skipping y_hat=90\n","Skipping y_hat=399\n","Skipping y_hat=40\n"," 39% 32/83 [00:00<00:01, 33.23it/s]Skipping y_hat=90\n"," 43% 36/83 [00:01<00:01, 33.38it/s]Skipping y_hat=990\n","Skipping y_hat=477\n","Skipping y_hat=630\n","Skipping y_hat=111\n"," 48% 40/83 [00:01<00:01, 33.23it/s]Skipping y_hat=296\n","Skipping y_hat=500\n","Skipping y_hat=1340\n","Skipping y_hat=540\n","Skipping y_hat=340\n"," 53% 44/83 [00:01<00:01, 32.83it/s]Skipping y_hat=660\n","Skipping y_hat=499\n","Skipping y_hat=207\n","Skipping y_hat=760\n","Skipping y_hat=1570\n"," 58% 48/83 [00:01<00:01, 32.56it/s]Skipping y_hat=1006\n","Skipping y_hat=396\n","Skipping y_hat=1000\n"," 63% 52/83 [00:01<00:00, 33.09it/s]Skipping y_hat=769\n","Skipping y_hat=500\n","Skipping y_hat=820\n","Skipping y_hat=299\n","Skipping y_hat=999\n","Skipping y_hat=700\n","Skipping y_hat=900\n","Skipping y_hat=930\n"," 67% 56/83 [00:01<00:00, 33.48it/s]Skipping y_hat=310\n","Skipping y_hat=1070\n"," 72% 60/83 [00:01<00:00, 33.82it/s]Skipping y_hat=170\n","Skipping y_hat=630\n"," 82% 68/83 [00:02<00:00, 33.99it/s]Skipping y_hat=140\n","Skipping y_hat=2198\n","Skipping y_hat=2044\n","Skipping y_hat=1473\n","Skipping y_hat=1505\n","Skipping y_hat=1428\n","Skipping y_hat=2006\n","Skipping y_hat=2773\n","Skipping y_hat=2942\n","Skipping y_hat=2097\n","Skipping y_hat=2295\n","Skipping y_hat=1422\n","Skipping y_hat=2199\n"," 87% 72/83 [00:02<00:00, 34.01it/s]Skipping y_hat=29\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=70\n"," 92% 76/83 [00:02<00:00, 33.45it/s]Skipping y_hat=2792\n","Skipping y_hat=1809\n","Skipping y_hat=1407\n","Skipping y_hat=1462\n","Skipping y_hat=1693\n","Skipping y_hat=977\n","Skipping y_hat=1702\n","Skipping y_hat=2201\n","Skipping y_hat=1420\n","Skipping y_hat=1601\n","Skipping y_hat=1269\n","Skipping y_hat=1710\n"," 96% 80/83 [00:02<00:00, 33.14it/s]Skipping y_hat=1794\n","100% 83/83 [00:02<00:00, 33.12it/s]\n","accuracy of 10000 examples: 9849/10000 (98.49%)\n","\n","Test Results:\n","test: 98.49%\n","\n","iter 43000: train loss 1.3333, val loss 1.3317\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=935\n","Skipping y_hat=800\n","Skipping y_hat=699\n","Skipping y_hat=732\n","Skipping y_hat=300\n","  5% 4/83 [00:00<00:02, 31.74it/s]Skipping y_hat=1230\n"," 10% 8/83 [00:00<00:02, 32.51it/s]Skipping y_hat=1120\n","Skipping y_hat=243\n","Skipping y_hat=399\n","Skipping y_hat=397\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 31.81it/s]Skipping y_hat=1300\n","Skipping y_hat=895\n","Skipping y_hat=301\n","Skipping y_hat=1301\n","Skipping y_hat=1296\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:02, 31.68it/s]Skipping y_hat=499\n","Skipping y_hat=600\n","Skipping y_hat=1100\n","Skipping y_hat=2500\n","Skipping y_hat=861\n","Skipping y_hat=1008\n","Skipping y_hat=897\n"," 24% 20/83 [00:00<00:01, 31.60it/s]Skipping y_hat=599\n","Skipping y_hat=1635\n","Skipping y_hat=2299\n","Skipping y_hat=1300\n"," 29% 24/83 [00:00<00:01, 31.75it/s]Skipping y_hat=1323\n","Skipping y_hat=1000\n","Skipping y_hat=1099\n","Skipping y_hat=1600\n"," 34% 28/83 [00:00<00:01, 31.95it/s]Skipping y_hat=300\n"," 43% 36/83 [00:01<00:01, 32.73it/s]Skipping y_hat=422\n","Skipping y_hat=194\n","Skipping y_hat=630\n","Skipping y_hat=1080\n"," 48% 40/83 [00:01<00:01, 32.78it/s]Skipping y_hat=700\n","Skipping y_hat=232\n","Skipping y_hat=399\n"," 53% 44/83 [00:01<00:01, 32.85it/s]Skipping y_hat=207\n","Skipping y_hat=295\n"," 63% 52/83 [00:01<00:00, 33.63it/s]Skipping y_hat=297\n","Skipping y_hat=300\n","Skipping y_hat=460\n","Skipping y_hat=700\n"," 67% 56/83 [00:01<00:00, 33.86it/s]Skipping y_hat=180\n"," 77% 64/83 [00:01<00:00, 34.33it/s]Skipping y_hat=739\n"," 82% 68/83 [00:02<00:00, 34.56it/s]Skipping y_hat=2929\n","Skipping y_hat=2367\n","Skipping y_hat=2594\n","Skipping y_hat=1010\n","Skipping y_hat=2044\n","Skipping y_hat=2323\n","Skipping y_hat=2491\n","Skipping y_hat=3155\n","Skipping y_hat=2538\n","Skipping y_hat=2029\n","Skipping y_hat=2577\n","Skipping y_hat=2101\n","Skipping y_hat=2199\n"," 87% 72/83 [00:02<00:00, 34.89it/s]Skipping y_hat=50\n"," 92% 76/83 [00:02<00:00, 35.41it/s]Skipping y_hat=2280\n","Skipping y_hat=2300\n","Skipping y_hat=2095\n","Skipping y_hat=2230\n","Skipping y_hat=1812\n","Skipping y_hat=2201\n","Skipping y_hat=1400\n","Skipping y_hat=2246\n","Skipping y_hat=2299\n"," 96% 80/83 [00:02<00:00, 35.54it/s]Skipping y_hat=2099\n","Skipping y_hat=2002\n","Skipping y_hat=1817\n","100% 83/83 [00:02<00:00, 33.70it/s]\n","accuracy of 10000 examples: 9893/10000 (98.92999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 39.16it/s]\n","accuracy of 10000 examples: 9893/10000 (98.92999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=562\n","Skipping y_hat=935\n","Skipping y_hat=699\n","  5% 4/83 [00:00<00:02, 34.91it/s]Skipping y_hat=1892\n","Skipping y_hat=1196\n","Skipping y_hat=1753\n","Skipping y_hat=501\n","Skipping y_hat=1300\n"," 10% 8/83 [00:00<00:02, 34.84it/s]Skipping y_hat=527\n","Skipping y_hat=1120\n","Skipping y_hat=399\n","Skipping y_hat=397\n","Skipping y_hat=300\n","Skipping y_hat=499\n"," 14% 12/83 [00:00<00:02, 34.86it/s]Skipping y_hat=1300\n","Skipping y_hat=895\n","Skipping y_hat=301\n","Skipping y_hat=820\n","Skipping y_hat=1099\n"," 19% 16/83 [00:00<00:01, 35.04it/s]Skipping y_hat=600\n","Skipping y_hat=1400\n","Skipping y_hat=2500\n","Skipping y_hat=861\n","Skipping y_hat=1008\n"," 24% 20/83 [00:00<00:01, 34.90it/s]Skipping y_hat=599\n","Skipping y_hat=1094\n","Skipping y_hat=1399\n","Skipping y_hat=1635\n","Skipping y_hat=1204\n","Skipping y_hat=2299\n"," 29% 24/83 [00:00<00:01, 34.70it/s]Skipping y_hat=1323\n","Skipping y_hat=1600\n","Skipping y_hat=1860\n","Skipping y_hat=1896\n"," 34% 28/83 [00:00<00:01, 33.71it/s]Skipping y_hat=300\n"," 43% 36/83 [00:01<00:01, 34.43it/s]Skipping y_hat=194\n","Skipping y_hat=630\n","Skipping y_hat=980\n","Skipping y_hat=350\n"," 48% 40/83 [00:01<00:01, 33.78it/s]Skipping y_hat=700\n","Skipping y_hat=232\n","Skipping y_hat=399\n","Skipping y_hat=201\n"," 53% 44/83 [00:01<00:01, 33.32it/s]Skipping y_hat=999\n","Skipping y_hat=207\n","Skipping y_hat=295\n"," 58% 48/83 [00:01<00:01, 32.97it/s]Skipping y_hat=666\n"," 63% 52/83 [00:01<00:00, 33.32it/s]Skipping y_hat=297\n","Skipping y_hat=205\n","Skipping y_hat=460\n","Skipping y_hat=700\n"," 72% 60/83 [00:01<00:00, 33.42it/s]Skipping y_hat=170\n","Skipping y_hat=1099\n"," 77% 64/83 [00:01<00:00, 33.56it/s]Skipping y_hat=739\n","Skipping y_hat=380\n"," 82% 68/83 [00:02<00:00, 33.52it/s]Skipping y_hat=2919\n","Skipping y_hat=2367\n","Skipping y_hat=3664\n","Skipping y_hat=2594\n","Skipping y_hat=2588\n","Skipping y_hat=1010\n","Skipping y_hat=2044\n","Skipping y_hat=2491\n","Skipping y_hat=3155\n","Skipping y_hat=2538\n","Skipping y_hat=2929\n","Skipping y_hat=2283\n","Skipping y_hat=2577\n","Skipping y_hat=2101\n"," 92% 76/83 [00:02<00:00, 34.25it/s]Skipping y_hat=2280\n","Skipping y_hat=2300\n","Skipping y_hat=2095\n","Skipping y_hat=603\n","Skipping y_hat=2230\n","Skipping y_hat=1812\n","Skipping y_hat=1400\n","Skipping y_hat=1289\n","Skipping y_hat=2299\n","100% 83/83 [00:02<00:00, 33.97it/s]\n","accuracy of 10000 examples: 9876/10000 (98.76%)\n","\n","Test Results:\n","test: 98.76%\n","\n","iter 44000: train loss 1.3294, val loss 1.3322\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=400\n","Skipping y_hat=700\n","Skipping y_hat=1500\n","Skipping y_hat=399\n","Skipping y_hat=817\n","  5% 4/83 [00:00<00:02, 31.81it/s]Skipping y_hat=627\n","Skipping y_hat=1377\n","Skipping y_hat=697\n","Skipping y_hat=501\n","Skipping y_hat=1279\n","Skipping y_hat=1300\n"," 10% 8/83 [00:00<00:02, 32.32it/s]Skipping y_hat=594\n","Skipping y_hat=799\n","Skipping y_hat=1400\n","Skipping y_hat=696\n","Skipping y_hat=300\n","Skipping y_hat=499\n","Skipping y_hat=999\n"," 14% 12/83 [00:00<00:02, 32.46it/s]Skipping y_hat=1300\n","Skipping y_hat=888\n","Skipping y_hat=800\n"," 19% 16/83 [00:00<00:02, 32.89it/s]Skipping y_hat=1092\n","Skipping y_hat=600\n","Skipping y_hat=1400\n","Skipping y_hat=1097\n","Skipping y_hat=1796\n","Skipping y_hat=1092\n","Skipping y_hat=1306\n","Skipping y_hat=1497\n","Skipping y_hat=2301\n","Skipping y_hat=1348\n"," 24% 20/83 [00:00<00:01, 32.79it/s]Skipping y_hat=1380\n","Skipping y_hat=2032\n","Skipping y_hat=2099\n","Skipping y_hat=1095\n"," 29% 24/83 [00:00<00:01, 32.83it/s]Skipping y_hat=700\n","Skipping y_hat=1795\n","Skipping y_hat=1994\n","Skipping y_hat=1296\n","Skipping y_hat=489\n","Skipping y_hat=122\n"," 34% 28/83 [00:00<00:01, 32.91it/s]Skipping y_hat=173\n","Skipping y_hat=820\n","Skipping y_hat=610\n"," 43% 36/83 [00:01<00:01, 33.81it/s]Skipping y_hat=274\n"," 48% 40/83 [00:01<00:01, 34.03it/s]Skipping y_hat=198\n","Skipping y_hat=201\n"," 58% 48/83 [00:01<00:01, 33.91it/s]Skipping y_hat=929\n","Skipping y_hat=1080\n","Skipping y_hat=379\n"," 63% 52/83 [00:01<00:00, 33.71it/s]Skipping y_hat=1043\n","Skipping y_hat=300\n","Skipping y_hat=400\n","Skipping y_hat=460\n","Skipping y_hat=700\n"," 77% 64/83 [00:01<00:00, 34.15it/s]Skipping y_hat=400\n","Skipping y_hat=299\n","Skipping y_hat=140\n"," 82% 68/83 [00:02<00:00, 34.07it/s]Skipping y_hat=1010\n","Skipping y_hat=2042\n","Skipping y_hat=2059\n","Skipping y_hat=948\n","Skipping y_hat=2025\n","Skipping y_hat=2199\n","Skipping y_hat=1626\n"," 92% 76/83 [00:02<00:00, 34.75it/s]Skipping y_hat=1497\n","Skipping y_hat=1497\n","Skipping y_hat=1600\n","Skipping y_hat=1693\n","Skipping y_hat=1860\n","Skipping y_hat=2300\n","Skipping y_hat=840\n","Skipping y_hat=2900\n","Skipping y_hat=1803\n","Skipping y_hat=1993\n","Skipping y_hat=2109\n"," 96% 80/83 [00:02<00:00, 34.65it/s]Skipping y_hat=1594\n","100% 83/83 [00:02<00:00, 33.86it/s]\n","accuracy of 10000 examples: 9905/10000 (99.05000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.66it/s]\n","accuracy of 10000 examples: 9896/10000 (98.96000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=562\n","Skipping y_hat=400\n","Skipping y_hat=201\n","Skipping y_hat=500\n","Skipping y_hat=167\n","Skipping y_hat=817\n","  5% 4/83 [00:00<00:02, 32.52it/s]Skipping y_hat=627\n","Skipping y_hat=697\n","Skipping y_hat=501\n","Skipping y_hat=1279\n","Skipping y_hat=1300\n"," 10% 8/83 [00:00<00:02, 33.35it/s]Skipping y_hat=594\n","Skipping y_hat=1400\n","Skipping y_hat=696\n","Skipping y_hat=1199\n","Skipping y_hat=899\n","Skipping y_hat=300\n","Skipping y_hat=499\n","Skipping y_hat=999\n"," 14% 12/83 [00:00<00:02, 33.13it/s]Skipping y_hat=1300\n","Skipping y_hat=1799\n","Skipping y_hat=800\n","Skipping y_hat=1798\n"," 19% 16/83 [00:00<00:02, 33.28it/s]Skipping y_hat=1506\n","Skipping y_hat=600\n","Skipping y_hat=1706\n","Skipping y_hat=1796\n","Skipping y_hat=1193\n","Skipping y_hat=1092\n","Skipping y_hat=1008\n","Skipping y_hat=1348\n"," 24% 20/83 [00:00<00:01, 33.49it/s]Skipping y_hat=1380\n","Skipping y_hat=1040\n","Skipping y_hat=1621\n","Skipping y_hat=706\n","Skipping y_hat=1798\n"," 29% 24/83 [00:00<00:01, 33.60it/s]Skipping y_hat=1994\n","Skipping y_hat=1099\n","Skipping y_hat=1840\n","Skipping y_hat=1296\n","Skipping y_hat=489\n"," 34% 28/83 [00:00<00:01, 33.85it/s]Skipping y_hat=173\n","Skipping y_hat=400\n","Skipping y_hat=820\n"," 43% 36/83 [00:01<00:01, 34.61it/s]Skipping y_hat=950\n","Skipping y_hat=274\n"," 48% 40/83 [00:01<00:01, 34.58it/s]Skipping y_hat=201\n","Skipping y_hat=200\n"," 53% 44/83 [00:01<00:01, 34.31it/s]Skipping y_hat=706\n","Skipping y_hat=207\n"," 58% 48/83 [00:01<00:01, 33.99it/s]Skipping y_hat=599\n","Skipping y_hat=1080\n"," 63% 52/83 [00:01<00:00, 33.84it/s]Skipping y_hat=1043\n","Skipping y_hat=769\n","Skipping y_hat=460\n","Skipping y_hat=700\n"," 77% 64/83 [00:01<00:00, 34.10it/s]Skipping y_hat=140\n"," 82% 68/83 [00:02<00:00, 34.24it/s]Skipping y_hat=1079\n","Skipping y_hat=1718\n","Skipping y_hat=1921\n","Skipping y_hat=1010\n","Skipping y_hat=948\n","Skipping y_hat=2029\n","Skipping y_hat=1805\n","Skipping y_hat=1626\n"," 92% 76/83 [00:02<00:00, 35.08it/s]Skipping y_hat=1497\n","Skipping y_hat=1600\n","Skipping y_hat=1693\n","Skipping y_hat=2300\n","Skipping y_hat=2204\n","Skipping y_hat=2305\n","Skipping y_hat=2403\n","Skipping y_hat=1803\n","Skipping y_hat=1200\n","Skipping y_hat=2732\n","Skipping y_hat=1640\n","Skipping y_hat=2209\n"," 96% 80/83 [00:02<00:00, 35.03it/s]Skipping y_hat=2020\n","Skipping y_hat=1701\n","Skipping y_hat=1594\n","100% 83/83 [00:02<00:00, 34.29it/s]\n","accuracy of 10000 examples: 9897/10000 (98.97%)\n","\n","Test Results:\n","test: 98.97%\n","\n","iter 45000: train loss 1.3363, val loss 1.3345\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1096\n","Skipping y_hat=1001\n","Skipping y_hat=500\n","Skipping y_hat=1289\n","Skipping y_hat=304\n","Skipping y_hat=1101\n","Skipping y_hat=1090\n","  5% 4/83 [00:00<00:02, 32.46it/s]Skipping y_hat=407\n","Skipping y_hat=501\n","Skipping y_hat=1350\n"," 10% 8/83 [00:00<00:02, 33.04it/s]Skipping y_hat=240\n","Skipping y_hat=1100\n","Skipping y_hat=405\n","Skipping y_hat=1199\n","Skipping y_hat=703\n","Skipping y_hat=403\n","Skipping y_hat=1000\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 32.58it/s]Skipping y_hat=903\n","Skipping y_hat=1300\n","Skipping y_hat=295\n","Skipping y_hat=610\n"," 19% 16/83 [00:00<00:02, 32.80it/s]Skipping y_hat=1300\n","Skipping y_hat=600\n","Skipping y_hat=1105\n","Skipping y_hat=430\n","Skipping y_hat=1703\n","Skipping y_hat=1110\n","Skipping y_hat=1100\n","Skipping y_hat=2500\n","Skipping y_hat=1393\n"," 24% 20/83 [00:00<00:01, 32.07it/s]Skipping y_hat=1602\n","Skipping y_hat=1800\n","Skipping y_hat=1502\n","Skipping y_hat=1701\n","Skipping y_hat=401\n","Skipping y_hat=1299\n","Skipping y_hat=1099\n"," 29% 24/83 [00:00<00:01, 32.34it/s]Skipping y_hat=1304\n","Skipping y_hat=401\n","Skipping y_hat=1299\n","Skipping y_hat=802\n","Skipping y_hat=1840\n"," 34% 28/83 [00:00<00:01, 32.05it/s]Skipping y_hat=903\n"," 39% 32/83 [00:00<00:01, 32.63it/s]Skipping y_hat=160\n"," 43% 36/83 [00:01<00:01, 33.27it/s]Skipping y_hat=800\n","Skipping y_hat=109\n","Skipping y_hat=899\n","Skipping y_hat=111\n"," 48% 40/83 [00:01<00:01, 33.01it/s]Skipping y_hat=1099\n","Skipping y_hat=1202\n"," 53% 44/83 [00:01<00:01, 33.36it/s]Skipping y_hat=100\n","Skipping y_hat=207\n","Skipping y_hat=807\n","Skipping y_hat=500\n"," 58% 48/83 [00:01<00:01, 33.42it/s]Skipping y_hat=600\n"," 63% 52/83 [00:01<00:00, 33.52it/s]Skipping y_hat=500\n","Skipping y_hat=618\n","Skipping y_hat=400\n","Skipping y_hat=601\n","Skipping y_hat=100\n","Skipping y_hat=700\n","Skipping y_hat=801\n"," 67% 56/83 [00:01<00:00, 33.56it/s]Skipping y_hat=999\n","Skipping y_hat=200\n"," 72% 60/83 [00:01<00:00, 33.70it/s]Skipping y_hat=203\n"," 77% 64/83 [00:01<00:00, 33.72it/s]Skipping y_hat=400\n","Skipping y_hat=201\n"," 82% 68/83 [00:02<00:00, 33.92it/s]Skipping y_hat=150\n","Skipping y_hat=2919\n","Skipping y_hat=1473\n","Skipping y_hat=2056\n","Skipping y_hat=2955\n","Skipping y_hat=2773\n","Skipping y_hat=2029\n","Skipping y_hat=2283\n","Skipping y_hat=2101\n","Skipping y_hat=1797\n"," 87% 72/83 [00:02<00:00, 34.33it/s]Skipping y_hat=20\n"," 92% 76/83 [00:02<00:00, 34.99it/s]Skipping y_hat=1497\n","Skipping y_hat=905\n","Skipping y_hat=1702\n","Skipping y_hat=1401\n","Skipping y_hat=2202\n","Skipping y_hat=600\n","Skipping y_hat=2104\n","Skipping y_hat=2299\n"," 96% 80/83 [00:02<00:00, 35.12it/s]Skipping y_hat=2099\n","Skipping y_hat=2002\n","100% 83/83 [00:02<00:00, 33.72it/s]\n","accuracy of 10000 examples: 9900/10000 (99.0%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.52it/s]\n","accuracy of 10000 examples: 9907/10000 (99.07000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1096\n","Skipping y_hat=500\n","Skipping y_hat=1500\n","Skipping y_hat=304\n","Skipping y_hat=1090\n","Skipping y_hat=1000\n","  5% 4/83 [00:00<00:02, 32.83it/s]Skipping y_hat=1220\n","Skipping y_hat=501\n","Skipping y_hat=1300\n","Skipping y_hat=1350\n"," 10% 8/83 [00:00<00:02, 32.74it/s]Skipping y_hat=1100\n","Skipping y_hat=405\n","Skipping y_hat=1199\n","Skipping y_hat=703\n","Skipping y_hat=493\n","Skipping y_hat=1000\n","Skipping y_hat=1730\n"," 14% 12/83 [00:00<00:02, 31.92it/s]Skipping y_hat=1300\n","Skipping y_hat=395\n","Skipping y_hat=1301\n","Skipping y_hat=720\n","Skipping y_hat=1804\n"," 19% 16/83 [00:00<00:02, 31.51it/s]Skipping y_hat=1105\n","Skipping y_hat=1300\n","Skipping y_hat=1105\n","Skipping y_hat=430\n","Skipping y_hat=1703\n","Skipping y_hat=1110\n","Skipping y_hat=1100\n","Skipping y_hat=2500\n","Skipping y_hat=1148\n"," 24% 20/83 [00:00<00:01, 31.50it/s]Skipping y_hat=1602\n","Skipping y_hat=1502\n","Skipping y_hat=1800\n","Skipping y_hat=1383\n","Skipping y_hat=997\n","Skipping y_hat=706\n","Skipping y_hat=1202\n","Skipping y_hat=401\n","Skipping y_hat=1099\n"," 29% 24/83 [00:00<00:01, 32.06it/s]Skipping y_hat=1304\n","Skipping y_hat=401\n","Skipping y_hat=1299\n","Skipping y_hat=802\n"," 34% 28/83 [00:00<00:01, 32.47it/s]Skipping y_hat=200\n"," 43% 36/83 [00:01<00:01, 33.70it/s]Skipping y_hat=800\n","Skipping y_hat=104\n","Skipping y_hat=111\n"," 48% 40/83 [00:01<00:01, 33.88it/s]Skipping y_hat=930\n","Skipping y_hat=1411\n","Skipping y_hat=311\n","Skipping y_hat=200\n"," 53% 44/83 [00:01<00:01, 33.40it/s]Skipping y_hat=501\n","Skipping y_hat=100\n","Skipping y_hat=207\n","Skipping y_hat=807\n"," 58% 48/83 [00:01<00:01, 32.94it/s]Skipping y_hat=1199\n","Skipping y_hat=600\n"," 63% 52/83 [00:01<00:00, 33.18it/s]Skipping y_hat=1002\n","Skipping y_hat=500\n","Skipping y_hat=100\n","Skipping y_hat=700\n"," 67% 56/83 [00:01<00:00, 33.19it/s]Skipping y_hat=208\n","Skipping y_hat=200\n"," 72% 60/83 [00:01<00:00, 33.71it/s]Skipping y_hat=200\n","Skipping y_hat=203\n"," 77% 64/83 [00:01<00:00, 33.76it/s]Skipping y_hat=400\n","Skipping y_hat=201\n"," 82% 68/83 [00:02<00:00, 33.69it/s]Skipping y_hat=106\n","Skipping y_hat=150\n","Skipping y_hat=1473\n","Skipping y_hat=2055\n","Skipping y_hat=1503\n","Skipping y_hat=2283\n","Skipping y_hat=2016\n","Skipping y_hat=2101\n","Skipping y_hat=1797\n"," 87% 72/83 [00:02<00:00, 33.90it/s]Skipping y_hat=20\n"," 92% 76/83 [00:02<00:00, 34.11it/s]Skipping y_hat=1497\n","Skipping y_hat=1100\n","Skipping y_hat=1802\n","Skipping y_hat=905\n","Skipping y_hat=1705\n","Skipping y_hat=1702\n","Skipping y_hat=1401\n","Skipping y_hat=2202\n","Skipping y_hat=1803\n","Skipping y_hat=600\n","Skipping y_hat=2299\n"," 96% 80/83 [00:02<00:00, 34.60it/s]Skipping y_hat=2099\n","Skipping y_hat=2002\n","100% 83/83 [00:02<00:00, 33.50it/s]\n","accuracy of 10000 examples: 9898/10000 (98.98%)\n","\n","Test Results:\n","test: 98.98%\n","\n","iter 46000: train loss 1.3302, val loss 1.3360\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1105\n","Skipping y_hat=1289\n","Skipping y_hat=1601\n","Skipping y_hat=1005\n","Skipping y_hat=801\n","Skipping y_hat=300\n","Skipping y_hat=1000\n","  5% 4/83 [00:00<00:02, 31.99it/s]Skipping y_hat=1200\n","Skipping y_hat=904\n","Skipping y_hat=905\n","Skipping y_hat=1059\n","Skipping y_hat=501\n"," 10% 8/83 [00:00<00:02, 32.66it/s]Skipping y_hat=1200\n","Skipping y_hat=1679\n","Skipping y_hat=1102\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 32.75it/s]Skipping y_hat=1300\n","Skipping y_hat=1303\n","Skipping y_hat=1007\n","Skipping y_hat=301\n","Skipping y_hat=1200\n","Skipping y_hat=1009\n","Skipping y_hat=1301\n","Skipping y_hat=1300\n","Skipping y_hat=700\n","Skipping y_hat=1119\n"," 19% 16/83 [00:00<00:02, 31.31it/s]Skipping y_hat=600\n","Skipping y_hat=2007\n","Skipping y_hat=1829\n","Skipping y_hat=1603\n","Skipping y_hat=2500\n","Skipping y_hat=1707\n","Skipping y_hat=1902\n","Skipping y_hat=897\n"," 24% 20/83 [00:00<00:02, 31.03it/s]Skipping y_hat=1309\n","Skipping y_hat=901\n","Skipping y_hat=1201\n","Skipping y_hat=1602\n","Skipping y_hat=1606\n","Skipping y_hat=1204\n","Skipping y_hat=1609\n","Skipping y_hat=1800\n","Skipping y_hat=1701\n","Skipping y_hat=905\n","Skipping y_hat=1300\n","Skipping y_hat=1901\n"," 29% 24/83 [00:00<00:01, 30.87it/s]Skipping y_hat=1300\n","Skipping y_hat=1405\n","Skipping y_hat=2104\n"," 34% 28/83 [00:00<00:01, 31.03it/s]Skipping y_hat=169\n"," 43% 36/83 [00:01<00:01, 32.61it/s]Skipping y_hat=805\n"," 48% 40/83 [00:01<00:01, 32.79it/s]Skipping y_hat=600\n","Skipping y_hat=893\n"," 53% 44/83 [00:01<00:01, 33.04it/s]Skipping y_hat=530\n"," 58% 48/83 [00:01<00:01, 33.01it/s]Skipping y_hat=1199\n","Skipping y_hat=1099\n","Skipping y_hat=600\n","Skipping y_hat=299\n"," 63% 52/83 [00:01<00:00, 33.14it/s]Skipping y_hat=502\n","Skipping y_hat=999\n"," 67% 56/83 [00:01<00:00, 33.17it/s]Skipping y_hat=169\n"," 72% 60/83 [00:01<00:00, 33.14it/s]Skipping y_hat=400\n","Skipping y_hat=139\n","Skipping y_hat=599\n"," 77% 64/83 [00:01<00:00, 33.19it/s]Skipping y_hat=699\n"," 82% 68/83 [00:02<00:00, 33.26it/s]Skipping y_hat=2764\n","Skipping y_hat=1844\n","Skipping y_hat=2607\n","Skipping y_hat=2860\n","Skipping y_hat=2199\n"," 92% 76/83 [00:02<00:00, 33.63it/s]Skipping y_hat=1602\n","Skipping y_hat=2304\n","Skipping y_hat=603\n","Skipping y_hat=2305\n","Skipping y_hat=1803\n","Skipping y_hat=2106\n","Skipping y_hat=1710\n","100% 83/83 [00:02<00:00, 32.81it/s]\n","accuracy of 10000 examples: 9907/10000 (99.07000000000001%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 39.03it/s]\n","accuracy of 10000 examples: 9904/10000 (99.03999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=894\n","Skipping y_hat=1289\n","Skipping y_hat=1601\n","Skipping y_hat=1005\n","Skipping y_hat=801\n","Skipping y_hat=300\n","Skipping y_hat=1000\n","  5% 4/83 [00:00<00:02, 34.09it/s]Skipping y_hat=1200\n","Skipping y_hat=904\n","Skipping y_hat=905\n","Skipping y_hat=1059\n","Skipping y_hat=501\n","Skipping y_hat=1300\n","Skipping y_hat=1069\n"," 10% 8/83 [00:00<00:02, 34.13it/s]Skipping y_hat=1400\n","Skipping y_hat=2907\n","Skipping y_hat=803\n","Skipping y_hat=1600\n","Skipping y_hat=1200\n","Skipping y_hat=1679\n","Skipping y_hat=1102\n","Skipping y_hat=300\n"," 14% 12/83 [00:00<00:02, 34.25it/s]Skipping y_hat=1300\n","Skipping y_hat=1099\n","Skipping y_hat=1303\n","Skipping y_hat=1007\n","Skipping y_hat=301\n","Skipping y_hat=1200\n","Skipping y_hat=1002\n","Skipping y_hat=1301\n","Skipping y_hat=1300\n","Skipping y_hat=1119\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:01, 34.48it/s]Skipping y_hat=499\n","Skipping y_hat=600\n","Skipping y_hat=2007\n","Skipping y_hat=1703\n","Skipping y_hat=1603\n","Skipping y_hat=2500\n","Skipping y_hat=1707\n","Skipping y_hat=1902\n"," 24% 20/83 [00:00<00:01, 34.23it/s]Skipping y_hat=1309\n","Skipping y_hat=901\n","Skipping y_hat=1201\n","Skipping y_hat=1602\n","Skipping y_hat=1606\n","Skipping y_hat=1204\n","Skipping y_hat=1311\n","Skipping y_hat=1604\n","Skipping y_hat=905\n","Skipping y_hat=1300\n","Skipping y_hat=1901\n"," 29% 24/83 [00:00<00:01, 34.00it/s]Skipping y_hat=1603\n","Skipping y_hat=1600\n","Skipping y_hat=1300\n","Skipping y_hat=1304\n","Skipping y_hat=1405\n"," 34% 28/83 [00:00<00:01, 34.07it/s]Skipping y_hat=400\n","Skipping y_hat=169\n"," 39% 32/83 [00:00<00:01, 34.00it/s]Skipping y_hat=79\n"," 43% 36/83 [00:01<00:01, 34.31it/s]Skipping y_hat=800\n"," 48% 40/83 [00:01<00:01, 34.14it/s]Skipping y_hat=600\n","Skipping y_hat=700\n","Skipping y_hat=893\n"," 53% 44/83 [00:01<00:01, 34.27it/s]Skipping y_hat=530\n"," 58% 48/83 [00:01<00:01, 34.36it/s]Skipping y_hat=1199\n","Skipping y_hat=1099\n","Skipping y_hat=600\n","Skipping y_hat=999\n"," 63% 52/83 [00:01<00:00, 34.43it/s]Skipping y_hat=502\n","Skipping y_hat=202\n"," 67% 56/83 [00:01<00:00, 34.58it/s]Skipping y_hat=1099\n","Skipping y_hat=169\n"," 72% 60/83 [00:01<00:00, 34.58it/s]Skipping y_hat=599\n","Skipping y_hat=600\n"," 82% 68/83 [00:01<00:00, 34.65it/s]Skipping y_hat=701\n","Skipping y_hat=2198\n","Skipping y_hat=3029\n","Skipping y_hat=2764\n","Skipping y_hat=2523\n","Skipping y_hat=2055\n","Skipping y_hat=2199\n"," 87% 72/83 [00:02<00:00, 34.76it/s]Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=80\n","Skipping y_hat=30\n"," 92% 76/83 [00:02<00:00, 35.17it/s]Skipping y_hat=2304\n","Skipping y_hat=1109\n","Skipping y_hat=2305\n","Skipping y_hat=2201\n","Skipping y_hat=2106\n","Skipping y_hat=2299\n"," 96% 80/83 [00:02<00:00, 35.26it/s]Skipping y_hat=2100\n","Skipping y_hat=1494\n","Skipping y_hat=1296\n","100% 83/83 [00:02<00:00, 34.64it/s]\n","accuracy of 10000 examples: 9884/10000 (98.83999999999999%)\n","\n","Test Results:\n","test: 98.84%\n","\n","iter 47000: train loss 1.3362, val loss 1.3297\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1105\n","Skipping y_hat=1096\n","Skipping y_hat=500\n","Skipping y_hat=990\n","Skipping y_hat=1383\n","Skipping y_hat=699\n","Skipping y_hat=182\n","Skipping y_hat=1506\n","Skipping y_hat=499\n","  5% 4/83 [00:00<00:02, 32.89it/s]Skipping y_hat=1200\n","Skipping y_hat=470\n"," 10% 8/83 [00:00<00:02, 33.65it/s]Skipping y_hat=1302\n","Skipping y_hat=899\n","Skipping y_hat=1200\n","Skipping y_hat=390\n","Skipping y_hat=499\n"," 14% 12/83 [00:00<00:02, 33.87it/s]Skipping y_hat=1390\n","Skipping y_hat=1200\n","Skipping y_hat=1202\n","Skipping y_hat=1301\n","Skipping y_hat=1504\n"," 19% 16/83 [00:00<00:01, 33.82it/s]Skipping y_hat=1300\n","Skipping y_hat=1595\n"," 24% 20/83 [00:00<00:01, 33.81it/s]Skipping y_hat=1198\n","Skipping y_hat=1309\n","Skipping y_hat=1201\n","Skipping y_hat=705\n","Skipping y_hat=1599\n","Skipping y_hat=1800\n","Skipping y_hat=1493\n","Skipping y_hat=1446\n"," 29% 24/83 [00:00<00:01, 33.52it/s]Skipping y_hat=1323\n","Skipping y_hat=1899\n","Skipping y_hat=1304\n"," 34% 28/83 [00:00<00:01, 33.84it/s]Skipping y_hat=120\n"," 39% 32/83 [00:00<00:01, 34.24it/s]Skipping y_hat=60\n","Skipping y_hat=560\n"," 43% 36/83 [00:01<00:01, 34.44it/s]Skipping y_hat=320\n","Skipping y_hat=989\n"," 48% 40/83 [00:01<00:01, 34.47it/s]Skipping y_hat=200\n"," 53% 44/83 [00:01<00:01, 34.46it/s]Skipping y_hat=602\n","Skipping y_hat=340\n"," 58% 48/83 [00:01<00:01, 34.49it/s]Skipping y_hat=599\n","Skipping y_hat=392\n"," 63% 52/83 [00:01<00:00, 34.37it/s]Skipping y_hat=502\n","Skipping y_hat=460\n"," 72% 60/83 [00:01<00:00, 34.65it/s]Skipping y_hat=960\n","Skipping y_hat=630\n","Skipping y_hat=170\n"," 77% 64/83 [00:01<00:00, 34.63it/s]Skipping y_hat=240\n","Skipping y_hat=299\n","Skipping y_hat=700\n"," 82% 68/83 [00:01<00:00, 34.63it/s]Skipping y_hat=770\n","Skipping y_hat=1503\n","Skipping y_hat=2199\n"," 87% 72/83 [00:02<00:00, 34.86it/s]Skipping y_hat=60\n"," 92% 76/83 [00:02<00:00, 35.17it/s]Skipping y_hat=1605\n","Skipping y_hat=2300\n","Skipping y_hat=905\n","Skipping y_hat=1902\n","Skipping y_hat=2403\n","Skipping y_hat=1803\n","Skipping y_hat=1230\n","Skipping y_hat=2209\n"," 96% 80/83 [00:02<00:00, 34.82it/s]Skipping y_hat=1794\n","Skipping y_hat=1594\n","100% 83/83 [00:02<00:00, 34.53it/s]\n","accuracy of 10000 examples: 9923/10000 (99.22999999999999%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.19it/s]\n","accuracy of 10000 examples: 9921/10000 (99.21%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=994\n","Skipping y_hat=1096\n","Skipping y_hat=500\n","Skipping y_hat=990\n","Skipping y_hat=1383\n","Skipping y_hat=699\n","Skipping y_hat=182\n","Skipping y_hat=1169\n","Skipping y_hat=1506\n","Skipping y_hat=499\n","  5% 4/83 [00:00<00:02, 31.14it/s]Skipping y_hat=1200\n","Skipping y_hat=470\n","Skipping y_hat=904\n","Skipping y_hat=697\n","Skipping y_hat=996\n"," 10% 8/83 [00:00<00:02, 31.91it/s]Skipping y_hat=1400\n","Skipping y_hat=1302\n","Skipping y_hat=899\n","Skipping y_hat=1200\n","Skipping y_hat=490\n","Skipping y_hat=999\n"," 14% 12/83 [00:00<00:02, 31.92it/s]Skipping y_hat=1300\n","Skipping y_hat=1410\n","Skipping y_hat=593\n","Skipping y_hat=1200\n","Skipping y_hat=1301\n","Skipping y_hat=809\n"," 19% 16/83 [00:00<00:02, 31.80it/s]Skipping y_hat=1300\n","Skipping y_hat=2007\n","Skipping y_hat=1595\n","Skipping y_hat=1208\n","Skipping y_hat=1294\n"," 24% 20/83 [00:00<00:02, 31.40it/s]Skipping y_hat=1389\n","Skipping y_hat=1198\n","Skipping y_hat=1340\n","Skipping y_hat=705\n","Skipping y_hat=1798\n","Skipping y_hat=1599\n","Skipping y_hat=1800\n","Skipping y_hat=1493\n","Skipping y_hat=1337\n"," 29% 24/83 [00:00<00:01, 31.56it/s]Skipping y_hat=1605\n"," 34% 28/83 [00:00<00:01, 32.00it/s]Skipping y_hat=110\n","Skipping y_hat=180\n"," 39% 32/83 [00:00<00:01, 32.71it/s]Skipping y_hat=170\n","Skipping y_hat=60\n","Skipping y_hat=560\n"," 43% 36/83 [00:01<00:01, 32.60it/s]Skipping y_hat=339\n","Skipping y_hat=422\n","Skipping y_hat=899\n"," 48% 40/83 [00:01<00:01, 32.57it/s]Skipping y_hat=200\n","Skipping y_hat=1249\n"," 58% 48/83 [00:01<00:01, 33.41it/s]Skipping y_hat=599\n","Skipping y_hat=999\n","Skipping y_hat=392\n","Skipping y_hat=1000\n"," 63% 52/83 [00:01<00:00, 33.34it/s]Skipping y_hat=502\n","Skipping y_hat=299\n","Skipping y_hat=293\n","Skipping y_hat=460\n"," 72% 60/83 [00:01<00:00, 33.30it/s]Skipping y_hat=889\n"," 77% 64/83 [00:01<00:00, 33.60it/s]Skipping y_hat=240\n","Skipping y_hat=130\n","Skipping y_hat=209\n"," 82% 68/83 [00:02<00:00, 33.19it/s]Skipping y_hat=770\n","Skipping y_hat=1525\n","Skipping y_hat=2055\n","Skipping y_hat=1503\n","Skipping y_hat=2101\n"," 87% 72/83 [00:02<00:00, 33.46it/s]Skipping y_hat=60\n"," 92% 76/83 [00:02<00:00, 33.93it/s]Skipping y_hat=1497\n","Skipping y_hat=2300\n","Skipping y_hat=2559\n","Skipping y_hat=905\n","Skipping y_hat=1405\n","Skipping y_hat=2403\n","Skipping y_hat=1230\n","Skipping y_hat=2209\n"," 96% 80/83 [00:02<00:00, 33.82it/s]Skipping y_hat=992\n","Skipping y_hat=2002\n","Skipping y_hat=1794\n","Skipping y_hat=1594\n","100% 83/83 [00:02<00:00, 32.96it/s]\n","accuracy of 10000 examples: 9903/10000 (99.03%)\n","\n","Test Results:\n","test: 99.03%\n","\n","iter 48000: train loss 1.3341, val loss 1.3323\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=955\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=1116\n","Skipping y_hat=801\n","  5% 4/83 [00:00<00:02, 33.41it/s]Skipping y_hat=907\n","Skipping y_hat=1007\n","Skipping y_hat=634\n","Skipping y_hat=1607\n","Skipping y_hat=501\n"," 10% 8/83 [00:00<00:02, 33.11it/s]Skipping y_hat=1039\n","Skipping y_hat=300\n","Skipping y_hat=1456\n","Skipping y_hat=264\n"," 14% 12/83 [00:00<00:02, 33.34it/s]Skipping y_hat=1969\n","Skipping y_hat=1200\n","Skipping y_hat=179\n","Skipping y_hat=905\n"," 19% 16/83 [00:00<00:01, 33.58it/s]Skipping y_hat=1095\n","Skipping y_hat=1829\n","Skipping y_hat=1299\n","Skipping y_hat=897\n"," 24% 20/83 [00:00<00:01, 33.45it/s]Skipping y_hat=1102\n","Skipping y_hat=1357\n","Skipping y_hat=1902\n"," 29% 24/83 [00:00<00:01, 33.37it/s]Skipping y_hat=895\n","Skipping y_hat=1899\n","Skipping y_hat=1258\n","Skipping y_hat=1600\n","Skipping y_hat=954\n","Skipping y_hat=199\n"," 43% 36/83 [00:01<00:01, 33.39it/s]Skipping y_hat=800\n","Skipping y_hat=349\n"," 48% 40/83 [00:01<00:01, 33.35it/s]Skipping y_hat=899\n","Skipping y_hat=658\n"," 53% 44/83 [00:01<00:01, 33.37it/s]Skipping y_hat=1239\n","Skipping y_hat=1220\n"," 63% 52/83 [00:01<00:00, 33.97it/s]Skipping y_hat=190\n"," 67% 56/83 [00:01<00:00, 34.26it/s]Skipping y_hat=599\n","Skipping y_hat=439\n","Skipping y_hat=999\n"," 72% 60/83 [00:01<00:00, 34.19it/s]Skipping y_hat=589\n"," 77% 64/83 [00:01<00:00, 34.28it/s]Skipping y_hat=209\n"," 82% 68/83 [00:02<00:00, 34.46it/s]Skipping y_hat=539\n","Skipping y_hat=2019\n","Skipping y_hat=2308\n","Skipping y_hat=2442\n","Skipping y_hat=2955\n","Skipping y_hat=2404\n","Skipping y_hat=2403\n","Skipping y_hat=2029\n","Skipping y_hat=2101\n"," 92% 76/83 [00:02<00:00, 35.06it/s]Skipping y_hat=1605\n","Skipping y_hat=1497\n","Skipping y_hat=2204\n","Skipping y_hat=950\n","Skipping y_hat=1640\n","Skipping y_hat=1269\n","Skipping y_hat=2109\n"," 96% 80/83 [00:02<00:00, 35.01it/s]Skipping y_hat=1355\n","Skipping y_hat=2002\n","Skipping y_hat=1606\n","100% 83/83 [00:02<00:00, 34.08it/s]\n","accuracy of 10000 examples: 9931/10000 (99.31%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 37.77it/s]\n","accuracy of 10000 examples: 9922/10000 (99.22%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=955\n","Skipping y_hat=900\n","Skipping y_hat=938\n","Skipping y_hat=1159\n","  5% 4/83 [00:00<00:02, 30.39it/s]Skipping y_hat=501\n"," 10% 8/83 [00:00<00:02, 31.34it/s]Skipping y_hat=1039\n","Skipping y_hat=1790\n","Skipping y_hat=300\n","Skipping y_hat=1456\n","Skipping y_hat=950\n"," 14% 12/83 [00:00<00:02, 31.51it/s]Skipping y_hat=1300\n","Skipping y_hat=1200\n","Skipping y_hat=207\n","Skipping y_hat=905\n"," 19% 16/83 [00:00<00:02, 31.86it/s]Skipping y_hat=1300\n","Skipping y_hat=1095\n","Skipping y_hat=1077\n","Skipping y_hat=1299\n","Skipping y_hat=1008\n","Skipping y_hat=863\n"," 24% 20/83 [00:00<00:02, 31.29it/s]Skipping y_hat=665\n","Skipping y_hat=1102\n","Skipping y_hat=2032\n","Skipping y_hat=1599\n","Skipping y_hat=1357\n","Skipping y_hat=1489\n"," 29% 24/83 [00:00<00:01, 30.97it/s]Skipping y_hat=895\n","Skipping y_hat=1258\n","Skipping y_hat=1608\n","Skipping y_hat=1256\n","Skipping y_hat=1600\n","Skipping y_hat=199\n"," 39% 32/83 [00:01<00:01, 32.42it/s]Skipping y_hat=81\n"," 43% 36/83 [00:01<00:01, 33.31it/s]Skipping y_hat=800\n","Skipping y_hat=658\n"," 48% 40/83 [00:01<00:01, 33.58it/s]Skipping y_hat=899\n","Skipping y_hat=460\n","Skipping y_hat=399\n","Skipping y_hat=658\n","Skipping y_hat=893\n"," 53% 44/83 [00:01<00:01, 33.22it/s]Skipping y_hat=908\n","Skipping y_hat=449\n","Skipping y_hat=1239\n","Skipping y_hat=295\n"," 58% 48/83 [00:01<00:01, 33.07it/s]Skipping y_hat=294\n","Skipping y_hat=1190\n"," 63% 52/83 [00:01<00:00, 33.33it/s]Skipping y_hat=300\n","Skipping y_hat=100\n"," 67% 56/83 [00:01<00:00, 33.06it/s]Skipping y_hat=599\n","Skipping y_hat=439\n","Skipping y_hat=999\n"," 72% 60/83 [00:01<00:00, 32.73it/s]Skipping y_hat=589\n"," 77% 64/83 [00:01<00:00, 33.20it/s]Skipping y_hat=209\n","Skipping y_hat=139\n"," 82% 68/83 [00:02<00:00, 33.40it/s]Skipping y_hat=539\n","Skipping y_hat=2308\n","Skipping y_hat=1830\n","Skipping y_hat=2491\n","Skipping y_hat=2955\n","Skipping y_hat=2404\n","Skipping y_hat=2403\n","Skipping y_hat=2029\n","Skipping y_hat=2062\n","Skipping y_hat=2101\n"," 92% 76/83 [00:02<00:00, 34.07it/s]Skipping y_hat=1497\n","Skipping y_hat=1501\n","Skipping y_hat=2300\n","Skipping y_hat=1900\n","Skipping y_hat=1803\n","Skipping y_hat=950\n","Skipping y_hat=2109\n"," 96% 80/83 [00:02<00:00, 33.68it/s]Skipping y_hat=1355\n","Skipping y_hat=2402\n","Skipping y_hat=1794\n","Skipping y_hat=1539\n","Skipping y_hat=1\n","100% 83/83 [00:02<00:00, 32.97it/s]\n","accuracy of 10000 examples: 9917/10000 (99.17%)\n","\n","Test Results:\n","test: 99.17%\n","\n","iter 49000: train loss 1.3329, val loss 1.3354\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1048\n","Skipping y_hat=1095\n","Skipping y_hat=607\n","Skipping y_hat=1601\n","Skipping y_hat=1005\n","Skipping y_hat=1599\n","Skipping y_hat=206\n","Skipping y_hat=801\n","  5% 4/83 [00:00<00:02, 32.89it/s]Skipping y_hat=805\n","Skipping y_hat=2077\n","Skipping y_hat=501\n","Skipping y_hat=1498\n"," 10% 8/83 [00:00<00:02, 33.01it/s]Skipping y_hat=835\n","Skipping y_hat=1400\n","Skipping y_hat=899\n","Skipping y_hat=284\n","Skipping y_hat=800\n"," 14% 12/83 [00:00<00:02, 33.71it/s]Skipping y_hat=980\n","Skipping y_hat=207\n","Skipping y_hat=610\n","Skipping y_hat=1209\n"," 19% 16/83 [00:00<00:01, 34.36it/s]Skipping y_hat=600\n","Skipping y_hat=1366\n","Skipping y_hat=1224\n"," 24% 20/83 [00:00<00:01, 34.24it/s]Skipping y_hat=1639\n","Skipping y_hat=1198\n","Skipping y_hat=1968\n","Skipping y_hat=1311\n","Skipping y_hat=1350\n","Skipping y_hat=1120\n","Skipping y_hat=2299\n","Skipping y_hat=962\n","Skipping y_hat=1300\n","Skipping y_hat=1901\n"," 29% 24/83 [00:00<00:01, 33.82it/s]Skipping y_hat=1491\n","Skipping y_hat=401\n","Skipping y_hat=1300\n","Skipping y_hat=1904\n"," 43% 36/83 [00:01<00:01, 34.35it/s]Skipping y_hat=1693\n","Skipping y_hat=920\n","Skipping y_hat=500\n"," 53% 44/83 [00:01<00:01, 34.27it/s]Skipping y_hat=100\n","Skipping y_hat=1093\n"," 58% 48/83 [00:01<00:01, 34.26it/s]Skipping y_hat=820\n","Skipping y_hat=480\n"," 63% 52/83 [00:01<00:00, 34.38it/s]Skipping y_hat=1492\n","Skipping y_hat=202\n"," 67% 56/83 [00:01<00:00, 34.59it/s]Skipping y_hat=599\n"," 72% 60/83 [00:01<00:00, 34.30it/s]Skipping y_hat=123\n"," 82% 68/83 [00:01<00:00, 34.50it/s]Skipping y_hat=2056\n","Skipping y_hat=3320\n","Skipping y_hat=3155\n","Skipping y_hat=3247\n","Skipping y_hat=1907\n","Skipping y_hat=1402\n","Skipping y_hat=2101\n","Skipping y_hat=2199\n"," 87% 72/83 [00:02<00:00, 34.74it/s]Skipping y_hat=60\n"," 92% 76/83 [00:02<00:00, 35.23it/s]Skipping y_hat=1414\n","Skipping y_hat=1903\n","Skipping y_hat=1578\n","Skipping y_hat=1203\n","Skipping y_hat=1802\n","Skipping y_hat=3304\n","Skipping y_hat=1714\n","Skipping y_hat=885\n","Skipping y_hat=603\n","Skipping y_hat=1900\n","Skipping y_hat=1985\n","Skipping y_hat=1401\n","Skipping y_hat=1803\n","Skipping y_hat=1249\n","Skipping y_hat=1710\n"," 96% 80/83 [00:02<00:00, 34.94it/s]Skipping y_hat=2007\n","Skipping y_hat=2100\n","Skipping y_hat=1151\n","Skipping y_hat=1078\n","100% 83/83 [00:02<00:00, 34.42it/s]\n","accuracy of 10000 examples: 9915/10000 (99.15%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.39it/s]\n","accuracy of 10000 examples: 9908/10000 (99.08%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1048\n","Skipping y_hat=1601\n","Skipping y_hat=565\n","Skipping y_hat=801\n","Skipping y_hat=528\n","Skipping y_hat=1162\n","  5% 4/83 [00:00<00:02, 32.67it/s]Skipping y_hat=287\n","Skipping y_hat=2077\n","Skipping y_hat=501\n","Skipping y_hat=1999\n","Skipping y_hat=1498\n"," 10% 8/83 [00:00<00:02, 33.34it/s]Skipping y_hat=370\n","Skipping y_hat=835\n","Skipping y_hat=1490\n","Skipping y_hat=1123\n","Skipping y_hat=691\n","Skipping y_hat=899\n","Skipping y_hat=499\n"," 14% 12/83 [00:00<00:02, 31.76it/s]Skipping y_hat=980\n","Skipping y_hat=397\n","Skipping y_hat=207\n","Skipping y_hat=1099\n","Skipping y_hat=610\n"," 19% 16/83 [00:00<00:02, 31.26it/s]Skipping y_hat=706\n","Skipping y_hat=600\n","Skipping y_hat=1203\n","Skipping y_hat=1706\n","Skipping y_hat=1267\n","Skipping y_hat=1299\n","Skipping y_hat=1100\n"," 24% 20/83 [00:00<00:01, 31.57it/s]Skipping y_hat=1389\n","Skipping y_hat=1968\n","Skipping y_hat=1006\n","Skipping y_hat=584\n","Skipping y_hat=1650\n","Skipping y_hat=1350\n","Skipping y_hat=2299\n","Skipping y_hat=962\n","Skipping y_hat=905\n","Skipping y_hat=1901\n"," 29% 24/83 [00:00<00:01, 31.94it/s]Skipping y_hat=401\n","Skipping y_hat=1899\n","Skipping y_hat=1300\n"," 43% 36/83 [00:01<00:01, 34.46it/s]Skipping y_hat=920\n","Skipping y_hat=500\n","Skipping y_hat=805\n"," 53% 44/83 [00:01<00:01, 34.92it/s]Skipping y_hat=100\n","Skipping y_hat=397\n"," 58% 48/83 [00:01<00:01, 34.98it/s]Skipping y_hat=820\n","Skipping y_hat=999\n"," 63% 52/83 [00:01<00:00, 34.94it/s]Skipping y_hat=1492\n","Skipping y_hat=100\n","Skipping y_hat=202\n"," 72% 60/83 [00:01<00:00, 34.96it/s]Skipping y_hat=126\n"," 82% 68/83 [00:02<00:00, 35.06it/s]Skipping y_hat=2478\n","Skipping y_hat=3320\n","Skipping y_hat=1160\n","Skipping y_hat=1907\n","Skipping y_hat=2199\n"," 87% 72/83 [00:02<00:00, 35.19it/s]Skipping y_hat=60\n"," 92% 76/83 [00:02<00:00, 35.42it/s]Skipping y_hat=1109\n","Skipping y_hat=901\n","Skipping y_hat=1714\n","Skipping y_hat=603\n","Skipping y_hat=1900\n","Skipping y_hat=1702\n","Skipping y_hat=1401\n","Skipping y_hat=1803\n","Skipping y_hat=1249\n","Skipping y_hat=2046\n","Skipping y_hat=1289\n","Skipping y_hat=1710\n"," 96% 80/83 [00:02<00:00, 35.34it/s]Skipping y_hat=2007\n","Skipping y_hat=2100\n","Skipping y_hat=2506\n","Skipping y_hat=1078\n","Skipping y_hat=2474\n","100% 83/83 [00:02<00:00, 34.25it/s]\n","accuracy of 10000 examples: 9913/10000 (99.13%)\n","\n","Test Results:\n","test: 99.13%\n","\n","iter 50000: train loss 1.3314, val loss 1.3273\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=652\n","Skipping y_hat=904\n","Skipping y_hat=1105\n","Skipping y_hat=690\n","Skipping y_hat=500\n","Skipping y_hat=729\n","Skipping y_hat=625\n","Skipping y_hat=699\n","Skipping y_hat=390\n","  5% 4/83 [00:00<00:02, 32.83it/s]Skipping y_hat=400\n","Skipping y_hat=470\n","Skipping y_hat=904\n","Skipping y_hat=905\n","Skipping y_hat=858\n","Skipping y_hat=1506\n","Skipping y_hat=501\n"," 10% 8/83 [00:00<00:02, 32.69it/s]Skipping y_hat=1590\n","Skipping y_hat=1007\n","Skipping y_hat=508\n","Skipping y_hat=1280\n","Skipping y_hat=1200\n"," 14% 12/83 [00:00<00:02, 33.24it/s]Skipping y_hat=1300\n","Skipping y_hat=1158\n","Skipping y_hat=1208\n","Skipping y_hat=700\n","Skipping y_hat=301\n","Skipping y_hat=1200\n","Skipping y_hat=640\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:02, 33.41it/s]Skipping y_hat=800\n","Skipping y_hat=1400\n","Skipping y_hat=1706\n","Skipping y_hat=1101\n","Skipping y_hat=1224\n"," 24% 20/83 [00:00<00:01, 33.42it/s]Skipping y_hat=1152\n","Skipping y_hat=1960\n","Skipping y_hat=1842\n","Skipping y_hat=1093\n","Skipping y_hat=932\n","Skipping y_hat=1650\n","Skipping y_hat=1204\n","Skipping y_hat=1050\n","Skipping y_hat=627\n","Skipping y_hat=701\n","Skipping y_hat=1102\n"," 29% 24/83 [00:00<00:01, 33.38it/s]Skipping y_hat=1603\n","Skipping y_hat=1875\n","Skipping y_hat=1099\n","Skipping y_hat=1600\n","Skipping y_hat=1840\n","Skipping y_hat=70\n"," 34% 28/83 [00:00<00:01, 33.24it/s]Skipping y_hat=400\n","Skipping y_hat=80\n"," 39% 32/83 [00:00<00:01, 33.63it/s]Skipping y_hat=79\n","Skipping y_hat=50\n","Skipping y_hat=499\n"," 43% 36/83 [00:01<00:01, 33.84it/s]Skipping y_hat=990\n","Skipping y_hat=800\n"," 48% 40/83 [00:01<00:01, 34.22it/s]Skipping y_hat=200\n"," 53% 44/83 [00:01<00:01, 34.27it/s]Skipping y_hat=260\n","Skipping y_hat=1239\n","Skipping y_hat=295\n"," 58% 48/83 [00:01<00:01, 34.28it/s]Skipping y_hat=460\n"," 63% 52/83 [00:01<00:00, 34.51it/s]Skipping y_hat=330\n","Skipping y_hat=1029\n","Skipping y_hat=1401\n","Skipping y_hat=1330\n","Skipping y_hat=165\n","Skipping y_hat=193\n"," 67% 56/83 [00:01<00:00, 34.37it/s]Skipping y_hat=175\n","Skipping y_hat=310\n"," 72% 60/83 [00:01<00:00, 34.58it/s]Skipping y_hat=850\n","Skipping y_hat=200\n","Skipping y_hat=1020\n","Skipping y_hat=170\n"," 77% 64/83 [00:01<00:00, 34.46it/s]Skipping y_hat=170\n"," 82% 68/83 [00:02<00:00, 34.11it/s]Skipping y_hat=770\n","Skipping y_hat=1550\n"," 87% 72/83 [00:02<00:00, 34.17it/s]Skipping y_hat=70\n","Skipping y_hat=60\n"," 92% 76/83 [00:02<00:00, 34.52it/s]Skipping y_hat=1903\n","Skipping y_hat=2300\n","Skipping y_hat=1702\n","Skipping y_hat=1803\n","Skipping y_hat=2300\n"," 96% 80/83 [00:02<00:00, 34.40it/s]Skipping y_hat=2402\n","Skipping y_hat=2506\n","Skipping y_hat=1150\n","100% 83/83 [00:02<00:00, 34.15it/s]\n","accuracy of 10000 examples: 9889/10000 (98.89%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.75it/s]\n","accuracy of 10000 examples: 9889/10000 (98.89%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=690\n","Skipping y_hat=500\n","Skipping y_hat=1006\n","Skipping y_hat=729\n","Skipping y_hat=625\n","Skipping y_hat=699\n","Skipping y_hat=1101\n","Skipping y_hat=1860\n","Skipping y_hat=390\n","  5% 4/83 [00:00<00:02, 34.11it/s]Skipping y_hat=904\n","Skipping y_hat=905\n","Skipping y_hat=750\n","Skipping y_hat=501\n"," 10% 8/83 [00:00<00:02, 34.19it/s]Skipping y_hat=594\n","Skipping y_hat=1030\n","Skipping y_hat=1200\n"," 14% 12/83 [00:00<00:02, 34.24it/s]Skipping y_hat=1300\n","Skipping y_hat=1158\n","Skipping y_hat=1208\n","Skipping y_hat=301\n","Skipping y_hat=1200\n","Skipping y_hat=308\n","Skipping y_hat=640\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:01, 34.44it/s]Skipping y_hat=1400\n","Skipping y_hat=1224\n","Skipping y_hat=1707\n","Skipping y_hat=1199\n","Skipping y_hat=1253\n"," 24% 20/83 [00:00<00:01, 34.24it/s]Skipping y_hat=599\n","Skipping y_hat=1152\n","Skipping y_hat=1960\n","Skipping y_hat=819\n","Skipping y_hat=1093\n","Skipping y_hat=895\n","Skipping y_hat=932\n","Skipping y_hat=1650\n","Skipping y_hat=1350\n","Skipping y_hat=1050\n","Skipping y_hat=627\n","Skipping y_hat=701\n","Skipping y_hat=935\n","Skipping y_hat=1102\n"," 29% 24/83 [00:00<00:01, 33.51it/s]Skipping y_hat=600\n","Skipping y_hat=1603\n","Skipping y_hat=801\n","Skipping y_hat=1099\n","Skipping y_hat=460\n","Skipping y_hat=1600\n","Skipping y_hat=1840\n"," 34% 28/83 [00:00<00:01, 33.59it/s]Skipping y_hat=400\n","Skipping y_hat=500\n"," 39% 32/83 [00:00<00:01, 33.66it/s]Skipping y_hat=79\n"," 43% 36/83 [00:01<00:01, 34.12it/s]Skipping y_hat=990\n","Skipping y_hat=800\n","Skipping y_hat=1160\n"," 48% 40/83 [00:01<00:01, 34.04it/s]Skipping y_hat=200\n","Skipping y_hat=1203\n","Skipping y_hat=508\n"," 53% 44/83 [00:01<00:01, 34.15it/s]Skipping y_hat=260\n","Skipping y_hat=1239\n","Skipping y_hat=295\n"," 58% 48/83 [00:01<00:01, 33.85it/s]Skipping y_hat=253\n"," 63% 52/83 [00:01<00:00, 34.17it/s]Skipping y_hat=330\n","Skipping y_hat=1330\n","Skipping y_hat=707\n"," 67% 56/83 [00:01<00:00, 34.28it/s]Skipping y_hat=310\n"," 72% 60/83 [00:01<00:00, 34.39it/s]Skipping y_hat=167\n","Skipping y_hat=850\n","Skipping y_hat=200\n"," 77% 64/83 [00:01<00:00, 34.62it/s]Skipping y_hat=780\n","Skipping y_hat=220\n","Skipping y_hat=170\n"," 82% 68/83 [00:01<00:00, 34.28it/s]Skipping y_hat=168\n","Skipping y_hat=2494\n","Skipping y_hat=3960\n","Skipping y_hat=2291\n","Skipping y_hat=1550\n","Skipping y_hat=2283\n","Skipping y_hat=2199\n"," 87% 72/83 [00:02<00:00, 34.68it/s]Skipping y_hat=70\n","Skipping y_hat=60\n","Skipping y_hat=40\n"," 92% 76/83 [00:02<00:00, 35.06it/s]Skipping y_hat=1565\n","Skipping y_hat=1903\n","Skipping y_hat=1693\n","Skipping y_hat=1799\n","Skipping y_hat=1967\n","Skipping y_hat=2300\n","Skipping y_hat=2201\n","Skipping y_hat=1803\n","Skipping y_hat=1706\n","Skipping y_hat=1710\n"," 96% 80/83 [00:02<00:00, 35.25it/s]Skipping y_hat=2099\n","Skipping y_hat=1604\n","Skipping y_hat=2402\n","Skipping y_hat=1666\n","100% 83/83 [00:02<00:00, 34.48it/s]\n","accuracy of 10000 examples: 9886/10000 (98.86%)\n","\n","Test Results:\n","test: 98.86%\n","\n","iter 51000: train loss 1.3280, val loss 1.3300\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1048\n","Skipping y_hat=1116\n","Skipping y_hat=300\n","  5% 4/83 [00:00<00:02, 34.30it/s]Skipping y_hat=904\n","Skipping y_hat=445\n","Skipping y_hat=697\n"," 10% 8/83 [00:00<00:02, 34.20it/s]Skipping y_hat=1253\n","Skipping y_hat=370\n","Skipping y_hat=517\n"," 14% 12/83 [00:00<00:02, 34.18it/s]Skipping y_hat=903\n","Skipping y_hat=1583\n"," 19% 16/83 [00:00<00:01, 34.58it/s]Skipping y_hat=600\n","Skipping y_hat=1400\n","Skipping y_hat=2199\n","Skipping y_hat=1731\n","Skipping y_hat=1109\n","Skipping y_hat=1146\n","Skipping y_hat=1458\n","Skipping y_hat=1387\n"," 24% 20/83 [00:00<00:01, 34.17it/s]Skipping y_hat=599\n","Skipping y_hat=2200\n","Skipping y_hat=1472\n","Skipping y_hat=1506\n","Skipping y_hat=1596\n","Skipping y_hat=1650\n","Skipping y_hat=1599\n","Skipping y_hat=1800\n","Skipping y_hat=706\n","Skipping y_hat=1488\n","Skipping y_hat=905\n","Skipping y_hat=2299\n","Skipping y_hat=1300\n"," 29% 24/83 [00:00<00:01, 33.91it/s]Skipping y_hat=1144\n","Skipping y_hat=1840\n"," 43% 36/83 [00:01<00:01, 34.29it/s]Skipping y_hat=800\n"," 58% 48/83 [00:01<00:01, 34.25it/s]Skipping y_hat=1000\n"," 63% 52/83 [00:01<00:00, 34.29it/s]Skipping y_hat=150\n"," 82% 68/83 [00:01<00:00, 34.25it/s]Skipping y_hat=3919\n","Skipping y_hat=810\n","Skipping y_hat=2087\n","Skipping y_hat=2055\n","Skipping y_hat=2045\n","Skipping y_hat=2483\n","Skipping y_hat=2199\n"," 92% 76/83 [00:02<00:00, 34.84it/s]Skipping y_hat=1497\n","Skipping y_hat=1497\n","Skipping y_hat=1003\n","Skipping y_hat=1802\n","Skipping y_hat=2204\n","Skipping y_hat=1940\n","Skipping y_hat=1702\n","Skipping y_hat=2230\n","Skipping y_hat=2299\n"," 96% 80/83 [00:02<00:00, 34.47it/s]Skipping y_hat=2099\n","Skipping y_hat=1666\n","Skipping y_hat=1701\n","100% 83/83 [00:02<00:00, 34.40it/s]\n","accuracy of 10000 examples: 9940/10000 (99.4%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 39.10it/s]\n","accuracy of 10000 examples: 9937/10000 (99.37%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=904\n","Skipping y_hat=500\n","Skipping y_hat=548\n","  5% 4/83 [00:00<00:02, 34.45it/s]Skipping y_hat=904\n","Skipping y_hat=445\n"," 10% 8/83 [00:00<00:02, 34.74it/s]Skipping y_hat=370\n","Skipping y_hat=997\n","Skipping y_hat=491\n"," 14% 12/83 [00:00<00:02, 34.60it/s]Skipping y_hat=1583\n"," 19% 16/83 [00:00<00:01, 34.84it/s]Skipping y_hat=600\n","Skipping y_hat=1590\n","Skipping y_hat=1731\n","Skipping y_hat=1109\n","Skipping y_hat=1193\n","Skipping y_hat=1387\n","Skipping y_hat=1260\n"," 24% 20/83 [00:00<00:01, 33.55it/s]Skipping y_hat=1472\n","Skipping y_hat=1506\n","Skipping y_hat=1842\n","Skipping y_hat=1800\n","Skipping y_hat=1350\n","Skipping y_hat=1488\n","Skipping y_hat=935\n","Skipping y_hat=1273\n","Skipping y_hat=905\n","Skipping y_hat=2299\n","Skipping y_hat=1300\n"," 29% 24/83 [00:00<00:01, 32.81it/s]Skipping y_hat=1840\n"," 43% 36/83 [00:01<00:01, 34.01it/s]Skipping y_hat=477\n","Skipping y_hat=800\n"," 48% 40/83 [00:01<00:01, 33.75it/s]Skipping y_hat=930\n","Skipping y_hat=201\n"," 58% 48/83 [00:01<00:01, 34.31it/s]Skipping y_hat=881\n","Skipping y_hat=999\n","Skipping y_hat=1080\n"," 82% 68/83 [00:01<00:00, 34.89it/s]Skipping y_hat=2919\n","Skipping y_hat=810\n","Skipping y_hat=1335\n","Skipping y_hat=2045\n","Skipping y_hat=2101\n","Skipping y_hat=2199\n"," 92% 76/83 [00:02<00:00, 35.20it/s]Skipping y_hat=1497\n","Skipping y_hat=1416\n","Skipping y_hat=1497\n","Skipping y_hat=1802\n","Skipping y_hat=1940\n","Skipping y_hat=1702\n","Skipping y_hat=2230\n"," 96% 80/83 [00:02<00:00, 34.86it/s]Skipping y_hat=2099\n","Skipping y_hat=1742\n","Skipping y_hat=39\n","100% 83/83 [00:02<00:00, 34.42it/s]\n","accuracy of 10000 examples: 9946/10000 (99.46000000000001%)\n","\n","Test Results:\n","test: 99.46%\n","\n","iter 52000: train loss 1.3386, val loss 1.3288\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=592\n","Skipping y_hat=500\n","Skipping y_hat=1599\n","Skipping y_hat=699\n","  5% 4/83 [00:00<00:02, 33.85it/s]Skipping y_hat=1200\n","Skipping y_hat=1699\n","Skipping y_hat=1051\n"," 10% 8/83 [00:00<00:02, 34.11it/s]Skipping y_hat=1400\n","Skipping y_hat=500\n","Skipping y_hat=899\n"," 19% 16/83 [00:00<00:01, 34.59it/s]Skipping y_hat=626\n","Skipping y_hat=1599\n","Skipping y_hat=1121\n","Skipping y_hat=1100\n","Skipping y_hat=1022\n","Skipping y_hat=1707\n","Skipping y_hat=1327\n"," 24% 20/83 [00:00<00:01, 34.17it/s]Skipping y_hat=599\n","Skipping y_hat=1201\n","Skipping y_hat=1204\n","Skipping y_hat=1357\n"," 29% 24/83 [00:00<00:01, 33.93it/s]Skipping y_hat=1199\n","Skipping y_hat=1197\n"," 39% 32/83 [00:00<00:01, 34.10it/s]Skipping y_hat=739\n","Skipping y_hat=279\n","Skipping y_hat=739\n"," 43% 36/83 [00:01<00:01, 34.34it/s]Skipping y_hat=1157\n","Skipping y_hat=104\n"," 48% 40/83 [00:01<00:01, 34.11it/s]Skipping y_hat=899\n","Skipping y_hat=939\n"," 53% 44/83 [00:01<00:01, 34.24it/s]Skipping y_hat=835\n"," 58% 48/83 [00:01<00:01, 34.43it/s]Skipping y_hat=797\n","Skipping y_hat=1199\n"," 63% 52/83 [00:01<00:00, 34.21it/s]Skipping y_hat=202\n"," 72% 60/83 [00:01<00:00, 34.17it/s]Skipping y_hat=890\n","Skipping y_hat=1069\n","Skipping y_hat=1059\n"," 82% 68/83 [00:01<00:00, 34.31it/s]Skipping y_hat=3029\n","Skipping y_hat=1844\n","Skipping y_hat=1228\n","Skipping y_hat=2055\n","Skipping y_hat=3247\n","Skipping y_hat=1402\n","Skipping y_hat=2577\n","Skipping y_hat=1999\n"," 92% 76/83 [00:02<00:00, 35.14it/s]Skipping y_hat=2409\n","Skipping y_hat=2300\n","Skipping y_hat=1202\n","Skipping y_hat=2305\n","Skipping y_hat=1822\n","Skipping y_hat=2299\n"," 96% 80/83 [00:02<00:00, 34.86it/s]Skipping y_hat=1727\n","Skipping y_hat=2099\n","Skipping y_hat=2402\n","Skipping y_hat=1699\n","Skipping y_hat=2370\n","100% 83/83 [00:02<00:00, 34.41it/s]\n","accuracy of 10000 examples: 9926/10000 (99.26%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.27it/s]\n","accuracy of 10000 examples: 9925/10000 (99.25%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=904\n","Skipping y_hat=1599\n","  5% 4/83 [00:00<00:02, 32.01it/s]Skipping y_hat=781\n","Skipping y_hat=1699\n"," 10% 8/83 [00:00<00:02, 31.90it/s]Skipping y_hat=1400\n","Skipping y_hat=930\n"," 19% 16/83 [00:00<00:02, 32.71it/s]Skipping y_hat=1100\n","Skipping y_hat=1707\n","Skipping y_hat=1221\n","Skipping y_hat=1327\n"," 24% 20/83 [00:00<00:01, 31.95it/s]Skipping y_hat=599\n","Skipping y_hat=1204\n","Skipping y_hat=1300\n"," 29% 24/83 [00:00<00:01, 31.46it/s]Skipping y_hat=1323\n","Skipping y_hat=1605\n"," 39% 32/83 [00:00<00:01, 32.88it/s]Skipping y_hat=739\n","Skipping y_hat=279\n","Skipping y_hat=739\n"," 43% 36/83 [00:01<00:01, 33.47it/s]Skipping y_hat=104\n"," 48% 40/83 [00:01<00:01, 33.46it/s]Skipping y_hat=198\n","Skipping y_hat=899\n","Skipping y_hat=939\n"," 53% 44/83 [00:01<00:01, 32.85it/s]Skipping y_hat=835\n","Skipping y_hat=207\n"," 58% 48/83 [00:01<00:01, 32.70it/s]Skipping y_hat=797\n","Skipping y_hat=659\n","Skipping y_hat=999\n"," 63% 52/83 [00:01<00:00, 32.39it/s]Skipping y_hat=202\n","Skipping y_hat=379\n"," 72% 60/83 [00:01<00:00, 32.80it/s]Skipping y_hat=890\n","Skipping y_hat=1069\n","Skipping y_hat=1059\n"," 82% 68/83 [00:02<00:00, 33.19it/s]Skipping y_hat=390\n","Skipping y_hat=1998\n","Skipping y_hat=1228\n","Skipping y_hat=2101\n"," 92% 76/83 [00:02<00:00, 34.61it/s]Skipping y_hat=109\n","Skipping y_hat=1639\n","Skipping y_hat=2409\n","Skipping y_hat=901\n","Skipping y_hat=2300\n","Skipping y_hat=2305\n","Skipping y_hat=2403\n","Skipping y_hat=1822\n","Skipping y_hat=2299\n"," 96% 80/83 [00:02<00:00, 34.81it/s]Skipping y_hat=1727\n","Skipping y_hat=1699\n","Skipping y_hat=2370\n","100% 83/83 [00:02<00:00, 33.27it/s]\n","accuracy of 10000 examples: 9932/10000 (99.32%)\n","\n","Test Results:\n","test: 99.32%\n","\n","iter 53000: train loss 1.3323, val loss 1.3205\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=542\n","Skipping y_hat=1105\n","Skipping y_hat=400\n","Skipping y_hat=840\n","Skipping y_hat=1000\n","Skipping y_hat=1050\n","  5% 4/83 [00:00<00:02, 32.17it/s]Skipping y_hat=1200\n","Skipping y_hat=1200\n","Skipping y_hat=1240\n","Skipping y_hat=1350\n"," 10% 8/83 [00:00<00:02, 32.27it/s]Skipping y_hat=1100\n","Skipping y_hat=1400\n","Skipping y_hat=851\n","Skipping y_hat=1200\n"," 14% 12/83 [00:00<00:02, 31.82it/s]Skipping y_hat=780\n","Skipping y_hat=1301\n","Skipping y_hat=1300\n","Skipping y_hat=1300\n","Skipping y_hat=1000\n"," 19% 16/83 [00:00<00:02, 31.95it/s]Skipping y_hat=499\n","Skipping y_hat=1400\n","Skipping y_hat=2500\n"," 24% 20/83 [00:00<00:01, 32.52it/s]Skipping y_hat=2200\n","Skipping y_hat=1380\n","Skipping y_hat=1389\n","Skipping y_hat=1309\n","Skipping y_hat=901\n","Skipping y_hat=1006\n","Skipping y_hat=701\n","Skipping y_hat=1300\n","Skipping y_hat=1901\n"," 29% 24/83 [00:00<00:01, 32.92it/s]Skipping y_hat=1393\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n"," 34% 28/83 [00:00<00:01, 33.29it/s]Skipping y_hat=180\n"," 39% 32/83 [00:00<00:01, 33.34it/s]Skipping y_hat=120\n","Skipping y_hat=700\n"," 43% 36/83 [00:01<00:01, 33.43it/s]Skipping y_hat=800\n","Skipping y_hat=246\n","Skipping y_hat=206\n"," 48% 40/83 [00:01<00:01, 33.23it/s]Skipping y_hat=146\n","Skipping y_hat=500\n","Skipping y_hat=200\n"," 58% 48/83 [00:01<00:01, 33.00it/s]Skipping y_hat=739\n","Skipping y_hat=1190\n"," 63% 52/83 [00:01<00:00, 33.45it/s]Skipping y_hat=1020\n"," 67% 56/83 [00:01<00:00, 33.85it/s]Skipping y_hat=460\n","Skipping y_hat=160\n","Skipping y_hat=200\n"," 72% 60/83 [00:01<00:00, 33.80it/s]Skipping y_hat=140\n","Skipping y_hat=100\n","Skipping y_hat=899\n"," 77% 64/83 [00:01<00:00, 33.87it/s]Skipping y_hat=400\n","Skipping y_hat=430\n","Skipping y_hat=160\n","Skipping y_hat=150\n"," 82% 68/83 [00:02<00:00, 33.75it/s]Skipping y_hat=810\n","Skipping y_hat=2973\n","Skipping y_hat=3009\n","Skipping y_hat=1422\n"," 87% 72/83 [00:02<00:00, 34.23it/s]Skipping y_hat=90\n","Skipping y_hat=60\n","Skipping y_hat=30\n"," 92% 76/83 [00:02<00:00, 34.70it/s]Skipping y_hat=1497\n","Skipping y_hat=1072\n","Skipping y_hat=2300\n","Skipping y_hat=2202\n","Skipping y_hat=1405\n","Skipping y_hat=2403\n"," 96% 80/83 [00:02<00:00, 34.83it/s]Skipping y_hat=2002\n","Skipping y_hat=2402\n","Skipping y_hat=1699\n","100% 83/83 [00:02<00:00, 33.70it/s]\n","accuracy of 10000 examples: 9918/10000 (99.18%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 38.92it/s]\n","accuracy of 10000 examples: 9916/10000 (99.16%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=542\n","Skipping y_hat=935\n","Skipping y_hat=400\n","Skipping y_hat=1500\n","Skipping y_hat=1000\n","Skipping y_hat=499\n","  5% 4/83 [00:00<00:02, 33.02it/s]Skipping y_hat=1200\n","Skipping y_hat=487\n","Skipping y_hat=501\n","Skipping y_hat=1200\n","Skipping y_hat=1240\n","Skipping y_hat=1300\n","Skipping y_hat=1350\n"," 10% 8/83 [00:00<00:02, 33.17it/s]Skipping y_hat=1100\n","Skipping y_hat=1400\n","Skipping y_hat=1200\n"," 14% 12/83 [00:00<00:02, 33.44it/s]Skipping y_hat=780\n","Skipping y_hat=207\n","Skipping y_hat=1301\n","Skipping y_hat=1300\n","Skipping y_hat=1504\n","Skipping y_hat=1300\n"," 19% 16/83 [00:00<00:02, 33.34it/s]Skipping y_hat=499\n","Skipping y_hat=1400\n","Skipping y_hat=2500\n","Skipping y_hat=2301\n","Skipping y_hat=1448\n"," 24% 20/83 [00:00<00:01, 33.21it/s]Skipping y_hat=2200\n","Skipping y_hat=1309\n","Skipping y_hat=1006\n","Skipping y_hat=701\n","Skipping y_hat=1300\n"," 29% 24/83 [00:00<00:01, 32.29it/s]Skipping y_hat=1393\n","Skipping y_hat=1600\n"," 39% 32/83 [00:00<00:01, 33.00it/s]Skipping y_hat=39\n","Skipping y_hat=120\n"," 43% 36/83 [00:01<00:01, 33.13it/s]Skipping y_hat=106\n","Skipping y_hat=800\n","Skipping y_hat=206\n"," 48% 40/83 [00:01<00:01, 32.77it/s]Skipping y_hat=500\n","Skipping y_hat=700\n","Skipping y_hat=201\n"," 53% 44/83 [00:01<00:01, 32.47it/s]Skipping y_hat=499\n","Skipping y_hat=207\n","Skipping y_hat=258\n","Skipping y_hat=1570\n"," 58% 48/83 [00:01<00:01, 32.37it/s]Skipping y_hat=720\n","Skipping y_hat=1190\n"," 63% 52/83 [00:01<00:00, 32.05it/s]Skipping y_hat=222\n","Skipping y_hat=1020\n"," 67% 56/83 [00:01<00:00, 32.16it/s]Skipping y_hat=460\n","Skipping y_hat=160\n","Skipping y_hat=620\n"," 72% 60/83 [00:01<00:00, 32.19it/s]Skipping y_hat=140\n","Skipping y_hat=100\n","Skipping y_hat=899\n"," 77% 64/83 [00:01<00:00, 32.35it/s]Skipping y_hat=430\n","Skipping y_hat=160\n","Skipping y_hat=150\n"," 82% 68/83 [00:02<00:00, 32.69it/s]Skipping y_hat=2019\n","Skipping y_hat=1503\n","Skipping y_hat=3009\n"," 87% 72/83 [00:02<00:00, 33.51it/s]Skipping y_hat=90\n","Skipping y_hat=60\n","Skipping y_hat=80\n","Skipping y_hat=30\n"," 92% 76/83 [00:02<00:00, 33.96it/s]Skipping y_hat=1497\n","Skipping y_hat=2300\n","Skipping y_hat=603\n","Skipping y_hat=600\n","Skipping y_hat=2403\n","Skipping y_hat=1601\n","Skipping y_hat=2299\n"," 96% 80/83 [00:02<00:00, 34.18it/s]Skipping y_hat=2100\n","Skipping y_hat=2002\n","100% 83/83 [00:02<00:00, 33.15it/s]\n","accuracy of 10000 examples: 9904/10000 (99.03999999999999%)\n","\n","Test Results:\n","test: 99.04%\n","\n","iter 54000: train loss 1.3330, val loss 1.3238\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1096\n","Skipping y_hat=1289\n","Skipping y_hat=1000\n","  5% 4/83 [00:00<00:02, 33.01it/s]Skipping y_hat=697\n","Skipping y_hat=1241\n"," 10% 8/83 [00:00<00:02, 33.21it/s]Skipping y_hat=1741\n","Skipping y_hat=802\n","Skipping y_hat=519\n","Skipping y_hat=1720\n","Skipping y_hat=1600\n","Skipping y_hat=1705\n","Skipping y_hat=600\n"," 14% 12/83 [00:00<00:02, 33.12it/s]Skipping y_hat=1300\n","Skipping y_hat=915\n","Skipping y_hat=1200\n","Skipping y_hat=1301\n","Skipping y_hat=1207\n"," 19% 16/83 [00:00<00:02, 33.20it/s]Skipping y_hat=1698\n","Skipping y_hat=706\n","Skipping y_hat=808\n","Skipping y_hat=1400\n","Skipping y_hat=1797\n","Skipping y_hat=1599\n","Skipping y_hat=1005\n","Skipping y_hat=1100\n","Skipping y_hat=1595\n","Skipping y_hat=1278\n","Skipping y_hat=2005\n","Skipping y_hat=908\n"," 24% 20/83 [00:00<00:01, 32.57it/s]Skipping y_hat=1797\n","Skipping y_hat=1494\n","Skipping y_hat=2932\n","Skipping y_hat=705\n","Skipping y_hat=1097\n","Skipping y_hat=1126\n","Skipping y_hat=1299\n","Skipping y_hat=1099\n"," 29% 24/83 [00:00<00:01, 32.79it/s]Skipping y_hat=1448\n","Skipping y_hat=1994\n","Skipping y_hat=801\n","Skipping y_hat=1299\n","Skipping y_hat=610\n"," 34% 28/83 [00:00<00:01, 33.12it/s]Skipping y_hat=529\n","Skipping y_hat=730\n"," 39% 32/83 [00:00<00:01, 33.20it/s]Skipping y_hat=779\n"," 43% 36/83 [00:01<00:01, 33.75it/s]Skipping y_hat=800\n","Skipping y_hat=1583\n","Skipping y_hat=104\n","Skipping y_hat=800\n","Skipping y_hat=1099\n"," 48% 40/83 [00:01<00:01, 33.73it/s]Skipping y_hat=1203\n","Skipping y_hat=1209\n","Skipping y_hat=399\n"," 58% 48/83 [00:01<00:01, 34.19it/s]Skipping y_hat=977\n","Skipping y_hat=1507\n"," 63% 52/83 [00:01<00:00, 33.99it/s]Skipping y_hat=544\n","Skipping y_hat=1029\n","Skipping y_hat=700\n","Skipping y_hat=707\n"," 67% 56/83 [00:01<00:00, 34.24it/s]Skipping y_hat=674\n"," 77% 64/83 [00:01<00:00, 34.17it/s]Skipping y_hat=400\n","Skipping y_hat=720\n"," 82% 68/83 [00:02<00:00, 34.10it/s]Skipping y_hat=846\n","Skipping y_hat=3960\n","Skipping y_hat=3045\n","Skipping y_hat=2942\n"," 92% 76/83 [00:02<00:00, 34.76it/s]Skipping y_hat=1600\n","Skipping y_hat=1780\n","Skipping y_hat=1999\n","Skipping y_hat=1802\n","Skipping y_hat=1793\n","Skipping y_hat=1401\n","Skipping y_hat=2104\n","Skipping y_hat=2201\n"," 96% 80/83 [00:02<00:00, 34.72it/s]Skipping y_hat=2099\n","Skipping y_hat=2002\n","Skipping y_hat=1794\n","Skipping y_hat=1610\n","Skipping y_hat=1594\n","100% 83/83 [00:02<00:00, 33.97it/s]\n","accuracy of 10000 examples: 9913/10000 (99.13%)\n","Using precomputed batches\n","Max number of tokens 5.\n","100% 81/81 [00:02<00:00, 39.04it/s]\n","accuracy of 10000 examples: 9908/10000 (99.08%)\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=1096\n","Skipping y_hat=1289\n","Skipping y_hat=771\n","Skipping y_hat=1569\n","Skipping y_hat=1506\n","Skipping y_hat=1000\n"," 10% 8/83 [00:00<00:02, 33.57it/s]Skipping y_hat=1741\n","Skipping y_hat=1400\n","Skipping y_hat=500\n","Skipping y_hat=600\n"," 14% 12/83 [00:00<00:02, 32.41it/s]Skipping y_hat=1300\n","Skipping y_hat=915\n","Skipping y_hat=1200\n"," 19% 16/83 [00:00<00:02, 32.57it/s]Skipping y_hat=706\n","Skipping y_hat=2207\n","Skipping y_hat=1400\n","Skipping y_hat=1100\n","Skipping y_hat=1595\n","Skipping y_hat=1132\n","Skipping y_hat=1404\n","Skipping y_hat=1654\n","Skipping y_hat=908\n"," 24% 20/83 [00:00<00:01, 31.56it/s]Skipping y_hat=1797\n","Skipping y_hat=705\n","Skipping y_hat=1798\n","Skipping y_hat=1126\n","Skipping y_hat=1180\n","Skipping y_hat=1099\n"," 29% 24/83 [00:00<00:01, 31.77it/s]Skipping y_hat=1448\n","Skipping y_hat=1850\n","Skipping y_hat=801\n","Skipping y_hat=1299\n","Skipping y_hat=1840\n","Skipping y_hat=1296\n"," 34% 28/83 [00:00<00:01, 32.36it/s]Skipping y_hat=480\n","Skipping y_hat=730\n"," 39% 32/83 [00:00<00:01, 32.88it/s]Skipping y_hat=967\n","Skipping y_hat=140\n","Skipping y_hat=779\n"," 43% 36/83 [00:01<00:01, 33.46it/s]Skipping y_hat=800\n","Skipping y_hat=1583\n","Skipping y_hat=104\n","Skipping y_hat=630\n"," 48% 40/83 [00:01<00:01, 33.75it/s]Skipping y_hat=1203\n","Skipping y_hat=600\n"," 53% 44/83 [00:01<00:01, 33.84it/s]Skipping y_hat=100\n"," 58% 48/83 [00:01<00:01, 34.07it/s]Skipping y_hat=240\n","Skipping y_hat=1146\n","Skipping y_hat=1507\n"," 63% 52/83 [00:01<00:00, 34.16it/s]Skipping y_hat=1029\n","Skipping y_hat=138\n","Skipping y_hat=700\n","Skipping y_hat=707\n"," 72% 60/83 [00:01<00:00, 34.45it/s]Skipping y_hat=926\n","Skipping y_hat=130\n"," 82% 68/83 [00:02<00:00, 33.77it/s]Skipping y_hat=178\n","Skipping y_hat=2044\n","Skipping y_hat=810\n","Skipping y_hat=2708\n","Skipping y_hat=2199\n"," 87% 72/83 [00:02<00:00, 33.57it/s]Skipping y_hat=129\n"," 92% 76/83 [00:02<00:00, 33.93it/s]Skipping y_hat=1497\n","Skipping y_hat=2280\n","Skipping y_hat=1600\n","Skipping y_hat=1693\n","Skipping y_hat=1802\n","Skipping y_hat=1793\n","Skipping y_hat=1401\n","Skipping y_hat=2104\n","Skipping y_hat=2201\n"," 96% 80/83 [00:02<00:00, 33.49it/s]Skipping y_hat=2099\n","Skipping y_hat=2002\n","Skipping y_hat=706\n","Skipping y_hat=1794\n","Skipping y_hat=1494\n","Skipping y_hat=1610\n","Skipping y_hat=1170\n","Skipping y_hat=1594\n","Skipping y_hat=1296\n","100% 83/83 [00:02<00:00, 33.35it/s]\n","accuracy of 10000 examples: 9914/10000 (99.14%)\n","\n","Test Results:\n","test: 99.14%\n","\n","iter 55000: train loss 1.3291, val loss 1.3339\n","Using precomputed batches\n","Max number of tokens 5.\n","  0% 0/83 [00:00<?, ?it/s]Skipping y_hat=652\n","Skipping y_hat=1096\n","Skipping y_hat=1001\n","Skipping y_hat=699\n","  6% 5/83 [00:00<00:02, 33.60it/s]\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n","    return func(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/addition/model.py\", line 214, in generate\n","    logits, _ = self(idx_cond)\n","                ^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/addition/model.py\", line 154, in forward\n","    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/addition/train_end_padding.py\", line 465, in <module>\n","    test_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","                                            ^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/addition/evaluation.py\", line 264, in evaluate_addition_batch\n","    return evaluate_addition_precomputed(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/addition/evaluation.py\", line 151, in evaluate_addition_precomputed\n","    y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n","    with ctx_factory():\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/grad_mode.py\", line 84, in __exit__\n","    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n","    \n","KeyboardInterrupt\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m4_operands_0_to_999_balanced_digit_plain_v4\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/rbf3dum8\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m4_operands_0_to_999_balanced_digit/plain_out_v4/wandb/run-20250623_201605-rbf3dum8/logs\u001b[0m\n"]}],"source":["!python train_end_padding.py 4_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"zyLD88utpuPt"},"source":["## Reproduce 2 Operands 0-99 Multiplication (Their Code, Their Data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":106,"status":"ok","timestamp":1750737021954,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"jkW7cJMjptoz","outputId":"5242cf32-4d33-441e-9d7a-f1af22c10f6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/teaching_arithmetic\n","bench.py\t\t  prompts\n","config\t\t\t  __pycache__\n","config2\t\t\t  README.md\n","config_gpt2\t\t  result_analysis.ipynb\n","configurator.py\t\t  run\n","data\t\t\t  run_gpt2\n","evaluate_additions.py\t  sample_addition_fewshot2.py\n","evaluate_models.py\t  sample_addition_fewshot.py\n","LICENSE\t\t\t  sample_addition.py\n","main_utils.py\t\t  sample.py\n","matrix_completion\t  test_addition.py\n","meta_all_ascii_chars.pkl  train_mixed.py\n","model.py\t\t  train_multi_task.py\n","myStart.ipynb\t\t  train_no527.py\n","out\t\t\t  train.py\n","out2\t\t\t  utils\n","plots\t\t\t  wandb\n"]}],"source":["%cd /content/drive/MyDrive/teaching_arithmetic\n","%pwd   # verify you’re in the right place\n","!ls    # should show train.py, 4_operands_addition.txt, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1851030,"status":"ok","timestamp":1750719483599,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"rdtzXjoKp-Aq","outputId":"3e1d8db1-f19d-431b-c4ed-01e8cd4890d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","correct: 66*13=858\n","outputs(x):  94*32=3948\n","4*\n","wrong  : 94*32=3948\n","correct: 94*32=3008\n","outputs(x):  14*66=1008\n","84\n","wrong  : 14*66=1008\n","correct: 14*66=924\n","outputs(x):  23*70=1980\n","59\n","wrong  : 23*70=1980\n","correct: 23*70=1610\n","outputs(x):  32*19=570\n","52*\n","wrong  : 32*19=570\n","correct: 32*19=608\n","outputs(x):  72*13=806\n","15*\n","wrong  : 72*13=806\n","correct: 72*13=936\n","outputs(x):  39*10=350\n","28*\n","wrong  : 39*10=350\n","correct: 39*10=390\n","outputs(x):  64*92=6256\n","77\n","wrong  : 64*92=6256\n","correct: 64*92=5888\n","outputs(x):  77*42=3654\n","73\n","wrong  : 77*42=3654\n","correct: 77*42=3234\n","outputs(x):  98*53=5144\n","14\n","wrong  : 98*53=5144\n","correct: 98*53=5194\n","outputs(x):  66*71=4752\n","33\n","wrong  : 66*71=4752\n","correct: 66*71=4686\n","outputs(x):  80*33=2673\n","66\n","wrong  : 80*33=2673\n","correct: 80*33=2640\n","outputs(x):  91*34=374\n","92*\n","wrong  : 91*34=374\n","correct: 91*34=3094\n","outputs(x):  47*40=1840\n","71\n","wrong  : 47*40=1840\n","correct: 47*40=1880\n","outputs(x):  27*50=1050\n","44\n","wrong  : 27*50=1050\n","correct: 27*50=1350\n","outputs(x):  19*73=1309\n","6*\n","wrong  : 19*73=1309\n","correct: 19*73=1387\n","outputs(x):  86*60=4920\n","0*\n","wrong  : 86*60=4920\n","correct: 86*60=5160\n","outputs(x):  71*53=3233\n","53\n","wrong  : 71*53=3233\n","correct: 71*53=3763\n","outputs(x):  16*14=1064\n","90\n","wrong  : 16*14=1064\n","correct: 16*14=224\n","outputs(x):  89*94=8416\n","3*\n","wrong  : 89*94=8416\n","correct: 89*94=8366\n","outputs(x):  39*44=1666\n","13\n","wrong  : 39*44=1666\n","correct: 39*44=1716\n","outputs(x):  69*71=5893\n","11\n","wrong  : 69*71=5893\n","correct: 69*71=4899\n","outputs(x):  62*35=1470\n","55\n","wrong  : 62*35=1470\n","correct: 62*35=2170\n","outputs(x):  72*15=180\n","21*\n","wrong  : 72*15=180\n","correct: 72*15=1080\n","outputs(x):  64*47=2162\n","13\n","wrong  : 64*47=2162\n","correct: 64*47=3008\n","outputs(x):  65*62=4412\n","73\n","wrong  : 65*62=4412\n","correct: 65*62=4030\n","outputs(x):  51*11=693\n","55*\n","wrong  : 51*11=693\n","correct: 51*11=561\n","outputs(x):  29*37=1363\n","12\n","wrong  : 29*37=1363\n","correct: 29*37=1073\n","outputs(x):  22*69=158\n","24*\n","wrong  : 22*69=158\n","correct: 22*69=1518\n","outputs(x):  88*92=7176\n","92\n","wrong  : 88*92=7176\n","correct: 88*92=8096\n","outputs(x):  96*58=5508\n","84\n","wrong  : 96*58=5508\n","correct: 96*58=5568\n","outputs(x):  41*18=798\n","55*\n","wrong  : 41*18=798\n","correct: 41*18=738\n","outputs(x):  77*39=3783\n","54\n","wrong  : 77*39=3783\n","correct: 77*39=3003\n","outputs(x):  61*75=4615\n","71\n","wrong  : 61*75=4615\n","correct: 61*75=4575\n","outputs(x):  66*37=2513\n","95\n","wrong  : 66*37=2513\n","correct: 66*37=2442\n","outputs(x):  32*43=1386\n","1*\n","wrong  : 32*43=1386\n","correct: 32*43=1376\n","outputs(x):  17*74=1332\n","99\n","wrong  : 17*74=1332\n","correct: 17*74=1258\n","outputs(x):  48*45=2070\n","1*\n","wrong  : 48*45=2070\n","correct: 48*45=2160\n","outputs(x):  87*44=3288\n","99\n","wrong  : 87*44=3288\n","correct: 87*44=3828\n","outputs(x):  90*19=1820\n","54\n","wrong  : 90*19=1820\n","correct: 90*19=1710\n","outputs(x):  57*28=1512\n","81\n","wrong  : 57*28=1512\n","correct: 57*28=1596\n","outputs(x):  59*46=2574\n","28\n","wrong  : 59*46=2574\n","correct: 59*46=2714\n","outputs(x):  19*22=408\n","14*\n","wrong  : 19*22=408\n","correct: 19*22=418\n","outputs(x):  92*85=8740\n","11\n","wrong  : 92*85=8740\n","correct: 92*85=7820\n","outputs(x):  45*16=880\n","44*\n","wrong  : 45*16=880\n","correct: 45*16=720\n","outputs(x):  41*30=1270\n","55\n","wrong  : 41*30=1270\n","correct: 41*30=1230\n","outputs(x):  44*28=1152\n","25\n","wrong  : 44*28=1152\n","correct: 44*28=1232\n","outputs(x):  56*71=3116\n","88\n","wrong  : 56*71=3116\n","correct: 56*71=3976\n","outputs(x):  25*46=1100\n","21\n","wrong  : 25*46=1100\n","correct: 25*46=1150\n","outputs(x):  43*82=3968\n","68\n","wrong  : 43*82=3968\n","correct: 43*82=3526\n","outputs(x):  42*43=172\n","79*\n","wrong  : 42*43=172\n","correct: 42*43=1806\n","outputs(x):  19*14=406\n","43*\n","wrong  : 19*14=406\n","correct: 19*14=266\n","outputs(x):  94*49=416\n","12*\n","wrong  : 94*49=416\n","correct: 94*49=4606\n","outputs(x):  56*26=1436\n","47\n","wrong  : 56*26=1436\n","correct: 56*26=1456\n","outputs(x):  17*86=1292\n","12\n","wrong  : 17*86=1292\n","correct: 17*86=1462\n","outputs(x):  16*38=684\n","38*\n","wrong  : 16*38=684\n","correct: 16*38=608\n","outputs(x):  36*60=2460\n","23\n","wrong  : 36*60=2460\n","correct: 36*60=2160\n","outputs(x):  95*47=4365\n","76\n","wrong  : 95*47=4365\n","correct: 95*47=4465\n","outputs(x):  33*89=2871\n","5*\n","wrong  : 33*89=2871\n","correct: 33*89=2937\n","outputs(x):  93*53=5141\n","18\n","wrong  : 93*53=5141\n","correct: 93*53=4929\n","outputs(x):  40*66=2970\n","60\n","wrong  : 40*66=2970\n","correct: 40*66=2640\n","outputs(x):  82*85=7055\n","83\n","wrong  : 82*85=7055\n","correct: 82*85=6970\n","outputs(x):  17*25=475\n","9*9\n","wrong  : 17*25=475\n","correct: 17*25=425\n","outputs(x):  41*38=1518\n","4*\n","wrong  : 41*38=1518\n","correct: 41*38=1558\n","outputs(x):  80*31=2418\n","7*\n","wrong  : 80*31=2418\n","correct: 80*31=2480\n","outputs(x):  18*99=178\n","37*\n","wrong  : 18*99=178\n","correct: 18*99=1782\n","outputs(x):  59*47=2633\n","15\n","wrong  : 59*47=2633\n","correct: 59*47=2773\n","outputs(x):  98*54=5104\n","44\n","wrong  : 98*54=5104\n","correct: 98*54=5292\n","outputs(x):  97*58=5742\n","13\n","wrong  : 97*58=5742\n","correct: 97*58=5626\n","outputs(x):  43*35=1595\n","30\n","wrong  : 43*35=1595\n","correct: 43*35=1505\n","outputs(x):  13*27=378\n","64*\n","wrong  : 13*27=378\n","correct: 13*27=351\n","outputs(x):  90*70=6440\n","26\n","wrong  : 90*70=6440\n","correct: 90*70=6300\n","outputs(x):  35*48=1440\n","57\n","wrong  : 35*48=1440\n","correct: 35*48=1680\n","outputs(x):  69*97=6633\n","87\n","wrong  : 69*97=6633\n","correct: 69*97=6693\n","outputs(x):  51*88=4288\n","14\n","wrong  : 51*88=4288\n","correct: 51*88=4488\n","outputs(x):  93*76=7220\n","94\n","wrong  : 93*76=7220\n","correct: 93*76=7068\n","outputs(x):  62*58=3168\n","91\n","wrong  : 62*58=3168\n","correct: 62*58=3596\n","outputs(x):  15*29=475\n","24*\n","wrong  : 15*29=475\n","correct: 15*29=435\n","outputs(x):  74*44=3344\n","35\n","wrong  : 74*44=3344\n","correct: 74*44=3256\n","outputs(x):  26*60=1240\n","59\n","wrong  : 26*60=1240\n","correct: 26*60=1560\n","outputs(x):  92*77=7314\n","58\n","wrong  : 92*77=7314\n","correct: 92*77=7084\n","outputs(x):  29*31=837\n","60*\n","wrong  : 29*31=837\n","correct: 29*31=899\n","outputs(x):  78*64=4352\n","54\n","wrong  : 78*64=4352\n","correct: 78*64=4992\n","outputs(x):  81*50=4250\n","62\n","wrong  : 81*50=4250\n","correct: 81*50=4050\n","outputs(x):  49*23=1104\n","6*\n","wrong  : 49*23=1104\n","correct: 49*23=1127\n","outputs(x):  90*11=1080\n","38\n","wrong  : 90*11=1080\n","correct: 90*11=990\n","outputs(x):  22*68=1476\n","70\n","wrong  : 22*68=1476\n","correct: 22*68=1496\n","outputs(x):  14*51=774\n","8*7\n","wrong  : 14*51=774\n","correct: 14*51=714\n","outputs(x):  15*41=656\n","88*\n","wrong  : 15*41=656\n","correct: 15*41=615\n","outputs(x):  35*16=400\n","65*\n","wrong  : 35*16=400\n","correct: 35*16=560\n","outputs(x):  99*13=247\n","86*\n","wrong  : 99*13=247\n","correct: 99*13=1287\n","outputs(x):  96*12=1140\n","62\n","wrong  : 96*12=1140\n","correct: 96*12=1152\n","outputs(x):  17*46=1196\n","63\n","wrong  : 17*46=1196\n","correct: 17*46=782\n","outputs(x):  34*50=1200\n","63\n","wrong  : 34*50=1200\n","correct: 34*50=1700\n","outputs(x):  89*59=5713\n","73\n","wrong  : 89*59=5713\n","correct: 89*59=5251\n","outputs(x):  78*16=128\n","80*\n","wrong  : 78*16=128\n","correct: 78*16=1248\n","outputs(x):  91*59=5487\n","38\n","wrong  : 91*59=5487\n","correct: 91*59=5369\n","outputs(x):  37*28=996\n","75*\n","wrong  : 37*28=996\n","correct: 37*28=1036\n","outputs(x):  52*88=4464\n","6*\n","wrong  : 52*88=4464\n","correct: 52*88=4576\n","outputs(x):  89*21=1449\n","59\n","wrong  : 89*21=1449\n","correct: 89*21=1869\n","outputs(x):  49*36=1404\n","97\n","wrong  : 49*36=1404\n","correct: 49*36=1764\n","outputs(x):  47*77=308\n","37*\n","wrong  : 47*77=308\n","correct: 47*77=3619\n","outputs(x):  64*84=5632\n","68\n","wrong  : 64*84=5632\n","correct: 64*84=5376\n","outputs(x):  73*42=3948\n","16\n","wrong  : 73*42=3948\n","correct: 73*42=3066\n","outputs(x):  50*83=4640\n","7*\n","wrong  : 50*83=4640\n","correct: 50*83=4150\n","outputs(x):  71*49=3408\n","14\n","wrong  : 71*49=3408\n","correct: 71*49=3479\n","outputs(x):  23*64=1518\n","59\n","wrong  : 23*64=1518\n","correct: 23*64=1472\n","outputs(x):  60*34=2720\n","16\n","wrong  : 60*34=2720\n","correct: 60*34=2040\n","outputs(x):  13*67=807\n","65*\n","wrong  : 13*67=807\n","correct: 13*67=871\n","outputs(x):  43*32=1056\n","18\n","wrong  : 43*32=1056\n","correct: 43*32=1376\n","outputs(x):  47*51=2447\n","18\n","wrong  : 47*51=2447\n","correct: 47*51=2397\n","outputs(x):  93*71=6461\n","81\n","wrong  : 93*71=6461\n","correct: 93*71=6603\n","outputs(x):  84*49=4214\n","17\n","wrong  : 84*49=4214\n","correct: 84*49=4116\n","outputs(x):  58*81=5128\n","26\n","wrong  : 58*81=5128\n","correct: 58*81=4698\n","outputs(x):  23*94=282\n","87*\n","wrong  : 23*94=282\n","correct: 23*94=2162\n","outputs(x):  77*79=5767\n","44\n","wrong  : 77*79=5767\n","correct: 77*79=6083\n","outputs(x):  46*97=4158\n","56\n","wrong  : 46*97=4158\n","correct: 46*97=4462\n","outputs(x):  52*14=768\n","81*\n","wrong  : 52*14=768\n","correct: 52*14=728\n","outputs(x):  26*66=1612\n","72\n","wrong  : 26*66=1612\n","correct: 26*66=1716\n","outputs(x):  34*63=2482\n","41\n","wrong  : 34*63=2482\n","correct: 34*63=2142\n","outputs(x):  57*39=2145\n","90\n","wrong  : 57*39=2145\n","correct: 57*39=2223\n","outputs(x):  14*71=1274\n","53\n","wrong  : 14*71=1274\n","correct: 14*71=994\n","outputs(x):  72*90=6080\n","43\n","wrong  : 72*90=6080\n","correct: 72*90=6480\n","outputs(x):  52*69=3458\n","21\n","wrong  : 52*69=3458\n","correct: 52*69=3588\n","outputs(x):  33*43=1729\n","63\n","wrong  : 33*43=1729\n","correct: 33*43=1419\n","outputs(x):  64*54=3402\n","15\n","wrong  : 64*54=3402\n","correct: 64*54=3456\n","outputs(x):  47*99=4663\n","55\n","wrong  : 47*99=4663\n","correct: 47*99=4653\n","outputs(x):  58*49=294\n","28*\n","wrong  : 58*49=294\n","correct: 58*49=2842\n","outputs(x):  53*39=2496\n","97\n","wrong  : 53*39=2496\n","correct: 53*39=2067\n"," 56% 14/25 [00:01<00:00, 11.28it/s]outputs(x):  99*65=6370\n","0*\n","wrong  : 99*65=6370\n","correct: 99*65=6435\n","outputs(x):  26*72=1944\n","94\n","wrong  : 26*72=1944\n","correct: 26*72=1872\n","outputs(x):  72*11=814\n","45*\n","wrong  : 72*11=814\n","correct: 72*11=792\n","outputs(x):  22*21=504\n","58*\n","wrong  : 22*21=504\n","correct: 22*21=462\n","outputs(x):  21*92=1742\n","13\n","wrong  : 21*92=1742\n","correct: 21*92=1932\n","outputs(x):  48*40=1840\n","71\n","wrong  : 48*40=1840\n","correct: 48*40=1920\n","outputs(x):  95*20=1700\n","39\n","wrong  : 95*20=1700\n","correct: 95*20=1900\n","outputs(x):  68*26=1508\n","49\n","wrong  : 68*26=1508\n","correct: 68*26=1768\n","outputs(x):  57*84=4620\n","23\n","wrong  : 57*84=4620\n","correct: 57*84=4788\n","outputs(x):  84*63=5628\n","51\n","wrong  : 84*63=5628\n","correct: 84*63=5292\n","outputs(x):  32*77=2233\n","30\n","wrong  : 32*77=2233\n","correct: 32*77=2464\n","outputs(x):  81*18=1536\n","25\n","wrong  : 81*18=1536\n","correct: 81*18=1458\n","outputs(x):  20*78=2340\n","1*\n","wrong  : 20*78=2340\n","correct: 20*78=1560\n","outputs(x):  74*42=3628\n","75\n","wrong  : 74*42=3628\n","correct: 74*42=3108\n","outputs(x):  38*34=1252\n","93\n","wrong  : 38*34=1252\n","correct: 38*34=1292\n","outputs(x):  36*55=2470\n","0*\n","wrong  : 36*55=2470\n","correct: 36*55=1980\n","outputs(x):  22*95=2905\n","43\n","wrong  : 22*95=2905\n","correct: 22*95=2090\n","outputs(x):  62*70=4960\n","12\n","wrong  : 62*70=4960\n","correct: 62*70=4340\n","outputs(x):  90*60=5600\n","1*\n","wrong  : 90*60=5600\n","correct: 90*60=5400\n","outputs(x):  91*48=4416\n","3*\n","wrong  : 91*48=4416\n","correct: 91*48=4368\n","outputs(x):  85*53=4055\n","39\n","wrong  : 85*53=4055\n","correct: 85*53=4505\n","outputs(x):  96*98=9506\n","85\n","wrong  : 96*98=9506\n","correct: 96*98=9408\n","outputs(x):  34*44=1256\n","38\n","wrong  : 34*44=1256\n","correct: 34*44=1496\n","outputs(x):  95*70=6590\n","9*\n","wrong  : 95*70=6590\n","correct: 95*70=6650\n","outputs(x):  22*55=1120\n","31\n","wrong  : 22*55=1120\n","correct: 22*55=1210\n","outputs(x):  42*11=506\n","98*\n","wrong  : 42*11=506\n","correct: 42*11=462\n","outputs(x):  33*54=1672\n","55\n","wrong  : 33*54=1672\n","correct: 33*54=1782\n","outputs(x):  76*69=5313\n","93\n","wrong  : 76*69=5313\n","correct: 76*69=5244\n","outputs(x):  45*67=2613\n","95\n","wrong  : 45*67=2613\n","correct: 45*67=3015\n","outputs(x):  82*49=3388\n","79\n","wrong  : 82*49=3388\n","correct: 82*49=4018\n","outputs(x):  30*72=2304\n","6*\n","wrong  : 30*72=2304\n","correct: 30*72=2160\n","outputs(x):  98*23=2024\n","19\n","wrong  : 98*23=2024\n","correct: 98*23=2254\n","outputs(x):  72*79=5767\n","44\n","wrong  : 72*79=5767\n","correct: 72*79=5688\n","outputs(x):  68*42=2436\n","47\n","wrong  : 68*42=2436\n","correct: 68*42=2856\n","outputs(x):  97*49=4263\n","33\n","wrong  : 97*49=4263\n","correct: 97*49=4753\n","outputs(x):  27*83=2407\n","16\n","wrong  : 27*83=2407\n","correct: 27*83=2241\n","outputs(x):  26*34=936\n","71*\n","wrong  : 26*34=936\n","correct: 26*34=884\n","outputs(x):  53*22=1272\n","53\n","wrong  : 53*22=1272\n","correct: 53*22=1166\n","outputs(x):  35*90=3240\n","52\n","wrong  : 35*90=3240\n","correct: 35*90=3150\n","outputs(x):  48*59=2242\n","64\n","wrong  : 48*59=2242\n","correct: 48*59=2832\n","outputs(x):  88*14=1372\n","7*\n","wrong  : 88*14=1372\n","correct: 88*14=1232\n","outputs(x):  57*16=972\n","99*\n","wrong  : 57*16=972\n","correct: 57*16=912\n","outputs(x):  53*70=3150\n","40\n","wrong  : 53*70=3150\n","correct: 53*70=3710\n","outputs(x):  17*52=1054\n","49\n","wrong  : 17*52=1054\n","correct: 17*52=884\n","outputs(x):  69*58=4582\n","33\n","wrong  : 69*58=4582\n","correct: 69*58=4002\n","outputs(x):  99*18=1728\n","73\n","wrong  : 99*18=1728\n","correct: 99*18=1782\n","outputs(x):  61*65=4615\n","71\n","wrong  : 61*65=4615\n","correct: 61*65=3965\n","outputs(x):  42*74=3182\n","67\n","wrong  : 42*74=3182\n","correct: 42*74=3108\n","outputs(x):  97*27=2565\n","40\n","wrong  : 97*27=2565\n","correct: 97*27=2619\n","outputs(x):  99*69=6039\n","54\n","wrong  : 99*69=6039\n","correct: 99*69=6831\n","outputs(x):  85*61=5795\n","67\n","wrong  : 85*61=5795\n","correct: 85*61=5185\n","outputs(x):  79*40=3920\n","70\n","wrong  : 79*40=3920\n","correct: 79*40=3160\n","outputs(x):  66*81=5612\n","28\n","wrong  : 66*81=5612\n","correct: 66*81=5346\n","outputs(x):  53*13=663\n","3*1\n","wrong  : 53*13=663\n","correct: 53*13=689\n","outputs(x):  27*43=1162\n","24\n","wrong  : 27*43=1162\n","correct: 27*43=1161\n","outputs(x):  55*86=4300\n","62\n","wrong  : 55*86=4300\n","correct: 55*86=4730\n","outputs(x):  83*31=299\n","90*\n","wrong  : 83*31=299\n","correct: 83*31=2573\n","outputs(x):  66*33=2508\n","80\n","wrong  : 66*33=2508\n","correct: 66*33=2178\n","outputs(x):  25*25=375\n","16*\n","wrong  : 25*25=375\n","correct: 25*25=625\n","outputs(x):  97*87=7569\n","10\n","wrong  : 97*87=7569\n","correct: 97*87=8439\n","outputs(x):  87*97=7663\n","82\n","wrong  : 87*97=7663\n","correct: 87*97=8439\n","outputs(x):  71*74=5994\n","31\n","wrong  : 71*74=5994\n","correct: 71*74=5254\n","outputs(x):  31*12=152\n","53*\n","wrong  : 31*12=152\n","correct: 31*12=372\n","outputs(x):  80*49=3160\n","19\n","wrong  : 80*49=3160\n","correct: 80*49=3920\n","outputs(x):  40*49=1760\n","4*\n","wrong  : 40*49=1760\n","correct: 40*49=1960\n","outputs(x):  26*96=2416\n","6*\n","wrong  : 26*96=2416\n","correct: 26*96=2496\n","outputs(x):  17*93=1679\n","24\n","wrong  : 17*93=1679\n","correct: 17*93=1581\n","outputs(x):  90*24=2720\n","16\n","wrong  : 90*24=2720\n","correct: 90*24=2160\n","outputs(x):  51*77=4047\n","19\n","wrong  : 51*77=4047\n","correct: 51*77=3927\n","outputs(x):  66*21=1449\n","59\n","wrong  : 66*21=1449\n","correct: 66*21=1386\n","outputs(x):  67*76=5852\n","68\n","wrong  : 67*76=5852\n","correct: 67*76=5092\n","outputs(x):  65*21=1995\n","38\n","wrong  : 65*21=1995\n","correct: 65*21=1365\n","outputs(x):  91*28=2604\n","69\n","wrong  : 91*28=2604\n","correct: 91*28=2548\n","outputs(x):  15*74=1140\n","37\n","wrong  : 15*74=1140\n","correct: 15*74=1110\n","outputs(x):  62*18=1152\n","25\n","wrong  : 62*18=1152\n","correct: 62*18=1116\n","outputs(x):  63*80=5120\n","92\n","wrong  : 63*80=5120\n","correct: 63*80=5040\n","outputs(x):  73*77=5467\n","7*\n","wrong  : 73*77=5467\n","correct: 73*77=5621\n","outputs(x):  46*35=1260\n","65\n","wrong  : 46*35=1260\n","correct: 46*35=1610\n","outputs(x):  98*50=5900\n","4*\n","wrong  : 98*50=5900\n","correct: 98*50=4900\n","outputs(x):  35*70=2100\n","35\n","wrong  : 35*70=2100\n","correct: 35*70=2450\n","outputs(x):  28*62=1612\n","72\n","wrong  : 28*62=1612\n","correct: 28*62=1736\n","outputs(x):  98*77=7766\n","9*\n","wrong  : 98*77=7766\n","correct: 98*77=7546\n","outputs(x):  81*91=7735\n","8*\n","wrong  : 81*91=7735\n","correct: 81*91=7371\n","outputs(x):  90*27=2590\n","7*\n","wrong  : 90*27=2590\n","correct: 90*27=2430\n","outputs(x):  93*38=3154\n","38\n","wrong  : 93*38=3154\n","correct: 93*38=3534\n","outputs(x):  58*54=3078\n","23\n","wrong  : 58*54=3078\n","correct: 58*54=3132\n","outputs(x):  16*73=1008\n","1*\n","wrong  : 16*73=1008\n","correct: 16*73=1168\n","outputs(x):  61*21=1701\n","91\n","wrong  : 61*21=1701\n","correct: 61*21=1281\n","outputs(x):  83*16=1168\n","60\n","wrong  : 83*16=1168\n","correct: 83*16=1328\n","outputs(x):  51*97=4047\n","19\n","wrong  : 51*97=4047\n","correct: 51*97=4947\n","outputs(x):  73*37=2923\n","11\n","wrong  : 73*37=2923\n","correct: 73*37=2701\n","outputs(x):  55*54=3078\n","23\n","wrong  : 55*54=3078\n","correct: 55*54=2970\n","outputs(x):  93*52=5148\n","6*\n","wrong  : 93*52=5148\n","correct: 93*52=4836\n","outputs(x):  90*38=3040\n","32\n","wrong  : 90*38=3040\n","correct: 90*38=3420\n","outputs(x):  73*80=6160\n","27\n","wrong  : 73*80=6160\n","correct: 73*80=5840\n","outputs(x):  21*28=568\n","57*\n","wrong  : 21*28=568\n","correct: 21*28=588\n","outputs(x):  61*28=1408\n","54\n","wrong  : 61*28=1408\n","correct: 61*28=1708\n","outputs(x):  97*36=3564\n","32\n","wrong  : 97*36=3564\n","correct: 97*36=3492\n","outputs(x):  44*78=3276\n","30\n","wrong  : 44*78=3276\n","correct: 44*78=3432\n","outputs(x):  67*57=3149\n","36\n","wrong  : 67*57=3149\n","correct: 67*57=3819\n","outputs(x):  73*69=4554\n","12\n","wrong  : 73*69=4554\n","correct: 73*69=5037\n","outputs(x):  27*34=936\n","71*\n","wrong  : 27*34=936\n","correct: 27*34=918\n","outputs(x):  34*32=1056\n","18\n","wrong  : 34*32=1056\n","correct: 34*32=1088\n","outputs(x):  13*57=57\n","7*4=\n","wrong  : 13*57=57\n","correct: 13*57=741\n","outputs(x):  14*78=1252\n","57\n","wrong  : 14*78=1252\n","correct: 14*78=1092\n","outputs(x):  41*59=2139\n","69\n","wrong  : 41*59=2139\n","correct: 41*59=2419\n","outputs(x):  53*99=4257\n","6*\n","wrong  : 53*99=4257\n","correct: 53*99=5247\n","outputs(x):  69*24=1106\n","63\n","wrong  : 69*24=1106\n","correct: 69*24=1656\n","outputs(x):  46*34=1668\n","59\n","wrong  : 46*34=1668\n","correct: 46*34=1564\n","outputs(x):  53*92=416\n","96*\n","wrong  : 53*92=416\n","correct: 53*92=4876\n","outputs(x):  92*21=1722\n","50\n","wrong  : 92*21=1722\n","correct: 92*21=1932\n","outputs(x):  58*56=3196\n","62\n","wrong  : 58*56=3196\n","correct: 58*56=3248\n","outputs(x):  38*22=682\n","36*\n","wrong  : 38*22=682\n","correct: 38*22=836\n","outputs(x):  91*55=455\n","83*\n","wrong  : 91*55=455\n","correct: 91*55=5005\n","outputs(x):  57*11=737\n","37*\n","wrong  : 57*11=737\n","correct: 57*11=627\n","outputs(x):  96*96=8928\n","53\n","wrong  : 96*96=8928\n","correct: 96*96=9216\n","outputs(x):  25*27=2295\n","86\n","wrong  : 25*27=2295\n","correct: 25*27=675\n","outputs(x):  37*75=2850\n","66\n","wrong  : 37*75=2850\n","correct: 37*75=2775\n","outputs(x):  38*42=1728\n","37\n","wrong  : 38*42=1728\n","correct: 38*42=1596\n","outputs(x):  15*12=1140\n","62\n","wrong  : 15*12=1140\n","correct: 15*12=180\n","outputs(x):  29*46=1104\n","18\n","wrong  : 29*46=1104\n","correct: 29*46=1334\n","outputs(x):  10*87=800\n","45*\n","wrong  : 10*87=800\n","correct: 10*87=870\n","outputs(x):  66*84=5616\n","12\n","wrong  : 66*84=5616\n","correct: 66*84=5544\n","outputs(x):  12*49=594\n","5*5\n","wrong  : 12*49=594\n","correct: 12*49=588\n","outputs(x):  72*83=6640\n","97\n","wrong  : 72*83=6640\n","correct: 72*83=5976\n","outputs(x):  72*61=4453\n","50\n","wrong  : 72*61=4453\n","correct: 72*61=4392\n","outputs(x):  30*32=1600\n","78\n","wrong  : 30*32=1600\n","correct: 30*32=960\n","outputs(x):  58*21=1008\n","40\n","wrong  : 58*21=1008\n","correct: 58*21=1218\n","outputs(x):  55*36=1840\n","75\n","wrong  : 55*36=1840\n","correct: 55*36=1980\n","outputs(x):  55*25=1350\n","91\n","wrong  : 55*25=1350\n","correct: 55*25=1375\n","outputs(x):  96*61=5734\n","2*\n","wrong  : 96*61=5734\n","correct: 96*61=5856\n","outputs(x):  90*69=6555\n","41\n","wrong  : 90*69=6555\n","correct: 90*69=6210\n","outputs(x):  78*78=624\n","35*\n","wrong  : 78*78=624\n","correct: 78*78=6084\n","outputs(x):  84*21=1944\n","82\n","wrong  : 84*21=1944\n","correct: 84*21=1764\n","outputs(x):  62*68=4284\n","58\n","wrong  : 62*68=4284\n","correct: 62*68=4216\n","outputs(x):  10*81=970\n","21*\n","wrong  : 10*81=970\n","correct: 10*81=810\n","outputs(x):  96*74=7252\n","13\n","wrong  : 96*74=7252\n","correct: 96*74=7104\n","outputs(x):  98*68=6468\n","1*\n","wrong  : 98*68=6468\n","correct: 98*68=6664\n","outputs(x):  85*12=1140\n","62\n","wrong  : 85*12=1140\n","correct: 85*12=1020\n","outputs(x):  88*40=3920\n","70\n","wrong  : 88*40=3920\n","correct: 88*40=3520\n","outputs(x):  82*83=7221\n","67\n","wrong  : 82*83=7221\n","correct: 82*83=6806\n","outputs(x):  78*30=2040\n","8*\n","wrong  : 78*30=2040\n","correct: 78*30=2340\n","outputs(x):  49*71=3053\n","26\n","wrong  : 49*71=3053\n","correct: 49*71=3479\n","outputs(x):  42*99=4851\n","17\n","wrong  : 42*99=4851\n","correct: 42*99=4158\n","outputs(x):  44*83=332\n","75*\n","wrong  : 44*83=332\n","correct: 44*83=3652\n","outputs(x):  11*44=444\n","8*0\n","wrong  : 11*44=444\n","correct: 11*44=484\n","outputs(x):  37*54=188\n","99*\n","wrong  : 37*54=188\n","correct: 37*54=1998\n","outputs(x):  65*80=5360\n","9*\n","wrong  : 65*80=5360\n","correct: 65*80=5200\n","outputs(x):  78*13=9114\n","74\n","wrong  : 78*13=9114\n","correct: 78*13=1014\n","outputs(x):  91*44=3960\n","12\n","wrong  : 91*44=3960\n","correct: 91*44=4004\n","outputs(x):  58*28=1568\n","8*\n","wrong  : 58*28=1568\n","correct: 58*28=1624\n","outputs(x):  13*54=742\n","23*\n","wrong  : 13*54=742\n","correct: 13*54=702\n","outputs(x):  28*65=1170\n","81\n","wrong  : 28*65=1170\n","correct: 28*65=1820\n","outputs(x):  37*84=3192\n","90\n","wrong  : 37*84=3192\n","correct: 37*84=3108\n","outputs(x):  43*19=76\n","94*8\n","wrong  : 43*19=76\n","correct: 43*19=817\n","outputs(x):  11*97=1947\n","3*\n","wrong  : 11*97=1947\n","correct: 11*97=1067\n","outputs(x):  61*71=3550\n","12\n","wrong  : 61*71=3550\n","correct: 61*71=4331\n","outputs(x):  14*85=1955\n","79\n","wrong  : 14*85=1955\n","correct: 14*85=1190\n","outputs(x):  83*63=567\n","42*\n","wrong  : 83*63=567\n","correct: 83*63=5229\n","outputs(x):  88*18=1602\n","86\n","wrong  : 88*18=1602\n","correct: 88*18=1584\n","outputs(x):  56*48=2208\n","81\n","wrong  : 56*48=2208\n","correct: 56*48=2688\n","outputs(x):  87*26=2112\n","15\n","wrong  : 87*26=2112\n","correct: 87*26=2262\n","outputs(x):  69*42=2858\n","81\n","wrong  : 69*42=2858\n","correct: 69*42=2898\n","outputs(x):  60*27=1610\n","56\n","wrong  : 60*27=1610\n","correct: 60*27=1620\n","outputs(x):  18*73=1278\n","19\n","wrong  : 18*73=1278\n","correct: 18*73=1314\n","outputs(x):  96*25=2300\n","23\n","wrong  : 96*25=2300\n","correct: 96*25=2400\n","outputs(x):  39*18=558\n","9*6\n","wrong  : 39*18=558\n","correct: 39*18=702\n","outputs(x):  12*69=804\n","24*\n","wrong  : 12*69=804\n","correct: 12*69=828\n","outputs(x):  45*22=910\n","2*9\n","wrong  : 45*22=910\n","correct: 45*22=990\n","outputs(x):  25*61=1645\n","75\n","wrong  : 25*61=1645\n","correct: 25*61=1525\n","outputs(x):  92*18=1674\n","54\n","wrong  : 92*18=1674\n","correct: 92*18=1656\n","outputs(x):  23*84=1972\n","99\n","wrong  : 23*84=1972\n","correct: 23*84=1932\n","outputs(x):  93*98=8372\n","29\n","wrong  : 93*98=8372\n","correct: 93*98=9114\n","outputs(x):  97*50=4550\n","71\n","wrong  : 97*50=4550\n","correct: 97*50=4850\n","outputs(x):  95*15=1350\n","91\n","wrong  : 95*15=1350\n","correct: 95*15=1425\n","outputs(x):  92*94=8742\n","21\n","wrong  : 92*94=8742\n","correct: 92*94=8648\n","outputs(x):  19*69=2001\n","46\n","wrong  : 19*69=2001\n","correct: 19*69=1311\n","outputs(x):  52*85=4560\n","66\n","wrong  : 52*85=4560\n","correct: 52*85=4420\n","outputs(x):  57*49=2693\n","95\n","wrong  : 57*49=2693\n","correct: 57*49=2793\n","outputs(x):  78*47=3196\n","64\n","wrong  : 78*47=3196\n","correct: 78*47=3666\n","outputs(x):  94*53=5134\n","88\n","wrong  : 94*53=5134\n","correct: 94*53=4982\n","outputs(x):  29*94=2632\n","63\n","wrong  : 29*94=2632\n","correct: 29*94=2726\n","outputs(x):  51*49=2695\n","67\n","wrong  : 51*49=2695\n","correct: 51*49=2499\n","outputs(x):  64*22=1628\n","75\n","wrong  : 64*22=1628\n","correct: 64*22=1408\n","outputs(x):  55*63=3380\n","38\n","wrong  : 55*63=3380\n","correct: 55*63=3465\n","outputs(x):  31*21=881\n","26*\n","wrong  : 31*21=881\n","correct: 31*21=651\n","outputs(x):  71*59=413\n","49*\n","wrong  : 71*59=413\n","correct: 71*59=4189\n","outputs(x):  17*12=156\n","53*\n","wrong  : 17*12=156\n","correct: 17*12=204\n","outputs(x):  58*88=5368\n","89\n","wrong  : 58*88=5368\n","correct: 58*88=5104\n","outputs(x):  46*31=1924\n","16\n","wrong  : 46*31=1924\n","correct: 46*31=1426\n","outputs(x):  88*85=7830\n","62\n","wrong  : 88*85=7830\n","correct: 88*85=7480\n","outputs(x):  18*35=590\n","6*2\n","wrong  : 18*35=590\n","correct: 18*35=630\n","outputs(x):  35*59=1775\n","94\n","wrong  : 35*59=1775\n","correct: 35*59=2065\n","outputs(x):  11*68=68\n","35*8\n","wrong  : 11*68=68\n","correct: 11*68=748\n","outputs(x):  90*85=8550\n","48\n","wrong  : 90*85=8550\n","correct: 90*85=7650\n","outputs(x):  65*86=5160\n","34\n","wrong  : 65*86=5160\n","correct: 65*86=5590\n","outputs(x):  94*26=2352\n","40\n","wrong  : 94*26=2352\n","correct: 94*26=2444\n","outputs(x):  63*90=6570\n","12\n","wrong  : 63*90=6570\n","correct: 63*90=5670\n","outputs(x):  55*21=1435\n","67\n","wrong  : 55*21=1435\n","correct: 55*21=1155\n","outputs(x):  86*74=6068\n","82\n","wrong  : 86*74=6068\n","correct: 86*74=6364\n","outputs(x):  56*45=2300\n","23\n","wrong  : 56*45=2300\n","correct: 56*45=2520\n","outputs(x):  75*81=6465\n","22\n","wrong  : 75*81=6465\n","correct: 75*81=6075\n","outputs(x):  91*84=7812\n","61\n","wrong  : 91*84=7812\n","correct: 91*84=7644\n","outputs(x):  31*52=1664\n","40\n","wrong  : 31*52=1664\n","correct: 31*52=1612\n","outputs(x):  62*87=4836\n","45\n","wrong  : 62*87=4836\n","correct: 62*87=5394\n","outputs(x):  50*62=3600\n","78\n","wrong  : 50*62=3600\n","correct: 50*62=3100\n","outputs(x):  89*25=2450\n","91\n","wrong  : 89*25=2450\n","correct: 89*25=2225\n","outputs(x):  79*18=1242\n","22\n","wrong  : 79*18=1242\n","correct: 79*18=1422\n","outputs(x):  38*86=3354\n","32\n","wrong  : 38*86=3354\n","correct: 38*86=3268\n","outputs(x):  60*62=3680\n","88\n","wrong  : 60*62=3680\n","correct: 60*62=3720\n","outputs(x):  35*94=3102\n","56\n","wrong  : 35*94=3102\n","correct: 35*94=3290\n","outputs(x):  71*37=2923\n","11\n","wrong  : 71*37=2923\n","correct: 71*37=2627\n","outputs(x):  52*86=4384\n","82\n","wrong  : 52*86=4384\n","correct: 52*86=4472\n","outputs(x):  94*19=1862\n","81\n","wrong  : 94*19=1862\n","correct: 94*19=1786\n","outputs(x):  22*97=294\n","5*7\n","wrong  : 22*97=294\n","correct: 22*97=2134\n","outputs(x):  82*63=5796\n","77\n","wrong  : 82*63=5796\n","correct: 82*63=5166\n","outputs(x):  25*26=670\n","66*\n","wrong  : 25*26=670\n","correct: 25*26=650\n","outputs(x):  63*67=4824\n","78\n","wrong  : 63*67=4824\n","correct: 63*67=4221\n","outputs(x):  56*30=1800\n","47\n","wrong  : 56*30=1800\n","correct: 56*30=1680\n","outputs(x):  91*98=8099\n","81\n","wrong  : 91*98=8099\n","correct: 91*98=8918\n","outputs(x):  38*15=490\n","74*\n","wrong  : 38*15=490\n","correct: 38*15=570\n","outputs(x):  47*10=460\n","16*\n","wrong  : 47*10=460\n","correct: 47*10=470\n","outputs(x):  73*87=6989\n","14\n","wrong  : 73*87=6989\n","correct: 73*87=6351\n","outputs(x):  20*71=1490\n","3*\n","wrong  : 20*71=1490\n","correct: 20*71=1420\n","outputs(x):  44*69=3476\n","9*\n","wrong  : 44*69=3476\n","correct: 44*69=3036\n","outputs(x):  60*77=6160\n","36\n","wrong  : 60*77=6160\n","correct: 60*77=4620\n","outputs(x):  11*70=700\n","46*\n","wrong  : 11*70=700\n","correct: 11*70=770\n","outputs(x):  56*38=2508\n","89\n","wrong  : 56*38=2508\n","correct: 56*38=2128\n","outputs(x):  15*17=225\n","12*\n","wrong  : 15*17=225\n","correct: 15*17=255\n","outputs(x):  51*45=2025\n","11\n","wrong  : 51*45=2025\n","correct: 51*45=2295\n","outputs(x):  81*35=2905\n","43\n","wrong  : 81*35=2905\n","correct: 81*35=2835\n","outputs(x):  83*18=1674\n","54\n","wrong  : 83*18=1674\n","correct: 83*18=1494\n","outputs(x):  47*65=2345\n","76\n","wrong  : 47*65=2345\n","correct: 47*65=3055\n","outputs(x):  25*84=2050\n","0*\n","wrong  : 25*84=2050\n","correct: 25*84=2100\n","outputs(x):  34*62=2356\n","14\n","wrong  : 34*62=2356\n","correct: 34*62=2108\n","outputs(x):  18*52=908\n","95*\n","wrong  : 18*52=908\n","correct: 18*52=936\n","outputs(x):  48*56=2744\n","27\n","wrong  : 48*56=2744\n","correct: 48*56=2688\n","outputs(x):  53*59=3717\n","81\n","wrong  : 53*59=3717\n","correct: 53*59=3127\n","outputs(x):  55*46=2070\n","13\n","wrong  : 55*46=2070\n","correct: 55*46=2530\n","outputs(x):  35*88=2870\n","98\n","wrong  : 35*88=2870\n","correct: 35*88=3080\n","outputs(x):  74*51=3294\n","4*\n","wrong  : 74*51=3294\n","correct: 74*51=3774\n","outputs(x):  87*14=1134\n","1*\n","wrong  : 87*14=1134\n","correct: 87*14=1218\n","outputs(x):  24*56=1232\n","12\n","wrong  : 24*56=1232\n","correct: 24*56=1344\n","outputs(x):  75*47=3195\n","86\n","wrong  : 75*47=3195\n","correct: 75*47=3525\n","outputs(x):  39*59=2001\n","46\n","wrong  : 39*59=2001\n","correct: 39*59=2301\n","outputs(x):  98*58=5104\n","44\n","wrong  : 98*58=5104\n","correct: 98*58=5684\n"," 64% 16/25 [00:01<00:00, 11.49it/s]outputs(x):  48*55=2490\n","58\n","wrong  : 48*55=2490\n","correct: 48*55=2640\n","outputs(x):  33*82=2736\n","60\n","wrong  : 33*82=2736\n","correct: 33*82=2706\n","outputs(x):  62*76=4836\n","87\n","wrong  : 62*76=4836\n","correct: 62*76=4712\n","outputs(x):  64*40=2160\n","49\n","wrong  : 64*40=2160\n","correct: 64*40=2560\n","outputs(x):  12*45=580\n","81*\n","wrong  : 12*45=580\n","correct: 12*45=540\n","outputs(x):  69*57=3477\n","2*\n","wrong  : 69*57=3477\n","correct: 69*57=3933\n","outputs(x):  64*68=4284\n","58\n","wrong  : 64*68=4284\n","correct: 64*68=4352\n","outputs(x):  90*63=5796\n","77\n","wrong  : 90*63=5796\n","correct: 90*63=5670\n","outputs(x):  27*80=2460\n","23\n","wrong  : 27*80=2460\n","correct: 27*80=2160\n","outputs(x):  45*30=1300\n","65\n","wrong  : 45*30=1300\n","correct: 45*30=1350\n","outputs(x):  15*69=1005\n","91\n","wrong  : 15*69=1005\n","correct: 15*69=1035\n","outputs(x):  35*17=425\n","12*\n","wrong  : 35*17=425\n","correct: 35*17=595\n","outputs(x):  52*61=3362\n","55\n","wrong  : 52*61=3362\n","correct: 52*61=3172\n","outputs(x):  94*74=7896\n","41\n","wrong  : 94*74=7896\n","correct: 94*74=6956\n","outputs(x):  63*14=938\n","13*\n","wrong  : 63*14=938\n","correct: 63*14=882\n","outputs(x):  63*75=4875\n","54\n","wrong  : 63*75=4875\n","correct: 63*75=4725\n","outputs(x):  38*11=408\n","42*\n","wrong  : 38*11=408\n","correct: 38*11=418\n","outputs(x):  68*63=456\n","74*\n","wrong  : 68*63=456\n","correct: 68*63=4284\n","outputs(x):  11*71=7461\n","81\n","wrong  : 11*71=7461\n","correct: 11*71=781\n","outputs(x):  13*68=1088\n","15\n","wrong  : 13*68=1088\n","correct: 13*68=884\n","outputs(x):  61*45=2705\n","9*\n","wrong  : 61*45=2705\n","correct: 61*45=2745\n","outputs(x):  43*15=765\n","33*\n","wrong  : 43*15=765\n","correct: 43*15=645\n","outputs(x):  34*75=2400\n","81\n","wrong  : 34*75=2400\n","correct: 34*75=2550\n","outputs(x):  29*67=2613\n","95\n","wrong  : 29*67=2613\n","correct: 29*67=1943\n","outputs(x):  27*92=2407\n","16\n","wrong  : 27*92=2407\n","correct: 27*92=2484\n","outputs(x):  52*64=3276\n","73\n","wrong  : 52*64=3276\n","correct: 52*64=3328\n","outputs(x):  54*86=4384\n","57\n","wrong  : 54*86=4384\n","correct: 54*86=4644\n","outputs(x):  86*51=4998\n","84\n","wrong  : 86*51=4998\n","correct: 86*51=4386\n","outputs(x):  51*68=3740\n","8*\n","wrong  : 51*68=3740\n","correct: 51*68=3468\n","outputs(x):  55*90=5310\n","78\n","wrong  : 55*90=5310\n","correct: 55*90=4950\n","outputs(x):  86*28=2352\n","40\n","wrong  : 86*28=2352\n","correct: 86*28=2408\n","outputs(x):  21*78=1768\n","67\n","wrong  : 21*78=1768\n","correct: 21*78=1638\n","outputs(x):  35*58=1910\n","29\n","wrong  : 35*58=1910\n","correct: 35*58=2030\n","outputs(x):  92*78=7254\n","45\n","wrong  : 92*78=7254\n","correct: 92*78=7176\n","outputs(x):  91*75=6945\n","31\n","wrong  : 91*75=6945\n","correct: 91*75=6825\n","outputs(x):  94*14=1372\n","7*\n","wrong  : 94*14=1372\n","correct: 94*14=1316\n","outputs(x):  47*35=1265\n","91\n","wrong  : 47*35=1265\n","correct: 47*35=1645\n","outputs(x):  44*43=1462\n","55\n","wrong  : 44*43=1462\n","correct: 44*43=1892\n","outputs(x):  82*52=4212\n","73\n","wrong  : 82*52=4212\n","correct: 82*52=4264\n","outputs(x):  79*10=910\n","7*7\n","wrong  : 79*10=910\n","correct: 79*10=790\n","outputs(x):  63*57=3534\n","76\n","wrong  : 63*57=3534\n","correct: 63*57=3591\n","outputs(x):  32*69=2001\n","46\n","wrong  : 32*69=2001\n","correct: 32*69=2208\n","outputs(x):  56*11=506\n","98*\n","wrong  : 56*11=506\n","correct: 56*11=616\n","outputs(x):  67*32=224\n","30*\n","wrong  : 67*32=224\n","correct: 67*32=2144\n","outputs(x):  28*32=608\n","34*\n","wrong  : 28*32=608\n","correct: 28*32=896\n","outputs(x):  37*86=3354\n","32\n","wrong  : 37*86=3354\n","correct: 37*86=3182\n","outputs(x):  14*45=580\n","73*\n","wrong  : 14*45=580\n","correct: 14*45=630\n","outputs(x):  58*93=4536\n","12\n","wrong  : 58*93=4536\n","correct: 58*93=5394\n","outputs(x):  32*38=1092\n","93\n","wrong  : 32*38=1092\n","correct: 32*38=1216\n","outputs(x):  24*14=616\n","88*\n","wrong  : 24*14=616\n","correct: 24*14=336\n","outputs(x):  39*50=1900\n","92\n","wrong  : 39*50=1900\n","correct: 39*50=1950\n","outputs(x):  74*89=6408\n","64\n","wrong  : 74*89=6408\n","correct: 74*89=6586\n","outputs(x):  30*10=350\n","28*\n","wrong  : 30*10=350\n","correct: 30*10=300\n","outputs(x):  24*23=598\n","38*\n","wrong  : 24*23=598\n","correct: 24*23=552\n","outputs(x):  75*48=3770\n","51\n","wrong  : 75*48=3770\n","correct: 75*48=3600\n","outputs(x):  35*40=120\n","3*9\n","wrong  : 35*40=120\n","correct: 35*40=1400\n","outputs(x):  12*96=1056\n","78\n","wrong  : 12*96=1056\n","correct: 12*96=1152\n","outputs(x):  21*25=270\n","22*\n","wrong  : 21*25=270\n","correct: 21*25=525\n","outputs(x):  53*53=2703\n","73\n","wrong  : 53*53=2703\n","correct: 53*53=2809\n","outputs(x):  24*12=1008\n","24\n","wrong  : 24*12=1008\n","correct: 24*12=288\n","outputs(x):  36*82=3220\n","78\n","wrong  : 36*82=3220\n","correct: 36*82=2952\n","outputs(x):  79*82=6396\n","77\n","wrong  : 79*82=6396\n","correct: 79*82=6478\n","outputs(x):  87*98=7644\n","7*\n","wrong  : 87*98=7644\n","correct: 87*98=8526\n","outputs(x):  70*59=3540\n","79\n","wrong  : 70*59=3540\n","correct: 70*59=4130\n","outputs(x):  57*36=2072\n","90\n","wrong  : 57*36=2072\n","correct: 57*36=2052\n","outputs(x):  50*39=2750\n","33\n","wrong  : 50*39=2750\n","correct: 50*39=1950\n","outputs(x):  30*30=1200\n","43\n","wrong  : 30*30=1200\n","correct: 30*30=900\n","outputs(x):  21*95=1955\n","79\n","wrong  : 21*95=1955\n","correct: 21*95=1995\n","outputs(x):  11*98=1748\n","99\n","wrong  : 11*98=1748\n","correct: 11*98=1078\n","outputs(x):  16*22=308\n","21*\n","wrong  : 16*22=308\n","correct: 16*22=352\n","outputs(x):  30*34=1120\n","32\n","wrong  : 30*34=1120\n","correct: 30*34=1020\n","outputs(x):  44*98=4292\n","11\n","wrong  : 44*98=4292\n","correct: 44*98=4312\n","outputs(x):  96*47=4136\n","4*\n","wrong  : 96*47=4136\n","correct: 96*47=4512\n","outputs(x):  22*49=1568\n","58\n","wrong  : 22*49=1568\n","correct: 22*49=1078\n","outputs(x):  51*92=4634\n","4*\n","wrong  : 51*92=4634\n","correct: 51*92=4692\n","outputs(x):  87*54=4374\n","49\n","wrong  : 87*54=4374\n","correct: 87*54=4698\n","outputs(x):  84*73=6646\n","99\n","wrong  : 84*73=6646\n","correct: 84*73=6132\n","outputs(x):  36*29=986\n","27*\n","wrong  : 36*29=986\n","correct: 36*29=1044\n","outputs(x):  67*44=2816\n","60\n","wrong  : 67*44=2816\n","correct: 67*44=2948\n","outputs(x):  63*18=1314\n","41\n","wrong  : 63*18=1314\n","correct: 63*18=1134\n","outputs(x):  77*47=3149\n","36\n","wrong  : 77*47=3149\n","correct: 77*47=3619\n","outputs(x):  71*79=5467\n","61\n","wrong  : 71*79=5467\n","correct: 71*79=5609\n","outputs(x):  29*84=2632\n","63\n","wrong  : 29*84=2632\n","correct: 29*84=2436\n","outputs(x):  78*56=4524\n","9*\n","wrong  : 78*56=4524\n","correct: 78*56=4368\n","outputs(x):  57*83=4648\n","62\n","wrong  : 57*83=4648\n","correct: 57*83=4731\n","outputs(x):  62*94=6016\n","3*\n","wrong  : 62*94=6016\n","correct: 62*94=5828\n","outputs(x):  56*25=1350\n","91\n","wrong  : 56*25=1350\n","correct: 56*25=1400\n","outputs(x):  26*24=672\n","10*\n","wrong  : 26*24=672\n","correct: 26*24=624\n","outputs(x):  57*32=1812\n","67\n","wrong  : 57*32=1812\n","correct: 57*32=1824\n","outputs(x):  51*50=2050\n","0*\n","wrong  : 51*50=2050\n","correct: 51*50=2550\n","outputs(x):  52*89=4868\n","7*\n","wrong  : 52*89=4868\n","correct: 52*89=4628\n","outputs(x):  98*38=3572\n","86\n","wrong  : 98*38=3572\n","correct: 98*38=3724\n","outputs(x):  93*50=5650\n","26\n","wrong  : 93*50=5650\n","correct: 93*50=4650\n","outputs(x):  48*85=3820\n","24\n","wrong  : 48*85=3820\n","correct: 48*85=4080\n","outputs(x):  46*86=3096\n","83\n","wrong  : 46*86=3096\n","correct: 46*86=3956\n","outputs(x):  20*74=1920\n","31\n","wrong  : 20*74=1920\n","correct: 20*74=1480\n","outputs(x):  14*42=676\n","69*\n","wrong  : 14*42=676\n","correct: 14*42=588\n","outputs(x):  37*56=2842\n","12\n","wrong  : 37*56=2842\n","correct: 37*56=2072\n","outputs(x):  70*77=5310\n","58\n","wrong  : 70*77=5310\n","correct: 70*77=5390\n","outputs(x):  15*16=250\n","98*\n","wrong  : 15*16=250\n","correct: 15*16=240\n","outputs(x):  70*38=2850\n","3*\n","wrong  : 70*38=2850\n","correct: 70*38=2660\n","outputs(x):  96*75=6900\n","27\n","wrong  : 96*75=6900\n","correct: 96*75=7200\n","outputs(x):  40*15=400\n","24*\n","wrong  : 40*15=400\n","correct: 40*15=600\n","outputs(x):  87*94=7224\n","66\n","wrong  : 87*94=7224\n","correct: 87*94=8178\n","outputs(x):  51*99=495\n","30*\n","wrong  : 51*99=495\n","correct: 51*99=5049\n","outputs(x):  57*68=3944\n","90\n","wrong  : 57*68=3944\n","correct: 57*68=3876\n","outputs(x):  46*60=2940\n","11\n","wrong  : 46*60=2940\n","correct: 46*60=2760\n","outputs(x):  55*75=3825\n","22\n","wrong  : 55*75=3825\n","correct: 55*75=4125\n","outputs(x):  42*72=3096\n","61\n","wrong  : 42*72=3096\n","correct: 42*72=3024\n","outputs(x):  55*88=4450\n","32\n","wrong  : 55*88=4450\n","correct: 55*88=4840\n","outputs(x):  22*70=1580\n","11\n","wrong  : 22*70=1580\n","correct: 22*70=1540\n","outputs(x):  39*28=1204\n","80\n","wrong  : 39*28=1204\n","correct: 39*28=1092\n","outputs(x):  85*77=6315\n","57\n","wrong  : 85*77=6315\n","correct: 85*77=6545\n","outputs(x):  30*61=1890\n","34\n","wrong  : 30*61=1890\n","correct: 30*61=1830\n","outputs(x):  71*14=1022\n","74\n","wrong  : 71*14=1022\n","correct: 71*14=994\n","outputs(x):  69*16=1274\n","51\n","wrong  : 69*16=1274\n","correct: 69*16=1104\n","outputs(x):  21*57=1007\n","74\n","wrong  : 21*57=1007\n","correct: 21*57=1197\n","outputs(x):  45*52=2400\n","15\n","wrong  : 45*52=2400\n","correct: 45*52=2340\n","outputs(x):  96*22=1892\n","49\n","wrong  : 96*22=1892\n","correct: 96*22=2112\n","outputs(x):  21*91=1701\n","95\n","wrong  : 21*91=1701\n","correct: 21*91=1911\n","outputs(x):  68*13=832\n","59*\n","wrong  : 68*13=832\n","correct: 68*13=884\n","outputs(x):  50*49=2950\n","95\n","wrong  : 50*49=2950\n","correct: 50*49=2450\n","outputs(x):  84*76=6068\n","82\n","wrong  : 84*76=6068\n","correct: 84*76=6384\n","outputs(x):  57*66=3422\n","79\n","wrong  : 57*66=3422\n","correct: 57*66=3762\n","outputs(x):  84*91=756\n","74*\n","wrong  : 84*91=756\n","correct: 84*91=7644\n","outputs(x):  84*95=8930\n","73\n","wrong  : 84*95=8930\n","correct: 84*95=7980\n","outputs(x):  50*66=3960\n","72\n","wrong  : 50*66=3960\n","correct: 50*66=3300\n","outputs(x):  20*82=2520\n","41\n","wrong  : 20*82=2520\n","correct: 20*82=1640\n","outputs(x):  78*21=1848\n","2*\n","wrong  : 78*21=1848\n","correct: 78*21=1638\n","outputs(x):  98*71=6106\n","53\n","wrong  : 98*71=6106\n","correct: 98*71=6958\n","outputs(x):  69*19=1243\n","8*\n","wrong  : 69*19=1243\n","correct: 69*19=1311\n","outputs(x):  23*30=2490\n","21\n","wrong  : 23*30=2490\n","correct: 23*30=690\n","outputs(x):  24*27=810\n","39*\n","wrong  : 24*27=810\n","correct: 24*27=648\n","outputs(x):  60*47=2350\n","73\n","wrong  : 60*47=2350\n","correct: 60*47=2820\n","outputs(x):  25*72=1940\n","3*\n","wrong  : 25*72=1940\n","correct: 25*72=1800\n","outputs(x):  87*51=4347\n","3*\n","wrong  : 87*51=4347\n","correct: 87*51=4437\n","outputs(x):  18*12=144\n","85*\n","wrong  : 18*12=144\n","correct: 18*12=216\n","outputs(x):  72*22=1628\n","75\n","wrong  : 72*22=1628\n","correct: 72*22=1584\n","outputs(x):  65*19=115\n","50*\n","wrong  : 65*19=115\n","correct: 65*19=1235\n","outputs(x):  90*50=4550\n","71\n","wrong  : 90*50=4550\n","correct: 90*50=4500\n","outputs(x):  18*48=846\n","16*\n","wrong  : 18*48=846\n","correct: 18*48=864\n","outputs(x):  94*83=7708\n","83\n","wrong  : 94*83=7708\n","correct: 94*83=7802\n","outputs(x):  12*84=1176\n","45\n","wrong  : 12*84=1176\n","correct: 12*84=1008\n","outputs(x):  47*29=1833\n","79\n","wrong  : 47*29=1833\n","correct: 47*29=1363\n","outputs(x):  32*97=3977\n","74\n","wrong  : 32*97=3977\n","correct: 32*97=3104\n","outputs(x):  93*69=6555\n","41\n","wrong  : 93*69=6555\n","correct: 93*69=6417\n","outputs(x):  66*82=5166\n","91\n","wrong  : 66*82=5166\n","correct: 66*82=5412\n","outputs(x):  71*21=1113\n","73\n","wrong  : 71*21=1113\n","correct: 71*21=1491\n","outputs(x):  59*89=5073\n","79\n","wrong  : 59*89=5073\n","correct: 59*89=5251\n","outputs(x):  24*73=1679\n","24\n","wrong  : 24*73=1679\n","correct: 24*73=1752\n","outputs(x):  61*95=5185\n","21\n","wrong  : 61*95=5185\n","correct: 61*95=5795\n","outputs(x):  65*45=3025\n","2*\n","wrong  : 65*45=3025\n","correct: 65*45=2925\n","outputs(x):  61*59=3481\n","36\n","wrong  : 61*59=3481\n","correct: 61*59=3599\n","outputs(x):  19*24=462\n","16*\n","wrong  : 19*24=462\n","correct: 19*24=456\n","outputs(x):  26*33=924\n","45*\n","wrong  : 26*33=924\n","correct: 26*33=858\n","outputs(x):  50*51=2050\n","0*\n","wrong  : 50*51=2050\n","correct: 50*51=2550\n","outputs(x):  70*64=4416\n","89\n","wrong  : 70*64=4416\n","correct: 70*64=4480\n","outputs(x):  31*32=1632\n","33\n","wrong  : 31*32=1632\n","correct: 31*32=992\n","outputs(x):  49*22=1274\n","51\n","wrong  : 49*22=1274\n","correct: 49*22=1078\n","outputs(x):  13*18=252\n","93*\n","wrong  : 13*18=252\n","correct: 13*18=234\n","outputs(x):  19*34=612\n","46*\n","wrong  : 19*34=612\n","correct: 19*34=646\n","outputs(x):  53*87=4437\n","82\n","wrong  : 53*87=4437\n","correct: 53*87=4611\n","outputs(x):  94*44=4224\n","96\n","wrong  : 94*44=4224\n","correct: 94*44=4136\n","outputs(x):  93*74=6808\n","14\n","wrong  : 93*74=6808\n","correct: 93*74=6882\n","outputs(x):  64*61=3294\n","5*\n","wrong  : 64*61=3294\n","correct: 64*61=3904\n","outputs(x):  48*32=128\n","60*\n","wrong  : 48*32=128\n","correct: 48*32=1536\n","outputs(x):  34*74=2672\n","55\n","wrong  : 34*74=2672\n","correct: 34*74=2516\n","outputs(x):  27*66=1518\n","59\n","wrong  : 27*66=1518\n","correct: 27*66=1782\n","outputs(x):  41*24=1464\n","76\n","wrong  : 41*24=1464\n","correct: 41*24=984\n","outputs(x):  76*82=656\n","85*\n","wrong  : 76*82=656\n","correct: 76*82=6232\n","outputs(x):  71*28=2216\n","56\n","wrong  : 71*28=2216\n","correct: 71*28=1988\n","outputs(x):  62*30=1980\n","47\n","wrong  : 62*30=1980\n","correct: 62*30=1860\n","outputs(x):  72*57=4674\n","50\n","wrong  : 72*57=4674\n","correct: 72*57=4104\n","outputs(x):  31*38=1054\n","5*\n","wrong  : 31*38=1054\n","correct: 31*38=1178\n","outputs(x):  20*93=1950\n","45\n","wrong  : 20*93=1950\n","correct: 20*93=1860\n","outputs(x):  64*76=4028\n","26\n","wrong  : 64*76=4028\n","correct: 64*76=4864\n","outputs(x):  19*54=1566\n","98\n","wrong  : 19*54=1566\n","correct: 19*54=1026\n","outputs(x):  45*29=1015\n","42\n","wrong  : 45*29=1015\n","correct: 45*29=1305\n","outputs(x):  61*27=1917\n","23\n","wrong  : 61*27=1917\n","correct: 61*27=1647\n","outputs(x):  72*59=4366\n","4*\n","wrong  : 72*59=4366\n","correct: 72*59=4248\n","outputs(x):  77*81=6391\n","55\n","wrong  : 77*81=6391\n","correct: 77*81=6237\n","outputs(x):  89*88=7744\n","74\n","wrong  : 89*88=7744\n","correct: 89*88=7832\n","outputs(x):  47*64=2268\n","81\n","wrong  : 47*64=2268\n","correct: 47*64=3008\n","outputs(x):  79*26=1794\n","2*\n","wrong  : 79*26=1794\n","correct: 79*26=2054\n","outputs(x):  49*98=4851\n","26\n","wrong  : 49*98=4851\n","correct: 49*98=4802\n","outputs(x):  77*97=7663\n","82\n","wrong  : 77*97=7663\n","correct: 77*97=7469\n","outputs(x):  22*87=1134\n","7*\n","wrong  : 22*87=1134\n","correct: 22*87=1914\n","outputs(x):  85*47=4165\n","54\n","wrong  : 85*47=4165\n","correct: 85*47=3995\n","outputs(x):  38*94=3666\n","67\n","wrong  : 38*94=3666\n","correct: 38*94=3572\n","outputs(x):  84*81=7209\n","58\n","wrong  : 84*81=7209\n","correct: 84*81=6804\n","outputs(x):  54*73=3402\n","72\n","wrong  : 54*73=3402\n","correct: 54*73=3942\n","outputs(x):  92*54=5148\n","6*\n","wrong  : 92*54=5148\n","correct: 92*54=4968\n","outputs(x):  28*45=1710\n","40\n","wrong  : 28*45=1710\n","correct: 28*45=1260\n","outputs(x):  15*30=500\n","41*\n","wrong  : 15*30=500\n","correct: 15*30=450\n","outputs(x):  42*63=2016\n","53\n","wrong  : 42*63=2016\n","correct: 42*63=2646\n","outputs(x):  37*94=3666\n","67\n","wrong  : 37*94=3666\n","correct: 37*94=3478\n","outputs(x):  64*97=6336\n","34\n","wrong  : 64*97=6336\n","correct: 64*97=6208\n","outputs(x):  43*64=2816\n","69\n","wrong  : 43*64=2816\n","correct: 43*64=2752\n","outputs(x):  75*28=2050\n","0*\n","wrong  : 75*28=2050\n","correct: 75*28=2100\n","outputs(x):  31*17=561\n","0*3\n","wrong  : 31*17=561\n","correct: 31*17=527\n","outputs(x):  15*42=644\n","10*\n","wrong  : 15*42=644\n","correct: 15*42=630\n","outputs(x):  47*17=833\n","95*\n","wrong  : 47*17=833\n","correct: 47*17=799\n","outputs(x):  55*12=612\n","90*\n","wrong  : 55*12=612\n","correct: 55*12=660\n","outputs(x):  72*40=3080\n","10\n","wrong  : 72*40=3080\n","correct: 72*40=2880\n","outputs(x):  92*45=4180\n","81\n","wrong  : 92*45=4180\n","correct: 92*45=4140\n","outputs(x):  11*53=53\n","71*6\n","wrong  : 11*53=53\n","correct: 11*53=583\n","outputs(x):  82*67=5628\n","51\n","wrong  : 82*67=5628\n","correct: 82*67=5494\n","outputs(x):  24*80=1440\n","84\n","wrong  : 24*80=1440\n","correct: 24*80=1920\n","outputs(x):  43*16=528\n","77*\n","wrong  : 43*16=528\n","correct: 43*16=688\n","outputs(x):  85*54=4050\n","0*\n","wrong  : 85*54=4050\n","correct: 85*54=4590\n","outputs(x):  32*27=812\n","14*\n","wrong  : 32*27=812\n","correct: 32*27=864\n","outputs(x):  23*54=128\n","37*\n","wrong  : 23*54=128\n","correct: 23*54=1242\n","outputs(x):  81*86=7138\n","16\n","wrong  : 81*86=7138\n","correct: 81*86=6966\n","outputs(x):  78*86=6624\n","18\n","wrong  : 78*86=6624\n","correct: 78*86=6708\n","outputs(x):  28*87=2523\n","99\n","wrong  : 28*87=2523\n","correct: 28*87=2436\n","outputs(x):  92*39=3627\n","45\n","wrong  : 92*39=3627\n","correct: 92*39=3588\n","outputs(x):  89*40=3880\n","10\n","wrong  : 89*40=3880\n","correct: 89*40=3560\n","outputs(x):  90*87=8004\n","59\n","wrong  : 90*87=8004\n","correct: 90*87=7830\n","outputs(x):  45*33=1815\n","51\n","wrong  : 45*33=1815\n","correct: 45*33=1485\n","outputs(x):  78*93=744\n","95*\n","wrong  : 78*93=744\n","correct: 78*93=7254\n","outputs(x):  75*80=6560\n","89\n","wrong  : 75*80=6560\n","correct: 75*80=6000\n","outputs(x):  22*93=2976\n","18\n","wrong  : 22*93=2976\n","correct: 22*93=2046\n","outputs(x):  30*84=2640\n","10\n","wrong  : 30*84=2640\n","correct: 30*84=2520\n","outputs(x):  19*88=176\n","98*\n","wrong  : 19*88=176\n","correct: 19*88=1672\n","outputs(x):  77*66=4812\n","61\n","wrong  : 77*66=4812\n","correct: 77*66=5082\n","outputs(x):  37*63=2551\n","58\n","wrong  : 37*63=2551\n","correct: 37*63=2331\n","outputs(x):  26*10=280\n","87*\n","wrong  : 26*10=280\n","correct: 26*10=260\n","outputs(x):  96*52=504\n","99*\n","wrong  : 96*52=504\n","correct: 96*52=4992\n","outputs(x):  91*18=1728\n","73\n","wrong  : 91*18=1728\n","correct: 91*18=1638\n","outputs(x):  18*13=494\n","9*3\n","wrong  : 18*13=494\n","correct: 18*13=234\n","outputs(x):  38*52=2496\n","95\n","wrong  : 38*52=2496\n","correct: 38*52=1976\n","outputs(x):  82*47=3384\n","76\n","wrong  : 82*47=3384\n","correct: 82*47=3854\n","outputs(x):  70*55=3300\n","12\n","wrong  : 70*55=3300\n","correct: 70*55=3850\n","outputs(x):  57*12=708\n","71*\n","wrong  : 57*12=708\n","correct: 57*12=684\n","outputs(x):  37*67=2812\n","47\n","wrong  : 37*67=2812\n","correct: 37*67=2479\n","outputs(x):  11*73=751\n","5*3\n","wrong  : 11*73=751\n","correct: 11*73=803\n","outputs(x):  69*55=3105\n","54\n","wrong  : 69*55=3105\n","correct: 69*55=3795\n","outputs(x):  87*38=3154\n","38\n","wrong  : 87*38=3154\n","correct: 87*38=3306\n","outputs(x):  80*94=7220\n","26\n","wrong  : 80*94=7220\n","correct: 80*94=7520\n","outputs(x):  81*98=7742\n","85\n","wrong  : 81*98=7742\n","correct: 81*98=7938\n","outputs(x):  39*52=2058\n","81\n","wrong  : 39*52=2058\n","correct: 39*52=2028\n","outputs(x):  38*51=2448\n","18\n","wrong  : 38*51=2448\n","correct: 38*51=1938\n","outputs(x):  60*57=3990\n","58\n","wrong  : 60*57=3990\n","correct: 60*57=3420\n"," 72% 18/25 [00:01<00:00, 11.92it/s]outputs(x):  96*92=9108\n","69\n","wrong  : 96*92=9108\n","correct: 96*92=8832\n","outputs(x):  31*13=393\n","55*\n","wrong  : 31*13=393\n","correct: 31*13=403\n","outputs(x):  89*29=2871\n","41\n","wrong  : 89*29=2871\n","correct: 89*29=2581\n","outputs(x):  83*28=2356\n","18\n","wrong  : 83*28=2356\n","correct: 83*28=2324\n","outputs(x):  82*73=6646\n","99\n","wrong  : 82*73=6646\n","correct: 82*73=5986\n","outputs(x):  44*67=2278\n","26\n","wrong  : 44*67=2278\n","correct: 44*67=2948\n","outputs(x):  23*12=324\n","72*\n","wrong  : 23*12=324\n","correct: 23*12=276\n","outputs(x):  69*76=5214\n","85\n","wrong  : 69*76=5214\n","correct: 69*76=5244\n","outputs(x):  76*54=4050\n","0*\n","wrong  : 76*54=4050\n","correct: 76*54=4104\n","outputs(x):  72*14=1064\n","90\n","wrong  : 72*14=1064\n","correct: 72*14=1008\n","outputs(x):  90*68=6256\n","9*\n","wrong  : 90*68=6256\n","correct: 90*68=6120\n","outputs(x):  70*97=6305\n","71\n","wrong  : 70*97=6305\n","correct: 70*97=6790\n","outputs(x):  32*85=2550\n","41\n","wrong  : 32*85=2550\n","correct: 32*85=2720\n","outputs(x):  81*82=6561\n","14\n","wrong  : 81*82=6561\n","correct: 81*82=6642\n","outputs(x):  63*41=2403\n","65\n","wrong  : 63*41=2403\n","correct: 63*41=2583\n","outputs(x):  32*30=1260\n","63\n","wrong  : 32*30=1260\n","correct: 32*30=960\n","outputs(x):  51*96=3936\n","46\n","wrong  : 51*96=3936\n","correct: 51*96=4896\n","outputs(x):  17*72=1944\n","94\n","wrong  : 17*72=1944\n","correct: 17*72=1224\n","outputs(x):  65*98=6350\n","72\n","wrong  : 65*98=6350\n","correct: 65*98=6370\n","outputs(x):  73*83=6789\n","14\n","wrong  : 73*83=6789\n","correct: 73*83=6059\n","outputs(x):  45*36=1660\n","14\n","wrong  : 45*36=1660\n","correct: 45*36=1620\n","outputs(x):  78*25=1850\n","3*\n","wrong  : 78*25=1850\n","correct: 78*25=1950\n","outputs(x):  74*25=1350\n","43\n","wrong  : 74*25=1350\n","correct: 74*25=1850\n","outputs(x):  31*66=2706\n","98\n","wrong  : 31*66=2706\n","correct: 31*66=2046\n","outputs(x):  77*34=2278\n","57\n","wrong  : 77*34=2278\n","correct: 77*34=2618\n","outputs(x):  37*40=1820\n","11\n","wrong  : 37*40=1820\n","correct: 37*40=1480\n","outputs(x):  70*91=6730\n","79\n","wrong  : 70*91=6730\n","correct: 70*91=6370\n","outputs(x):  56*73=3358\n","57\n","wrong  : 56*73=3358\n","correct: 56*73=4088\n","outputs(x):  88*54=432\n","79*\n","wrong  : 88*54=432\n","correct: 88*54=4752\n","outputs(x):  77*11=869\n","63*\n","wrong  : 77*11=869\n","correct: 77*11=847\n","outputs(x):  77*21=1407\n","67\n","wrong  : 77*21=1407\n","correct: 77*21=1617\n","outputs(x):  95*72=6980\n","94\n","wrong  : 95*72=6980\n","correct: 95*72=6840\n","outputs(x):  86*47=4136\n","4*\n","wrong  : 86*47=4136\n","correct: 86*47=4042\n","outputs(x):  27*45=1170\n","54\n","wrong  : 27*45=1170\n","correct: 27*45=1215\n","outputs(x):  71*62=4464\n","82\n","wrong  : 71*62=4464\n","correct: 71*62=4402\n","outputs(x):  31*71=2059\n","91\n","wrong  : 31*71=2059\n","correct: 31*71=2201\n","outputs(x):  61*70=4480\n","3*\n","wrong  : 61*70=4480\n","correct: 61*70=4270\n","outputs(x):  55*42=210\n","76*\n","wrong  : 55*42=210\n","correct: 55*42=2310\n","outputs(x):  62*10=720\n","6*1\n","wrong  : 62*10=720\n","correct: 62*10=620\n","outputs(x):  44*94=3102\n","56\n","wrong  : 44*94=3102\n","correct: 44*94=4136\n","outputs(x):  98*32=2848\n","16\n","wrong  : 98*32=2848\n","correct: 98*32=3136\n","outputs(x):  92*93=8742\n","72\n","wrong  : 92*93=8742\n","correct: 92*93=8556\n","outputs(x):  58*44=2992\n","45\n","wrong  : 58*44=2992\n","correct: 58*44=2552\n","outputs(x):  28*63=1638\n","82\n","wrong  : 28*63=1638\n","correct: 28*63=1764\n","outputs(x):  20*27=560\n","52*\n","wrong  : 20*27=560\n","correct: 20*27=540\n","outputs(x):  64*72=4864\n","85\n","wrong  : 64*72=4864\n","correct: 64*72=4608\n","outputs(x):  46*78=342\n","37*\n","wrong  : 46*78=342\n","correct: 46*78=3588\n","outputs(x):  70*95=6550\n","48\n","wrong  : 70*95=6550\n","correct: 70*95=6650\n","outputs(x):  41*46=2806\n","23\n","wrong  : 41*46=2806\n","correct: 41*46=1886\n","outputs(x):  89*17=1429\n","78\n","wrong  : 89*17=1429\n","correct: 89*17=1513\n","outputs(x):  23*43=92\n","97*2\n","wrong  : 23*43=92\n","correct: 23*43=989\n","outputs(x):  91*90=8370\n","78\n","wrong  : 91*90=8370\n","correct: 91*90=8190\n","outputs(x):  42*27=1058\n","69\n","wrong  : 42*27=1058\n","correct: 42*27=1134\n","outputs(x):  43*40=1840\n","71\n","wrong  : 43*40=1840\n","correct: 43*40=1720\n","outputs(x):  90*47=470\n","95*\n","wrong  : 90*47=470\n","correct: 90*47=4230\n","outputs(x):  24*59=1372\n","9*\n","wrong  : 24*59=1372\n","correct: 24*59=1416\n","outputs(x):  84*24=2256\n","38\n","wrong  : 84*24=2256\n","correct: 84*24=2016\n","outputs(x):  39*48=1912\n","43\n","wrong  : 39*48=1912\n","correct: 39*48=1872\n","outputs(x):  19*11=187\n","56*\n","wrong  : 19*11=187\n","correct: 19*11=209\n","outputs(x):  95*39=3315\n","46\n","wrong  : 95*39=3315\n","correct: 95*39=3705\n","outputs(x):  98*16=1440\n","57\n","wrong  : 98*16=1440\n","correct: 98*16=1568\n","outputs(x):  82*95=7055\n","39\n","wrong  : 82*95=7055\n","correct: 82*95=7790\n","outputs(x):  14*76=988\n","19*\n","wrong  : 14*76=988\n","correct: 14*76=1064\n","outputs(x):  52*81=4293\n","31\n","wrong  : 52*81=4293\n","correct: 52*81=4212\n","outputs(x):  44*96=4128\n","60\n","wrong  : 44*96=4128\n","correct: 44*96=4224\n","outputs(x):  59*84=5796\n","62\n","wrong  : 59*84=5796\n","correct: 59*84=4956\n","outputs(x):  97*48=4272\n","48\n","wrong  : 97*48=4272\n","correct: 97*48=4656\n","outputs(x):  77*98=7644\n","7*\n","wrong  : 77*98=7644\n","correct: 77*98=7546\n","outputs(x):  73*23=1449\n","54\n","wrong  : 73*23=1449\n","correct: 73*23=1679\n","outputs(x):  53*19=1387\n","82\n","wrong  : 53*19=1387\n","correct: 53*19=1007\n","outputs(x):  61*80=5280\n","10\n","wrong  : 61*80=5280\n","correct: 61*80=4880\n","outputs(x):  48*20=1260\n","68\n","wrong  : 48*20=1260\n","correct: 48*20=960\n","outputs(x):  68*54=4352\n","54\n","wrong  : 68*54=4352\n","correct: 68*54=3672\n","outputs(x):  44*68=3264\n","33\n","wrong  : 44*68=3264\n","correct: 44*68=2992\n","outputs(x):  55*99=495\n","30*\n","wrong  : 55*99=495\n","correct: 55*99=5445\n","outputs(x):  42*42=1806\n","99\n","wrong  : 42*42=1806\n","correct: 42*42=1764\n","outputs(x):  47*52=2496\n","95\n","wrong  : 47*52=2496\n","correct: 47*52=2444\n","outputs(x):  98*81=7128\n","26\n","wrong  : 98*81=7128\n","correct: 98*81=7938\n","outputs(x):  83*74=5994\n","31\n","wrong  : 83*74=5994\n","correct: 83*74=6142\n","outputs(x):  84*58=4272\n","44\n","wrong  : 84*58=4272\n","correct: 84*58=4872\n","outputs(x):  51*64=3202\n","72\n","wrong  : 51*64=3202\n","correct: 51*64=3264\n","outputs(x):  20*22=680\n","35*\n","wrong  : 20*22=680\n","correct: 20*22=440\n","outputs(x):  56*67=3819\n","96\n","wrong  : 56*67=3819\n","correct: 56*67=3752\n","outputs(x):  70*49=3185\n","66\n","wrong  : 70*49=3185\n","correct: 70*49=3430\n","outputs(x):  33*45=1035\n","13\n","wrong  : 33*45=1035\n","correct: 33*45=1485\n","outputs(x):  39*96=3360\n","43\n","wrong  : 39*96=3360\n","correct: 39*96=3744\n","outputs(x):  41*27=1095\n","45\n","wrong  : 41*27=1095\n","correct: 41*27=1107\n","outputs(x):  26*22=528\n","97*\n","wrong  : 26*22=528\n","correct: 26*22=572\n","outputs(x):  88*19=1862\n","81\n","wrong  : 88*19=1862\n","correct: 88*19=1672\n","outputs(x):  98*26=2574\n","78\n","wrong  : 98*26=2574\n","correct: 98*26=2548\n","outputs(x):  85*95=7225\n","33\n","wrong  : 85*95=7225\n","correct: 85*95=8075\n","outputs(x):  41*34=1734\n","3*\n","wrong  : 41*34=1734\n","correct: 41*34=1394\n","outputs(x):  54*80=4860\n","57\n","wrong  : 54*80=4860\n","correct: 54*80=4320\n","outputs(x):  94*33=3168\n","2*\n","wrong  : 94*33=3168\n","correct: 94*33=3102\n","outputs(x):  52*55=3410\n","6*\n","wrong  : 52*55=3410\n","correct: 52*55=2860\n","outputs(x):  44*72=3384\n","8*\n","wrong  : 44*72=3384\n","correct: 44*72=3168\n","outputs(x):  45*51=2805\n","95\n","wrong  : 45*51=2805\n","correct: 45*51=2295\n","outputs(x):  67*38=2508\n","89\n","wrong  : 67*38=2508\n","correct: 67*38=2546\n","outputs(x):  99*48=4272\n","48\n","wrong  : 99*48=4272\n","correct: 99*48=4752\n","outputs(x):  32*11=374\n","99*\n","wrong  : 32*11=374\n","correct: 32*11=352\n","outputs(x):  18*32=608\n","34*\n","wrong  : 18*32=608\n","correct: 18*32=576\n","outputs(x):  34*87=2926\n","16\n","wrong  : 34*87=2926\n","correct: 34*87=2958\n","outputs(x):  19*52=5148\n","6*\n","wrong  : 19*52=5148\n","correct: 19*52=988\n","outputs(x):  20*36=1080\n","37\n","wrong  : 20*36=1080\n","correct: 20*36=720\n","outputs(x):  29*99=261\n","70*\n","wrong  : 29*99=261\n","correct: 29*99=2871\n","outputs(x):  44*44=1848\n","21\n","wrong  : 44*44=1848\n","correct: 44*44=1936\n","outputs(x):  19*20=1700\n","39\n","wrong  : 19*20=1700\n","correct: 19*20=380\n","outputs(x):  43*36=1924\n","55\n","wrong  : 43*36=1924\n","correct: 43*36=1548\n","outputs(x):  96*91=8918\n","40\n","wrong  : 96*91=8918\n","correct: 96*91=8736\n","outputs(x):  15*99=1185\n","68\n","wrong  : 15*99=1185\n","correct: 15*99=1485\n","outputs(x):  92*96=8928\n","53\n","wrong  : 92*96=8928\n","correct: 92*96=8832\n","outputs(x):  52*73=3726\n","76\n","wrong  : 52*73=3726\n","correct: 52*73=3796\n","outputs(x):  41*20=900\n","4*6\n","wrong  : 41*20=900\n","correct: 41*20=820\n","outputs(x):  69*31=2418\n","7*\n","wrong  : 69*31=2418\n","correct: 69*31=2139\n","outputs(x):  73*67=4819\n","29\n","wrong  : 73*67=4819\n","correct: 73*67=4891\n","outputs(x):  45*32=160\n","66*\n","wrong  : 45*32=160\n","correct: 45*32=1440\n","outputs(x):  69*11=869\n","38*\n","wrong  : 69*11=869\n","correct: 69*11=759\n","outputs(x):  36*22=682\n","36*\n","wrong  : 36*22=682\n","correct: 36*22=792\n","outputs(x):  20*59=1770\n","94\n","wrong  : 20*59=1770\n","correct: 20*59=1180\n","outputs(x):  66*94=6016\n","3*\n","wrong  : 66*94=6016\n","correct: 66*94=6204\n","outputs(x):  59*28=1593\n","12\n","wrong  : 59*28=1593\n","correct: 59*28=1652\n","outputs(x):  65*90=5310\n","54\n","wrong  : 65*90=5310\n","correct: 65*90=5850\n","outputs(x):  61*63=3403\n","7*\n","wrong  : 61*63=3403\n","correct: 61*63=3843\n","outputs(x):  76*86=6020\n","8*\n","wrong  : 76*86=6020\n","correct: 76*86=6536\n","outputs(x):  83*65=5915\n","47\n","wrong  : 83*65=5915\n","correct: 83*65=5395\n","outputs(x):  28*39=1482\n","40\n","wrong  : 28*39=1482\n","correct: 28*39=1092\n","outputs(x):  61*23=1426\n","76\n","wrong  : 61*23=1426\n","correct: 61*23=1403\n","outputs(x):  93*67=6365\n","91\n","wrong  : 93*67=6365\n","correct: 93*67=6231\n","outputs(x):  79*21=1449\n","59\n","wrong  : 79*21=1449\n","correct: 79*21=1659\n","outputs(x):  91*39=3627\n","45\n","wrong  : 91*39=3627\n","correct: 91*39=3549\n","outputs(x):  21*24=544\n","5*1\n","wrong  : 21*24=544\n","correct: 21*24=504\n","outputs(x):  51*83=4725\n","11\n","wrong  : 51*83=4725\n","correct: 51*83=4233\n","outputs(x):  59*16=888\n","46*\n","wrong  : 59*16=888\n","correct: 59*16=944\n","outputs(x):  22*15=310\n","55*\n","wrong  : 22*15=310\n","correct: 22*15=330\n","outputs(x):  70*25=1500\n","28\n","wrong  : 70*25=1500\n","correct: 70*25=1750\n","outputs(x):  60*87=4350\n","41\n","wrong  : 60*87=4350\n","correct: 60*87=5220\n","outputs(x):  27*54=1404\n","63\n","wrong  : 27*54=1404\n","correct: 27*54=1458\n","outputs(x):  23*46=1104\n","18\n","wrong  : 23*46=1104\n","correct: 23*46=1058\n","outputs(x):  87*20=1700\n","39\n","wrong  : 87*20=1700\n","correct: 87*20=1740\n","outputs(x):  93*57=5244\n","5*\n","wrong  : 93*57=5244\n","correct: 93*57=5301\n","outputs(x):  25*55=1225\n","92\n","wrong  : 25*55=1225\n","correct: 25*55=1375\n","outputs(x):  46*75=3150\n","15\n","wrong  : 46*75=3150\n","correct: 46*75=3450\n","outputs(x):  33*92=3956\n","52\n","wrong  : 33*92=3956\n","correct: 33*92=3036\n","outputs(x):  34*10=430\n","32*\n","wrong  : 34*10=430\n","correct: 34*10=340\n","outputs(x):  21*37=703\n","83*\n","wrong  : 21*37=703\n","correct: 21*37=777\n","outputs(x):  50*13=710\n","87*\n","wrong  : 50*13=710\n","correct: 50*13=650\n","outputs(x):  76*46=3404\n","13\n","wrong  : 76*46=3404\n","correct: 76*46=3496\n","outputs(x):  71*81=5893\n","42\n","wrong  : 71*81=5893\n","correct: 71*81=5751\n","outputs(x):  98*21=1848\n","2*\n","wrong  : 98*21=1848\n","correct: 98*21=2058\n","outputs(x):  99*94=94\n","93*2\n","wrong  : 99*94=94\n","correct: 99*94=9306\n","outputs(x):  82*15=1260\n","39\n","wrong  : 82*15=1260\n","correct: 82*15=1230\n"," 80% 20/25 [00:01<00:00, 12.87it/s]outputs(x):  0*47=470\n","66*\n","wrong  : 0*47=470\n","correct: 0*47=0\n","outputs(x):  4*90=4860\n","57\n","wrong  : 4*90=4860\n","correct: 4*90=360\n","outputs(x):  39*3=177\n","15*\n","wrong  : 39*3=177\n","correct: 39*3=117\n","outputs(x):  44*8=32\n","72*2\n","wrong  : 44*8=32\n","correct: 44*8=352\n","outputs(x):  70*5=300\n","87*\n","wrong  : 70*5=300\n","correct: 70*5=350\n","outputs(x):  2*38=2436\n","47\n","wrong  : 2*38=2436\n","correct: 2*38=76\n","outputs(x):  5*12=850\n","95*\n","wrong  : 5*12=850\n","correct: 5*12=60\n","outputs(x):  42*4=124\n","23*\n","wrong  : 42*4=124\n","correct: 42*4=168\n","outputs(x):  9*86=6106\n","66\n","wrong  : 9*86=6106\n","correct: 9*86=774\n","outputs(x):  20*2=20\n","49*8\n","wrong  : 20*2=20\n","correct: 20*2=40\n","outputs(x):  9*36=2844\n","49\n","wrong  : 9*36=2844\n","correct: 9*36=324\n","outputs(x):  35*9=270\n","59*\n","wrong  : 35*9=270\n","correct: 35*9=315\n","outputs(x):  29*4=124\n","23*\n","wrong  : 29*4=124\n","correct: 29*4=116\n","outputs(x):  0*96=7680\n","93\n","wrong  : 0*96=7680\n","correct: 0*96=0\n","outputs(x):  5*61=5795\n","67\n","wrong  : 5*61=5795\n","correct: 5*61=305\n","outputs(x):  7*62=2394\n","40\n","wrong  : 7*62=2394\n","correct: 7*62=434\n","outputs(x):  4*30=2220\n","52\n","wrong  : 4*30=2220\n","correct: 4*30=120\n","outputs(x):  6*50=1900\n","51\n","wrong  : 6*50=1900\n","correct: 6*50=300\n","outputs(x):  34*2=36\n","90*1\n","wrong  : 34*2=36\n","correct: 34*2=68\n","outputs(x):  26*6=16\n","0*4=\n","wrong  : 26*6=16\n","correct: 26*6=156\n","outputs(x):  0*45=1800\n","4*\n","wrong  : 0*45=1800\n","correct: 0*45=0\n","outputs(x):  73*6=48\n","11*4\n","wrong  : 73*6=48\n","correct: 73*6=438\n","outputs(x):  99*4=36\n","83*6\n","wrong  : 99*4=36\n","correct: 99*4=396\n","outputs(x):  17*7=126\n","99*\n","wrong  : 17*7=126\n","correct: 17*7=119\n","outputs(x):  3*36=828\n","68*\n","wrong  : 3*36=828\n","correct: 3*36=108\n","outputs(x):  5*98=7350\n","72\n","wrong  : 5*98=7350\n","correct: 5*98=490\n","outputs(x):  4*85=3740\n","15\n","wrong  : 4*85=3740\n","correct: 4*85=340\n","outputs(x):  9*47=3243\n","73\n","wrong  : 9*47=3243\n","correct: 9*47=423\n","outputs(x):  4*67=1608\n","49\n","wrong  : 4*67=1608\n","correct: 4*67=268\n","outputs(x):  92*5=310\n","97*\n","wrong  : 92*5=310\n","correct: 92*5=460\n","outputs(x):  8*51=2958\n","30\n","wrong  : 8*51=2958\n","correct: 8*51=408\n","outputs(x):  8*47=2162\n","13\n","wrong  : 8*47=2162\n","correct: 8*47=376\n","outputs(x):  5*50=3750\n","97\n","wrong  : 5*50=3750\n","correct: 5*50=250\n","outputs(x):  88*5=40\n","43*5\n","wrong  : 88*5=40\n","correct: 88*5=440\n","outputs(x):  0*30=1800\n","41\n","wrong  : 0*30=1800\n","correct: 0*30=0\n","outputs(x):  8*28=2184\n","41\n","wrong  : 8*28=2184\n","correct: 8*28=224\n","outputs(x):  9*11=869\n","4*5\n","wrong  : 9*11=869\n","correct: 9*11=99\n","outputs(x):  6*60=960\n","46*\n","wrong  : 6*60=960\n","correct: 6*60=360\n","outputs(x):  1*34=1734\n","3*\n","wrong  : 1*34=1734\n","correct: 1*34=34\n","outputs(x):  5*11=935\n","22*\n","wrong  : 5*11=935\n","correct: 5*11=55\n","outputs(x):  26*1=2\n","1*92=\n","wrong  : 26*1=2\n","correct: 26*1=26\n","outputs(x):  1*24=1944\n","44\n","wrong  : 1*24=1944\n","correct: 1*24=24\n","outputs(x):  2*68=6256\n","9*\n","wrong  : 2*68=6256\n","correct: 2*68=136\n","outputs(x):  14*8=16\n","0*49\n","wrong  : 14*8=16\n","correct: 14*8=112\n","outputs(x):  13*4=56\n","39*5\n","wrong  : 13*4=56\n","correct: 13*4=52\n","outputs(x):  32*5=150\n","47*\n","wrong  : 32*5=150\n","correct: 32*5=160\n","outputs(x):  0*71=4970\n","18\n","wrong  : 0*71=4970\n","correct: 0*71=0\n","outputs(x):  1*23=1173\n","3*\n","wrong  : 1*23=1173\n","correct: 1*23=23\n","outputs(x):  5*95=6270\n","52\n","wrong  : 5*95=6270\n","correct: 5*95=475\n","outputs(x):  5*85=7225\n","33\n","wrong  : 5*85=7225\n","correct: 5*85=425\n","outputs(x):  1*70=2170\n","17\n","wrong  : 1*70=2170\n","correct: 1*70=70\n","outputs(x):  4*13=312\n","52*\n","wrong  : 4*13=312\n","correct: 4*13=52\n","outputs(x):  90*8=80\n","80*4\n","wrong  : 90*8=80\n","correct: 90*8=720\n","outputs(x):  4*11=1003\n","41\n","wrong  : 4*11=1003\n","correct: 4*11=44\n","outputs(x):  8*91=8008\n","36\n","wrong  : 8*91=8008\n","correct: 8*91=728\n","outputs(x):  6*55=880\n","73*\n","wrong  : 6*55=880\n","correct: 6*55=330\n","outputs(x):  0*55=550\n","79*\n","wrong  : 0*55=550\n","correct: 0*55=0\n","outputs(x):  76*7=56\n","75*3\n","wrong  : 76*7=56\n","correct: 76*7=532\n","outputs(x):  96*9=684\n","11*\n","wrong  : 96*9=684\n","correct: 96*9=864\n","outputs(x):  26*9=144\n","80*\n","wrong  : 26*9=144\n","correct: 26*9=234\n","outputs(x):  3*21=483\n","35*\n","wrong  : 3*21=483\n","correct: 3*21=63\n","outputs(x):  8*25=1500\n","28\n","wrong  : 8*25=1500\n","correct: 8*25=200\n","outputs(x):  69*1=59\n","65*3\n","wrong  : 69*1=59\n","correct: 69*1=69\n","outputs(x):  0*21=210\n","28*\n","wrong  : 0*21=210\n","correct: 0*21=0\n","outputs(x):  4*46=0\n","9*6=5\n","wrong  : 4*46=0\n","correct: 4*46=184\n","outputs(x):  21*7=143\n","9*9\n","wrong  : 21*7=143\n","correct: 21*7=147\n","outputs(x):  84*2=16\n","5*76\n","wrong  : 84*2=16\n","correct: 84*2=168\n","outputs(x):  4*79=3476\n","60\n","wrong  : 4*79=3476\n","correct: 4*79=316\n","outputs(x):  3*38=2014\n","76\n","wrong  : 3*38=2014\n","correct: 3*38=114\n","outputs(x):  59*7=357\n","42*\n","wrong  : 59*7=357\n","correct: 59*7=413\n","outputs(x):  2*58=3016\n","18\n","wrong  : 2*58=3016\n","correct: 2*58=116\n","outputs(x):  6*37=3182\n","32\n","wrong  : 6*37=3182\n","correct: 6*37=222\n","outputs(x):  3*77=1001\n","26\n","wrong  : 3*77=1001\n","correct: 3*77=231\n","outputs(x):  4*47=470\n","95*\n","wrong  : 4*47=470\n","correct: 4*47=188\n","outputs(x):  14*7=168\n","38*\n","wrong  : 14*7=168\n","correct: 14*7=98\n","outputs(x):  67*6=42\n","67*9\n","wrong  : 67*6=42\n","correct: 67*6=402\n","outputs(x):  1*78=6396\n","23\n","wrong  : 1*78=6396\n","correct: 1*78=78\n","outputs(x):  33*3=9\n","31*18\n","wrong  : 33*3=9\n","correct: 33*3=99\n","outputs(x):  12*8=88\n","86*4\n","wrong  : 12*8=88\n","correct: 12*8=96\n","outputs(x):  77*6=42\n","67*9\n","wrong  : 77*6=42\n","correct: 77*6=462\n","outputs(x):  22*4=8\n","59*52\n","wrong  : 22*4=8\n","correct: 22*4=88\n","outputs(x):  8*74=2664\n","69\n","wrong  : 8*74=2664\n","correct: 8*74=592\n","outputs(x):  1*29=609\n","91*\n","wrong  : 1*29=609\n","correct: 1*29=29\n","outputs(x):  39*4=152\n","72*\n","wrong  : 39*4=152\n","correct: 39*4=156\n","outputs(x):  32*9=276\n","71*\n","wrong  : 32*9=276\n","correct: 32*9=288\n","outputs(x):  73*7=553\n","25*\n","wrong  : 73*7=553\n","correct: 73*7=511\n","outputs(x):  25*3=65\n","42*2\n","wrong  : 25*3=65\n","correct: 25*3=75\n","outputs(x):  79*2=138\n","18*\n","wrong  : 79*2=138\n","correct: 79*2=158\n","outputs(x):  5*31=2805\n","95\n","wrong  : 5*31=2805\n","correct: 5*31=155\n","outputs(x):  6*40=640\n","38*\n","wrong  : 6*40=640\n","correct: 6*40=240\n","outputs(x):  68*9=684\n","11*\n","wrong  : 68*9=684\n","correct: 68*9=612\n","outputs(x):  36*9=342\n","78*\n","wrong  : 36*9=342\n","correct: 36*9=324\n","outputs(x):  45*5=25\n","94*3\n","wrong  : 45*5=25\n","correct: 45*5=225\n","outputs(x):  27*6=102\n","0*4\n","wrong  : 27*6=102\n","correct: 27*6=162\n","outputs(x):  8*62=2356\n","14\n","wrong  : 8*62=2356\n","correct: 8*62=496\n","outputs(x):  36*8=272\n","63*\n","wrong  : 36*8=272\n","correct: 36*8=288\n","outputs(x):  1*19=1729\n","63\n","wrong  : 1*19=1729\n","correct: 1*19=19\n","outputs(x):  7*98=9506\n","85\n","wrong  : 7*98=9506\n","correct: 7*98=686\n","outputs(x):  9*35=2065\n","0*\n","wrong  : 9*35=2065\n","correct: 9*35=315\n","outputs(x):  77*5=35\n","79*6\n","wrong  : 77*5=35\n","correct: 77*5=385\n","outputs(x):  8*94=2632\n","63\n","wrong  : 8*94=2632\n","correct: 8*94=752\n","outputs(x):  63*3=249\n","73*\n","wrong  : 63*3=249\n","correct: 63*3=189\n","outputs(x):  2*49=1568\n","58\n","wrong  : 2*49=1568\n","correct: 2*49=98\n","outputs(x):  1*12=152\n","25*\n","wrong  : 1*12=152\n","correct: 1*12=12\n","outputs(x):  1*13=143\n","60*\n","wrong  : 1*13=143\n","correct: 1*13=13\n","outputs(x):  88*7=686\n","21*\n","wrong  : 88*7=686\n","correct: 88*7=616\n","outputs(x):  46*9=342\n","78*\n","wrong  : 46*9=342\n","correct: 46*9=414\n","outputs(x):  0*64=5760\n","19\n","wrong  : 0*64=5760\n","correct: 0*64=0\n","outputs(x):  3*76=4028\n","26\n","wrong  : 3*76=4028\n","correct: 3*76=228\n","outputs(x):  75*3=255\n","5*3\n","wrong  : 75*3=255\n","correct: 75*3=225\n","outputs(x):  0*76=5320\n","47\n","wrong  : 0*76=5320\n","correct: 0*76=0\n","outputs(x):  4*76=4324\n","60\n","wrong  : 4*76=4324\n","correct: 4*76=304\n","outputs(x):  4*58=4292\n","11\n","wrong  : 4*58=4292\n","correct: 4*58=232\n","outputs(x):  47*6=294\n","94*\n","wrong  : 47*6=294\n","correct: 47*6=282\n","outputs(x):  66*8=488\n","45*\n","wrong  : 66*8=488\n","correct: 66*8=528\n","outputs(x):  49*2=14\n","20*1\n","wrong  : 49*2=14\n","correct: 49*2=98\n","outputs(x):  9*34=1666\n","13\n","wrong  : 9*34=1666\n","correct: 9*34=306\n","outputs(x):  53*5=15\n","2*60\n","wrong  : 53*5=15\n","correct: 53*5=265\n","outputs(x):  37*6=102\n","0*4\n","wrong  : 37*6=102\n","correct: 37*6=222\n","outputs(x):  0*40=2000\n","14\n","wrong  : 0*40=2000\n","correct: 0*40=0\n","outputs(x):  18*2=32\n","98*3\n","wrong  : 18*2=32\n","correct: 18*2=36\n","outputs(x):  92*2=24\n","17*6\n","wrong  : 92*2=24\n","correct: 92*2=184\n","outputs(x):  4*17=238\n","69*\n","wrong  : 4*17=238\n","correct: 4*17=68\n","outputs(x):  2*24=768\n","81*\n","wrong  : 2*24=768\n","correct: 2*24=48\n","outputs(x):  2*50=3100\n","76\n","wrong  : 2*50=3100\n","correct: 2*50=100\n","outputs(x):  0*69=1932\n","21\n","wrong  : 0*69=1932\n","correct: 0*69=0\n","outputs(x):  6*88=1408\n","54\n","wrong  : 6*88=1408\n","correct: 6*88=528\n","outputs(x):  12*6=12\n","43*9\n","wrong  : 12*6=12\n","correct: 12*6=72\n","outputs(x):  1*38=378\n","85*\n","wrong  : 1*38=378\n","correct: 1*38=38\n","outputs(x):  5*62=3570\n","30\n","wrong  : 5*62=3570\n","correct: 5*62=310\n","outputs(x):  9*39=3627\n","45\n","wrong  : 9*39=3627\n","correct: 9*39=351\n","outputs(x):  97*6=594\n","96*\n","wrong  : 97*6=594\n","correct: 97*6=582\n","outputs(x):  7*93=4371\n","0*\n","wrong  : 7*93=4371\n","correct: 7*93=651\n","outputs(x):  1*16=320\n","26*\n","wrong  : 1*16=320\n","correct: 1*16=16\n","outputs(x):  93*2=291\n","78*\n","wrong  : 93*2=291\n","correct: 93*2=186\n","outputs(x):  9*62=5518\n","70\n","wrong  : 9*62=5518\n","correct: 9*62=558\n","outputs(x):  37*4=108\n","95*\n","wrong  : 37*4=108\n","correct: 37*4=148\n","outputs(x):  44*1=4\n","58*32\n","wrong  : 44*1=4\n","correct: 44*1=44\n","outputs(x):  9*83=3237\n","88\n","wrong  : 9*83=3237\n","correct: 9*83=747\n","outputs(x):  53*3=156\n","67*\n","wrong  : 53*3=156\n","correct: 53*3=159\n","outputs(x):  4*23=1472\n","14\n","wrong  : 4*23=1472\n","correct: 4*23=92\n","outputs(x):  7*87=7569\n","10\n","wrong  : 7*87=7569\n","correct: 7*87=609\n","outputs(x):  1*65=5915\n","47\n","wrong  : 1*65=5915\n","correct: 1*65=65\n","outputs(x):  7*99=7623\n","94\n","wrong  : 7*99=7623\n","correct: 7*99=693\n","outputs(x):  4*62=3968\n","68\n","wrong  : 4*62=3968\n","correct: 4*62=248\n","outputs(x):  15*5=105\n","1*3\n","wrong  : 15*5=105\n","correct: 15*5=75\n","outputs(x):  2*69=4416\n","12\n","wrong  : 2*69=4416\n","correct: 2*69=138\n","outputs(x):  42*3=12\n","28*9\n","wrong  : 42*3=12\n","correct: 42*3=126\n","outputs(x):  72*6=42\n","39*8\n","wrong  : 72*6=42\n","correct: 72*6=432\n","outputs(x):  90*7=610\n","38*\n","wrong  : 90*7=610\n","correct: 90*7=630\n","outputs(x):  6*81=5427\n","97\n","wrong  : 6*81=5427\n","correct: 6*81=486\n","outputs(x):  9*54=1566\n","10\n","wrong  : 9*54=1566\n","correct: 9*54=486\n","outputs(x):  79*1=7\n","17*65\n","wrong  : 79*1=7\n","correct: 79*1=79\n","outputs(x):  4*63=1512\n","74\n","wrong  : 4*63=1512\n","correct: 4*63=252\n","outputs(x):  7*38=2886\n","78\n","wrong  : 7*38=2886\n","correct: 7*38=266\n","outputs(x):  1*85=765\n","7*5\n","wrong  : 1*85=765\n","correct: 1*85=85\n","outputs(x):  52*2=106\n","42*\n","wrong  : 52*2=106\n","correct: 52*2=104\n","outputs(x):  5*20=1200\n","43\n","wrong  : 5*20=1200\n","correct: 5*20=100\n","outputs(x):  8*66=5148\n","17\n","wrong  : 8*66=5148\n","correct: 8*66=528\n","outputs(x):  2*96=2304\n","90\n","wrong  : 2*96=2304\n","correct: 2*96=192\n","outputs(x):  7*11=187\n","56*\n","wrong  : 7*11=187\n","correct: 7*11=77\n","outputs(x):  4*96=1344\n","67\n","wrong  : 4*96=1344\n","correct: 4*96=384\n","outputs(x):  1*77=6003\n","67\n","wrong  : 1*77=6003\n","correct: 1*77=77\n","outputs(x):  1*63=693\n","95*\n","wrong  : 1*63=693\n","correct: 1*63=63\n","outputs(x):  54*3=132\n","58*\n","wrong  : 54*3=132\n","correct: 54*3=162\n","outputs(x):  3*30=2190\n","76\n","wrong  : 3*30=2190\n","correct: 3*30=90\n","outputs(x):  32*3=216\n","98*\n","wrong  : 32*3=216\n","correct: 32*3=96\n","outputs(x):  8*88=8624\n","11\n","wrong  : 8*88=8624\n","correct: 8*88=704\n","outputs(x):  1*36=3276\n","82\n","wrong  : 1*36=3276\n","correct: 1*36=36\n","outputs(x):  4*35=1540\n","92\n","wrong  : 4*35=1540\n","correct: 4*35=140\n","outputs(x):  83*1=93\n","58*2\n","wrong  : 83*1=93\n","correct: 83*1=83\n","outputs(x):  30*5=105\n","78*\n","wrong  : 30*5=105\n","correct: 30*5=150\n","outputs(x):  26*8=24\n","21*3\n","wrong  : 26*8=24\n","correct: 26*8=208\n","outputs(x):  4*56=5264\n","4*\n","wrong  : 4*56=5264\n","correct: 4*56=224\n","outputs(x):  9*12=708\n","71*\n","wrong  : 9*12=708\n","correct: 9*12=108\n","outputs(x):  27*2=6\n","35*54\n","wrong  : 27*2=6\n","correct: 27*2=54\n","outputs(x):  82*7=624\n","41*\n","wrong  : 82*7=624\n","correct: 82*7=574\n","outputs(x):  15*6=102\n","0*4\n","wrong  : 15*6=102\n","correct: 15*6=90\n","outputs(x):  64*4=28\n","36*3\n","wrong  : 64*4=28\n","correct: 64*4=256\n","outputs(x):  38*1=36\n","64*1\n","wrong  : 38*1=36\n","correct: 38*1=38\n","outputs(x):  70*9=670\n","90*\n","wrong  : 70*9=670\n","correct: 70*9=630\n","outputs(x):  3*93=2409\n","41\n","wrong  : 3*93=2409\n","correct: 3*93=279\n","outputs(x):  5*80=7600\n","25\n","wrong  : 5*80=7600\n","correct: 5*80=400\n","outputs(x):  9*73=3577\n","5*\n","wrong  : 9*73=3577\n","correct: 9*73=657\n","outputs(x):  89*5=45\n","62*2\n","wrong  : 89*5=45\n","correct: 89*5=445\n","outputs(x):  22*6=124\n","59*\n","wrong  : 22*6=124\n","correct: 22*6=132\n","outputs(x):  3*66=6138\n","67\n","wrong  : 3*66=6138\n","correct: 3*66=198\n","outputs(x):  2*22=1628\n","75\n","wrong  : 2*22=1628\n","correct: 2*22=44\n","outputs(x):  83*2=146\n","90*\n","wrong  : 83*2=146\n","correct: 83*2=166\n","outputs(x):  8*39=1482\n","40\n","wrong  : 8*39=1482\n","correct: 8*39=312\n","outputs(x):  74*9=76\n","94*8\n","wrong  : 74*9=76\n","correct: 74*9=666\n","outputs(x):  1*40=3640\n","66\n","wrong  : 1*40=3640\n","correct: 1*40=40\n","outputs(x):  0*70=3500\n","11\n","wrong  : 0*70=3500\n","correct: 0*70=0\n","outputs(x):  8*52=3016\n","19\n","wrong  : 8*52=3016\n","correct: 8*52=416\n","outputs(x):  3*26=2158\n","13\n","wrong  : 3*26=2158\n","correct: 3*26=78\n","outputs(x):  96*8=68\n","35*8\n","wrong  : 96*8=68\n","correct: 96*8=768\n","outputs(x):  1*64=1344\n","70\n","wrong  : 1*64=1344\n","correct: 1*64=64\n","outputs(x):  8*73=2044\n","9*\n","wrong  : 8*73=2044\n","correct: 8*73=584\n","outputs(x):  1*39=1209\n","4*\n","wrong  : 1*39=1209\n","correct: 1*39=39\n","outputs(x):  3*96=7968\n","89\n","wrong  : 3*96=7968\n","correct: 3*96=288\n","outputs(x):  3*14=1022\n","74\n","wrong  : 3*14=1022\n","correct: 3*14=42\n","outputs(x):  52*7=315\n","76*\n","wrong  : 52*7=315\n","correct: 52*7=364\n","outputs(x):  85*7=55\n","39*7\n","wrong  : 85*7=55\n","correct: 85*7=595\n","outputs(x):  1*10=210\n","28*\n","wrong  : 1*10=210\n","correct: 1*10=10\n","outputs(x):  6*63=1638\n","82\n","wrong  : 6*63=1638\n","correct: 6*63=378\n","outputs(x):  9*65=5785\n","70\n","wrong  : 9*65=5785\n","correct: 9*65=585\n","outputs(x):  94*9=874\n","41*\n","wrong  : 94*9=874\n","correct: 94*9=846\n","outputs(x):  3*19=1387\n","82\n","wrong  : 3*19=1387\n","correct: 3*19=57\n","outputs(x):  8*65=5070\n","56\n","wrong  : 8*65=5070\n","correct: 8*65=520\n","outputs(x):  55*7=315\n","76*\n","wrong  : 55*7=315\n","correct: 55*7=385\n","outputs(x):  46*4=224\n","11*\n","wrong  : 46*4=224\n","correct: 46*4=184\n","outputs(x):  5*77=1925\n","78\n","wrong  : 5*77=1925\n","correct: 5*77=385\n","outputs(x):  6*66=1056\n","59\n","wrong  : 6*66=1056\n","correct: 6*66=396\n","outputs(x):  2*41=3362\n","55\n","wrong  : 2*41=3362\n","correct: 2*41=82\n","outputs(x):  0*59=1770\n","94\n","wrong  : 0*59=1770\n","correct: 0*59=0\n","outputs(x):  1*80=6160\n","27\n","wrong  : 1*80=6160\n","correct: 1*80=80\n","outputs(x):  2*15=315\n","55*\n","wrong  : 2*15=315\n","correct: 2*15=30\n","outputs(x):  33*5=15\n","2*60\n","wrong  : 33*5=15\n","correct: 33*5=165\n","outputs(x):  11*9=979\n","83*\n","wrong  : 11*9=979\n","correct: 11*9=99\n","outputs(x):  90*5=405\n","41*\n","wrong  : 90*5=405\n","correct: 90*5=450\n","outputs(x):  80*5=405\n","41*\n","wrong  : 80*5=405\n","correct: 80*5=400\n","outputs(x):  3*11=693\n","55*\n","wrong  : 3*11=693\n","correct: 3*11=33\n","outputs(x):  35*4=108\n","2*8\n","wrong  : 35*4=108\n","correct: 35*4=140\n","outputs(x):  5*66=4290\n","48\n","wrong  : 5*66=4290\n","correct: 5*66=330\n","outputs(x):  8*83=6474\n","35\n","wrong  : 8*83=6474\n","correct: 8*83=664\n"," 88% 22/25 [00:01<00:00, 12.66it/s]outputs(x):  6*76=5016\n","67\n","wrong  : 6*76=5016\n","correct: 6*76=456\n","outputs(x):  33*9=279\n","5*6\n","wrong  : 33*9=279\n","correct: 33*9=297\n","outputs(x):  9*40=2760\n","38\n","wrong  : 9*40=2760\n","correct: 9*40=360\n","outputs(x):  2*90=8370\n","78\n","wrong  : 2*90=8370\n","correct: 2*90=180\n","outputs(x):  91*1=97\n","68*1\n","wrong  : 91*1=97\n","correct: 91*1=91\n","outputs(x):  9*42=1806\n","71\n","wrong  : 9*42=1806\n","correct: 9*42=378\n","outputs(x):  52*6=372\n","42*\n","wrong  : 52*6=372\n","correct: 52*6=312\n","outputs(x):  3*52=5146\n","31\n","wrong  : 3*52=5146\n","correct: 3*52=156\n","outputs(x):  8*59=2242\n","64\n","wrong  : 8*59=2242\n","correct: 8*59=472\n","outputs(x):  1*88=6958\n","3*\n","wrong  : 1*88=6958\n","correct: 1*88=88\n","outputs(x):  6*39=3182\n","32\n","wrong  : 6*39=3182\n","correct: 6*39=234\n","outputs(x):  2*36=2232\n","23\n","wrong  : 2*36=2232\n","correct: 2*36=72\n","outputs(x):  1*30=330\n","30*\n","wrong  : 1*30=330\n","correct: 1*30=30\n","outputs(x):  53*8=416\n","6*1\n","wrong  : 53*8=416\n","correct: 53*8=424\n","outputs(x):  3*42=1806\n","8*\n","wrong  : 3*42=1806\n","correct: 3*42=126\n","outputs(x):  46*3=108\n","38*\n","wrong  : 46*3=108\n","correct: 46*3=138\n","outputs(x):  98*9=162\n","93*\n","wrong  : 98*9=162\n","correct: 98*9=882\n","outputs(x):  5*59=2655\n","45\n","wrong  : 5*59=2655\n","correct: 5*59=295\n","outputs(x):  59*4=294\n","21*\n","wrong  : 59*4=294\n","correct: 59*4=236\n","outputs(x):  31*2=6\n","35*54\n","wrong  : 31*2=6\n","correct: 31*2=62\n","outputs(x):  7*96=1632\n","51\n","wrong  : 7*96=1632\n","correct: 7*96=672\n","outputs(x):  41*2=94\n","88*7\n","wrong  : 41*2=94\n","correct: 41*2=82\n","outputs(x):  29*6=194\n","94*\n","wrong  : 29*6=194\n","correct: 29*6=174\n","outputs(x):  5*52=2970\n","18\n","wrong  : 5*52=2970\n","correct: 5*52=260\n","outputs(x):  65*9=50\n","21*2\n","wrong  : 65*9=50\n","correct: 65*9=585\n","outputs(x):  6*53=5088\n","70\n","wrong  : 6*53=5088\n","correct: 6*53=318\n","outputs(x):  24*1=2\n","1*92=\n","wrong  : 24*1=2\n","correct: 24*1=24\n","outputs(x):  34*9=278\n","69*\n","wrong  : 34*9=278\n","correct: 34*9=306\n","outputs(x):  3*90=2970\n","21\n","wrong  : 3*90=2970\n","correct: 3*90=270\n","outputs(x):  1*51=3111\n","80\n","wrong  : 1*51=3111\n","correct: 1*51=51\n","outputs(x):  9*99=7821\n","45\n","wrong  : 9*99=7821\n","correct: 9*99=891\n","outputs(x):  66*9=504\n","39*\n","wrong  : 66*9=504\n","correct: 66*9=594\n","outputs(x):  3*25=1675\n","42\n","wrong  : 3*25=1675\n","correct: 3*25=75\n","outputs(x):  63*7=45\n","31*7\n","wrong  : 63*7=45\n","correct: 63*7=441\n","outputs(x):  0*15=300\n","24*\n","wrong  : 0*15=300\n","correct: 0*15=0\n","outputs(x):  81*6=546\n","18*\n","wrong  : 81*6=546\n","correct: 81*6=486\n","outputs(x):  1*58=4582\n","33\n","wrong  : 1*58=4582\n","correct: 1*58=58\n","outputs(x):  20*6=180\n","25*\n","wrong  : 20*6=180\n","correct: 20*6=120\n","outputs(x):  3*49=637\n","73*\n","wrong  : 3*49=637\n","correct: 3*49=147\n","outputs(x):  26*5=10\n","19*1\n","wrong  : 26*5=10\n","correct: 26*5=130\n","outputs(x):  29*8=240\n","69*\n","wrong  : 29*8=240\n","correct: 29*8=232\n","outputs(x):  8*79=2212\n","44\n","wrong  : 8*79=2212\n","correct: 8*79=632\n","outputs(x):  0*20=1400\n","44\n","wrong  : 0*20=1400\n","correct: 0*20=0\n","outputs(x):  27*5=105\n","1*3\n","wrong  : 27*5=105\n","correct: 27*5=135\n","outputs(x):  0*73=2300\n","63\n","wrong  : 0*73=2300\n","correct: 0*73=0\n","outputs(x):  9*77=2233\n","30\n","wrong  : 9*77=2233\n","correct: 9*77=693\n","outputs(x):  65*3=215\n","16*\n","wrong  : 65*3=215\n","correct: 65*3=195\n","outputs(x):  8*40=1160\n","71\n","wrong  : 8*40=1160\n","correct: 8*40=320\n","outputs(x):  7*88=5896\n","98\n","wrong  : 7*88=5896\n","correct: 7*88=616\n","outputs(x):  5*39=2145\n","90\n","wrong  : 5*39=2145\n","correct: 5*39=195\n","outputs(x):  46*8=32\n","72*2\n","wrong  : 46*8=32\n","correct: 46*8=368\n","outputs(x):  30*3=60\n","96*2\n","wrong  : 30*3=60\n","correct: 30*3=90\n","outputs(x):  14*9=16\n","99*1\n","wrong  : 14*9=16\n","correct: 14*9=126\n","outputs(x):  95*5=45\n","62*2\n","wrong  : 95*5=45\n","correct: 95*5=475\n","outputs(x):  1*73=6643\n","85\n","wrong  : 1*73=6643\n","correct: 1*73=73\n","outputs(x):  0*10=200\n","68*\n","wrong  : 0*10=200\n","correct: 0*10=0\n","outputs(x):  8*41=1558\n","44\n","wrong  : 8*41=1558\n","correct: 8*41=328\n","outputs(x):  7*60=3420\n","84\n","wrong  : 7*60=3420\n","correct: 7*60=420\n","outputs(x):  9*28=2212\n","5*\n","wrong  : 9*28=2212\n","correct: 9*28=252\n","outputs(x):  92*4=336\n","28*\n","wrong  : 92*4=336\n","correct: 92*4=368\n","outputs(x):  13*3=23\n","68*6\n","wrong  : 13*3=23\n","correct: 13*3=39\n","outputs(x):  4*73=2482\n","29\n","wrong  : 4*73=2482\n","correct: 4*73=292\n","outputs(x):  7*80=6160\n","27\n","wrong  : 7*80=6160\n","correct: 7*80=560\n","outputs(x):  55*8=400\n","24*\n","wrong  : 55*8=400\n","correct: 55*8=440\n","outputs(x):  6*94=3102\n","56\n","wrong  : 6*94=3102\n","correct: 6*94=564\n","outputs(x):  20*4=120\n","53*\n","wrong  : 20*4=120\n","correct: 20*4=80\n","outputs(x):  97*2=14\n","32*1\n","wrong  : 97*2=14\n","correct: 97*2=194\n","outputs(x):  27*7=207\n","70*\n","wrong  : 27*7=207\n","correct: 27*7=189\n","outputs(x):  0*61=1860\n","75\n","wrong  : 0*61=1860\n","correct: 0*61=0\n","outputs(x):  23*2=6\n","35*54\n","wrong  : 23*2=6\n","correct: 23*2=46\n","outputs(x):  4*36=3384\n","80\n","wrong  : 4*36=3384\n","correct: 4*36=144\n","outputs(x):  76*3=255\n","5*3\n","wrong  : 76*3=255\n","correct: 76*3=228\n","outputs(x):  29*7=261\n","70*\n","wrong  : 29*7=261\n","correct: 29*7=203\n","outputs(x):  1*47=517\n","0*0\n","wrong  : 1*47=517\n","correct: 1*47=47\n","outputs(x):  67*3=221\n","67*\n","wrong  : 67*3=221\n","correct: 67*3=201\n","outputs(x):  7*29=783\n","49*\n","wrong  : 7*29=783\n","correct: 7*29=203\n","outputs(x):  6*68=1088\n","15\n","wrong  : 6*68=1088\n","correct: 6*68=408\n","outputs(x):  83*4=292\n","22*\n","wrong  : 83*4=292\n","correct: 83*4=332\n","outputs(x):  33*2=6\n","35*54\n","wrong  : 33*2=6\n","correct: 33*2=66\n","outputs(x):  7*28=2716\n","67\n","wrong  : 7*28=2716\n","correct: 7*28=196\n","outputs(x):  5*24=1250\n","76\n","wrong  : 5*24=1250\n","correct: 5*24=120\n","outputs(x):  6*75=4200\n","30\n","wrong  : 6*75=4200\n","correct: 6*75=450\n","outputs(x):  2*48=3456\n","66\n","wrong  : 2*48=3456\n","correct: 2*48=96\n","outputs(x):  51*9=477\n","6*1\n","wrong  : 51*9=477\n","correct: 51*9=459\n","outputs(x):  39*2=18\n","70*7\n","wrong  : 39*2=18\n","correct: 39*2=78\n","outputs(x):  7*58=3886\n","97\n","wrong  : 7*58=3886\n","correct: 7*58=406\n","outputs(x):  6*85=3060\n","4*\n","wrong  : 6*85=3060\n","correct: 6*85=510\n","outputs(x):  2*77=1694\n","43\n","wrong  : 2*77=1694\n","correct: 2*77=154\n","outputs(x):  7*75=1275\n","59\n","wrong  : 7*75=1275\n","correct: 7*75=525\n","outputs(x):  5*10=250\n","98*\n","wrong  : 5*10=250\n","correct: 5*10=50\n","outputs(x):  71*3=217\n","73*\n","wrong  : 71*3=217\n","correct: 71*3=213\n","outputs(x):  1*46=2806\n","23\n","wrong  : 1*46=2806\n","correct: 1*46=46\n","outputs(x):  9*23=1357\n","76\n","wrong  : 9*23=1357\n","correct: 9*23=207\n","outputs(x):  8*53=742\n","23*\n","wrong  : 8*53=742\n","correct: 8*53=424\n","outputs(x):  88*4=32\n","51*3\n","wrong  : 88*4=32\n","correct: 88*4=352\n","outputs(x):  5*72=1300\n","8*\n","wrong  : 5*72=1300\n","correct: 5*72=360\n","outputs(x):  3*65=1650\n","43\n","wrong  : 3*65=1650\n","correct: 3*65=195\n","outputs(x):  94*2=88\n","8*75\n","wrong  : 94*2=88\n","correct: 94*2=188\n","outputs(x):  4*60=560\n","67*\n","wrong  : 4*60=560\n","correct: 4*60=240\n","outputs(x):  54*8=486\n","6*7\n","wrong  : 54*8=486\n","correct: 54*8=432\n","outputs(x):  19*5=195\n","30*\n","wrong  : 19*5=195\n","correct: 19*5=95\n","outputs(x):  23*3=9\n","31*18\n","wrong  : 23*3=9\n","correct: 23*3=69\n","outputs(x):  8*67=1742\n","31\n","wrong  : 8*67=1742\n","correct: 8*67=536\n","outputs(x):  59*2=138\n","18*\n","wrong  : 59*2=138\n","correct: 59*2=118\n","outputs(x):  27*3=51\n","5*30\n","wrong  : 27*3=51\n","correct: 27*3=81\n","outputs(x):  5*35=1225\n","92\n","wrong  : 5*35=1225\n","correct: 5*35=175\n","outputs(x):  31*3=133\n","99*\n","wrong  : 31*3=133\n","correct: 31*3=93\n","outputs(x):  5*81=7695\n","78\n","wrong  : 5*81=7695\n","correct: 5*81=405\n","outputs(x):  35*7=215\n","5*1\n","wrong  : 35*7=215\n","correct: 35*7=245\n","outputs(x):  83*6=554\n","75*\n","wrong  : 83*6=554\n","correct: 83*6=498\n","outputs(x):  50*4=120\n","53*\n","wrong  : 50*4=120\n","correct: 50*4=200\n","outputs(x):  13*6=102\n","9*4\n","wrong  : 13*6=102\n","correct: 13*6=78\n","outputs(x):  4*52=3848\n","12\n","wrong  : 4*52=3848\n","correct: 4*52=208\n","outputs(x):  11*4=4\n","49*5=\n","wrong  : 11*4=4\n","correct: 11*4=44\n","outputs(x):  4*80=5120\n","92\n","wrong  : 4*80=5120\n","correct: 4*80=320\n","outputs(x):  12*7=88\n","86*4\n","wrong  : 12*7=88\n","correct: 12*7=84\n","outputs(x):  6*31=832\n","59*\n","wrong  : 6*31=832\n","correct: 6*31=186\n","outputs(x):  0*66=660\n","99*\n","wrong  : 0*66=660\n","correct: 0*66=0\n","outputs(x):  83*9=729\n","37*\n","wrong  : 83*9=729\n","correct: 83*9=747\n","outputs(x):  8*24=672\n","10*\n","wrong  : 8*24=672\n","correct: 8*24=192\n","outputs(x):  9*27=2673\n","41\n","wrong  : 9*27=2673\n","correct: 9*27=243\n","outputs(x):  6*25=1650\n","33\n","wrong  : 6*25=1650\n","correct: 6*25=150\n","outputs(x):  68*7=56\n","75*3\n","wrong  : 68*7=56\n","correct: 68*7=476\n","outputs(x):  88*9=78\n","92*5\n","wrong  : 88*9=78\n","correct: 88*9=792\n","outputs(x):  7*25=1675\n","89\n","wrong  : 7*25=1675\n","correct: 7*25=175\n","outputs(x):  96*6=602\n","30*\n","wrong  : 96*6=602\n","correct: 96*6=576\n","outputs(x):  64*7=469\n","39*\n","wrong  : 64*7=469\n","correct: 64*7=448\n","outputs(x):  8*15=870\n","12*\n","wrong  : 8*15=870\n","correct: 8*15=120\n","outputs(x):  2*61=1342\n","41\n","wrong  : 2*61=1342\n","correct: 2*61=122\n","outputs(x):  0*11=370\n","91*\n","wrong  : 0*11=370\n","correct: 0*11=0\n","outputs(x):  3*22=1606\n","32\n","wrong  : 3*22=1606\n","correct: 3*22=66\n","outputs(x):  77*2=14\n","30*9\n","wrong  : 77*2=14\n","correct: 77*2=154\n","outputs(x):  8*36=3168\n","79\n","wrong  : 8*36=3168\n","correct: 8*36=288\n","outputs(x):  3*78=1014\n","14\n","wrong  : 3*78=1014\n","correct: 3*78=234\n","outputs(x):  2*35=2520\n","57\n","wrong  : 2*35=2520\n","correct: 2*35=70\n","outputs(x):  41*9=378\n","74*\n","wrong  : 41*9=378\n","correct: 41*9=369\n","outputs(x):  0*90=7200\n","60\n","wrong  : 0*90=7200\n","correct: 0*90=0\n","outputs(x):  7*76=2812\n","47\n","wrong  : 7*76=2812\n","correct: 7*76=532\n","outputs(x):  72*2=24\n","17*6\n","wrong  : 72*2=24\n","correct: 72*2=144\n","outputs(x):  7*42=2814\n","27\n","wrong  : 7*42=2814\n","correct: 7*42=294\n","outputs(x):  65*6=36\n","86*7\n","wrong  : 65*6=36\n","correct: 65*6=390\n","outputs(x):  24*9=1372\n","79\n","wrong  : 24*9=1372\n","correct: 24*9=216\n","outputs(x):  2*51=612\n","88*\n","wrong  : 2*51=612\n","correct: 2*51=102\n","outputs(x):  5*68=3060\n","81\n","wrong  : 5*68=3060\n","correct: 5*68=340\n","outputs(x):  51*5=305\n","1*3\n","wrong  : 51*5=305\n","correct: 51*5=255\n","outputs(x):  9*72=1368\n","48\n","wrong  : 9*72=1368\n","correct: 9*72=648\n","outputs(x):  9*46=3634\n","46\n","wrong  : 9*46=3634\n","correct: 9*46=414\n","outputs(x):  13*2=22\n","68*6\n","wrong  : 13*2=22\n","correct: 13*2=26\n","outputs(x):  6*43=4128\n","28\n","wrong  : 6*43=4128\n","correct: 6*43=258\n","outputs(x):  37*3=114\n","83*\n","wrong  : 37*3=114\n","correct: 37*3=111\n","outputs(x):  7*31=837\n","60*\n","wrong  : 7*31=837\n","correct: 7*31=217\n","outputs(x):  3*34=2720\n","16\n","wrong  : 3*34=2720\n","correct: 3*34=102\n","outputs(x):  0*23=690\n","70*\n","wrong  : 0*23=690\n","correct: 0*23=0\n","outputs(x):  9*80=1520\n","8*\n","wrong  : 9*80=1520\n","correct: 9*80=720\n","outputs(x):  7*30=2310\n","58\n","wrong  : 7*30=2310\n","correct: 7*30=210\n","outputs(x):  77*3=21\n","46*1\n","wrong  : 77*3=21\n","correct: 77*3=231\n","outputs(x):  6*10=460\n","16*\n","wrong  : 6*10=460\n","correct: 6*10=60\n","outputs(x):  25*7=195\n","38*\n","wrong  : 25*7=195\n","correct: 25*7=175\n","outputs(x):  76*8=684\n","11*\n","wrong  : 76*8=684\n","correct: 76*8=608\n","outputs(x):  21*9=179\n","12*\n","wrong  : 21*9=179\n","correct: 21*9=189\n","outputs(x):  7*50=3850\n","66\n","wrong  : 7*50=3850\n","correct: 7*50=350\n","outputs(x):  7*74=6512\n","16\n","wrong  : 7*74=6512\n","correct: 7*74=518\n","outputs(x):  7*71=1207\n","14\n","wrong  : 7*71=1207\n","correct: 7*71=497\n","outputs(x):  74*3=216\n","98*\n","wrong  : 74*3=216\n","correct: 74*3=222\n","outputs(x):  47*4=28\n","36*3\n","wrong  : 47*4=28\n","correct: 47*4=188\n","outputs(x):  26*3=108\n","38*\n","wrong  : 26*3=108\n","correct: 26*3=78\n","outputs(x):  8*30=540\n","67*\n","wrong  : 8*30=540\n","correct: 8*30=240\n","outputs(x):  1*67=1407\n","45\n","wrong  : 1*67=1407\n","correct: 1*67=67\n","outputs(x):  69*8=512\n","23*\n","wrong  : 69*8=512\n","correct: 69*8=552\n","outputs(x):  2*57=2231\n","34\n","wrong  : 2*57=2231\n","correct: 2*57=114\n","outputs(x):  5*16=880\n","44*\n","wrong  : 5*16=880\n","correct: 5*16=80\n","outputs(x):  1*18=378\n","85*\n","wrong  : 1*18=378\n","correct: 1*18=18\n","outputs(x):  23*6=18\n","50*9\n","wrong  : 23*6=18\n","correct: 23*6=138\n","outputs(x):  1*82=3280\n","94\n","wrong  : 1*82=3280\n","correct: 1*82=82\n","outputs(x):  72*5=310\n","97*\n","wrong  : 72*5=310\n","correct: 72*5=360\n","outputs(x):  2*14=168\n","37*\n","wrong  : 2*14=168\n","correct: 2*14=28\n","outputs(x):  87*9=873\n","50*\n","wrong  : 87*9=873\n","correct: 87*9=783\n","outputs(x):  5*46=2070\n","13\n","wrong  : 5*46=2070\n","correct: 5*46=230\n","outputs(x):  24*3=12\n","28*9\n","wrong  : 24*3=12\n","correct: 24*3=72\n","outputs(x):  16*7=102\n","0*4\n","wrong  : 16*7=102\n","correct: 16*7=112\n","outputs(x):  85*4=336\n","28*\n","wrong  : 85*4=336\n","correct: 85*4=340\n","outputs(x):  8*33=594\n","81*\n","wrong  : 8*33=594\n","correct: 8*33=264\n","outputs(x):  9*84=5796\n","62\n","wrong  : 9*84=5796\n","correct: 9*84=756\n","outputs(x):  34*6=18\n","50*9\n","wrong  : 34*6=18\n","correct: 34*6=204\n","outputs(x):  64*3=132\n","58*\n","wrong  : 64*3=132\n","correct: 64*3=192\n","outputs(x):  98*8=752\n","64*\n","wrong  : 98*8=752\n","correct: 98*8=784\n","outputs(x):  65*5=380\n","4*2\n","wrong  : 65*5=380\n","correct: 65*5=325\n","outputs(x):  5*18=630\n","24*\n","wrong  : 5*18=630\n","correct: 5*18=90\n","outputs(x):  6*24=1824\n","16\n","wrong  : 6*24=1824\n","correct: 6*24=144\n","outputs(x):  0*51=4080\n","38\n","wrong  : 0*51=4080\n","correct: 0*51=0\n","outputs(x):  7*35=2345\n","76\n","wrong  : 7*35=2345\n","correct: 7*35=245\n","outputs(x):  0*83=6640\n","97\n","wrong  : 0*83=6640\n","correct: 0*83=0\n","outputs(x):  1*37=703\n","83*\n","wrong  : 1*37=703\n","correct: 1*37=37\n","outputs(x):  6*12=540\n","45*\n","wrong  : 6*12=540\n","correct: 6*12=72\n","outputs(x):  9*90=8010\n","38\n","wrong  : 9*90=8010\n","correct: 9*90=810\n","outputs(x):  65*7=45\n","31*6\n","wrong  : 65*7=45\n","correct: 65*7=455\n","outputs(x):  88*8=64\n","94*4\n","wrong  : 88*8=64\n","correct: 88*8=704\n","outputs(x):  53*7=417\n","12*\n","wrong  : 53*7=417\n","correct: 53*7=371\n","outputs(x):  17*2=24\n","17*6\n","wrong  : 17*2=24\n","correct: 17*2=34\n","outputs(x):  6*36=936\n","4*5\n","wrong  : 6*36=936\n","correct: 6*36=216\n","outputs(x):  1*45=1705\n","9*\n","wrong  : 1*45=1705\n","correct: 1*45=45\n","outputs(x):  84*3=246\n","48*\n","wrong  : 84*3=246\n","correct: 84*3=252\n","outputs(x):  5*38=1290\n","76\n","wrong  : 5*38=1290\n","correct: 5*38=190\n","outputs(x):  1*14=1274\n","19\n","wrong  : 1*14=1274\n","correct: 1*14=14\n","outputs(x):  2*54=2268\n","49\n","wrong  : 2*54=2268\n","correct: 2*54=108\n","outputs(x):  92*9=812\n","14*\n","wrong  : 92*9=812\n","correct: 92*9=828\n","outputs(x):  32*6=18\n","50*9\n","wrong  : 32*6=18\n","correct: 32*6=192\n","outputs(x):  72*8=416\n","6*1\n","wrong  : 72*8=416\n","correct: 72*8=576\n","outputs(x):  13*8=12\n","35*0\n","wrong  : 13*8=12\n","correct: 13*8=104\n","outputs(x):  5*96=7200\n","23\n","wrong  : 5*96=7200\n","correct: 5*96=480\n","outputs(x):  2*47=3384\n","76\n","wrong  : 2*47=3384\n","correct: 2*47=94\n","outputs(x):  37*7=21\n","89*6\n","wrong  : 37*7=21\n","correct: 37*7=259\n","outputs(x):  1*43=387\n","54*\n","wrong  : 1*43=387\n","correct: 1*43=43\n","outputs(x):  77*8=696\n","64*\n","wrong  : 77*8=696\n","correct: 77*8=616\n","outputs(x):  8*60=4680\n","77\n","wrong  : 8*60=4680\n","correct: 8*60=480\n","outputs(x):  2*55=660\n","23*\n","wrong  : 2*55=660\n","correct: 2*55=110\n","outputs(x):  8*34=2652\n","56\n","wrong  : 8*34=2652\n","correct: 8*34=272\n","outputs(x):  3*81=2753\n","38\n","wrong  : 3*81=2753\n","correct: 3*81=243\n","outputs(x):  37*1=3\n","91*64\n","wrong  : 37*1=3\n","correct: 37*1=37\n","outputs(x):  3*20=860\n","98*\n","wrong  : 3*20=860\n","correct: 3*20=60\n","outputs(x):  37*5=195\n","30*\n","wrong  : 37*5=195\n","correct: 37*5=185\n","outputs(x):  19*6=18\n","18*5\n","wrong  : 19*6=18\n","correct: 19*6=114\n","outputs(x):  99*5=455\n","83*\n","wrong  : 99*5=455\n","correct: 99*5=495\n","outputs(x):  0*57=570\n","73*\n","wrong  : 0*57=570\n","correct: 0*57=0\n","outputs(x):  7*24=1128\n","75\n","wrong  : 7*24=1128\n","correct: 7*24=168\n","outputs(x):  79*8=624\n","41*\n","wrong  : 79*8=624\n","correct: 79*8=632\n","outputs(x):  56*3=18\n","18*3\n","wrong  : 56*3=18\n","correct: 56*3=168\n","outputs(x):  5*15=825\n","88*\n","wrong  : 5*15=825\n","correct: 5*15=75\n"," 96% 24/25 [00:02<00:00, 12.26it/s]outputs(x):  42*6=246\n","23*\n","wrong  : 42*6=246\n","correct: 42*6=252\n","outputs(x):  2*45=1080\n","19\n","wrong  : 2*45=1080\n","correct: 2*45=90\n","outputs(x):  9*79=4661\n","35\n","wrong  : 9*79=4661\n","correct: 9*79=711\n","outputs(x):  9*18=1242\n","22\n","wrong  : 9*18=1242\n","correct: 9*18=162\n","outputs(x):  58*8=448\n","83*\n","wrong  : 58*8=448\n","correct: 58*8=464\n","outputs(x):  1*59=1770\n","94\n","wrong  : 1*59=1770\n","correct: 1*59=59\n","outputs(x):  2*65=5330\n","6*\n","wrong  : 2*65=5330\n","correct: 2*65=130\n","outputs(x):  75*8=680\n","35*\n","wrong  : 75*8=680\n","correct: 75*8=600\n","outputs(x):  94*3=276\n","91*\n","wrong  : 94*3=276\n","correct: 94*3=282\n","outputs(x):  8*10=580\n","73*\n","wrong  : 8*10=580\n","correct: 8*10=80\n","outputs(x):  6*51=3111\n","80\n","wrong  : 6*51=3111\n","correct: 6*51=306\n","outputs(x):  86*3=28\n","99*9\n","wrong  : 86*3=28\n","correct: 86*3=258\n","outputs(x):  91*8=88\n","86*4\n","wrong  : 91*8=88\n","correct: 91*8=728\n","outputs(x):  3*43=3999\n","98\n","wrong  : 3*43=3999\n","correct: 3*43=129\n","outputs(x):  0*48=2880\n","56\n","wrong  : 0*48=2880\n","correct: 0*48=0\n","outputs(x):  2*23=1186\n","25\n","wrong  : 2*23=1186\n","correct: 2*23=46\n","outputs(x):  28*8=312\n","43*\n","wrong  : 28*8=312\n","correct: 28*8=224\n","outputs(x):  2*29=638\n","25*\n","wrong  : 2*29=638\n","correct: 2*29=58\n","outputs(x):  9*71=2059\n","91\n","wrong  : 9*71=2059\n","correct: 9*71=639\n","outputs(x):  2*28=1456\n","70\n","wrong  : 2*28=1456\n","correct: 2*28=56\n","outputs(x):  6*14=1064\n","90\n","wrong  : 6*14=1064\n","correct: 6*14=84\n","outputs(x):  87*6=468\n","93*\n","wrong  : 87*6=468\n","correct: 87*6=522\n","outputs(x):  58*4=224\n","11*\n","wrong  : 58*4=224\n","correct: 58*4=232\n","outputs(x):  7*40=680\n","69*\n","wrong  : 7*40=680\n","correct: 7*40=280\n","outputs(x):  6*13=390\n","26*\n","wrong  : 6*13=390\n","correct: 6*13=78\n","100% 25/25 [00:02<00:00, 12.20it/s]\n","accuracy of 3000 examples: 157/3000 (5.233333333333333%)\n","{'carry0': 11.648351648351648, 'carry1': 3.6404160475482916, 'carry2': 0.2688172043010753, 'carry3': nan, 'carry4': nan, 'carry5': nan}\n","evaluating addition from: FILE:data/bal/train_multiplication_3000.txt\n","Evaluating Addition using test data file: data/bal/train_multiplication_3000.txt\n","100% 3000/3000 [00:00<00:00, 21036.17it/s]\n","100% 24/24 [00:00<00:00, 28.11it/s]\n","accuracy of 3000 examples: 2732/3000 (91.06666666666666%)\n","{'carry0': 85.98130841121495, 'carry1': 90.96477794793262, 'carry2': 97.94801641586868, 'carry3': nan, 'carry4': nan, 'carry5': nan}\n","step 5000: train loss 0.0316, val loss 12.8791\n","iter 5000: loss 0.0330, time 32468.22ms, mfu 5.85%\n","saving final checkpoint to out2/multiplication_plain\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mplain\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/multiplication/runs/rk73t7u9\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250623_222718-rk73t7u9/logs\u001b[0m\n"]}],"source":["!python train.py config2/multiplication/plain/train_addition_bal.py"]},{"cell_type":"markdown","metadata":{"id":"TpnV8IAgZ0QF"},"source":["## 0-to-999 times 1-digit multiplication (balanced data)"]},{"cell_type":"markdown","metadata":{"id":"XFYuOesGl5bK"},"source":["Plain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2464,"status":"ok","timestamp":1751228252900,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"sNLMc1pYVkR1","outputId":"b7b2f7a6-03f4-4f9d-d6f0-b6a7f6452ea0"},"outputs":[{"name":"stdout","output_type":"stream","text":["# train a miniature character-level shakespeare model\n","# good for debugging and playing on macbooks and such\n","\n","out_dir = 'out2/multiplication_plain_0_to_999_times_1_digit_bal'\n","eval_interval = 250 # keep frequent because we'll overfit\n","eval_iters = 200\n","log_interval = 10 # don't print too too often\n","\n","# we expect to overfit on this small dataset, so only save when val improves\n","always_save_checkpoint = False\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'multiplication'\n","wandb_run_name = 'plain_0_to_999_times_1_digit_bal'\n","\n","data_type='text'\n","data_format='plain'\n","operator='*'\n","dataset = 'bal'\n","batch_size = 256\n","block_size = 256 # context of up to 256 previous characters\n","train_data_path = '0_to_999_times_1_digit_train_3000.txt'\n","# val_data_path = 'val.bin'\n","ckpt_path_name = 'ckpt.pt'\n","eval_addition = True\n","start = \"FILE:data/bal/0_to_999_times_1_digit_test_3000.txt\" #\"FILE:data/bal/test_multiplication_7000.txt\"\n","eval_addition_train = True\n","# start_train = \"FILE:data/multiplication/plain/train_examples_3000_trainprompt.txt\"\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually\n","beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n","\n","warmup_iters = 100 # not super necessary potentially\n","\n","device='cuda:0'\n","\n","# on macbook also add\n","# device = 'cpu'  # run on cpu only\n","# compile = False # do not torch compile the model\n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval1 = True\n","early_eval_interval1 = 5\n","early_eval_iters1 = 1000\n","\n","more_early_eval2 = False\n","early_eval_interval2 = 25\n","early_eval_iters2 = 1200"]}],"source":["%cat config2/multiplication/plain/train_addition_bal.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":767018,"status":"ok","timestamp":1751236101594,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"b9njQEWWZzor","outputId":"ded40ac7-73c2-45d6-daca-7cdca3616f28"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","wrong  : 847*7=6629\n","correct: 847*7=5929\n","outputs(x):  762*6=4532\n","52\n","wrong  : 762*6=4532\n","correct: 762*6=4572\n","outputs(x):  788*7=5526\n","23\n","wrong  : 788*7=5526\n","correct: 788*7=5516\n","outputs(x):  137*5=5\n","575*6\n","wrong  : 137*5=5\n","correct: 137*5=685\n","outputs(x):  162*7=1184\n","14\n","wrong  : 162*7=1184\n","correct: 162*7=1134\n","outputs(x):  823*3=249\n","752\n","wrong  : 823*3=249\n","correct: 823*3=2469\n","outputs(x):  104*9=90\n","458*\n","wrong  : 104*9=90\n","correct: 104*9=936\n","outputs(x):  179*4=756\n","303\n","wrong  : 179*4=756\n","correct: 179*4=716\n","outputs(x):  299*9=2601\n","54\n","wrong  : 299*9=2601\n","correct: 299*9=2691\n","outputs(x):  254*8=1232\n","76\n","wrong  : 254*8=1232\n","correct: 254*8=2032\n","outputs(x):  431*9=3987\n","70\n","wrong  : 431*9=3987\n","correct: 431*9=3879\n","outputs(x):  936*7=6524\n","23\n","wrong  : 936*7=6524\n","correct: 936*7=6552\n","outputs(x):  103*6=606\n","96*\n","wrong  : 103*6=606\n","correct: 103*6=618\n","outputs(x):  736*9=6804\n","21\n","wrong  : 736*9=6804\n","correct: 736*9=6624\n","outputs(x):  985*9=8855\n","60\n","wrong  : 985*9=8855\n","correct: 985*9=8865\n","outputs(x):  955*7=6615\n","58\n","wrong  : 955*7=6615\n","correct: 955*7=6685\n","outputs(x):  446*2=292\n","827\n","wrong  : 446*2=292\n","correct: 446*2=892\n","outputs(x):  519*6=3054\n","50\n","wrong  : 519*6=3054\n","correct: 519*6=3114\n","outputs(x):  869*8=7712\n","19\n","wrong  : 869*8=7712\n","correct: 869*8=6952\n","outputs(x):  105*8=820\n","230\n","wrong  : 105*8=820\n","correct: 105*8=840\n","outputs(x):  829*6=4934\n","14\n","wrong  : 829*6=4934\n","correct: 829*6=4974\n","outputs(x):  248*9=2052\n","71\n","wrong  : 248*9=2052\n","correct: 248*9=2232\n","outputs(x):  825*9=7445\n","97\n","wrong  : 825*9=7445\n","correct: 825*9=7425\n","outputs(x):  520*7=3540\n","99\n","wrong  : 520*7=3540\n","correct: 520*7=3640\n","outputs(x):  836*7=5152\n","23\n","wrong  : 836*7=5152\n","correct: 836*7=5852\n","outputs(x):  581*9=5299\n","14\n","wrong  : 581*9=5299\n","correct: 581*9=5229\n","outputs(x):  558*9=5892\n","29\n","wrong  : 558*9=5892\n","correct: 558*9=5022\n","outputs(x):  666*6=396\n","922\n","wrong  : 666*6=396\n","correct: 666*6=3996\n","outputs(x):  332*2=64\n","527*\n","wrong  : 332*2=64\n","correct: 332*2=664\n","outputs(x):  322*7=2224\n","94\n","wrong  : 322*7=2224\n","correct: 322*7=2254\n","outputs(x):  910*5=4500\n","77\n","wrong  : 910*5=4500\n","correct: 910*5=4550\n","outputs(x):  521*4=2044\n","89\n","wrong  : 521*4=2044\n","correct: 521*4=2084\n","outputs(x):  500*6=3060\n","51\n","wrong  : 500*6=3060\n","correct: 500*6=3000\n","outputs(x):  494*9=4496\n","35\n","wrong  : 494*9=4496\n","correct: 494*9=4446\n","outputs(x):  437*7=3759\n","89\n","wrong  : 437*7=3759\n","correct: 437*7=3059\n","outputs(x):  271*7=1217\n","73\n","wrong  : 271*7=1217\n","correct: 271*7=1897\n","outputs(x):  266*7=1822\n","40\n","wrong  : 266*7=1822\n","correct: 266*7=1862\n","outputs(x):  379*8=2232\n","87\n","wrong  : 379*8=2232\n","correct: 379*8=3032\n","outputs(x):  327*3=921\n","281\n","wrong  : 327*3=921\n","correct: 327*3=981\n","outputs(x):  267*2=552\n","792\n","wrong  : 267*2=552\n","correct: 267*2=534\n","outputs(x):  178*9=1502\n","62\n","wrong  : 178*9=1502\n","correct: 178*9=1602\n","outputs(x):  787*3=2331\n","9*\n","wrong  : 787*3=2331\n","correct: 787*3=2361\n","outputs(x):  972*9=8716\n","19\n","wrong  : 972*9=8716\n","correct: 972*9=8748\n","outputs(x):  610*3=1800\n","51\n","wrong  : 610*3=1800\n","correct: 610*3=1830\n","outputs(x):  833*4=3372\n","62\n","wrong  : 833*4=3372\n","correct: 833*4=3332\n","outputs(x):  793*9=7147\n","57\n","wrong  : 793*9=7147\n","correct: 793*9=7137\n","outputs(x):  348*7=2416\n","80\n","wrong  : 348*7=2416\n","correct: 348*7=2436\n","outputs(x):  100*7=70\n","130*\n","wrong  : 100*7=70\n","correct: 100*7=700\n","outputs(x):  429*8=3352\n","56\n","wrong  : 429*8=3352\n","correct: 429*8=3432\n","outputs(x):  483*8=3824\n","85\n","wrong  : 483*8=3824\n","correct: 483*8=3864\n","outputs(x):  237*2=274\n","30*\n","wrong  : 237*2=274\n","correct: 237*2=474\n","outputs(x):  921*4=3644\n","83\n","wrong  : 921*4=3644\n","correct: 921*4=3684\n","outputs(x):  962*8=7776\n","37\n","wrong  : 962*8=7776\n","correct: 962*8=7696\n","outputs(x):  633*6=3858\n","3*\n","wrong  : 633*6=3858\n","correct: 633*6=3798\n","outputs(x):  294*4=1136\n","63\n","wrong  : 294*4=1136\n","correct: 294*4=1176\n","  8% 2/24 [00:00<00:01, 17.11it/s]outputs(x):  569*9=5061\n","84\n","wrong  : 569*9=5061\n","correct: 569*9=5121\n","outputs(x):  863*8=686\n","780\n","wrong  : 863*8=686\n","correct: 863*8=6904\n","outputs(x):  593*3=1729\n","42\n","wrong  : 593*3=1729\n","correct: 593*3=1779\n","outputs(x):  876*8=7896\n","25\n","wrong  : 876*8=7896\n","correct: 876*8=7008\n","outputs(x):  372*7=2624\n","83\n","wrong  : 372*7=2624\n","correct: 372*7=2604\n","outputs(x):  872*3=2676\n","37\n","wrong  : 872*3=2676\n","correct: 872*3=2616\n","outputs(x):  773*2=1746\n","87\n","wrong  : 773*2=1746\n","correct: 773*2=1546\n","outputs(x):  117*6=1302\n","54\n","wrong  : 117*6=1302\n","correct: 117*6=702\n","outputs(x):  546*7=3722\n","64\n","wrong  : 546*7=3722\n","correct: 546*7=3822\n","outputs(x):  282*7=1994\n","51\n","wrong  : 282*7=1994\n","correct: 282*7=1974\n","outputs(x):  188*2=36\n","166*\n","wrong  : 188*2=36\n","correct: 188*2=376\n","outputs(x):  806*7=5632\n","40\n","wrong  : 806*7=5632\n","correct: 806*7=5642\n","outputs(x):  272*8=2976\n","68\n","wrong  : 272*8=2976\n","correct: 272*8=2176\n","outputs(x):  740*9=6860\n","30\n","wrong  : 740*9=6860\n","correct: 740*9=6660\n","outputs(x):  347*7=2422\n","66\n","wrong  : 347*7=2422\n","correct: 347*7=2429\n","outputs(x):  644*9=576\n","280\n","wrong  : 644*9=576\n","correct: 644*9=5796\n","outputs(x):  444*5=2270\n","66\n","wrong  : 444*5=2270\n","correct: 444*5=2220\n","outputs(x):  699*7=483\n","357\n","wrong  : 699*7=483\n","correct: 699*7=4893\n","outputs(x):  194*7=1388\n","98\n","wrong  : 194*7=1388\n","correct: 194*7=1358\n","outputs(x):  333*6=798\n","212\n","wrong  : 333*6=798\n","correct: 333*6=1998\n","outputs(x):  343*9=2987\n","26\n","wrong  : 343*9=2987\n","correct: 343*9=3087\n","outputs(x):  932*9=8488\n","73\n","wrong  : 932*9=8488\n","correct: 932*9=8388\n","outputs(x):  914*2=1888\n","34\n","wrong  : 914*2=1888\n","correct: 914*2=1828\n","outputs(x):  376*4=1464\n","71\n","wrong  : 376*4=1464\n","correct: 376*4=1504\n","outputs(x):  312*3=93\n","556*\n","wrong  : 312*3=93\n","correct: 312*3=936\n","outputs(x):  459*7=3133\n","22\n","wrong  : 459*7=3133\n","correct: 459*7=3213\n","outputs(x):  879*9=801\n","431\n","wrong  : 879*9=801\n","correct: 879*9=7911\n","outputs(x):  157*9=1313\n","69\n","wrong  : 157*9=1313\n","correct: 157*9=1413\n","outputs(x):  318*8=2504\n","77\n","wrong  : 318*8=2504\n","correct: 318*8=2544\n","outputs(x):  605*4=2460\n","98\n","wrong  : 605*4=2460\n","correct: 605*4=2420\n","outputs(x):  732*7=5154\n","69\n","wrong  : 732*7=5154\n","correct: 732*7=5124\n","outputs(x):  435*9=3195\n","71\n","wrong  : 435*9=3195\n","correct: 435*9=3915\n","outputs(x):  144*9=1316\n","59\n","wrong  : 144*9=1316\n","correct: 144*9=1296\n","outputs(x):  726*9=6454\n","69\n","wrong  : 726*9=6454\n","correct: 726*9=6534\n","outputs(x):  745*8=6760\n","14\n","wrong  : 745*8=6760\n","correct: 745*8=5960\n","outputs(x):  284*3=542\n","293\n","wrong  : 284*3=542\n","correct: 284*3=852\n","outputs(x):  639*3=1907\n","95\n","wrong  : 639*3=1907\n","correct: 639*3=1917\n","outputs(x):  506*8=408\n","908\n","wrong  : 506*8=408\n","correct: 506*8=4048\n","outputs(x):  642*9=5788\n","\n","4\n","wrong  : 642*9=5788\n","correct: 642*9=5778\n","outputs(x):  765*6=4050\n","29\n","wrong  : 765*6=4050\n","correct: 765*6=4590\n","outputs(x):  298*7=2786\n","61\n","wrong  : 298*7=2786\n","correct: 298*7=2086\n","outputs(x):  328*4=1272\n","96\n","wrong  : 328*4=1272\n","correct: 328*4=1312\n","outputs(x):  255*4=1000\n","38\n","wrong  : 255*4=1000\n","correct: 255*4=1020\n","outputs(x):  167*5=845\n","539\n","wrong  : 167*5=845\n","correct: 167*5=835\n","outputs(x):  666*7=462\n","302\n","wrong  : 666*7=462\n","correct: 666*7=4662\n","outputs(x):  503*9=4577\n","83\n","wrong  : 503*9=4577\n","correct: 503*9=4527\n","outputs(x):  822*7=574\n","71*\n","wrong  : 822*7=574\n","correct: 822*7=5754\n","outputs(x):  511*3=153\n","57*\n","wrong  : 511*3=153\n","correct: 511*3=1533\n","outputs(x):  147*3=422\n","3*7\n","wrong  : 147*3=422\n","correct: 147*3=441\n","outputs(x):  831*7=5117\n","63\n","wrong  : 831*7=5117\n","correct: 831*7=5817\n","outputs(x):  103*5=605\n","666\n","wrong  : 103*5=605\n","correct: 103*5=515\n","outputs(x):  409*7=2843\n","85\n","wrong  : 409*7=2843\n","correct: 409*7=2863\n","outputs(x):  441*6=2046\n","89\n","wrong  : 441*6=2046\n","correct: 441*6=2646\n","outputs(x):  411*7=2387\n","76\n","wrong  : 411*7=2387\n","correct: 411*7=2877\n","outputs(x):  901*9=8190\n","73\n","wrong  : 901*9=8190\n","correct: 901*9=8109\n","outputs(x):  582*7=4774\n","58\n","wrong  : 582*7=4774\n","correct: 582*7=4074\n","outputs(x):  923*4=372\n","5*0\n","wrong  : 923*4=372\n","correct: 923*4=3692\n","outputs(x):  965*6=570\n","167\n","wrong  : 965*6=570\n","correct: 965*6=5790\n","outputs(x):  929*8=736\n","194\n","wrong  : 929*8=736\n","correct: 929*8=7432\n","outputs(x):  158*3=465\n","849\n","wrong  : 158*3=465\n","correct: 158*3=474\n","outputs(x):  548*3=1614\n","90\n","wrong  : 548*3=1614\n","correct: 548*3=1644\n","outputs(x):  196*3=598\n","33*\n","wrong  : 196*3=598\n","correct: 196*3=588\n","outputs(x):  408*7=2842\n","35\n","wrong  : 408*7=2842\n","correct: 408*7=2856\n","outputs(x):  174*9=1466\n","40\n","wrong  : 174*9=1466\n","correct: 174*9=1566\n","outputs(x):  925*8=7360\n","65\n","wrong  : 925*8=7360\n","correct: 925*8=7400\n","outputs(x):  782*7=5374\n","71\n","wrong  : 782*7=5374\n","correct: 782*7=5474\n","outputs(x):  875*9=7065\n","84\n","wrong  : 875*9=7065\n","correct: 875*9=7875\n","outputs(x):  452*6=2772\n","37\n","wrong  : 452*6=2772\n","correct: 452*6=2712\n","outputs(x):  170*9=1430\n","55\n","wrong  : 170*9=1430\n","correct: 170*9=1530\n","outputs(x):  669*8=552\n","945\n","wrong  : 669*8=552\n","correct: 669*8=5352\n","outputs(x):  326*9=2034\n","80\n","wrong  : 326*9=2034\n","correct: 326*9=2934\n","outputs(x):  889*8=7912\n","41\n","wrong  : 889*8=7912\n","correct: 889*8=7112\n","outputs(x):  925*6=550\n","232\n","wrong  : 925*6=550\n","correct: 925*6=5550\n","outputs(x):  777*5=385\n","531\n","wrong  : 777*5=385\n","correct: 777*5=3885\n","outputs(x):  359*6=2114\n","22\n","wrong  : 359*6=2114\n","correct: 359*6=2154\n","outputs(x):  820*7=5760\n","8*\n","wrong  : 820*7=5760\n","correct: 820*7=5740\n","outputs(x):  531*8=4488\n","63\n","wrong  : 531*8=4488\n","correct: 531*8=4248\n","outputs(x):  224*2=488\n","550\n","wrong  : 224*2=488\n","correct: 224*2=448\n","outputs(x):  787*9=783\n","384\n","wrong  : 787*9=783\n","correct: 787*9=7083\n","outputs(x):  223*7=161\n","241\n","wrong  : 223*7=161\n","correct: 223*7=1561\n","outputs(x):  796*6=4782\n","77\n","wrong  : 796*6=4782\n","correct: 796*6=4776\n","outputs(x):  282*8=2296\n","33\n","wrong  : 282*8=2296\n","correct: 282*8=2256\n","outputs(x):  521*2=1022\n","28\n","wrong  : 521*2=1022\n","correct: 521*2=1042\n","outputs(x):  349*3=1097\n","10\n","wrong  : 349*3=1097\n","correct: 349*3=1047\n","outputs(x):  326*4=1344\n","77\n","wrong  : 326*4=1344\n","correct: 326*4=1304\n","outputs(x):  475*6=2270\n","66\n","wrong  : 475*6=2270\n","correct: 475*6=2850\n","outputs(x):  669*7=483\n","357\n","wrong  : 669*7=483\n","correct: 669*7=4683\n","outputs(x):  117*2=34\n","567*\n","wrong  : 117*2=34\n","correct: 117*2=234\n","outputs(x):  593*9=537\n","479\n","wrong  : 593*9=537\n","correct: 593*9=5337\n","outputs(x):  907*6=5422\n","90\n","wrong  : 907*6=5422\n","correct: 907*6=5442\n","outputs(x):  225*7=1475\n","55\n","wrong  : 225*7=1475\n","correct: 225*7=1575\n","outputs(x):  156*7=1162\n","53\n","wrong  : 156*7=1162\n","correct: 156*7=1092\n","outputs(x):  928*3=294\n","88*\n","wrong  : 928*3=294\n","correct: 928*3=2784\n","outputs(x):  129*4=2116\n","78\n","wrong  : 129*4=2116\n","correct: 129*4=516\n","outputs(x):  116*4=644\n","300\n","wrong  : 116*4=644\n","correct: 116*4=464\n","outputs(x):  771*8=6288\n","13\n","wrong  : 771*8=6288\n","correct: 771*8=6168\n","outputs(x):  735*6=4470\n","72\n","wrong  : 735*6=4470\n","correct: 735*6=4410\n","outputs(x):  444*8=3592\n","96\n","wrong  : 444*8=3592\n","correct: 444*8=3552\n","outputs(x):  966*6=576\n","69*\n","wrong  : 966*6=576\n","correct: 966*6=5796\n","outputs(x):  609*8=4072\n","48\n","wrong  : 609*8=4072\n","correct: 609*8=4872\n","outputs(x):  224*8=1352\n","89\n","wrong  : 224*8=1352\n","correct: 224*8=1792\n","outputs(x):  355*5=1725\n","49\n","wrong  : 355*5=1725\n","correct: 355*5=1775\n","outputs(x):  215*7=1435\n","55\n","wrong  : 215*7=1435\n","correct: 215*7=1505\n","outputs(x):  132*5=65\n","337*\n","wrong  : 132*5=65\n","correct: 132*5=660\n","outputs(x):  248*7=1988\n","94\n","wrong  : 248*7=1988\n","correct: 248*7=1736\n","outputs(x):  795*3=2375\n","72\n","wrong  : 795*3=2375\n","correct: 795*3=2385\n","outputs(x):  310*3=90\n","345*\n","wrong  : 310*3=90\n","correct: 310*3=930\n","outputs(x):  248*3=72\n","569*\n","wrong  : 248*3=72\n","correct: 248*3=744\n","outputs(x):  863*6=5138\n","70\n","wrong  : 863*6=5138\n","correct: 863*6=5178\n","outputs(x):  318*3=1254\n","96\n","wrong  : 318*3=1254\n","correct: 318*3=954\n","outputs(x):  783*6=4638\n","43\n","wrong  : 783*6=4638\n","correct: 783*6=4698\n","outputs(x):  607*8=5656\n","17\n","wrong  : 607*8=5656\n","correct: 607*8=4856\n"," 17% 4/24 [00:00<00:01, 16.17it/s]outputs(x):  715*8=5760\n","72\n","wrong  : 715*8=5760\n","correct: 715*8=5720\n","outputs(x):  802*3=2410\n","53\n","wrong  : 802*3=2410\n","correct: 802*3=2406\n","outputs(x):  679*9=6093\n","10\n","wrong  : 679*9=6093\n","correct: 679*9=6111\n","outputs(x):  785*3=2325\n","74\n","wrong  : 785*3=2325\n","correct: 785*3=2355\n","outputs(x):  755*9=6705\n","48\n","wrong  : 755*9=6705\n","correct: 755*9=6795\n","outputs(x):  846*9=7576\n","44\n","wrong  : 846*9=7576\n","correct: 846*9=7614\n","outputs(x):  182*7=1284\n","29\n","wrong  : 182*7=1284\n","correct: 182*7=1274\n","outputs(x):  517*4=2868\n","41\n","wrong  : 517*4=2868\n","correct: 517*4=2068\n","outputs(x):  297*9=2693\n","48\n","wrong  : 297*9=2693\n","correct: 297*9=2673\n","outputs(x):  883*9=7987\n","73\n","wrong  : 883*9=7987\n","correct: 883*9=7947\n","outputs(x):  979*4=388\n","915\n","wrong  : 979*4=388\n","correct: 979*4=3916\n","outputs(x):  584*5=290\n","804\n","wrong  : 584*5=290\n","correct: 584*5=2920\n","outputs(x):  787*6=4122\n","21\n","wrong  : 787*6=4122\n","correct: 787*6=4722\n","outputs(x):  228*8=1644\n","78\n","wrong  : 228*8=1644\n","correct: 228*8=1824\n","outputs(x):  449*7=3843\n","85\n","wrong  : 449*7=3843\n","correct: 449*7=3143\n","outputs(x):  635*6=3170\n","39\n","wrong  : 635*6=3170\n","correct: 635*6=3810\n","outputs(x):  189*6=1194\n","76\n","wrong  : 189*6=1194\n","correct: 189*6=1134\n","outputs(x):  143*5=725\n","145\n","wrong  : 143*5=725\n","correct: 143*5=715\n","outputs(x):  961*7=6867\n","17\n","wrong  : 961*7=6867\n","correct: 961*7=6727\n","outputs(x):  889*9=801\n","431\n","wrong  : 889*9=801\n","correct: 889*9=8001\n","outputs(x):  193*6=114\n","19*\n","wrong  : 193*6=114\n","correct: 193*6=1158\n","outputs(x):  768*7=5346\n","50\n","wrong  : 768*7=5346\n","correct: 768*7=5376\n","outputs(x):  775*8=600\n","263\n","wrong  : 775*8=600\n","correct: 775*8=6200\n","outputs(x):  618*4=2872\n","59\n","wrong  : 618*4=2872\n","correct: 618*4=2472\n","outputs(x):  954*9=8506\n","71\n","wrong  : 954*9=8506\n","correct: 954*9=8586\n","outputs(x):  272*4=1108\n","32\n","wrong  : 272*4=1108\n","correct: 272*4=1088\n","outputs(x):  413*9=3787\n","5*\n","wrong  : 413*9=3787\n","correct: 413*9=3717\n","outputs(x):  978*4=3112\n","79\n","wrong  : 978*4=3112\n","correct: 978*4=3912\n","outputs(x):  862*9=7728\n","29\n","wrong  : 862*9=7728\n","correct: 862*9=7758\n","outputs(x):  786*3=2308\n","60\n","wrong  : 786*3=2308\n","correct: 786*3=2358\n","outputs(x):  154*4=620\n","481\n","wrong  : 154*4=620\n","correct: 154*4=616\n","outputs(x):  574*9=516\n","288\n","wrong  : 574*9=516\n","correct: 574*9=5166\n","outputs(x):  476*2=932\n","237\n","wrong  : 476*2=932\n","correct: 476*2=952\n","outputs(x):  495*9=4065\n","0*\n","wrong  : 495*9=4065\n","correct: 495*9=4455\n","outputs(x):  921*2=1882\n","50\n","wrong  : 921*2=1882\n","correct: 921*2=1842\n","outputs(x):  153*7=945\n","71*\n","wrong  : 153*7=945\n","correct: 153*7=1071\n","outputs(x):  647*6=3862\n","61\n","wrong  : 647*6=3862\n","correct: 647*6=3882\n","outputs(x):  100*9=90\n","458*\n","wrong  : 100*9=90\n","correct: 100*9=900\n","outputs(x):  246*9=2314\n","10\n","wrong  : 246*9=2314\n","correct: 246*9=2214\n","outputs(x):  686*4=2704\n","61\n","wrong  : 686*4=2704\n","correct: 686*4=2744\n","outputs(x):  819*8=652\n","7*8\n","wrong  : 819*8=652\n","correct: 819*8=6552\n","outputs(x):  119*6=709\n","943\n","wrong  : 119*6=709\n","correct: 119*6=714\n","outputs(x):  513*7=3577\n","4*\n","wrong  : 513*7=3577\n","correct: 513*7=3591\n","outputs(x):  453*3=1329\n","44\n","wrong  : 453*3=1329\n","correct: 453*3=1359\n","outputs(x):  433*8=344\n","990\n","wrong  : 433*8=344\n","correct: 433*8=3464\n","outputs(x):  716*8=5368\n","48\n","wrong  : 716*8=5368\n","correct: 716*8=5728\n","outputs(x):  348*9=3852\n","77\n","wrong  : 348*9=3852\n","correct: 348*9=3132\n","outputs(x):  835*6=4950\n","14\n","wrong  : 835*6=4950\n","correct: 835*6=5010\n","outputs(x):  785*8=680\n","230\n","wrong  : 785*8=680\n","correct: 785*8=6280\n","outputs(x):  174*6=1644\n","78\n","wrong  : 174*6=1644\n","correct: 174*6=1044\n","outputs(x):  542*9=4818\n","15\n","wrong  : 542*9=4818\n","correct: 542*9=4878\n","outputs(x):  376*9=3414\n","21\n","wrong  : 376*9=3414\n","correct: 376*9=3384\n","outputs(x):  659*9=5861\n","38\n","wrong  : 659*9=5861\n","correct: 659*9=5931\n","outputs(x):  465*8=3240\n","64\n","wrong  : 465*8=3240\n","correct: 465*8=3720\n","outputs(x):  387*8=3146\n","50\n","wrong  : 387*8=3146\n","correct: 387*8=3096\n","outputs(x):  261*8=2888\n","47\n","wrong  : 261*8=2888\n","correct: 261*8=2088\n","outputs(x):  757*7=4039\n","47\n","wrong  : 757*7=4039\n","correct: 757*7=5299\n","outputs(x):  229*8=1112\n","63\n","wrong  : 229*8=1112\n","correct: 229*8=1832\n","outputs(x):  448*8=3504\n","97\n","wrong  : 448*8=3504\n","correct: 448*8=3584\n","outputs(x):  158*6=908\n","961\n","wrong  : 158*6=908\n","correct: 158*6=948\n","outputs(x):  775*2=1750\n","21\n","wrong  : 775*2=1750\n","correct: 775*2=1550\n","outputs(x):  566*4=2224\n","54\n","wrong  : 566*4=2224\n","correct: 566*4=2264\n","outputs(x):  660*4=2680\n","10\n","wrong  : 660*4=2680\n","correct: 660*4=2640\n","outputs(x):  735*4=2900\n","28\n","wrong  : 735*4=2900\n","correct: 735*4=2940\n","outputs(x):  508*7=3566\n","57\n","wrong  : 508*7=3566\n","correct: 508*7=3556\n","outputs(x):  344*7=2478\n","67\n","wrong  : 344*7=2478\n","correct: 344*7=2408\n","outputs(x):  426*8=3368\n","17\n","wrong  : 426*8=3368\n","correct: 426*8=3408\n","outputs(x):  462*8=3776\n","44\n","wrong  : 462*8=3776\n","correct: 462*8=3696\n","outputs(x):  191*3=57\n","791*\n","wrong  : 191*3=57\n","correct: 191*3=573\n","outputs(x):  707*9=6343\n","86\n","wrong  : 707*9=6343\n","correct: 707*9=6363\n","outputs(x):  795*9=715\n","884\n","wrong  : 795*9=715\n","correct: 795*9=7155\n","outputs(x):  796*9=7194\n","76\n","wrong  : 796*9=7194\n","correct: 796*9=7164\n","outputs(x):  319*8=2752\n","16\n","wrong  : 319*8=2752\n","correct: 319*8=2552\n","outputs(x):  975*6=5860\n","72\n","wrong  : 975*6=5860\n","correct: 975*6=5850\n","outputs(x):  839*6=534\n","242\n","wrong  : 839*6=534\n","correct: 839*6=5034\n","outputs(x):  615*9=5515\n","63\n","wrong  : 615*9=5515\n","correct: 615*9=5535\n","outputs(x):  835*3=2595\n","98\n","wrong  : 835*3=2595\n","correct: 835*3=2505\n","outputs(x):  859*9=771\n","977\n","wrong  : 859*9=771\n","correct: 859*9=7731\n","outputs(x):  158*9=1322\n","83\n","wrong  : 158*9=1322\n","correct: 158*9=1422\n","outputs(x):  284*9=2592\n","86\n","wrong  : 284*9=2592\n","correct: 284*9=2556\n","outputs(x):  633*3=1999\n","13\n","wrong  : 633*3=1999\n","correct: 633*3=1899\n","outputs(x):  367*7=2539\n","72\n","wrong  : 367*7=2539\n","correct: 367*7=2569\n","outputs(x):  255*6=150\n","232\n","wrong  : 255*6=150\n","correct: 255*6=1530\n","outputs(x):  721*7=5057\n","35\n","wrong  : 721*7=5057\n","correct: 721*7=5047\n","outputs(x):  621*2=1222\n","11\n","wrong  : 621*2=1222\n","correct: 621*2=1242\n","outputs(x):  404*9=3616\n","99\n","wrong  : 404*9=3616\n","correct: 404*9=3636\n","outputs(x):  971*3=293\n","50*\n","wrong  : 971*3=293\n","correct: 971*3=2913\n","outputs(x):  525*8=4120\n","52\n","wrong  : 525*8=4120\n","correct: 525*8=4200\n","outputs(x):  366*4=1344\n","77\n","wrong  : 366*4=1344\n","correct: 366*4=1464\n","outputs(x):  472*9=4288\n","95\n","wrong  : 472*9=4288\n","correct: 472*9=4248\n","outputs(x):  213*8=1304\n","79\n","wrong  : 213*8=1304\n","correct: 213*8=1704\n","outputs(x):  926*7=6472\n","55\n","wrong  : 926*7=6472\n","correct: 926*7=6482\n","outputs(x):  900*8=720\n","248\n","wrong  : 900*8=720\n","correct: 900*8=7200\n","outputs(x):  517*8=456\n","969\n","wrong  : 517*8=456\n","correct: 517*8=4136\n","outputs(x):  614*8=4928\n","21\n","wrong  : 614*8=4928\n","correct: 614*8=4912\n","outputs(x):  477*8=3896\n","53\n","wrong  : 477*8=3896\n","correct: 477*8=3816\n","outputs(x):  236*3=678\n","617\n","wrong  : 236*3=678\n","correct: 236*3=708\n","outputs(x):  682*6=4752\n","52\n","wrong  : 682*6=4752\n","correct: 682*6=4092\n","outputs(x):  827*9=7483\n","38\n","wrong  : 827*9=7483\n","correct: 827*9=7443\n","outputs(x):  114*6=624\n","786\n","wrong  : 114*6=624\n","correct: 114*6=684\n","outputs(x):  239*9=2011\n","54\n","wrong  : 239*9=2011\n","correct: 239*9=2151\n","outputs(x):  344*6=2044\n","31\n","wrong  : 344*6=2044\n","correct: 344*6=2064\n","outputs(x):  926*8=7368\n","20\n","wrong  : 926*8=7368\n","correct: 926*8=7408\n","outputs(x):  963*2=1726\n","64\n","wrong  : 963*2=1726\n","correct: 963*2=1926\n","outputs(x):  375*3=1105\n","21\n","wrong  : 375*3=1105\n","correct: 375*3=1125\n","outputs(x):  429*9=3841\n","85\n","wrong  : 429*9=3841\n","correct: 429*9=3861\n","outputs(x):  676*9=6004\n","97\n","wrong  : 676*9=6004\n","correct: 676*9=6084\n","outputs(x):  202*4=888\n","922\n","wrong  : 202*4=888\n","correct: 202*4=808\n","outputs(x):  305*8=2400\n","77\n","wrong  : 305*8=2400\n","correct: 305*8=2440\n","outputs(x):  977*2=1944\n","85\n","wrong  : 977*2=1944\n","correct: 977*2=1954\n"," 25% 6/24 [00:00<00:01, 16.29it/s]outputs(x):  422*9=3978\n","84\n","wrong  : 422*9=3978\n","correct: 422*9=3798\n","outputs(x):  417*7=2939\n","95\n","wrong  : 417*7=2939\n","correct: 417*7=2919\n","outputs(x):  499*2=1698\n","44\n","wrong  : 499*2=1698\n","correct: 499*2=998\n","outputs(x):  439*9=3977\n","70\n","wrong  : 439*9=3977\n","correct: 439*9=3951\n","outputs(x):  493*3=1497\n","30\n","wrong  : 493*3=1497\n","correct: 493*3=1479\n","outputs(x):  961*4=3044\n","32\n","wrong  : 961*4=3044\n","correct: 961*4=3844\n","outputs(x):  187*4=708\n","328\n","wrong  : 187*4=708\n","correct: 187*4=748\n","outputs(x):  376*7=2629\n","93\n","wrong  : 376*7=2629\n","correct: 376*7=2632\n","outputs(x):  881*6=5346\n","68\n","wrong  : 881*6=5346\n","correct: 881*6=5286\n","outputs(x):  830*7=5620\n","43\n","wrong  : 830*7=5620\n","correct: 830*7=5810\n","outputs(x):  326*3=98\n","398*\n","wrong  : 326*3=98\n","correct: 326*3=978\n","outputs(x):  412*4=168\n","221\n","wrong  : 412*4=168\n","correct: 412*4=1648\n","outputs(x):  710*1=70\n","814*\n","wrong  : 710*1=70\n","correct: 710*1=710\n","outputs(x):  313*7=2171\n","4*\n","wrong  : 313*7=2171\n","correct: 313*7=2191\n","outputs(x):  773*3=2019\n","14\n","wrong  : 773*3=2019\n","correct: 773*3=2319\n","outputs(x):  330*9=2770\n","55\n","wrong  : 330*9=2770\n","correct: 330*9=2970\n","outputs(x):  538*6=3268\n","64\n","wrong  : 538*6=3268\n","correct: 538*6=3228\n","outputs(x):  714*8=5792\n","30\n","wrong  : 714*8=5792\n","correct: 714*8=5712\n","outputs(x):  122*6=6732\n","18\n","wrong  : 122*6=6732\n","correct: 122*6=732\n","outputs(x):  579*8=4712\n","55\n","wrong  : 579*8=4712\n","correct: 579*8=4632\n","outputs(x):  878*3=2664\n","81\n","wrong  : 878*3=2664\n","correct: 878*3=2634\n","outputs(x):  833*6=5298\n","1*\n","wrong  : 833*6=5298\n","correct: 833*6=4998\n","outputs(x):  483*3=1499\n","30\n","wrong  : 483*3=1499\n","correct: 483*3=1449\n","outputs(x):  867*7=6039\n","16\n","wrong  : 867*7=6039\n","correct: 867*7=6069\n","outputs(x):  536*9=5724\n","93\n","wrong  : 536*9=5724\n","correct: 536*9=4824\n","outputs(x):  933*9=8377\n","61\n","wrong  : 933*9=8377\n","correct: 933*9=8397\n","outputs(x):  869*1=89\n","484*\n","wrong  : 869*1=89\n","correct: 869*1=869\n","outputs(x):  881*5=4905\n","42\n","wrong  : 881*5=4905\n","correct: 881*5=4405\n","outputs(x):  224*6=144\n","97*\n","wrong  : 224*6=144\n","correct: 224*6=1344\n","outputs(x):  683*6=4038\n","81\n","wrong  : 683*6=4038\n","correct: 683*6=4098\n","outputs(x):  205*5=1075\n","22\n","wrong  : 205*5=1075\n","correct: 205*5=1025\n","outputs(x):  875*4=340\n","434\n","wrong  : 875*4=340\n","correct: 875*4=3500\n","outputs(x):  579*7=403\n","560\n","wrong  : 579*7=403\n","correct: 579*7=4053\n","outputs(x):  934*7=6608\n","60\n","wrong  : 934*7=6608\n","correct: 934*7=6538\n","outputs(x):  383*9=3497\n","12\n","wrong  : 383*9=3497\n","correct: 383*9=3447\n","outputs(x):  107*9=1033\n","89\n","wrong  : 107*9=1033\n","correct: 107*9=963\n","outputs(x):  792*7=5524\n","38\n","wrong  : 792*7=5524\n","correct: 792*7=5544\n","outputs(x):  654*4=2696\n","38\n","wrong  : 654*4=2696\n","correct: 654*4=2616\n","outputs(x):  663*8=5384\n","54\n","wrong  : 663*8=5384\n","correct: 663*8=5304\n","outputs(x):  848*9=7592\n","81\n","wrong  : 848*9=7592\n","correct: 848*9=7632\n","outputs(x):  866*8=688\n","610\n","wrong  : 866*8=688\n","correct: 866*8=6928\n","outputs(x):  182*5=960\n","948\n","wrong  : 182*5=960\n","correct: 182*5=910\n","outputs(x):  805*8=6400\n","77\n","wrong  : 805*8=6400\n","correct: 805*8=6440\n","outputs(x):  886*9=7994\n","68\n","wrong  : 886*9=7994\n","correct: 886*9=7974\n","outputs(x):  313*9=2897\n","83\n","wrong  : 313*9=2897\n","correct: 313*9=2817\n","outputs(x):  220*8=160\n","194\n","wrong  : 220*8=160\n","correct: 220*8=1760\n","outputs(x):  485*3=145\n","245\n","wrong  : 485*3=145\n","correct: 485*3=1455\n","outputs(x):  566*7=3992\n","20\n","wrong  : 566*7=3992\n","correct: 566*7=3962\n","outputs(x):  746*3=2298\n","65\n","wrong  : 746*3=2298\n","correct: 746*3=2238\n","outputs(x):  128*4=492\n","909\n","wrong  : 128*4=492\n","correct: 128*4=512\n","outputs(x):  979*6=5834\n","27\n","wrong  : 979*6=5834\n","correct: 979*6=5874\n","outputs(x):  674*9=6046\n","48\n","wrong  : 674*9=6046\n","correct: 674*9=6066\n","outputs(x):  115*2=30\n","75*8\n","wrong  : 115*2=30\n","correct: 115*2=230\n","outputs(x):  110*6=60\n","972*\n","wrong  : 110*6=60\n","correct: 110*6=660\n","outputs(x):  160*3=318\n","412\n","wrong  : 160*3=318\n","correct: 160*3=480\n","outputs(x):  631*3=1913\n","85\n","wrong  : 631*3=1913\n","correct: 631*3=1893\n","outputs(x):  634*9=5786\n","63\n","wrong  : 634*9=5786\n","correct: 634*9=5706\n","outputs(x):  675*9=6055\n","48\n","wrong  : 675*9=6055\n","correct: 675*9=6075\n","outputs(x):  763*6=4038\n","81\n","wrong  : 763*6=4038\n","correct: 763*6=4578\n","outputs(x):  740*7=520\n","464\n","wrong  : 740*7=520\n","correct: 740*7=5180\n","outputs(x):  103*7=714\n","182\n","wrong  : 103*7=714\n","correct: 103*7=721\n","outputs(x):  680*7=4060\n","94\n","wrong  : 680*7=4060\n","correct: 680*7=4760\n","outputs(x):  107*4=628\n","429\n","wrong  : 107*4=628\n","correct: 107*4=428\n","outputs(x):  833*3=2589\n","74\n","wrong  : 833*3=2589\n","correct: 833*3=2499\n","outputs(x):  363*8=2584\n","84\n","wrong  : 363*8=2584\n","correct: 363*8=2904\n","outputs(x):  935*9=8475\n","65\n","wrong  : 935*9=8475\n","correct: 935*9=8415\n","outputs(x):  624*6=3736\n","58\n","wrong  : 624*6=3736\n","correct: 624*6=3744\n","outputs(x):  546*3=1668\n","83\n","wrong  : 546*3=1668\n","correct: 546*3=1638\n","outputs(x):  706*7=4242\n","28\n","wrong  : 706*7=4242\n","correct: 706*7=4942\n","outputs(x):  855*3=2595\n","98\n","wrong  : 855*3=2595\n","correct: 855*3=2565\n","outputs(x):  628*3=1804\n","56\n","wrong  : 628*3=1804\n","correct: 628*3=1884\n","outputs(x):  499*6=2934\n","91\n","wrong  : 499*6=2934\n","correct: 499*6=2994\n","outputs(x):  717*6=4262\n","70\n","wrong  : 717*6=4262\n","correct: 717*6=4302\n","outputs(x):  423*3=1299\n","13\n","wrong  : 423*3=1299\n","correct: 423*3=1269\n","outputs(x):  204*8=1672\n","61\n","wrong  : 204*8=1672\n","correct: 204*8=1632\n","outputs(x):  708*3=2144\n","23\n","wrong  : 708*3=2144\n","correct: 708*3=2124\n","outputs(x):  848*7=6188\n","14\n","wrong  : 848*7=6188\n","correct: 848*7=5936\n","outputs(x):  856*7=6055\n","41\n","wrong  : 856*7=6055\n","correct: 856*7=5992\n","outputs(x):  923*3=2799\n","80\n","wrong  : 923*3=2799\n","correct: 923*3=2769\n","outputs(x):  113*5=555\n","66*\n","wrong  : 113*5=555\n","correct: 113*5=565\n","outputs(x):  153*5=75\n","187*\n","wrong  : 153*5=75\n","correct: 153*5=765\n","outputs(x):  314*3=1242\n","78\n","wrong  : 314*3=1242\n","correct: 314*3=942\n","outputs(x):  892*7=6274\n","40\n","wrong  : 892*7=6274\n","correct: 892*7=6244\n","outputs(x):  414*8=3152\n","48\n","wrong  : 414*8=3152\n","correct: 414*8=3312\n","outputs(x):  943*8=7504\n","56\n","wrong  : 943*8=7504\n","correct: 943*8=7544\n","outputs(x):  425*6=2570\n","55\n","wrong  : 425*6=2570\n","correct: 425*6=2550\n","outputs(x):  993*3=2679\n","62\n","wrong  : 993*3=2679\n","correct: 993*3=2979\n","outputs(x):  806*9=7794\n","68\n","wrong  : 806*9=7794\n","correct: 806*9=7254\n","outputs(x):  166*9=144\n","767\n","wrong  : 166*9=144\n","correct: 166*9=1494\n","outputs(x):  183*6=1698\n","62\n","wrong  : 183*6=1698\n","correct: 183*6=1098\n","outputs(x):  167*7=1199\n","82\n","wrong  : 167*7=1199\n","correct: 167*7=1169\n","outputs(x):  575*9=5135\n","26\n","wrong  : 575*9=5135\n","correct: 575*9=5175\n","outputs(x):  259*6=1594\n","11\n","wrong  : 259*6=1594\n","correct: 259*6=1554\n","outputs(x):  102*3=316\n","519\n","wrong  : 102*3=316\n","correct: 102*3=306\n","outputs(x):  651*3=1353\n","39\n","wrong  : 651*3=1353\n","correct: 651*3=1953\n","outputs(x):  218*6=1328\n","48\n","wrong  : 218*6=1328\n","correct: 218*6=1308\n","outputs(x):  415*2=825\n","441\n","wrong  : 415*2=825\n","correct: 415*2=830\n","outputs(x):  420*2=80\n","234*\n","wrong  : 420*2=80\n","correct: 420*2=840\n","outputs(x):  172*3=526\n","234\n","wrong  : 172*3=526\n","correct: 172*3=516\n","outputs(x):  513*4=2292\n","96\n","wrong  : 513*4=2292\n","correct: 513*4=2052\n","outputs(x):  768*8=6944\n","48\n","wrong  : 768*8=6944\n","correct: 768*8=6144\n"," 33% 8/24 [00:00<00:00, 16.02it/s]outputs(x):  356*8=2048\n","82\n","wrong  : 356*8=2048\n","correct: 356*8=2848\n","outputs(x):  335*9=2015\n","12\n","wrong  : 335*9=2015\n","correct: 335*9=3015\n","outputs(x):  256*3=78\n","619*\n","wrong  : 256*3=78\n","correct: 256*3=768\n","outputs(x):  797*9=713\n","928\n","wrong  : 797*9=713\n","correct: 797*9=7173\n","outputs(x):  623*7=4421\n","38\n","wrong  : 623*7=4421\n","correct: 623*7=4361\n","outputs(x):  349*9=3941\n","60\n","wrong  : 349*9=3941\n","correct: 349*9=3141\n","outputs(x):  595*9=5365\n","72\n","wrong  : 595*9=5365\n","correct: 595*9=5355\n","outputs(x):  667*8=5344\n","46\n","wrong  : 667*8=5344\n","correct: 667*8=5336\n","outputs(x):  147*9=1313\n","67\n","wrong  : 147*9=1313\n","correct: 147*9=1323\n","outputs(x):  320*4=1200\n","71\n","wrong  : 320*4=1200\n","correct: 320*4=1280\n","outputs(x):  927*4=3908\n","41\n","wrong  : 927*4=3908\n","correct: 927*4=3708\n","outputs(x):  463*9=4177\n","83\n","wrong  : 463*9=4177\n","correct: 463*9=4167\n","outputs(x):  999*5=4495\n","36\n","wrong  : 999*5=4495\n","correct: 999*5=4995\n","outputs(x):  983*8=784\n","324\n","wrong  : 983*8=784\n","correct: 983*8=7864\n","outputs(x):  138*9=1222\n","14\n","wrong  : 138*9=1222\n","correct: 138*9=1242\n","outputs(x):  460*8=3280\n","99\n","wrong  : 460*8=3280\n","correct: 460*8=3680\n","outputs(x):  683*4=2772\n","70\n","wrong  : 683*4=2772\n","correct: 683*4=2732\n","outputs(x):  761*9=6869\n","30\n","wrong  : 761*9=6869\n","correct: 761*9=6849\n","outputs(x):  988*8=7184\n","48\n","wrong  : 988*8=7184\n","correct: 988*8=7904\n","outputs(x):  111*4=644\n","300\n","wrong  : 111*4=644\n","correct: 111*4=444\n","outputs(x):  755*5=375\n","966\n","wrong  : 755*5=375\n","correct: 755*5=3775\n","outputs(x):  317*6=1842\n","90\n","wrong  : 317*6=1842\n","correct: 317*6=1902\n","outputs(x):  519*9=4591\n","85\n","wrong  : 519*9=4591\n","correct: 519*9=4671\n","outputs(x):  754*9=6866\n","36\n","wrong  : 754*9=6866\n","correct: 754*9=6786\n","outputs(x):  263*8=2744\n","93\n","wrong  : 263*8=2744\n","correct: 263*8=2104\n","outputs(x):  259*3=887\n","756\n","wrong  : 259*3=887\n","correct: 259*3=777\n","outputs(x):  483*6=2958\n","79\n","wrong  : 483*6=2958\n","correct: 483*6=2898\n","outputs(x):  665*9=5995\n","33\n","wrong  : 665*9=5995\n","correct: 665*9=5985\n","outputs(x):  366*3=1998\n","20\n","wrong  : 366*3=1998\n","correct: 366*3=1098\n","outputs(x):  907*9=8193\n","73\n","wrong  : 907*9=8193\n","correct: 907*9=8163\n","outputs(x):  415*7=2925\n","74\n","wrong  : 415*7=2925\n","correct: 415*7=2905\n","outputs(x):  912*4=3688\n","89\n","wrong  : 912*4=3688\n","correct: 912*4=3648\n","outputs(x):  507*8=406\n","908\n","wrong  : 507*8=406\n","correct: 507*8=4056\n","outputs(x):  254*9=2116\n","78\n","wrong  : 254*9=2116\n","correct: 254*9=2286\n","outputs(x):  302*5=1500\n","35\n","wrong  : 302*5=1500\n","correct: 302*5=1510\n","outputs(x):  984*2=1948\n","64\n","wrong  : 984*2=1948\n","correct: 984*2=1968\n","outputs(x):  251*9=2359\n","72\n","wrong  : 251*9=2359\n","correct: 251*9=2259\n","outputs(x):  110*9=90\n","458*\n","wrong  : 110*9=90\n","correct: 110*9=990\n","outputs(x):  689*3=2307\n","94\n","wrong  : 689*3=2307\n","correct: 689*3=2067\n","outputs(x):  622*9=5688\n","90\n","wrong  : 622*9=5688\n","correct: 622*9=5598\n","outputs(x):  755*2=150\n","676\n","wrong  : 755*2=150\n","correct: 755*2=1510\n","outputs(x):  617*7=4279\n","87\n","wrong  : 617*7=4279\n","correct: 617*7=4319\n","outputs(x):  264*8=2072\n","33\n","wrong  : 264*8=2072\n","correct: 264*8=2112\n","outputs(x):  356*7=2555\n","39\n","wrong  : 356*7=2555\n","correct: 356*7=2492\n","outputs(x):  819*4=3556\n","48\n","wrong  : 819*4=3556\n","correct: 819*4=3276\n","outputs(x):  854*7=6888\n","18\n","wrong  : 854*7=6888\n","correct: 854*7=5978\n","outputs(x):  296*3=898\n","947\n","wrong  : 296*3=898\n","correct: 296*3=888\n","outputs(x):  341*9=2169\n","76\n","wrong  : 341*9=2169\n","correct: 341*9=3069\n","outputs(x):  117*5=535\n","0*3\n","wrong  : 117*5=535\n","correct: 117*5=585\n","outputs(x):  356*6=2112\n","65\n","wrong  : 356*6=2112\n","correct: 356*6=2136\n","outputs(x):  564*7=3918\n","66\n","wrong  : 564*7=3918\n","correct: 564*7=3948\n","outputs(x):  918*9=8242\n","22\n","wrong  : 918*9=8242\n","correct: 918*9=8262\n","outputs(x):  125*4=410\n","53*\n","wrong  : 125*4=410\n","correct: 125*4=500\n","outputs(x):  544*3=1662\n","86\n","wrong  : 544*3=1662\n","correct: 544*3=1632\n","outputs(x):  182*9=1628\n","22\n","wrong  : 182*9=1628\n","correct: 182*9=1638\n","outputs(x):  867*8=6136\n","58\n","wrong  : 867*8=6136\n","correct: 867*8=6936\n","outputs(x):  336*3=1908\n","98\n","wrong  : 336*3=1908\n","correct: 336*3=1008\n","outputs(x):  137*7=931\n","932\n","wrong  : 137*7=931\n","correct: 137*7=959\n","outputs(x):  935*6=5670\n","49\n","wrong  : 935*6=5670\n","correct: 935*6=5610\n","outputs(x):  478*9=4282\n","35\n","wrong  : 478*9=4282\n","correct: 478*9=4302\n","outputs(x):  963*3=2989\n","35\n","wrong  : 963*3=2989\n","correct: 963*3=2889\n","outputs(x):  797*3=2931\n","9*\n","wrong  : 797*3=2931\n","correct: 797*3=2391\n","outputs(x):  338*7=2326\n","92\n","wrong  : 338*7=2326\n","correct: 338*7=2366\n","outputs(x):  317*7=2639\n","97\n","wrong  : 317*7=2639\n","correct: 317*7=2219\n","outputs(x):  421*4=1644\n","20\n","wrong  : 421*4=1644\n","correct: 421*4=1684\n","outputs(x):  155*7=105\n","917\n","wrong  : 155*7=105\n","correct: 155*7=1085\n","outputs(x):  304*3=1212\n","17\n","wrong  : 304*3=1212\n","correct: 304*3=912\n","outputs(x):  573*9=5137\n","23\n","wrong  : 573*9=5137\n","correct: 573*9=5157\n","outputs(x):  442*8=3576\n","83\n","wrong  : 442*8=3576\n","correct: 442*8=3536\n","outputs(x):  885*3=2675\n","79\n","wrong  : 885*3=2675\n","correct: 885*3=2655\n","outputs(x):  453*6=2118\n","19\n","wrong  : 453*6=2118\n","correct: 453*6=2718\n","outputs(x):  987*3=2931\n","9*\n","wrong  : 987*3=2931\n","correct: 987*3=2961\n","outputs(x):  111*6=646\n","397\n","wrong  : 111*6=646\n","correct: 111*6=666\n","outputs(x):  724*3=2112\n","63\n","wrong  : 724*3=2112\n","correct: 724*3=2172\n","outputs(x):  256*6=1516\n","38\n","wrong  : 256*6=1516\n","correct: 256*6=1536\n","outputs(x):  456*2=932\n","237\n","wrong  : 456*2=932\n","correct: 456*2=912\n","outputs(x):  316*3=1848\n","40\n","wrong  : 316*3=1848\n","correct: 316*3=948\n","outputs(x):  570*5=2350\n","38\n","wrong  : 570*5=2350\n","correct: 570*5=2850\n","outputs(x):  510*5=250\n","403\n","wrong  : 510*5=250\n","correct: 510*5=2550\n","outputs(x):  269*9=2341\n","12\n","wrong  : 269*9=2341\n","correct: 269*9=2421\n","outputs(x):  671*9=6099\n","11\n","wrong  : 671*9=6099\n","correct: 671*9=6039\n","outputs(x):  338*4=1552\n","22\n","wrong  : 338*4=1552\n","correct: 338*4=1352\n","outputs(x):  227*9=2083\n","16\n","wrong  : 227*9=2083\n","correct: 227*9=2043\n","outputs(x):  715*7=5245\n","54\n","wrong  : 715*7=5245\n","correct: 715*7=5005\n","outputs(x):  385*9=3095\n","72\n","wrong  : 385*9=3095\n","correct: 385*9=3465\n","outputs(x):  776*7=5322\n","23\n","wrong  : 776*7=5322\n","correct: 776*7=5432\n","outputs(x):  774*7=5348\n","65\n","wrong  : 774*7=5348\n","correct: 774*7=5418\n","outputs(x):  974*9=8266\n","58\n","wrong  : 974*9=8266\n","correct: 974*9=8766\n","outputs(x):  277*4=1148\n","31\n","wrong  : 277*4=1148\n","correct: 277*4=1108\n","outputs(x):  229*9=2601\n","54\n","wrong  : 229*9=2601\n","correct: 229*9=2061\n","outputs(x):  962*3=2976\n","59\n","wrong  : 962*3=2976\n","correct: 962*3=2886\n","outputs(x):  684*9=6176\n","33\n","wrong  : 684*9=6176\n","correct: 684*9=6156\n","outputs(x):  528*7=3776\n","23\n","wrong  : 528*7=3776\n","correct: 528*7=3696\n","outputs(x):  171*4=688\n","116\n","wrong  : 171*4=688\n","correct: 171*4=684\n","outputs(x):  486*6=2976\n","59\n","wrong  : 486*6=2976\n","correct: 486*6=2916\n","outputs(x):  102*9=1808\n","37\n","wrong  : 102*9=1808\n","correct: 102*9=918\n","outputs(x):  169*8=1592\n","85\n","wrong  : 169*8=1592\n","correct: 169*8=1352\n","outputs(x):  332*3=96\n","555*\n","wrong  : 332*3=96\n","correct: 332*3=996\n","outputs(x):  954*7=6688\n","18\n","wrong  : 954*7=6688\n","correct: 954*7=6678\n","outputs(x):  121*6=1276\n","91\n","wrong  : 121*6=1276\n","correct: 121*6=726\n","outputs(x):  527*3=1551\n","20\n","wrong  : 527*3=1551\n","correct: 527*3=1581\n","outputs(x):  688*9=6182\n","18\n","wrong  : 688*9=6182\n","correct: 688*9=6192\n","outputs(x):  119*9=1971\n","81\n","wrong  : 119*9=1971\n","correct: 119*9=1071\n","outputs(x):  480*3=140\n","212\n","wrong  : 480*3=140\n","correct: 480*3=1440\n","outputs(x):  389*9=3411\n","29\n","wrong  : 389*9=3411\n","correct: 389*9=3501\n","outputs(x):  694*7=4838\n","58\n","wrong  : 694*7=4838\n","correct: 694*7=4858\n"," 42% 10/24 [00:00<00:00, 15.67it/s]outputs(x):  154*5=750\n","61*\n","wrong  : 154*5=750\n","correct: 154*5=770\n","outputs(x):  794*3=2392\n","44\n","wrong  : 794*3=2392\n","correct: 794*3=2382\n","outputs(x):  473*8=3704\n","52\n","wrong  : 473*8=3704\n","correct: 473*8=3784\n","outputs(x):  788*6=4128\n","52\n","wrong  : 788*6=4128\n","correct: 788*6=4728\n","outputs(x):  847*6=5022\n","28\n","wrong  : 847*6=5022\n","correct: 847*6=5082\n","outputs(x):  275*5=1325\n","51\n","wrong  : 275*5=1325\n","correct: 275*5=1375\n","outputs(x):  549*3=1697\n","60\n","wrong  : 549*3=1697\n","correct: 549*3=1647\n","outputs(x):  853*3=2599\n","13\n","wrong  : 853*3=2599\n","correct: 853*3=2559\n","outputs(x):  418*6=2488\n","63\n","wrong  : 418*6=2488\n","correct: 418*6=2508\n","outputs(x):  237*9=2033\n","65\n","wrong  : 237*9=2033\n","correct: 237*9=2133\n","outputs(x):  966*3=2988\n","0*\n","wrong  : 966*3=2988\n","correct: 966*3=2898\n","outputs(x):  756*8=6016\n","53\n","wrong  : 756*8=6016\n","correct: 756*8=6048\n","outputs(x):  425*7=2955\n","74\n","wrong  : 425*7=2955\n","correct: 425*7=2975\n","outputs(x):  226*5=1330\n","34\n","wrong  : 226*5=1330\n","correct: 226*5=1130\n","outputs(x):  180*4=752\n","373\n","wrong  : 180*4=752\n","correct: 180*4=720\n","outputs(x):  794*8=632\n","707\n","wrong  : 794*8=632\n","correct: 794*8=6352\n","outputs(x):  387*2=756\n","777\n","wrong  : 387*2=756\n","correct: 387*2=774\n","outputs(x):  723*9=657\n","304\n","wrong  : 723*9=657\n","correct: 723*9=6507\n","outputs(x):  419*3=1297\n","13\n","wrong  : 419*3=1297\n","correct: 419*3=1257\n","outputs(x):  280*3=540\n","86*\n","wrong  : 280*3=540\n","correct: 280*3=840\n","outputs(x):  511*4=2844\n","20\n","wrong  : 511*4=2844\n","correct: 511*4=2044\n","outputs(x):  109*5=535\n","0*3\n","wrong  : 109*5=535\n","correct: 109*5=545\n","outputs(x):  774*2=1748\n","75\n","wrong  : 774*2=1748\n","correct: 774*2=1548\n","outputs(x):  152*5=750\n","61*\n","wrong  : 152*5=750\n","correct: 152*5=760\n","outputs(x):  909*7=6343\n","15\n","wrong  : 909*7=6343\n","correct: 909*7=6363\n","outputs(x):  395*7=2745\n","37\n","wrong  : 395*7=2745\n","correct: 395*7=2765\n","outputs(x):  512*4=2088\n","53\n","wrong  : 512*4=2088\n","correct: 512*4=2048\n","outputs(x):  913*7=6531\n","79\n","wrong  : 913*7=6531\n","correct: 913*7=6391\n","outputs(x):  944*8=7592\n","81\n","wrong  : 944*8=7592\n","correct: 944*8=7552\n","outputs(x):  564*3=1092\n","10\n","wrong  : 564*3=1092\n","correct: 564*3=1692\n","outputs(x):  191*7=1379\n","23\n","wrong  : 191*7=1379\n","correct: 191*7=1337\n","outputs(x):  926*6=556\n","312\n","wrong  : 926*6=556\n","correct: 926*6=5556\n","outputs(x):  196*7=1362\n","40\n","wrong  : 196*7=1362\n","correct: 196*7=1372\n","outputs(x):  437*3=1241\n","53\n","wrong  : 437*3=1241\n","correct: 437*3=1311\n","outputs(x):  825*4=320\n","383\n","wrong  : 825*4=320\n","correct: 825*4=3300\n","outputs(x):  949*3=2897\n","85\n","wrong  : 949*3=2897\n","correct: 949*3=2847\n","outputs(x):  141*3=1243\n","75\n","wrong  : 141*3=1243\n","correct: 141*3=423\n","outputs(x):  393*3=1199\n","80\n","wrong  : 393*3=1199\n","correct: 393*3=1179\n","outputs(x):  312*8=256\n","791\n","wrong  : 312*8=256\n","correct: 312*8=2496\n","outputs(x):  332*9=2088\n","51\n","wrong  : 332*9=2088\n","correct: 332*9=2988\n","outputs(x):  574*5=2370\n","86\n","wrong  : 574*5=2370\n","correct: 574*5=2870\n","outputs(x):  910*3=270\n","909\n","wrong  : 910*3=270\n","correct: 910*3=2730\n","outputs(x):  106*8=812\n","198\n","wrong  : 106*8=812\n","correct: 106*8=848\n","outputs(x):  737*3=2511\n","87\n","wrong  : 737*3=2511\n","correct: 737*3=2211\n","outputs(x):  525*9=4785\n","13\n","wrong  : 525*9=4785\n","correct: 525*9=4725\n","outputs(x):  129*8=916\n","134\n","wrong  : 129*8=916\n","correct: 129*8=1032\n","outputs(x):  373*9=3337\n","27\n","wrong  : 373*9=3337\n","correct: 373*9=3357\n","outputs(x):  419*6=2094\n","96\n","wrong  : 419*6=2094\n","correct: 419*6=2514\n","outputs(x):  992*8=796\n","318\n","wrong  : 992*8=796\n","correct: 992*8=7936\n","outputs(x):  455*6=2770\n","41\n","wrong  : 455*6=2770\n","correct: 455*6=2730\n","outputs(x):  574*8=4536\n","85\n","wrong  : 574*8=4536\n","correct: 574*8=4592\n","outputs(x):  489*2=96\n","901*\n","wrong  : 489*2=96\n","correct: 489*2=978\n","outputs(x):  433*6=2558\n","1*\n","wrong  : 433*6=2558\n","correct: 433*6=2598\n","outputs(x):  348*8=3504\n","97\n","wrong  : 348*8=3504\n","correct: 348*8=2784\n","outputs(x):  604*9=5426\n","94\n","wrong  : 604*9=5426\n","correct: 604*9=5436\n","outputs(x):  761*8=6888\n","32\n","wrong  : 761*8=6888\n","correct: 761*8=6088\n","outputs(x):  371*7=2317\n","77\n","wrong  : 371*7=2317\n","correct: 371*7=2597\n","outputs(x):  483*9=4387\n","75\n","wrong  : 483*9=4387\n","correct: 483*9=4347\n","outputs(x):  368*8=2144\n","52\n","wrong  : 368*8=2144\n","correct: 368*8=2944\n","outputs(x):  336*8=2528\n","14\n","wrong  : 336*8=2528\n","correct: 336*8=2688\n","outputs(x):  630*6=3180\n","60\n","wrong  : 630*6=3180\n","correct: 630*6=3780\n","outputs(x):  147*7=1329\n","44\n","wrong  : 147*7=1329\n","correct: 147*7=1029\n","outputs(x):  152*7=1024\n","33\n","wrong  : 152*7=1024\n","correct: 152*7=1064\n","outputs(x):  718*8=5704\n","53\n","wrong  : 718*8=5704\n","correct: 718*8=5744\n","outputs(x):  272*7=1204\n","95\n","wrong  : 272*7=1204\n","correct: 272*7=1904\n","outputs(x):  636*8=5098\n","25\n","wrong  : 636*8=5098\n","correct: 636*8=5088\n","outputs(x):  567*7=3999\n","29\n","wrong  : 567*7=3999\n","correct: 567*7=3969\n","outputs(x):  613*3=1849\n","12\n","wrong  : 613*3=1849\n","correct: 613*3=1839\n","outputs(x):  957*7=6679\n","53\n","wrong  : 957*7=6679\n","correct: 957*7=6699\n","outputs(x):  110*1=10\n","4*8=\n","wrong  : 110*1=10\n","correct: 110*1=110\n","outputs(x):  489*9=4341\n","82\n","wrong  : 489*9=4341\n","correct: 489*9=4401\n","outputs(x):  465*6=2770\n","41\n","wrong  : 465*6=2770\n","correct: 465*6=2790\n","outputs(x):  488*4=1992\n","39\n","wrong  : 488*4=1992\n","correct: 488*4=1952\n","outputs(x):  461*8=3288\n","83\n","wrong  : 461*8=3288\n","correct: 461*8=3688\n","outputs(x):  889*3=267\n","627\n","wrong  : 889*3=267\n","correct: 889*3=2667\n","outputs(x):  774*8=6224\n","83\n","wrong  : 774*8=6224\n","correct: 774*8=6192\n","outputs(x):  668*6=4608\n","77\n","wrong  : 668*6=4608\n","correct: 668*6=4008\n","outputs(x):  991*3=2970\n","16\n","wrong  : 991*3=2970\n","correct: 991*3=2973\n","outputs(x):  104*8=812\n","198\n","wrong  : 104*8=812\n","correct: 104*8=832\n","outputs(x):  174*8=1492\n","38\n","wrong  : 174*8=1492\n","correct: 174*8=1392\n","outputs(x):  195*7=135\n","928\n","wrong  : 195*7=135\n","correct: 195*7=1365\n","outputs(x):  566*9=5004\n","73\n","wrong  : 566*9=5004\n","correct: 566*9=5094\n","outputs(x):  143*7=991\n","241\n","wrong  : 143*7=991\n","correct: 143*7=1001\n","outputs(x):  267*3=709\n","168\n","wrong  : 267*3=709\n","correct: 267*3=801\n","outputs(x):  995*9=855\n","875\n","wrong  : 995*9=855\n","correct: 995*9=8955\n","outputs(x):  775*9=6955\n","38\n","wrong  : 775*9=6955\n","correct: 775*9=6975\n","outputs(x):  338*9=2142\n","22\n","wrong  : 338*9=2142\n","correct: 338*9=3042\n","outputs(x):  916*3=2788\n","24\n","wrong  : 916*3=2788\n","correct: 916*3=2748\n","outputs(x):  829*4=3116\n","66\n","wrong  : 829*4=3116\n","correct: 829*4=3316\n","outputs(x):  783*8=5584\n","54\n","wrong  : 783*8=5584\n","correct: 783*8=6264\n","outputs(x):  629*4=2116\n","78\n","wrong  : 629*4=2116\n","correct: 629*4=2516\n","outputs(x):  678*9=6912\n","27\n","wrong  : 678*9=6912\n","correct: 678*9=6102\n","outputs(x):  272*3=813\n","354\n","wrong  : 272*3=813\n","correct: 272*3=816\n","outputs(x):  669*4=2716\n","38\n","wrong  : 669*4=2716\n","correct: 669*4=2676\n","outputs(x):  702*4=2888\n","80\n","wrong  : 702*4=2888\n","correct: 702*4=2808\n"," 50% 12/24 [00:00<00:00, 15.75it/s]outputs(x):  829*2=1698\n","44\n","wrong  : 829*2=1698\n","correct: 829*2=1658\n","outputs(x):  971*7=6277\n","13\n","wrong  : 971*7=6277\n","correct: 971*7=6797\n","outputs(x):  136*6=876\n","66*\n","wrong  : 136*6=876\n","correct: 136*6=816\n","outputs(x):  535*6=3230\n","89\n","wrong  : 535*6=3230\n","correct: 535*6=3210\n","outputs(x):  522*7=3164\n","15\n","wrong  : 522*7=3164\n","correct: 522*7=3654\n","outputs(x):  414*7=2938\n","95\n","wrong  : 414*7=2938\n","correct: 414*7=2898\n","outputs(x):  867*6=5122\n","21\n","wrong  : 867*6=5122\n","correct: 867*6=5202\n","outputs(x):  840*7=5800\n","43\n","wrong  : 840*7=5800\n","correct: 840*7=5880\n","outputs(x):  296*9=2684\n","40\n","wrong  : 296*9=2684\n","correct: 296*9=2664\n","outputs(x):  973*8=784\n","324\n","wrong  : 973*8=784\n","correct: 973*8=7784\n","outputs(x):  777*8=6136\n","58\n","wrong  : 777*8=6136\n","correct: 777*8=6216\n","outputs(x):  674*8=5372\n","62\n","wrong  : 674*8=5372\n","correct: 674*8=5392\n","outputs(x):  963*6=5858\n","3*\n","wrong  : 963*6=5858\n","correct: 963*6=5778\n","outputs(x):  355*8=2800\n","2*\n","wrong  : 355*8=2800\n","correct: 355*8=2840\n","outputs(x):  131*5=555\n","66*\n","wrong  : 131*5=555\n","correct: 131*5=655\n","outputs(x):  994*9=8046\n","48\n","wrong  : 994*9=8046\n","correct: 994*9=8946\n","outputs(x):  392*3=1196\n","47\n","wrong  : 392*3=1196\n","correct: 392*3=1176\n","outputs(x):  924*7=6478\n","62\n","wrong  : 924*7=6478\n","correct: 924*7=6468\n","outputs(x):  765*9=6855\n","30\n","wrong  : 765*9=6855\n","correct: 765*9=6885\n","outputs(x):  789*9=7921\n","40\n","wrong  : 789*9=7921\n","correct: 789*9=7101\n","outputs(x):  855*2=1750\n","21\n","wrong  : 855*2=1750\n","correct: 855*2=1710\n","outputs(x):  432*2=84\n","604*\n","wrong  : 432*2=84\n","correct: 432*2=864\n","outputs(x):  231*2=446\n","685\n","wrong  : 231*2=446\n","correct: 231*2=462\n","outputs(x):  473*9=4237\n","25\n","wrong  : 473*9=4237\n","correct: 473*9=4257\n","outputs(x):  378*7=2546\n","89\n","wrong  : 378*7=2546\n","correct: 378*7=2646\n","outputs(x):  686*6=4128\n","52\n","wrong  : 686*6=4128\n","correct: 686*6=4116\n","outputs(x):  941*9=8549\n","27\n","wrong  : 941*9=8549\n","correct: 941*9=8469\n","outputs(x):  877*4=3488\n","47\n","wrong  : 877*4=3488\n","correct: 877*4=3508\n","outputs(x):  208*7=1466\n","99\n","wrong  : 208*7=1466\n","correct: 208*7=1456\n","outputs(x):  507*7=3539\n","98\n","wrong  : 507*7=3539\n","correct: 507*7=3549\n","outputs(x):  218*4=88\n","120*\n","wrong  : 218*4=88\n","correct: 218*4=872\n","outputs(x):  225*9=205\n","232\n","wrong  : 225*9=205\n","correct: 225*9=2025\n","outputs(x):  874*7=6188\n","14\n","wrong  : 874*7=6188\n","correct: 874*7=6118\n","outputs(x):  617*4=2868\n","41\n","wrong  : 617*4=2868\n","correct: 617*4=2468\n","outputs(x):  619*9=5521\n","21\n","wrong  : 619*9=5521\n","correct: 619*9=5571\n","outputs(x):  366*6=2156\n","84\n","wrong  : 366*6=2156\n","correct: 366*6=2196\n","outputs(x):  977*7=6079\n","56\n","wrong  : 977*7=6079\n","correct: 977*7=6839\n","outputs(x):  959*6=574\n","75*\n","wrong  : 959*6=574\n","correct: 959*6=5754\n","outputs(x):  259*9=2311\n","12\n","wrong  : 259*9=2311\n","correct: 259*9=2331\n","outputs(x):  306*9=2744\n","93\n","wrong  : 306*9=2744\n","correct: 306*9=2754\n","outputs(x):  294*3=897\n","6*7\n","wrong  : 294*3=897\n","correct: 294*3=882\n","outputs(x):  493*2=878\n","73*\n","wrong  : 493*2=878\n","correct: 493*2=986\n","outputs(x):  413*7=2961\n","97\n","wrong  : 413*7=2961\n","correct: 413*7=2891\n","outputs(x):  970*7=6730\n","42\n","wrong  : 970*7=6730\n","correct: 970*7=6790\n","outputs(x):  788*9=6912\n","27\n","wrong  : 788*9=6912\n","correct: 788*9=7092\n","outputs(x):  716*9=644\n","300\n","wrong  : 716*9=644\n","correct: 716*9=6444\n","outputs(x):  172*5=850\n","95*\n","wrong  : 172*5=850\n","correct: 172*5=860\n","outputs(x):  496*7=3402\n","37\n","wrong  : 496*7=3402\n","correct: 496*7=3472\n","outputs(x):  136*7=931\n","932\n","wrong  : 136*7=931\n","correct: 136*7=952\n","outputs(x):  985*6=5890\n","72\n","wrong  : 985*6=5890\n","correct: 985*6=5910\n","outputs(x):  832*9=7408\n","59\n","wrong  : 832*9=7408\n","correct: 832*9=7488\n","outputs(x):  836*8=6768\n","77\n","wrong  : 836*8=6768\n","correct: 836*8=6688\n","outputs(x):  578*9=5112\n","16\n","wrong  : 578*9=5112\n","correct: 578*9=5202\n","outputs(x):  366*8=2888\n","47\n","wrong  : 366*8=2888\n","correct: 366*8=2928\n","outputs(x):  394*3=1422\n","48\n","wrong  : 394*3=1422\n","correct: 394*3=1182\n","outputs(x):  709*7=5363\n","50\n","wrong  : 709*7=5363\n","correct: 709*7=4963\n","outputs(x):  234*3=692\n","518\n","wrong  : 234*3=692\n","correct: 234*3=702\n","outputs(x):  448*2=96\n","901*\n","wrong  : 448*2=96\n","correct: 448*2=896\n","outputs(x):  653*7=4581\n","10\n","wrong  : 653*7=4581\n","correct: 653*7=4571\n","outputs(x):  111*7=147\n","5*3\n","wrong  : 111*7=147\n","correct: 111*7=777\n","outputs(x):  772*9=6088\n","77\n","wrong  : 772*9=6088\n","correct: 772*9=6948\n","outputs(x):  279*3=897\n","6*7\n","wrong  : 279*3=897\n","correct: 279*3=837\n","outputs(x):  390*8=2320\n","53\n","wrong  : 390*8=2320\n","correct: 390*8=3120\n","outputs(x):  487*2=96\n","901*\n","wrong  : 487*2=96\n","correct: 487*2=974\n","outputs(x):  121*9=1909\n","73\n","wrong  : 121*9=1909\n","correct: 121*9=1089\n","outputs(x):  991*2=1382\n","56\n","wrong  : 991*2=1382\n","correct: 991*2=1982\n","outputs(x):  833*7=5811\n","56\n","wrong  : 833*7=5811\n","correct: 833*7=5831\n","outputs(x):  744*7=5228\n","85\n","wrong  : 744*7=5228\n","correct: 744*7=5208\n","outputs(x):  355*4=140\n","879\n","wrong  : 355*4=140\n","correct: 355*4=1420\n","outputs(x):  749*6=4474\n","72\n","wrong  : 749*6=4474\n","correct: 749*6=4494\n","outputs(x):  668*4=272\n","976\n","wrong  : 668*4=272\n","correct: 668*4=2672\n","outputs(x):  967*7=6739\n","10\n","wrong  : 967*7=6739\n","correct: 967*7=6769\n","outputs(x):  846*7=672\n","69*\n","wrong  : 846*7=672\n","correct: 846*7=5922\n","outputs(x):  454*7=3168\n","80\n","wrong  : 454*7=3168\n","correct: 454*7=3178\n","outputs(x):  411*3=1293\n","73\n","wrong  : 411*3=1293\n","correct: 411*3=1233\n","outputs(x):  565*5=2325\n","74\n","wrong  : 565*5=2325\n","correct: 565*5=2825\n","outputs(x):  576*3=1758\n","70\n","wrong  : 576*3=1758\n","correct: 576*3=1728\n","outputs(x):  727*8=5776\n","40\n","wrong  : 727*8=5776\n","correct: 727*8=5816\n","outputs(x):  305*7=2155\n","63\n","wrong  : 305*7=2155\n","correct: 305*7=2135\n","outputs(x):  327*5=1685\n","56\n","wrong  : 327*5=1685\n","correct: 327*5=1635\n","outputs(x):  832*2=1644\n","71\n","wrong  : 832*2=1644\n","correct: 832*2=1664\n","outputs(x):  698*8=5504\n","53\n","wrong  : 698*8=5504\n","correct: 698*8=5584\n","outputs(x):  660*6=4080\n","97\n","wrong  : 660*6=4080\n","correct: 660*6=3960\n","outputs(x):  814*7=5688\n","68\n","wrong  : 814*7=5688\n","correct: 814*7=5698\n","outputs(x):  127*9=1943\n","79\n","wrong  : 127*9=1943\n","correct: 127*9=1143\n","outputs(x):  128*8=904\n","36*\n","wrong  : 128*8=904\n","correct: 128*8=1024\n","outputs(x):  843*9=7507\n","57\n","wrong  : 843*9=7507\n","correct: 843*9=7587\n","outputs(x):  869*6=5134\n","22\n","wrong  : 869*6=5134\n","correct: 869*6=5214\n","outputs(x):  417*2=82\n","408*\n","wrong  : 417*2=82\n","correct: 417*2=834\n","outputs(x):  589*3=1707\n","66\n","wrong  : 589*3=1707\n","correct: 589*3=1767\n","outputs(x):  746*6=4436\n","95\n","wrong  : 746*6=4436\n","correct: 746*6=4476\n","outputs(x):  117*9=1033\n","89\n","wrong  : 117*9=1033\n","correct: 117*9=1053\n"," 58% 14/24 [00:00<00:00, 16.26it/s]outputs(x):  221*2=42\n","603*\n","wrong  : 221*2=42\n","correct: 221*2=442\n","outputs(x):  222*8=176\n","361\n","wrong  : 222*8=176\n","correct: 222*8=1776\n","outputs(x):  108*7=706\n","534\n","wrong  : 108*7=706\n","correct: 108*7=756\n","outputs(x):  282*3=546\n","234\n","wrong  : 282*3=546\n","correct: 282*3=846\n","outputs(x):  841*7=6167\n","86\n","wrong  : 841*7=6167\n","correct: 841*7=5887\n","outputs(x):  254*7=1738\n","67\n","wrong  : 254*7=1738\n","correct: 254*7=1778\n","outputs(x):  333*3=1299\n","13\n","wrong  : 333*3=1299\n","correct: 333*3=999\n","outputs(x):  535*3=1005\n","21\n","wrong  : 535*3=1005\n","correct: 535*3=1605\n","outputs(x):  149*7=963\n","732\n","wrong  : 149*7=963\n","correct: 149*7=1043\n","outputs(x):  108*5=535\n","0*3\n","wrong  : 108*5=535\n","correct: 108*5=540\n","outputs(x):  129*6=114\n","19*\n","wrong  : 129*6=114\n","correct: 129*6=774\n","outputs(x):  246*7=1422\n","48\n","wrong  : 246*7=1422\n","correct: 246*7=1722\n","outputs(x):  169*6=1094\n","96\n","wrong  : 169*6=1094\n","correct: 169*6=1014\n","outputs(x):  278*8=2264\n","71\n","wrong  : 278*8=2264\n","correct: 278*8=2224\n","outputs(x):  109*8=1672\n","61\n","wrong  : 109*8=1672\n","correct: 109*8=872\n","outputs(x):  409*9=3641\n","85\n","wrong  : 409*9=3641\n","correct: 409*9=3681\n","outputs(x):  435*8=3080\n","40\n","wrong  : 435*8=3080\n","correct: 435*8=3480\n","outputs(x):  914*9=8146\n","45\n","wrong  : 914*9=8146\n","correct: 914*9=8226\n","outputs(x):  110*4=1200\n","71\n","wrong  : 110*4=1200\n","correct: 110*4=440\n","outputs(x):  419*7=2633\n","71\n","wrong  : 419*7=2633\n","correct: 419*7=2933\n","outputs(x):  428*7=3416\n","80\n","wrong  : 428*7=3416\n","correct: 428*7=2996\n","outputs(x):  303*5=1505\n","47\n","wrong  : 303*5=1505\n","correct: 303*5=1515\n","outputs(x):  628*8=4224\n","65\n","wrong  : 628*8=4224\n","correct: 628*8=5024\n","outputs(x):  968*7=6716\n","24\n","wrong  : 968*7=6716\n","correct: 968*7=6776\n","outputs(x):  100*4=800\n","649\n","wrong  : 100*4=800\n","correct: 100*4=400\n","outputs(x):  249*7=1963\n","73\n","wrong  : 249*7=1963\n","correct: 249*7=1743\n","outputs(x):  566*6=3456\n","49\n","wrong  : 566*6=3456\n","correct: 566*6=3396\n","outputs(x):  134*7=968\n","730\n","wrong  : 134*7=968\n","correct: 134*7=938\n","outputs(x):  612*6=3612\n","19\n","wrong  : 612*6=3612\n","correct: 612*6=3672\n","outputs(x):  523*9=4787\n","63\n","wrong  : 523*9=4787\n","correct: 523*9=4707\n","outputs(x):  140*3=312\n","513\n","wrong  : 140*3=312\n","correct: 140*3=420\n","outputs(x):  838*3=2544\n","82\n","wrong  : 838*3=2544\n","correct: 838*3=2514\n","outputs(x):  942*3=2876\n","12\n","wrong  : 942*3=2876\n","correct: 942*3=2826\n","outputs(x):  614*7=4988\n","46\n","wrong  : 614*7=4988\n","correct: 614*7=4298\n","outputs(x):  112*9=1908\n","73\n","wrong  : 112*9=1908\n","correct: 112*9=1008\n","outputs(x):  267*8=2296\n","74\n","wrong  : 267*8=2296\n","correct: 267*8=2136\n","outputs(x):  101*4=840\n","7*2\n","wrong  : 101*4=840\n","correct: 101*4=404\n","outputs(x):  158*7=126\n","601\n","wrong  : 158*7=126\n","correct: 158*7=1106\n","outputs(x):  122*7=86\n","478*\n","wrong  : 122*7=86\n","correct: 122*7=854\n","outputs(x):  739*2=1438\n","48\n","wrong  : 739*2=1438\n","correct: 739*2=1478\n","outputs(x):  206*7=1422\n","48\n","wrong  : 206*7=1422\n","correct: 206*7=1442\n","outputs(x):  965*3=2995\n","48\n","wrong  : 965*3=2995\n","correct: 965*3=2895\n","outputs(x):  533*6=3258\n","61\n","wrong  : 533*6=3258\n","correct: 533*6=3198\n","outputs(x):  641*3=1383\n","34\n","wrong  : 641*3=1383\n","correct: 641*3=1923\n","outputs(x):  915*8=720\n","248\n","wrong  : 915*8=720\n","correct: 915*8=7320\n","outputs(x):  204*4=836\n","540\n","wrong  : 204*4=836\n","correct: 204*4=816\n","outputs(x):  386*7=2602\n","\n","4\n","wrong  : 386*7=2602\n","correct: 386*7=2702\n","outputs(x):  986*7=672\n","69*\n","wrong  : 986*7=672\n","correct: 986*7=6902\n","outputs(x):  427*4=1068\n","83\n","wrong  : 427*4=1068\n","correct: 427*4=1708\n","outputs(x):  292*3=896\n","947\n","wrong  : 292*3=896\n","correct: 292*3=876\n","outputs(x):  731*3=213\n","842\n","wrong  : 731*3=213\n","correct: 731*3=2193\n","outputs(x):  556*8=448\n","306\n","wrong  : 556*8=448\n","correct: 556*8=4448\n","outputs(x):  667*7=4609\n","77\n","wrong  : 667*7=4609\n","correct: 667*7=4669\n","outputs(x):  500*3=1560\n","86\n","wrong  : 500*3=1560\n","correct: 500*3=1500\n","outputs(x):  328*9=2052\n","71\n","wrong  : 328*9=2052\n","correct: 328*9=2952\n","outputs(x):  444*9=4896\n","35\n","wrong  : 444*9=4896\n","correct: 444*9=3996\n","outputs(x):  859*3=25\n","780*\n","wrong  : 859*3=25\n","correct: 859*3=2577\n","outputs(x):  587*7=4129\n","21\n","wrong  : 587*7=4129\n","correct: 587*7=4109\n","outputs(x):  861*7=6167\n","86\n","wrong  : 861*7=6167\n","correct: 861*7=6027\n","outputs(x):  914*7=6488\n","13\n","wrong  : 914*7=6488\n","correct: 914*7=6398\n","outputs(x):  386*6=2376\n","91\n","wrong  : 386*6=2376\n","correct: 386*6=2316\n","outputs(x):  157*8=1266\n","79\n","wrong  : 157*8=1266\n","correct: 157*8=1256\n","outputs(x):  138*3=404\n","336\n","wrong  : 138*3=404\n","correct: 138*3=414\n","outputs(x):  232*2=44\n","348*\n","wrong  : 232*2=44\n","correct: 232*2=464\n","outputs(x):  353*8=2664\n","81\n","wrong  : 353*8=2664\n","correct: 353*8=2824\n","outputs(x):  864*7=6068\n","66\n","wrong  : 864*7=6068\n","correct: 864*7=6048\n","outputs(x):  369*7=2513\n","76\n","wrong  : 369*7=2513\n","correct: 369*7=2583\n","outputs(x):  773*7=5391\n","12\n","wrong  : 773*7=5391\n","correct: 773*7=5411\n","outputs(x):  816*7=5622\n","20\n","wrong  : 816*7=5622\n","correct: 816*7=5712\n","outputs(x):  127*6=1662\n","74\n","wrong  : 127*6=1662\n","correct: 127*6=762\n","outputs(x):  566*8=4488\n","63\n","wrong  : 566*8=4488\n","correct: 566*8=4528\n","outputs(x):  162*3=498\n","969\n","wrong  : 162*3=498\n","correct: 162*3=486\n","outputs(x):  974*6=5864\n","68\n","wrong  : 974*6=5864\n","correct: 974*6=5844\n","outputs(x):  753*9=6867\n","30\n","wrong  : 753*9=6867\n","correct: 753*9=6777\n","outputs(x):  470*9=4270\n","25\n","wrong  : 470*9=4270\n","correct: 470*9=4230\n","outputs(x):  438*6=2028\n","21\n","wrong  : 438*6=2028\n","correct: 438*6=2628\n","outputs(x):  182*4=756\n","303\n","wrong  : 182*4=756\n","correct: 182*4=728\n","outputs(x):  430*9=3980\n","74\n","wrong  : 430*9=3980\n","correct: 430*9=3870\n","outputs(x):  713*7=5117\n","63\n","wrong  : 713*7=5117\n","correct: 713*7=4991\n","outputs(x):  314*7=2918\n","90\n","wrong  : 314*7=2918\n","correct: 314*7=2198\n","outputs(x):  668*3=1104\n","14\n","wrong  : 668*3=1104\n","correct: 668*3=2004\n","outputs(x):  540*9=4800\n","68\n","wrong  : 540*9=4800\n","correct: 540*9=4860\n","outputs(x):  876*6=5226\n","81\n","wrong  : 876*6=5226\n","correct: 876*6=5256\n","outputs(x):  502*9=4568\n","33\n","wrong  : 502*9=4568\n","correct: 502*9=4518\n","outputs(x):  674*7=4018\n","80\n","wrong  : 674*7=4018\n","correct: 674*7=4718\n","outputs(x):  591*3=1713\n","92\n","wrong  : 591*3=1713\n","correct: 591*3=1773\n","outputs(x):  347*6=2022\n","88\n","wrong  : 347*6=2022\n","correct: 347*6=2082\n","outputs(x):  244*6=1444\n","55\n","wrong  : 244*6=1444\n","correct: 244*6=1464\n","outputs(x):  852*3=2576\n","12\n","wrong  : 852*3=2576\n","correct: 852*3=2556\n","outputs(x):  606*9=5434\n","46\n","wrong  : 606*9=5434\n","correct: 606*9=5454\n","outputs(x):  915*4=3620\n","96\n","wrong  : 915*4=3620\n","correct: 915*4=3660\n","outputs(x):  443*8=344\n","990\n","wrong  : 443*8=344\n","correct: 443*8=3544\n","outputs(x):  184*4=676\n","492\n","wrong  : 184*4=676\n","correct: 184*4=736\n","outputs(x):  866*7=602\n","\n","12\n","wrong  : 866*7=602\n","correct: 866*7=6062\n","outputs(x):  227*5=135\n","928\n","wrong  : 227*5=135\n","correct: 227*5=1135\n","outputs(x):  821*2=1622\n","88\n","wrong  : 821*2=1622\n","correct: 821*2=1642\n","outputs(x):  179*9=1571\n","69\n","wrong  : 179*9=1571\n","correct: 179*9=1611\n"," 67% 16/24 [00:00<00:00, 16.25it/s]outputs(x):  233*7=161\n","241\n","wrong  : 233*7=161\n","correct: 233*7=1631\n","outputs(x):  874*8=6972\n","61\n","wrong  : 874*8=6972\n","correct: 874*8=6992\n","outputs(x):  658*7=4536\n","85\n","wrong  : 658*7=4536\n","correct: 658*7=4606\n","outputs(x):  233*2=446\n","685\n","wrong  : 233*2=446\n","correct: 233*2=466\n","outputs(x):  431*7=2317\n","77\n","wrong  : 431*7=2317\n","correct: 431*7=3017\n","outputs(x):  891*7=6277\n","87\n","wrong  : 891*7=6277\n","correct: 891*7=6237\n","outputs(x):  137*3=401\n","786\n","wrong  : 137*3=401\n","correct: 137*3=411\n","outputs(x):  624*7=4918\n","66\n","wrong  : 624*7=4918\n","correct: 624*7=4368\n","outputs(x):  586*7=3402\n","37\n","wrong  : 586*7=3402\n","correct: 586*7=4102\n","outputs(x):  710*8=560\n","4*3\n","wrong  : 710*8=560\n","correct: 710*8=5680\n","outputs(x):  894*3=2652\n","62\n","wrong  : 894*3=2652\n","correct: 894*3=2682\n","outputs(x):  245*9=2105\n","97\n","wrong  : 245*9=2105\n","correct: 245*9=2205\n","outputs(x):  167*9=1483\n","35\n","wrong  : 167*9=1483\n","correct: 167*9=1503\n","outputs(x):  336*6=2028\n","21\n","wrong  : 336*6=2028\n","correct: 336*6=2016\n","outputs(x):  186*4=704\n","31*\n","wrong  : 186*4=704\n","correct: 186*4=744\n","outputs(x):  888*7=6296\n","60\n","wrong  : 888*7=6296\n","correct: 888*7=6216\n","outputs(x):  167*3=528\n","329\n","wrong  : 167*3=528\n","correct: 167*3=501\n","outputs(x):  172*8=1456\n","39\n","wrong  : 172*8=1456\n","correct: 172*8=1376\n","outputs(x):  907*3=2741\n","52\n","wrong  : 907*3=2741\n","correct: 907*3=2721\n","outputs(x):  427*7=2909\n","46\n","wrong  : 427*7=2909\n","correct: 427*7=2989\n","outputs(x):  630*3=1990\n","73\n","wrong  : 630*3=1990\n","correct: 630*3=1890\n","outputs(x):  276*7=1302\n","28\n","wrong  : 276*7=1302\n","correct: 276*7=1932\n","outputs(x):  686*7=4732\n","60\n","wrong  : 686*7=4732\n","correct: 686*7=4802\n","outputs(x):  660*7=4600\n","90\n","wrong  : 660*7=4600\n","correct: 660*7=4620\n","outputs(x):  509*9=4531\n","50\n","wrong  : 509*9=4531\n","correct: 509*9=4581\n","outputs(x):  808*7=5640\n","8*\n","wrong  : 808*7=5640\n","correct: 808*7=5656\n","outputs(x):  733*9=6577\n","60\n","wrong  : 733*9=6577\n","correct: 733*9=6597\n","outputs(x):  870*7=5460\n","8*\n","wrong  : 870*7=5460\n","correct: 870*7=6090\n","outputs(x):  845*7=6615\n","58\n","wrong  : 845*7=6615\n","correct: 845*7=5915\n","outputs(x):  193*7=1311\n","65\n","wrong  : 193*7=1311\n","correct: 193*7=1351\n","outputs(x):  667*3=2601\n","99\n","wrong  : 667*3=2601\n","correct: 667*3=2001\n","outputs(x):  718*6=4268\n","35\n","wrong  : 718*6=4268\n","correct: 718*6=4308\n","outputs(x):  376*8=2208\n","75\n","wrong  : 376*8=2208\n","correct: 376*8=3008\n","outputs(x):  219*8=1352\n","89\n","wrong  : 219*8=1352\n","correct: 219*8=1752\n","outputs(x):  962*9=8628\n","93\n","wrong  : 962*9=8628\n","correct: 962*9=8658\n","outputs(x):  764*6=444\n","155\n","wrong  : 764*6=444\n","correct: 764*6=4584\n","outputs(x):  850*8=680\n","230\n","wrong  : 850*8=680\n","correct: 850*8=6800\n","outputs(x):  569*6=3354\n","50\n","wrong  : 569*6=3354\n","correct: 569*6=3414\n","outputs(x):  381*9=3439\n","55\n","wrong  : 381*9=3439\n","correct: 381*9=3429\n","outputs(x):  655*2=130\n","511\n","wrong  : 655*2=130\n","correct: 655*2=1310\n","outputs(x):  227*7=1582\n","40\n","wrong  : 227*7=1582\n","correct: 227*7=1589\n","outputs(x):  340*7=240\n","439\n","wrong  : 340*7=240\n","correct: 340*7=2380\n","outputs(x):  372*9=3378\n","21\n","wrong  : 372*9=3378\n","correct: 372*9=3348\n","outputs(x):  983*6=5838\n","36\n","wrong  : 983*6=5838\n","correct: 983*6=5898\n","outputs(x):  685*3=2075\n","22\n","wrong  : 685*3=2075\n","correct: 685*3=2055\n","outputs(x):  197*5=975\n","937\n","wrong  : 197*5=975\n","correct: 197*5=985\n","outputs(x):  750*9=7650\n","44\n","wrong  : 750*9=7650\n","correct: 750*9=6750\n","outputs(x):  386*9=3414\n","21\n","wrong  : 386*9=3414\n","correct: 386*9=3474\n","outputs(x):  708*7=4946\n","6*\n","wrong  : 708*7=4946\n","correct: 708*7=4956\n","outputs(x):  395*8=3180\n","40\n","wrong  : 395*8=3180\n","correct: 395*8=3160\n","outputs(x):  403*4=1212\n","24\n","wrong  : 403*4=1212\n","correct: 403*4=1612\n","outputs(x):  962*7=6744\n","43\n","wrong  : 962*7=6744\n","correct: 962*7=6734\n","outputs(x):  484*7=3488\n","34\n","wrong  : 484*7=3488\n","correct: 484*7=3388\n","outputs(x):  331*9=2997\n","26\n","wrong  : 331*9=2997\n","correct: 331*9=2979\n","outputs(x):  173*3=511\n","86*\n","wrong  : 173*3=511\n","correct: 173*3=519\n","outputs(x):  533*8=4424\n","24\n","wrong  : 533*8=4424\n","correct: 533*8=4264\n","outputs(x):  110*5=55\n","76*2\n","wrong  : 110*5=55\n","correct: 110*5=550\n","outputs(x):  375*4=140\n","879\n","wrong  : 375*4=140\n","correct: 375*4=1500\n","outputs(x):  316*7=2912\n","98\n","wrong  : 316*7=2912\n","correct: 316*7=2212\n","outputs(x):  814*8=6432\n","51\n","wrong  : 814*8=6432\n","correct: 814*8=6512\n","outputs(x):  818*8=6504\n","67\n","wrong  : 818*8=6504\n","correct: 818*8=6544\n","outputs(x):  299*7=2793\n","79\n","wrong  : 299*7=2793\n","correct: 299*7=2093\n","outputs(x):  966*8=7768\n","37\n","wrong  : 966*8=7768\n","correct: 966*8=7728\n","outputs(x):  103*8=8184\n","20\n","wrong  : 103*8=8184\n","correct: 103*8=824\n","outputs(x):  367*9=3203\n","89\n","wrong  : 367*9=3203\n","correct: 367*9=3303\n","outputs(x):  925*3=2475\n","72\n","wrong  : 925*3=2475\n","correct: 925*3=2775\n","outputs(x):  737*2=1454\n","82\n","wrong  : 737*2=1454\n","correct: 737*2=1474\n","outputs(x):  881*9=7329\n","35\n","wrong  : 881*9=7329\n","correct: 881*9=7929\n","outputs(x):  959*7=6633\n","15\n","wrong  : 959*7=6633\n","correct: 959*7=6713\n","outputs(x):  460*7=3280\n","99\n","wrong  : 460*7=3280\n","correct: 460*7=3220\n","outputs(x):  777*3=2931\n","9*\n","wrong  : 777*3=2931\n","correct: 777*3=2331\n","outputs(x):  616*3=1248\n","8*\n","wrong  : 616*3=1248\n","correct: 616*3=1848\n","outputs(x):  177*7=1379\n","23\n","wrong  : 177*7=1379\n","correct: 177*7=1239\n","outputs(x):  317*3=1251\n","52\n","wrong  : 317*3=1251\n","correct: 317*3=951\n","outputs(x):  544*7=3068\n","80\n","wrong  : 544*7=3068\n","correct: 544*7=3808\n","outputs(x):  940*7=6620\n","93\n","wrong  : 940*7=6620\n","correct: 940*7=6580\n","outputs(x):  896*7=6282\n","18\n","wrong  : 896*7=6282\n","correct: 896*7=6272\n","outputs(x):  794*4=3196\n","84\n","wrong  : 794*4=3196\n","correct: 794*4=3176\n","outputs(x):  320*2=60\n","286*\n","wrong  : 320*2=60\n","correct: 320*2=640\n","outputs(x):  631*6=3186\n","45\n","wrong  : 631*6=3186\n","correct: 631*6=3786\n","outputs(x):  826*8=7088\n","46\n","wrong  : 826*8=7088\n","correct: 826*8=6608\n","outputs(x):  365*3=1905\n","7*\n","wrong  : 365*3=1905\n","correct: 365*3=1095\n","outputs(x):  712*8=576\n","29*\n","wrong  : 712*8=576\n","correct: 712*8=5696\n","outputs(x):  562*7=3984\n","86\n","wrong  : 562*7=3984\n","correct: 562*7=3934\n","outputs(x):  226*6=156\n","845\n","wrong  : 226*6=156\n","correct: 226*6=1356\n","outputs(x):  172*9=1558\n","92\n","wrong  : 172*9=1558\n","correct: 172*9=1548\n","outputs(x):  576*8=4616\n","87\n","wrong  : 576*8=4616\n","correct: 576*8=4608\n","outputs(x):  112*5=60\n","922*\n","wrong  : 112*5=60\n","correct: 112*5=560\n","outputs(x):  737*6=4038\n","81\n","wrong  : 737*6=4038\n","correct: 737*6=4422\n","outputs(x):  488*8=384\n","578\n","wrong  : 488*8=384\n","correct: 488*8=3904\n","outputs(x):  318*9=2852\n","77\n","wrong  : 318*9=2852\n","correct: 318*9=2862\n","outputs(x):  712*3=213\n","842\n","wrong  : 712*3=213\n","correct: 712*3=2136\n","outputs(x):  652*7=4574\n","58\n","wrong  : 652*7=4574\n","correct: 652*7=4564\n","outputs(x):  820*8=6080\n","56\n","wrong  : 820*8=6080\n","correct: 820*8=6560\n","outputs(x):  373*8=2964\n","20\n","wrong  : 373*8=2964\n","correct: 373*8=2984\n","outputs(x):  905*9=8105\n","59\n","wrong  : 905*9=8105\n","correct: 905*9=8145\n","outputs(x):  740*2=140\n","564\n","wrong  : 740*2=140\n","correct: 740*2=1480\n","outputs(x):  418*9=3722\n","67\n","wrong  : 418*9=3722\n","correct: 418*9=3762\n","outputs(x):  183*9=1677\n","62\n","wrong  : 183*9=1677\n","correct: 183*9=1647\n"," 75% 18/24 [00:01<00:00, 16.47it/s]outputs(x):  374*2=694\n","588\n","wrong  : 374*2=694\n","correct: 374*2=748\n","outputs(x):  116*9=1944\n","79\n","wrong  : 116*9=1944\n","correct: 116*9=1044\n","outputs(x):  109*9=1881\n","63\n","wrong  : 109*9=1881\n","correct: 109*9=981\n","outputs(x):  608*9=5440\n","69\n","wrong  : 608*9=5440\n","correct: 608*9=5472\n","outputs(x):  548*7=3368\n","55\n","wrong  : 548*7=3368\n","correct: 548*7=3836\n","outputs(x):  846*3=2588\n","73\n","wrong  : 846*3=2588\n","correct: 846*3=2538\n","outputs(x):  391*7=2797\n","76\n","wrong  : 391*7=2797\n","correct: 391*7=2737\n","outputs(x):  479*8=3912\n","46\n","wrong  : 479*8=3912\n","correct: 479*8=3832\n","outputs(x):  672*9=6056\n","41\n","wrong  : 672*9=6056\n","correct: 672*9=6048\n","outputs(x):  656*4=2224\n","54\n","wrong  : 656*4=2224\n","correct: 656*4=2624\n","outputs(x):  841*9=7729\n","30\n","wrong  : 841*9=7729\n","correct: 841*9=7569\n","outputs(x):  133*8=104\n","813\n","wrong  : 133*8=104\n","correct: 133*8=1064\n","outputs(x):  834*3=252\n","287\n","wrong  : 834*3=252\n","correct: 834*3=2502\n","outputs(x):  784*9=7956\n","84\n","wrong  : 784*9=7956\n","correct: 784*9=7056\n","outputs(x):  405*9=3695\n","71\n","wrong  : 405*9=3695\n","correct: 405*9=3645\n","outputs(x):  575*7=3895\n","99\n","wrong  : 575*7=3895\n","correct: 575*7=4025\n","outputs(x):  908*9=8181\n","18\n","wrong  : 908*9=8181\n","correct: 908*9=8172\n","outputs(x):  266*8=2088\n","51\n","wrong  : 266*8=2088\n","correct: 266*8=2128\n","outputs(x):  214*4=84\n","197*\n","wrong  : 214*4=84\n","correct: 214*4=856\n","outputs(x):  989*7=6893\n","93\n","wrong  : 989*7=6893\n","correct: 989*7=6923\n","outputs(x):  748*9=672\n","78*\n","wrong  : 748*9=672\n","correct: 748*9=6732\n","outputs(x):  665*3=195\n","128\n","wrong  : 665*3=195\n","correct: 665*3=1995\n","outputs(x):  258*7=1726\n","60\n","wrong  : 258*7=1726\n","correct: 258*7=1806\n","outputs(x):  577*9=513\n","268\n","wrong  : 577*9=513\n","correct: 577*9=5193\n","outputs(x):  423*9=3987\n","70\n","wrong  : 423*9=3987\n","correct: 423*9=3807\n","outputs(x):  444*7=3068\n","85\n","wrong  : 444*7=3068\n","correct: 444*7=3108\n","outputs(x):  230*9=2080\n","56\n","wrong  : 230*9=2080\n","correct: 230*9=2070\n","outputs(x):  508*9=4522\n","72\n","wrong  : 508*9=4522\n","correct: 508*9=4572\n","outputs(x):  380*8=3120\n","59\n","wrong  : 380*8=3120\n","correct: 380*8=3040\n","outputs(x):  323*9=2997\n","26\n","wrong  : 323*9=2997\n","correct: 323*9=2907\n","outputs(x):  447*8=376\n","635\n","wrong  : 447*8=376\n","correct: 447*8=3576\n","outputs(x):  100*5=520\n","961\n","wrong  : 100*5=520\n","correct: 100*5=500\n","outputs(x):  288*3=894\n","147\n","wrong  : 288*3=894\n","correct: 288*3=864\n","outputs(x):  239*4=916\n","134\n","wrong  : 239*4=916\n","correct: 239*4=956\n","outputs(x):  342*7=2424\n","62\n","wrong  : 342*7=2424\n","correct: 342*7=2394\n","outputs(x):  505*7=3555\n","56\n","wrong  : 505*7=3555\n","correct: 505*7=3535\n","outputs(x):  310*4=1200\n","71\n","wrong  : 310*4=1200\n","correct: 310*4=1240\n","outputs(x):  156*3=498\n","968\n","wrong  : 156*3=498\n","correct: 156*3=468\n","outputs(x):  491*3=1477\n","13\n","wrong  : 491*3=1477\n","correct: 491*3=1473\n","outputs(x):  489*3=1497\n","30\n","wrong  : 489*3=1497\n","correct: 489*3=1467\n","outputs(x):  842*7=5768\n","84\n","wrong  : 842*7=5768\n","correct: 842*7=5894\n","outputs(x):  469*9=4041\n","82\n","wrong  : 469*9=4041\n","correct: 469*9=4221\n","outputs(x):  384*9=3906\n","82\n","wrong  : 384*9=3906\n","correct: 384*9=3456\n","outputs(x):  760*8=6160\n","34\n","wrong  : 760*8=6160\n","correct: 760*8=6080\n","outputs(x):  322*8=256\n","791\n","wrong  : 322*8=256\n","correct: 322*8=2576\n","outputs(x):  718*3=2134\n","86\n","wrong  : 718*3=2134\n","correct: 718*3=2154\n","outputs(x):  565*7=3995\n","72\n","wrong  : 565*7=3995\n","correct: 565*7=3955\n","outputs(x):  813*8=6584\n","72\n","wrong  : 813*8=6584\n","correct: 813*8=6504\n","outputs(x):  322*6=1992\n","33\n","wrong  : 322*6=1992\n","correct: 322*6=1932\n","outputs(x):  620*8=4860\n","98\n","wrong  : 620*8=4860\n","correct: 620*8=4960\n","outputs(x):  839*7=5733\n","51\n","wrong  : 839*7=5733\n","correct: 839*7=5873\n","outputs(x):  215*9=1125\n","34\n","wrong  : 215*9=1125\n","correct: 215*9=1935\n","outputs(x):  619*7=493\n","507\n","wrong  : 619*7=493\n","correct: 619*7=4333\n","outputs(x):  548*9=4931\n","12\n","wrong  : 548*9=4931\n","correct: 548*9=4932\n","outputs(x):  172*4=708\n","328\n","wrong  : 172*4=708\n","correct: 172*4=688\n","outputs(x):  733*6=438\n","295\n","wrong  : 733*6=438\n","correct: 733*6=4398\n","outputs(x):  453*9=4977\n","73\n","wrong  : 453*9=4977\n","correct: 453*9=4077\n","outputs(x):  921*9=8298\n","68\n","wrong  : 921*9=8298\n","correct: 921*9=8289\n","outputs(x):  611*3=1863\n","12\n","wrong  : 611*3=1863\n","correct: 611*3=1833\n","outputs(x):  857*8=7616\n","21\n","wrong  : 857*8=7616\n","correct: 857*8=6856\n","outputs(x):  742*8=5736\n","57\n","wrong  : 742*8=5736\n","correct: 742*8=5936\n","outputs(x):  697*9=6279\n","53\n","wrong  : 697*9=6279\n","correct: 697*9=6273\n","outputs(x):  628*9=5632\n","90\n","wrong  : 628*9=5632\n","correct: 628*9=5652\n","outputs(x):  904*7=6352\n","62\n","wrong  : 904*7=6352\n","correct: 904*7=6328\n","outputs(x):  177*3=521\n","356\n","wrong  : 177*3=521\n","correct: 177*3=531\n","outputs(x):  734*9=6866\n","30\n","wrong  : 734*9=6866\n","correct: 734*9=6606\n","outputs(x):  687*7=4732\n","60\n","wrong  : 687*7=4732\n","correct: 687*7=4809\n","outputs(x):  521*7=3177\n","4*\n","wrong  : 521*7=3177\n","correct: 521*7=3647\n","outputs(x):  194*9=1716\n","48\n","wrong  : 194*9=1716\n","correct: 194*9=1746\n","outputs(x):  566*5=280\n","837\n","wrong  : 566*5=280\n","correct: 566*5=2830\n","outputs(x):  119*4=76\n","987*\n","wrong  : 119*4=76\n","correct: 119*4=476\n","outputs(x):  955*4=3980\n","46\n","wrong  : 955*4=3980\n","correct: 955*4=3820\n","outputs(x):  718*5=3500\n","8*\n","wrong  : 718*5=3500\n","correct: 718*5=3590\n","outputs(x):  380*6=2080\n","96\n","wrong  : 380*6=2080\n","correct: 380*6=2280\n","outputs(x):  258*3=755\n","530\n","wrong  : 258*3=755\n","correct: 258*3=774\n","outputs(x):  152*8=1200\n","58\n","wrong  : 152*8=1200\n","correct: 152*8=1216\n","outputs(x):  442*6=2052\n","29\n","wrong  : 442*6=2052\n","correct: 442*6=2652\n","outputs(x):  430*7=3710\n","58\n","wrong  : 430*7=3710\n","correct: 430*7=3010\n","outputs(x):  575*5=2375\n","22\n","wrong  : 575*5=2375\n","correct: 575*5=2875\n","outputs(x):  426*4=1104\n","53\n","wrong  : 426*4=1104\n","correct: 426*4=1704\n","outputs(x):  719*9=6453\n","86\n","wrong  : 719*9=6453\n","correct: 719*9=6471\n","outputs(x):  233*9=2997\n","26\n","wrong  : 233*9=2997\n","correct: 233*9=2097\n","outputs(x):  741*9=6729\n","51\n","wrong  : 741*9=6729\n","correct: 741*9=6669\n","outputs(x):  379*7=2793\n","79\n","wrong  : 379*7=2793\n","correct: 379*7=2653\n","outputs(x):  268*7=1176\n","60\n","wrong  : 268*7=1176\n","correct: 268*7=1876\n","outputs(x):  961*3=2983\n","55\n","wrong  : 961*3=2983\n","correct: 961*3=2883\n","outputs(x):  648*3=1404\n","33\n","wrong  : 648*3=1404\n","correct: 648*3=1944\n","outputs(x):  274*4=1196\n","67\n","wrong  : 274*4=1196\n","correct: 274*4=1096\n","outputs(x):  155*5=75\n","187*\n","wrong  : 155*5=75\n","correct: 155*5=775\n","outputs(x):  311*7=217\n","509\n","wrong  : 311*7=217\n","correct: 311*7=2177\n","outputs(x):  686*8=5348\n","46\n","wrong  : 686*8=5348\n","correct: 686*8=5488\n","outputs(x):  427*9=3853\n","87\n","wrong  : 427*9=3853\n","correct: 427*9=3843\n","outputs(x):  813*7=5621\n","20\n","wrong  : 813*7=5621\n","correct: 813*7=5691\n","outputs(x):  118*3=54\n","866*\n","wrong  : 118*3=54\n","correct: 118*3=354\n","outputs(x):  772*3=2116\n","52\n","wrong  : 772*3=2116\n","correct: 772*3=2316\n","outputs(x):  231*9=209\n","768\n","wrong  : 231*9=209\n","correct: 231*9=2079\n","outputs(x):  877*8=7816\n","87\n","wrong  : 877*8=7816\n","correct: 877*8=7016\n","outputs(x):  922*7=6944\n","43\n","wrong  : 922*7=6944\n","correct: 922*7=6454\n"," 83% 20/24 [00:01<00:00, 16.70it/s]outputs(x):  109*4=676\n","492\n","wrong  : 109*4=676\n","correct: 109*4=436\n","outputs(x):  492*7=3404\n","71\n","wrong  : 492*7=3404\n","correct: 492*7=3444\n","outputs(x):  148*6=84\n","540*\n","wrong  : 148*6=84\n","correct: 148*6=888\n","outputs(x):  624*8=5072\n","62\n","wrong  : 624*8=5072\n","correct: 624*8=4992\n","outputs(x):  169*7=1123\n","68\n","wrong  : 169*7=1123\n","correct: 169*7=1183\n","outputs(x):  264*3=78\n","619*\n","wrong  : 264*3=78\n","correct: 264*3=792\n","outputs(x):  196*5=480\n","520\n","wrong  : 196*5=480\n","correct: 196*5=980\n","outputs(x):  778*6=468\n","598\n","wrong  : 778*6=468\n","correct: 778*6=4668\n","outputs(x):  828*3=24\n","839*\n","wrong  : 828*3=24\n","correct: 828*3=2484\n","outputs(x):  855*8=7640\n","53\n","wrong  : 855*8=7640\n","correct: 855*8=6840\n","outputs(x):  789*4=3556\n","48\n","wrong  : 789*4=3556\n","correct: 789*4=3156\n","outputs(x):  994*7=6668\n","83\n","wrong  : 994*7=6668\n","correct: 994*7=6958\n","outputs(x):  608*4=2464\n","98\n","wrong  : 608*4=2464\n","correct: 608*4=2432\n","outputs(x):  975*7=6055\n","41\n","wrong  : 975*7=6055\n","correct: 975*7=6825\n","outputs(x):  876*4=3464\n","55\n","wrong  : 876*4=3464\n","correct: 876*4=3504\n","outputs(x):  726*3=2188\n","48\n","wrong  : 726*3=2188\n","correct: 726*3=2178\n","outputs(x):  466*8=3768\n","20\n","wrong  : 466*8=3768\n","correct: 466*8=3728\n","outputs(x):  720*7=5050\n","30\n","wrong  : 720*7=5050\n","correct: 720*7=5040\n","outputs(x):  924*6=5564\n","95\n","wrong  : 924*6=5564\n","correct: 924*6=5544\n","outputs(x):  287*7=1729\n","42\n","wrong  : 287*7=1729\n","correct: 287*7=2009\n","outputs(x):  759*6=4574\n","72\n","wrong  : 759*6=4574\n","correct: 759*6=4554\n","outputs(x):  176*2=34\n","567*\n","wrong  : 176*2=34\n","correct: 176*2=352\n","outputs(x):  639*9=5661\n","18\n","wrong  : 639*9=5661\n","correct: 639*9=5751\n","outputs(x):  933*8=7448\n","37\n","wrong  : 933*8=7448\n","correct: 933*8=7464\n","outputs(x):  675*8=5360\n","32\n","wrong  : 675*8=5360\n","correct: 675*8=5400\n","outputs(x):  339*7=2393\n","79\n","wrong  : 339*7=2393\n","correct: 339*7=2373\n","outputs(x):  529*8=4152\n","80\n","wrong  : 529*8=4152\n","correct: 529*8=4232\n","outputs(x):  518*4=2872\n","59\n","wrong  : 518*4=2872\n","correct: 518*4=2072\n","outputs(x):  255*3=755\n","530\n","wrong  : 255*3=755\n","correct: 255*3=765\n","outputs(x):  784*7=5278\n","85\n","wrong  : 784*7=5278\n","correct: 784*7=5488\n","outputs(x):  364*9=3116\n","63\n","wrong  : 364*9=3116\n","correct: 364*9=3276\n","outputs(x):  711*7=5397\n","19\n","wrong  : 711*7=5397\n","correct: 711*7=4977\n","outputs(x):  393*7=2731\n","79\n","wrong  : 393*7=2731\n","correct: 393*7=2751\n","outputs(x):  366*7=2522\n","34\n","wrong  : 366*7=2522\n","correct: 366*7=2562\n","outputs(x):  128*3=374\n","871\n","wrong  : 128*3=374\n","correct: 128*3=384\n","outputs(x):  579*6=3434\n","99\n","wrong  : 579*6=3434\n","correct: 579*6=3474\n","outputs(x):  176*9=144\n","767\n","wrong  : 176*9=144\n","correct: 176*9=1584\n","outputs(x):  849*9=8541\n","91\n","wrong  : 849*9=8541\n","correct: 849*9=7641\n","outputs(x):  636*6=3756\n","85\n","wrong  : 636*6=3756\n","correct: 636*6=3816\n","outputs(x):  205*3=621\n","454\n","wrong  : 205*3=621\n","correct: 205*3=615\n","outputs(x):  669*9=6921\n","27\n","wrong  : 669*9=6921\n","correct: 669*9=6021\n","outputs(x):  860*7=5360\n","96\n","wrong  : 860*7=5360\n","correct: 860*7=6020\n","outputs(x):  936*6=5676\n","42\n","wrong  : 936*6=5676\n","correct: 936*6=5616\n","outputs(x):  259*7=1113\n","45\n","wrong  : 259*7=1113\n","correct: 259*7=1813\n","outputs(x):  170*3=520\n","464\n","wrong  : 170*3=520\n","correct: 170*3=510\n","outputs(x):  279*7=1463\n","91\n","wrong  : 279*7=1463\n","correct: 279*7=1953\n","outputs(x):  379*6=2234\n","97\n","wrong  : 379*6=2234\n","correct: 379*6=2274\n","outputs(x):  300*7=2170\n","82\n","wrong  : 300*7=2170\n","correct: 300*7=2100\n","outputs(x):  407*9=4033\n","81\n","wrong  : 407*9=4033\n","correct: 407*9=3663\n","outputs(x):  198*9=1792\n","44\n","wrong  : 198*9=1792\n","correct: 198*9=1782\n","outputs(x):  428*4=1192\n","38\n","wrong  : 428*4=1192\n","correct: 428*4=1712\n","outputs(x):  659*3=1971\n","35\n","wrong  : 659*3=1971\n","correct: 659*3=1977\n","outputs(x):  701*7=4207\n","82\n","wrong  : 701*7=4207\n","correct: 701*7=4907\n","outputs(x):  777*2=1754\n","16\n","wrong  : 777*2=1754\n","correct: 777*2=1554\n","outputs(x):  476*4=1864\n","69\n","wrong  : 476*4=1864\n","correct: 476*4=1904\n","outputs(x):  924*3=2862\n","37\n","wrong  : 924*3=2862\n","correct: 924*3=2772\n","outputs(x):  319*6=1834\n","39\n","wrong  : 319*6=1834\n","correct: 319*6=1914\n","outputs(x):  783*7=5201\n","16\n","wrong  : 783*7=5201\n","correct: 783*7=5481\n","outputs(x):  779*5=3395\n","72\n","wrong  : 779*5=3395\n","correct: 779*5=3895\n","outputs(x):  132*4=452\n","882\n","wrong  : 132*4=452\n","correct: 132*4=528\n","outputs(x):  441*9=3929\n","14\n","wrong  : 441*9=3929\n","correct: 441*9=3969\n","outputs(x):  565*3=1965\n","71\n","wrong  : 565*3=1965\n","correct: 565*3=1695\n","outputs(x):  494*7=3473\n","10\n","wrong  : 494*7=3473\n","correct: 494*7=3458\n","outputs(x):  488*2=96\n","901*\n","wrong  : 488*2=96\n","correct: 488*2=976\n","outputs(x):  534*3=1662\n","86\n","wrong  : 534*3=1662\n","correct: 534*3=1602\n","outputs(x):  335*2=707\n","534\n","wrong  : 335*2=707\n","correct: 335*2=670\n","outputs(x):  643*7=4711\n","66\n","wrong  : 643*7=4711\n","correct: 643*7=4501\n","outputs(x):  115*4=640\n","310\n","wrong  : 115*4=640\n","correct: 115*4=460\n","outputs(x):  923*7=6531\n","79\n","wrong  : 923*7=6531\n","correct: 923*7=6461\n","outputs(x):  477*9=4233\n","31\n","wrong  : 477*9=4233\n","correct: 477*9=4293\n","outputs(x):  466*7=3222\n","32\n","wrong  : 466*7=3222\n","correct: 466*7=3262\n","outputs(x):  275*9=245\n","549\n","wrong  : 275*9=245\n","correct: 275*9=2475\n","outputs(x):  125*7=1435\n","55\n","wrong  : 125*7=1435\n","correct: 125*7=875\n","outputs(x):  705*9=6355\n","38\n","wrong  : 705*9=6355\n","correct: 705*9=6345\n","outputs(x):  138*6=708\n","943\n","wrong  : 138*6=708\n","correct: 138*6=828\n","outputs(x):  797*7=5589\n","62\n","wrong  : 797*7=5589\n","correct: 797*7=5579\n"," 92% 22/24 [00:01<00:00, 17.09it/s]outputs(x):  96*6=3576\n","38\n","wrong  : 96*6=3576\n","correct: 96*6=576\n","outputs(x):  93*2=386\n","245\n","wrong  : 93*2=386\n","correct: 93*2=186\n","outputs(x):  91*8=4728\n","34\n","wrong  : 91*8=4728\n","correct: 91*8=728\n","outputs(x):  44*5=4720\n","39\n","wrong  : 44*5=4720\n","correct: 44*5=220\n","outputs(x):  81*7=3367\n","26\n","wrong  : 81*7=3367\n","correct: 81*7=567\n","outputs(x):  30*6=4380\n","97\n","wrong  : 30*6=4380\n","correct: 30*6=180\n","outputs(x):  14*1=414\n","308\n","wrong  : 14*1=414\n","correct: 14*1=14\n","outputs(x):  36*8=2528\n","14\n","wrong  : 36*8=2528\n","correct: 36*8=288\n","outputs(x):  28*6=4368\n","35\n","wrong  : 28*6=4368\n","correct: 28*6=168\n","outputs(x):  37*5=1185\n","74\n","wrong  : 37*5=1185\n","correct: 37*5=185\n","outputs(x):  49*7=3843\n","85\n","wrong  : 49*7=3843\n","correct: 49*7=343\n","outputs(x):  95*3=2925\n","74\n","wrong  : 95*3=2925\n","correct: 95*3=285\n","outputs(x):  46*8=7568\n","14\n","wrong  : 46*8=7568\n","correct: 46*8=368\n","outputs(x):  98*9=2682\n","45\n","wrong  : 98*9=2682\n","correct: 98*9=882\n","outputs(x):  16*4=2664\n","81\n","wrong  : 16*4=2664\n","correct: 16*4=64\n","outputs(x):  16*3=1248\n","8*\n","wrong  : 16*3=1248\n","correct: 16*3=48\n","outputs(x):  21*6=4926\n","60\n","wrong  : 21*6=4926\n","correct: 21*6=126\n","outputs(x):  43*5=2465\n","49\n","wrong  : 43*5=2465\n","correct: 43*5=215\n","outputs(x):  75*7=6055\n","41\n","wrong  : 75*7=6055\n","correct: 75*7=525\n","outputs(x):  59*9=4131\n","50\n","wrong  : 59*9=4131\n","correct: 59*9=531\n","outputs(x):  47*2=1294\n","57\n","wrong  : 47*2=1294\n","correct: 47*2=94\n","outputs(x):  14*9=1026\n","83\n","wrong  : 14*9=1026\n","correct: 14*9=126\n","outputs(x):  83*9=1677\n","62\n","wrong  : 83*9=1677\n","correct: 83*9=747\n","outputs(x):  34*7=1638\n","67\n","wrong  : 34*7=1638\n","correct: 34*7=238\n","outputs(x):  40*8=1120\n","24\n","wrong  : 40*8=1120\n","correct: 40*8=320\n","outputs(x):  24*5=3620\n","98\n","wrong  : 24*5=3620\n","correct: 24*5=120\n","outputs(x):  24*7=1498\n","44\n","wrong  : 24*7=1498\n","correct: 24*7=168\n","outputs(x):  47*4=1388\n","98\n","wrong  : 47*4=1388\n","correct: 47*4=188\n","outputs(x):  40*1=540\n","168\n","wrong  : 40*1=540\n","correct: 40*1=40\n","outputs(x):  21*5=1155\n","44\n","wrong  : 21*5=1155\n","correct: 21*5=105\n","outputs(x):  20*7=1540\n","30\n","wrong  : 20*7=1540\n","correct: 20*7=140\n","outputs(x):  97*7=2779\n","76\n","wrong  : 97*7=2779\n","correct: 97*7=679\n","outputs(x):  58*7=3396\n","29\n","wrong  : 58*7=3396\n","correct: 58*7=406\n","outputs(x):  38*7=664\n","413\n","wrong  : 38*7=664\n","correct: 38*7=266\n","outputs(x):  17*9=103\n","570\n","wrong  : 17*9=103\n","correct: 17*9=153\n","outputs(x):  71*5=1355\n","41\n","wrong  : 71*5=1355\n","correct: 71*5=355\n","outputs(x):  25*1=925\n","174\n","wrong  : 25*1=925\n","correct: 25*1=25\n","outputs(x):  55*6=3930\n","55\n","wrong  : 55*6=3930\n","correct: 55*6=330\n","outputs(x):  56*6=930\n","283\n","wrong  : 56*6=930\n","correct: 56*6=336\n","outputs(x):  51*7=3757\n","8*\n","wrong  : 51*7=3757\n","correct: 51*7=357\n","outputs(x):  58*1=758\n","705\n","wrong  : 58*1=758\n","correct: 58*1=58\n","outputs(x):  90*9=1710\n","58\n","wrong  : 90*9=1710\n","correct: 90*9=810\n","outputs(x):  22*1=422\n","343\n","wrong  : 22*1=422\n","correct: 22*1=22\n","outputs(x):  82*3=1146\n","60\n","wrong  : 82*3=1146\n","correct: 82*3=246\n","outputs(x):  36*1=336\n","889\n","wrong  : 36*1=336\n","correct: 36*1=36\n","outputs(x):  10*5=4050\n","44\n","wrong  : 10*5=4050\n","correct: 10*5=50\n","outputs(x):  62*7=4844\n","13\n","wrong  : 62*7=4844\n","correct: 62*7=434\n","outputs(x):  88*7=4126\n","92\n","wrong  : 88*7=4126\n","correct: 88*7=616\n","outputs(x):  13*6=3078\n","37\n","wrong  : 13*6=3078\n","correct: 13*6=78\n","outputs(x):  63*8=2744\n","93\n","wrong  : 63*8=2744\n","correct: 63*8=504\n","outputs(x):  38*6=2028\n","21\n","wrong  : 38*6=2028\n","correct: 38*6=228\n","outputs(x):  75*9=3005\n","24\n","wrong  : 75*9=3005\n","correct: 75*9=675\n","outputs(x):  82*8=7056\n","23\n","wrong  : 82*8=7056\n","correct: 82*8=656\n","outputs(x):  93*8=3144\n","23\n","wrong  : 93*8=3144\n","correct: 93*8=744\n","outputs(x):  67*8=296\n","87*\n","wrong  : 67*8=296\n","correct: 67*8=536\n","outputs(x):  49*8=5992\n","89\n","wrong  : 49*8=5992\n","correct: 49*8=392\n","outputs(x):  22*9=2898\n","94\n","wrong  : 22*9=2898\n","correct: 22*9=198\n","outputs(x):  52*7=5264\n","54\n","wrong  : 52*7=5264\n","correct: 52*7=364\n","outputs(x):  41*3=723\n","442\n","wrong  : 41*3=723\n","correct: 41*3=123\n","outputs(x):  35*9=205\n","249\n","wrong  : 35*9=205\n","correct: 35*9=315\n","outputs(x):  78*7=3346\n","50\n","wrong  : 78*7=3346\n","correct: 78*7=546\n","outputs(x):  14*7=1498\n","44\n","wrong  : 14*7=1498\n","correct: 14*7=98\n","outputs(x):  50*9=4950\n","14\n","wrong  : 50*9=4950\n","correct: 50*9=450\n","outputs(x):  60*1=360\n","867\n","wrong  : 60*1=360\n","correct: 60*1=60\n","outputs(x):  19*8=4152\n","80\n","wrong  : 19*8=4152\n","correct: 19*8=152\n","outputs(x):  35*6=810\n","17*\n","wrong  : 35*6=810\n","correct: 35*6=210\n","outputs(x):  59*3=1377\n","26\n","wrong  : 59*3=1377\n","correct: 59*3=177\n","outputs(x):  87*3=1131\n","15\n","wrong  : 87*3=1131\n","correct: 87*3=261\n","outputs(x):  96*4=2784\n","61\n","wrong  : 96*4=2784\n","correct: 96*4=384\n","outputs(x):  45*4=2980\n","20\n","wrong  : 45*4=2980\n","correct: 45*4=180\n","outputs(x):  50*1=750\n","872\n","wrong  : 50*1=750\n","correct: 50*1=50\n","outputs(x):  16*1=516\n","288\n","wrong  : 16*1=516\n","correct: 16*1=16\n","outputs(x):  55*5=1275\n","22\n","wrong  : 55*5=1275\n","correct: 55*5=275\n","outputs(x):  68*1=568\n","19*\n","wrong  : 68*1=568\n","correct: 68*1=68\n","outputs(x):  37*2=274\n","30*\n","wrong  : 37*2=274\n","correct: 37*2=74\n","outputs(x):  94*1=694\n","611\n","wrong  : 94*1=694\n","correct: 94*1=94\n","outputs(x):  13*7=2271\n","34\n","wrong  : 13*7=2271\n","correct: 13*7=91\n","outputs(x):  92*3=2676\n","37\n","wrong  : 92*3=2676\n","correct: 92*3=276\n","outputs(x):  54*7=1188\n","48\n","wrong  : 54*7=1188\n","correct: 54*7=378\n","outputs(x):  41*5=705\n","609\n","wrong  : 41*5=705\n","correct: 41*5=205\n","outputs(x):  71*7=3397\n","19\n","wrong  : 71*7=3397\n","correct: 71*7=497\n","outputs(x):  62*2=924\n","493\n","wrong  : 62*2=924\n","correct: 62*2=124\n","outputs(x):  27*3=2481\n","12\n","wrong  : 27*3=2481\n","correct: 27*3=81\n","outputs(x):  21*3=1863\n","12\n","wrong  : 21*3=1863\n","correct: 21*3=63\n","outputs(x):  23*2=1246\n","3*\n","wrong  : 23*2=1246\n","correct: 23*2=46\n","outputs(x):  27*7=1659\n","79\n","wrong  : 27*7=1659\n","correct: 27*7=189\n","outputs(x):  64*2=928\n","713\n","wrong  : 64*2=928\n","correct: 64*2=128\n","outputs(x):  87*1=287\n","951\n","wrong  : 87*1=287\n","correct: 87*1=87\n","outputs(x):  28*4=2912\n","49\n","wrong  : 28*4=2912\n","correct: 28*4=112\n","outputs(x):  36*2=872\n","428\n","wrong  : 36*2=872\n","correct: 36*2=72\n","outputs(x):  55*7=3885\n","9*\n","wrong  : 55*7=3885\n","correct: 55*7=385\n","outputs(x):  36*5=2180\n","80\n","wrong  : 36*5=2180\n","correct: 36*5=180\n","outputs(x):  81*9=2449\n","55\n","wrong  : 81*9=2449\n","correct: 81*9=729\n","outputs(x):  72*7=2304\n","28\n","wrong  : 72*7=2304\n","correct: 72*7=504\n","outputs(x):  61*3=1383\n","34\n","wrong  : 61*3=1383\n","correct: 61*3=183\n","outputs(x):  18*5=1090\n","95\n","wrong  : 18*5=1090\n","correct: 18*5=90\n","outputs(x):  74*4=3496\n","6*\n","wrong  : 74*4=3496\n","correct: 74*4=296\n","outputs(x):  99*3=1497\n","30\n","wrong  : 99*3=1497\n","correct: 99*3=297\n","outputs(x):  38*3=1914\n","54\n","wrong  : 38*3=1914\n","correct: 38*3=114\n","outputs(x):  48*3=1364\n","11\n","wrong  : 48*3=1364\n","correct: 48*3=144\n","outputs(x):  41*7=3787\n","85\n","wrong  : 41*7=3787\n","correct: 41*7=287\n","outputs(x):  62*3=1386\n","97\n","wrong  : 62*3=1386\n","correct: 62*3=186\n","outputs(x):  78*4=1192\n","38\n","wrong  : 78*4=1192\n","correct: 78*4=312\n","outputs(x):  58*5=1790\n","4*\n","wrong  : 58*5=1790\n","correct: 58*5=290\n","outputs(x):  78*5=4890\n","37\n","wrong  : 78*5=4890\n","correct: 78*5=390\n","outputs(x):  57*7=2559\n","72\n","wrong  : 57*7=2559\n","correct: 57*7=399\n","outputs(x):  69*5=845\n","539\n","wrong  : 69*5=845\n","correct: 69*5=345\n","outputs(x):  63*4=652\n","132\n","wrong  : 63*4=652\n","correct: 63*4=252\n","outputs(x):  58*2=516\n","277\n","wrong  : 58*2=516\n","correct: 58*2=116\n","outputs(x):  94*3=582\n","959\n","wrong  : 94*3=582\n","correct: 94*3=282\n","outputs(x):  99*7=5523\n","45\n","wrong  : 99*7=5523\n","correct: 99*7=693\n","outputs(x):  62*8=5296\n","84\n","wrong  : 62*8=5296\n","correct: 62*8=496\n","outputs(x):  39*4=3756\n","71\n","wrong  : 39*4=3756\n","correct: 39*4=156\n","outputs(x):  25*5=625\n","200\n","wrong  : 25*5=625\n","correct: 25*5=125\n","outputs(x):  69*2=1738\n","59\n","wrong  : 69*2=1738\n","correct: 69*2=138\n","outputs(x):  45*5=2725\n","40\n","wrong  : 45*5=2725\n","correct: 45*5=225\n","outputs(x):  33*1=333\n","690\n","wrong  : 33*1=333\n","correct: 33*1=33\n","outputs(x):  60*7=1120\n","46\n","wrong  : 60*7=1120\n","correct: 60*7=420\n","outputs(x):  12*6=1272\n","96\n","wrong  : 12*6=1272\n","correct: 12*6=72\n","outputs(x):  27*8=5096\n","84\n","wrong  : 27*8=5096\n","correct: 27*8=216\n","outputs(x):  55*1=555\n","41*\n","wrong  : 55*1=555\n","correct: 55*1=55\n","outputs(x):  39*7=3773\n","50\n","wrong  : 39*7=3773\n","correct: 39*7=273\n","outputs(x):  50*2=1100\n","89\n","wrong  : 50*2=1100\n","correct: 50*2=100\n","outputs(x):  96*2=1192\n","44\n","wrong  : 96*2=1192\n","correct: 96*2=192\n","outputs(x):  22*4=3688\n","89\n","wrong  : 22*4=3688\n","correct: 22*4=88\n","outputs(x):  86*4=1144\n","67\n","wrong  : 86*4=1144\n","correct: 86*4=344\n","outputs(x):  50*7=5950\n","69\n","wrong  : 50*7=5950\n","correct: 50*7=350\n","outputs(x):  73*2=1746\n","87\n","wrong  : 73*2=1746\n","correct: 73*2=146\n","outputs(x):  43*3=1029\n","13\n","wrong  : 43*3=1029\n","correct: 43*3=129\n","outputs(x):  48*4=2992\n","77\n","wrong  : 48*4=2992\n","correct: 48*4=192\n","outputs(x):  12*4=848\n","731\n","wrong  : 12*4=848\n","correct: 12*4=48\n","outputs(x):  78*3=1134\n","86\n","wrong  : 78*3=1134\n","correct: 78*3=234\n","outputs(x):  39*3=927\n","305\n","wrong  : 39*3=927\n","correct: 39*3=117\n","outputs(x):  28*5=4140\n","31\n","wrong  : 28*5=4140\n","correct: 28*5=140\n","outputs(x):  24*8=992\n","831\n","wrong  : 24*8=992\n","correct: 24*8=192\n","outputs(x):  40*4=2920\n","12\n","wrong  : 40*4=2920\n","correct: 40*4=160\n","outputs(x):  32*6=1992\n","33\n","wrong  : 32*6=1992\n","correct: 32*6=192\n","outputs(x):  48*5=4740\n","91\n","wrong  : 48*5=4740\n","correct: 48*5=240\n","outputs(x):  32*1=732\n","9*2\n","wrong  : 32*1=732\n","correct: 32*1=32\n","outputs(x):  65*9=1485\n","98\n","wrong  : 65*9=1485\n","correct: 65*9=585\n","outputs(x):  13*3=639\n","220\n","wrong  : 13*3=639\n","correct: 13*3=39\n","outputs(x):  58*3=1374\n","11\n","wrong  : 58*3=1374\n","correct: 58*3=174\n","outputs(x):  52*6=3312\n","32\n","wrong  : 52*6=3312\n","correct: 52*6=312\n","outputs(x):  36*6=5676\n","42\n","wrong  : 36*6=5676\n","correct: 36*6=216\n","outputs(x):  80*8=600\n","263\n","wrong  : 80*8=600\n","correct: 80*8=640\n","outputs(x):  46*1=346\n","411\n","wrong  : 46*1=346\n","correct: 46*1=46\n","outputs(x):  54*6=924\n","216\n","wrong  : 54*6=924\n","correct: 54*6=324\n","outputs(x):  84*1=484\n","945\n","wrong  : 84*1=484\n","correct: 84*1=84\n","outputs(x):  30*1=830\n","273\n","wrong  : 30*1=830\n","correct: 30*1=30\n","outputs(x):  59*4=3436\n","90\n","wrong  : 59*4=3436\n","correct: 59*4=236\n","outputs(x):  95*1=795\n","533\n","wrong  : 95*1=795\n","correct: 95*1=95\n","outputs(x):  89*7=4123\n","68\n","wrong  : 89*7=4123\n","correct: 89*7=623\n","outputs(x):  56*1=856\n","659\n","wrong  : 56*1=856\n","correct: 56*1=56\n","outputs(x):  72*2=4\n","949*8\n","wrong  : 72*2=4\n","correct: 72*2=144\n","outputs(x):  45*3=435\n","690\n","wrong  : 45*3=435\n","correct: 45*3=135\n","outputs(x):  59*6=1514\n","1*\n","wrong  : 59*6=1514\n","correct: 59*6=354\n","outputs(x):  72*4=2288\n","69\n","wrong  : 72*4=2288\n","correct: 72*4=288\n","outputs(x):  46*6=876\n","66*\n","wrong  : 46*6=876\n","correct: 46*6=276\n","outputs(x):  92*2=384\n","369\n","wrong  : 92*2=384\n","correct: 92*2=184\n","outputs(x):  14*3=1842\n","12\n","wrong  : 14*3=1842\n","correct: 14*3=42\n","outputs(x):  97*3=1191\n","92\n","wrong  : 97*3=1191\n","correct: 97*3=291\n","outputs(x):  44*1=144\n","495\n","wrong  : 44*1=144\n","correct: 44*1=44\n","outputs(x):  12*3=1596\n","47\n","wrong  : 12*3=1596\n","correct: 12*3=36\n","outputs(x):  53*9=1377\n","26\n","wrong  : 53*9=1377\n","correct: 53*9=477\n","outputs(x):  77*3=1431\n","1*\n","wrong  : 77*3=1431\n","correct: 77*3=231\n","outputs(x):  23*6=4938\n","14\n","wrong  : 23*6=4938\n","correct: 23*6=138\n","outputs(x):  81*2=1562\n","75\n","wrong  : 81*2=1562\n","correct: 81*2=162\n","outputs(x):  18*4=1272\n","96\n","wrong  : 18*4=1272\n","correct: 18*4=72\n","outputs(x):  55*4=3420\n","0*\n","wrong  : 55*4=3420\n","correct: 55*4=220\n","outputs(x):  52*5=2250\n","58\n","wrong  : 52*5=2250\n","correct: 52*5=260\n","outputs(x):  61*6=3966\n","20\n","wrong  : 61*6=3966\n","correct: 61*6=366\n","outputs(x):  22*6=1392\n","28\n","wrong  : 22*6=1392\n","correct: 22*6=132\n","outputs(x):  91*4=2764\n","20\n","wrong  : 91*4=2764\n","correct: 91*4=364\n","outputs(x):  86*9=3434\n","85\n","wrong  : 86*9=3434\n","correct: 86*9=774\n","outputs(x):  32*9=2088\n","51\n","wrong  : 32*9=2088\n","correct: 32*9=288\n","outputs(x):  25*2=450\n","653\n","wrong  : 25*2=450\n","correct: 25*2=50\n","outputs(x):  76*4=2704\n","61\n","wrong  : 76*4=2704\n","correct: 76*4=304\n","outputs(x):  81*3=1143\n","15\n","wrong  : 81*3=1143\n","correct: 81*3=243\n","outputs(x):  14*5=1070\n","38\n","wrong  : 14*5=1070\n","correct: 14*5=70\n","outputs(x):  85*3=2655\n","68\n","wrong  : 85*3=2655\n","correct: 85*3=255\n","outputs(x):  98*6=5388\n","45\n","wrong  : 98*6=5388\n","correct: 98*6=588\n","outputs(x):  98*2=1196\n","67\n","wrong  : 98*2=1196\n","correct: 98*2=196\n","outputs(x):  29*8=1112\n","63\n","wrong  : 29*8=1112\n","correct: 29*8=232\n","outputs(x):  14*2=1888\n","34\n","wrong  : 14*2=1888\n","correct: 14*2=28\n","outputs(x):  71*1=871\n","113\n","wrong  : 71*1=871\n","correct: 71*1=71\n","outputs(x):  17*6=3702\n","80\n","wrong  : 17*6=3702\n","correct: 17*6=102\n","outputs(x):  45*8=760\n","779\n","wrong  : 45*8=760\n","correct: 45*8=360\n","outputs(x):  90*6=1140\n","14\n","wrong  : 90*6=1140\n","correct: 90*6=540\n","outputs(x):  70*5=4850\n","40\n","wrong  : 70*5=4850\n","correct: 70*5=350\n","outputs(x):  57*1=557\n","557\n","wrong  : 57*1=557\n","correct: 57*1=57\n","outputs(x):  64*8=1152\n","89\n","wrong  : 64*8=1152\n","correct: 64*8=512\n","outputs(x):  87*6=4122\n","21\n","wrong  : 87*6=4122\n","correct: 87*6=522\n","outputs(x):  52*8=7616\n","21\n","wrong  : 52*8=7616\n","correct: 52*8=416\n","outputs(x):  73*7=1211\n","64\n","wrong  : 73*7=1211\n","correct: 73*7=511\n","outputs(x):  72*9=844\n","408\n","wrong  : 72*9=844\n","correct: 72*9=648\n","outputs(x):  23*8=2584\n","84\n","wrong  : 23*8=2584\n","correct: 23*8=184\n","outputs(x):  69*9=5521\n","92\n","wrong  : 69*9=5521\n","correct: 69*9=621\n","outputs(x):  11*7=3577\n","4*\n","wrong  : 11*7=3577\n","correct: 11*7=77\n","outputs(x):  49*6=2094\n","96\n","wrong  : 49*6=2094\n","correct: 49*6=294\n","outputs(x):  57*5=1785\n","46\n","wrong  : 57*5=1785\n","correct: 57*5=285\n","outputs(x):  79*7=3493\n","90\n","wrong  : 79*7=3493\n","correct: 79*7=553\n","outputs(x):  97*8=4776\n","55\n","wrong  : 97*8=4776\n","correct: 97*8=776\n","outputs(x):  26*4=3304\n","63\n","wrong  : 26*4=3304\n","correct: 26*4=104\n","outputs(x):  92*4=2288\n","69\n","wrong  : 92*4=2288\n","correct: 92*4=368\n","outputs(x):  85*6=2970\n","79\n","wrong  : 85*6=2970\n","correct: 85*6=510\n","100% 24/24 [00:01<00:00, 16.00it/s]\n","accuracy of 3000 examples: 1718/3000 (57.266666666666666%)\n","{'carry0': 73.07921381774865, 'carry1': 34.90328006728343, 'carry2': 57.14285714285714, 'carry3': 61.53846153846154, 'carry4': nan, 'carry5': nan}\n","evaluating addition from: FILE:data/bal/0_to_999_times_1_digit_train_3000.txt\n","Evaluating Addition using test data file: data/bal/0_to_999_times_1_digit_train_3000.txt\n","100% 3000/3000 [00:00<00:00, 19822.91it/s]\n","100% 25/25 [00:00<00:00, 27.65it/s]\n","accuracy of 3000 examples: 2625/3000 (87.5%)\n","{'carry0': 88.34834834834835, 'carry1': 86.47058823529412, 'carry2': 84.61538461538461, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 5000: train loss 0.0317, val loss 10.9967\n","iter 5000: loss 0.0336, time 31363.44ms, mfu 6.04%\n","saving final checkpoint to out2/multiplication_plain_0_to_999_times_1_digit_bal\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mplain_0_to_999_times_1_digit_bal\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/multiplication/runs/kii98emb\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250629_201823-kii98emb/logs\u001b[0m\n"]}],"source":["!python train.py config2/multiplication/plain/train_addition_bal.py"]},{"cell_type":"markdown","metadata":{"id":"FpllaDa5l6VM"},"source":["Reverse"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":71447,"status":"ok","timestamp":1751227309645,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"o8S9C32fl7Zx","outputId":"2b4e3949-b53d-4db7-ef69-3153405aa3e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n"," 33% 8/24 [00:00<00:00, 18.81it/s]outputs(x):  $239*9=1362$\n","$\n","wrong  : 239*9=2631\n","correct: 239*9=2151\n","outputs(x):  $457*6=2492$\n","$\n","wrong  : 457*6=2942\n","correct: 457*6=2742\n","outputs(x):  $533*8=4663$\n","$\n","wrong  : 533*8=3664\n","correct: 533*8=4264\n","outputs(x):  $119*6=415$\n","$4\n","wrong  : 119*6=514\n","correct: 119*6=714\n","outputs(x):  $525*8=0044$\n","$\n","wrong  : 525*8=4400\n","correct: 525*8=4200\n","outputs(x):  $103*4=218$\n","$1\n","wrong  : 103*4=812\n","correct: 103*4=412\n","outputs(x):  $396*8=8653$\n","$\n","wrong  : 396*8=3568\n","correct: 396*8=3168\n","outputs(x):  $529*8=2514$\n","$\n","wrong  : 529*8=4152\n","correct: 529*8=4232\n","outputs(x):  $788*7=6196$\n","$\n","wrong  : 788*7=6916\n","correct: 788*7=5516\n","outputs(x):  $220*8=0612$\n","$\n","wrong  : 220*8=2160\n","correct: 220*8=1760\n","outputs(x):  $383*9=7452$\n","$\n","wrong  : 383*9=2547\n","correct: 383*9=3447\n","outputs(x):  $627*6=2653$\n","$\n","wrong  : 627*6=3562\n","correct: 627*6=3762\n","outputs(x):  $670*9=0316$\n","$\n","wrong  : 670*9=6130\n","correct: 670*9=6030\n","outputs(x):  $386*7=2032$\n","$\n","wrong  : 386*7=2302\n","correct: 386*7=2702\n","outputs(x):  $751*6=6053$\n","$\n","wrong  : 751*6=3506\n","correct: 751*6=4506\n","outputs(x):  $971*3=3182$\n","$\n","wrong  : 971*3=2813\n","correct: 971*3=2913\n","outputs(x):  $246*9=4132$\n","$\n","wrong  : 246*9=2314\n","correct: 246*9=2214\n","outputs(x):  $959*6=4595$\n","$\n","wrong  : 959*6=5954\n","correct: 959*6=5754\n","outputs(x):  $814*8=2136$\n","$\n","wrong  : 814*8=6312\n","correct: 814*8=6512\n","outputs(x):  $248*9=2382$\n","$\n","wrong  : 248*9=2832\n","correct: 248*9=2232\n","outputs(x):  $271*7=7932$\n","$\n","wrong  : 271*7=2397\n","correct: 271*7=1897\n","outputs(x):  $456*8=8443$\n","$\n","wrong  : 456*8=3448\n","correct: 456*8=3648\n","outputs(x):  $841*9=9677$\n","$\n","wrong  : 841*9=7769\n","correct: 841*9=7569\n","outputs(x):  $755*9=5967$\n","$\n","wrong  : 755*9=7695\n","correct: 755*9=6795\n","outputs(x):  $181*7=7611$\n","$\n","wrong  : 181*7=1167\n","correct: 181*7=1267\n","outputs(x):  $374*8=2782$\n","$\n","wrong  : 374*8=2872\n","correct: 374*8=2992\n","outputs(x):  $916*3=8422$\n","$\n","wrong  : 916*3=2248\n","correct: 916*3=2748\n","outputs(x):  $309*7=3652$\n","$\n","wrong  : 309*7=2563\n","correct: 309*7=2163\n","outputs(x):  $772*8=6735$\n","$\n","wrong  : 772*8=5376\n","correct: 772*8=6176\n","outputs(x):  $788*9=2996$\n","$\n","wrong  : 788*9=6992\n","correct: 788*9=7092\n","outputs(x):  $368*8=4452$\n","$\n","wrong  : 368*8=2544\n","correct: 368*8=2944\n","outputs(x):  $473*8=4833$\n","$\n","wrong  : 473*8=3384\n","correct: 473*8=3784\n","outputs(x):  $686*8=8805$\n","$\n","wrong  : 686*8=5088\n","correct: 686*8=5488\n","outputs(x):  $892*7=4416$\n","$\n","wrong  : 892*7=6144\n","correct: 892*7=6244\n","outputs(x):  $342*7=4962$\n","$\n","wrong  : 342*7=2694\n","correct: 342*7=2394\n","outputs(x):  $644*3=2381$\n","$\n","wrong  : 644*3=1832\n","correct: 644*3=1932\n","outputs(x):  $360*6=0691$\n","$\n","wrong  : 360*6=1960\n","correct: 360*6=2160\n","outputs(x):  $371*7=7932$\n","$\n","wrong  : 371*7=2397\n","correct: 371*7=2597\n","outputs(x):  $761*8=8826$\n","$\n","wrong  : 761*8=6288\n","correct: 761*8=6088\n","outputs(x):  $153*7=1731$\n","$\n","wrong  : 153*7=1371\n","correct: 153*7=1071\n","outputs(x):  $829*4=6192$\n","$\n","wrong  : 829*4=2916\n","correct: 829*4=3316\n","outputs(x):  $732*9=8846$\n","$\n","wrong  : 732*9=6488\n","correct: 732*9=6588\n","outputs(x):  $407*9=3624$\n","$\n","wrong  : 407*9=4263\n","correct: 407*9=3663\n","outputs(x):  $713*7=1924$\n","$\n","wrong  : 713*7=4291\n","correct: 713*7=4991\n","outputs(x):  $636*6=6173$\n","$\n","wrong  : 636*6=3716\n","correct: 636*6=3816\n","outputs(x):  $907*3=1292$\n","$\n","wrong  : 907*3=2921\n","correct: 907*3=2721\n","outputs(x):  $898*7=6816$\n","$\n","wrong  : 898*7=6186\n","correct: 898*7=6286\n","outputs(x):  $711*7=7735$\n","$\n","wrong  : 711*7=5377\n","correct: 711*7=4977\n","outputs(x):  $548*9=2384$\n","$\n","wrong  : 548*9=4832\n","correct: 548*9=4932\n","outputs(x):  $974*3=2282$\n","$\n","wrong  : 974*3=2822\n","correct: 974*3=2922\n","outputs(x):  $680*8=0425$\n","$\n","wrong  : 680*8=5240\n","correct: 680*8=5440\n","outputs(x):  $595*8=0654$\n","$\n","wrong  : 595*8=4560\n","correct: 595*8=4760\n","outputs(x):  $813*7=1946$\n","$\n","wrong  : 813*7=6491\n","correct: 813*7=5691\n","outputs(x):  $903*5=5104$\n","$\n","wrong  : 903*5=4015\n","correct: 903*5=4515\n","outputs(x):  $961*3=3852$\n","$\n","wrong  : 961*3=2583\n","correct: 961*3=2883\n","outputs(x):  $960*9=0477$\n","$\n","wrong  : 960*9=7740\n","correct: 960*9=8640\n","outputs(x):  $367*7=9623$\n","$\n","wrong  : 367*7=3269\n","correct: 367*7=2569\n","outputs(x):  $888*2=6751$\n","$\n","wrong  : 888*2=1576\n","correct: 888*2=1776\n","outputs(x):  $310*9=0981$\n","$\n","wrong  : 310*9=1890\n","correct: 310*9=2790\n","outputs(x):  $768*7=6754$\n","$\n","wrong  : 768*7=4576\n","correct: 768*7=5376\n","outputs(x):  $330*7=0152$\n","$\n","wrong  : 330*7=2510\n","correct: 330*7=2310\n","outputs(x):  $144*9=6981$\n","$\n","wrong  : 144*9=1896\n","correct: 144*9=1296\n","outputs(x):  $914*7=8946$\n","$\n","wrong  : 914*7=6498\n","correct: 914*7=6398\n","outputs(x):  $121*6=626$\n","$3\n","wrong  : 121*6=626\n","correct: 121*6=726\n","outputs(x):  $356*8=8442$\n","$\n","wrong  : 356*8=2448\n","correct: 356*8=2848\n","outputs(x):  $267*3=106$\n","$5\n","wrong  : 267*3=601\n","correct: 267*3=801\n","outputs(x):  $100*7=008$\n","$5\n","wrong  : 100*7=800\n","correct: 100*7=700\n","outputs(x):  $376*9=4823$\n","$\n","wrong  : 376*9=3284\n","correct: 376*9=3384\n"," 42% 10/24 [00:00<00:00, 18.33it/s]outputs(x):  $962*8=6987$\n","$\n","wrong  : 962*8=7896\n","correct: 962*8=7696\n","outputs(x):  $606*9=4585$\n","$\n","wrong  : 606*9=5854\n","correct: 606*9=5454\n","outputs(x):  $358*6=8422$\n","$\n","wrong  : 358*6=2248\n","correct: 358*6=2148\n","outputs(x):  $886*7=2085$\n","$\n","wrong  : 886*7=5802\n","correct: 886*7=6202\n","outputs(x):  $366*8=8252$\n","$\n","wrong  : 366*8=2528\n","correct: 366*8=2928\n","outputs(x):  $549*3=7471$\n","$\n","wrong  : 549*3=1747\n","correct: 549*3=1647\n","outputs(x):  $854*7=8725$\n","$\n","wrong  : 854*7=5278\n","correct: 854*7=5978\n","outputs(x):  $856*7=2975$\n","$\n","wrong  : 856*7=5792\n","correct: 856*7=5992\n","outputs(x):  $907*9=3628$\n","$\n","wrong  : 907*9=8263\n","correct: 907*9=8163\n","outputs(x):  $965*3=5952$\n","$\n","wrong  : 965*3=2595\n","correct: 965*3=2895\n","outputs(x):  $569*6=4104$\n","$\n","wrong  : 569*6=4014\n","correct: 569*6=3414\n","outputs(x):  $855*8=0467$\n","$\n","wrong  : 855*8=7640\n","correct: 855*8=6840\n","outputs(x):  $904*8=2347$\n","$\n","wrong  : 904*8=7432\n","correct: 904*8=7232\n","outputs(x):  $901*3=3021$\n","$\n","wrong  : 901*3=1203\n","correct: 901*3=2703\n","outputs(x):  $749*7=3485$\n","$\n","wrong  : 749*7=5843\n","correct: 749*7=5243\n","outputs(x):  $850*8=0067$\n","$\n","wrong  : 850*8=7600\n","correct: 850*8=6800\n","outputs(x):  $466*8=8253$\n","$\n","wrong  : 466*8=3528\n","correct: 466*8=3728\n","outputs(x):  $674*9=6624$\n","$\n","wrong  : 674*9=4266\n","correct: 674*9=6066\n","outputs(x):  $526*8=8044$\n","$\n","wrong  : 526*8=4408\n","correct: 526*8=4208\n","outputs(x):  $150*7=059$\n","$6\n","wrong  : 150*7=950\n","correct: 150*7=1050\n","outputs(x):  $313*7=1922$\n","$\n","wrong  : 313*7=2291\n","correct: 313*7=2191\n","outputs(x):  $831*7=7156$\n","$\n","wrong  : 831*7=6517\n","correct: 831*7=5817\n","outputs(x):  $422*4=8863$\n","$\n","wrong  : 422*4=3688\n","correct: 422*4=1688\n","outputs(x):  $841*7=7875$\n","$\n","wrong  : 841*7=5787\n","correct: 841*7=5887\n","outputs(x):  $136*7=2531$\n","$\n","wrong  : 136*7=1352\n","correct: 136*7=952\n","outputs(x):  $431*7=7133$\n","$\n","wrong  : 431*7=3317\n","correct: 431*7=3017\n","outputs(x):  $748*9=2356$\n","$\n","wrong  : 748*9=6532\n","correct: 748*9=6732\n","outputs(x):  $762*6=2793$\n","$\n","wrong  : 762*6=3972\n","correct: 762*6=4572\n","outputs(x):  $686*7=2044$\n","$\n","wrong  : 686*7=4402\n","correct: 686*7=4802\n","outputs(x):  $993*6=8592$\n","$\n","wrong  : 993*6=2958\n","correct: 993*6=5958\n","outputs(x):  $198*9=2861$\n","$\n","wrong  : 198*9=1682\n","correct: 198*9=1782\n","outputs(x):  $315*8=0272$\n","$\n","wrong  : 315*8=2720\n","correct: 315*8=2520\n","outputs(x):  $820*8=0676$\n","$\n","wrong  : 820*8=6760\n","correct: 820*8=6560\n","outputs(x):  $477*9=3934$\n","$\n","wrong  : 477*9=4393\n","correct: 477*9=4293\n","outputs(x):  $228*8=4261$\n","$\n","wrong  : 228*8=1624\n","correct: 228*8=1824\n","outputs(x):  $913*7=1946$\n","$\n","wrong  : 913*7=6491\n","correct: 913*7=6391\n","outputs(x):  $862*9=857$\n","$9\n","wrong  : 862*9=758\n","correct: 862*9=7758\n","outputs(x):  $219*8=2594$\n","$\n","wrong  : 219*8=4952\n","correct: 219*8=1752\n","outputs(x):  $955*4=0263$\n","$\n","wrong  : 955*4=3620\n","correct: 955*4=3820\n","outputs(x):  $789*9=1076$\n","$\n","wrong  : 789*9=6701\n","correct: 789*9=7101\n","outputs(x):  $312*8=6982$\n","$\n","wrong  : 312*8=2896\n","correct: 312*8=2496\n","outputs(x):  $723*9=7046$\n","$\n","wrong  : 723*9=6407\n","correct: 723*9=6507\n","outputs(x):  $567*5=5332$\n","$\n","wrong  : 567*5=2335\n","correct: 567*5=2835\n","outputs(x):  $381*9=9283$\n","$\n","wrong  : 381*9=3829\n","correct: 381*9=3429\n","outputs(x):  $249*7=3441$\n","$\n","wrong  : 249*7=1443\n","correct: 249*7=1743\n","outputs(x):  $306*9=4513$\n","$\n","wrong  : 306*9=3154\n","correct: 306*9=2754\n","outputs(x):  $944*8=2577$\n","$\n","wrong  : 944*8=7752\n","correct: 944*8=7552\n","outputs(x):  $639*9=1365$\n","$\n","wrong  : 639*9=5631\n","correct: 639*9=5751\n","outputs(x):  $574*9=6635$\n","$\n","wrong  : 574*9=5366\n","correct: 574*9=5166\n","outputs(x):  $395*8=0632$\n","$\n","wrong  : 395*8=2360\n","correct: 395*8=3160\n","outputs(x):  $107*4=826$\n","$4\n","wrong  : 107*4=628\n","correct: 107*4=428\n","outputs(x):  $415*7=5013$\n","$\n","wrong  : 415*7=3105\n","correct: 415*7=2905\n","outputs(x):  $629*8=2594$\n","$\n","wrong  : 629*8=4952\n","correct: 629*8=5032\n","outputs(x):  $243*5=5111$\n","$\n","wrong  : 243*5=1115\n","correct: 243*5=1215\n","outputs(x):  $701*7=7035$\n","$\n","wrong  : 701*7=5307\n","correct: 701*7=4907\n","outputs(x):  $643*7=1024$\n","$\n","wrong  : 643*7=4201\n","correct: 643*7=4501\n","outputs(x):  $296*9=4652$\n","$\n","wrong  : 296*9=2564\n","correct: 296*9=2664\n","outputs(x):  $564*8=2554$\n","$\n","wrong  : 564*8=4552\n","correct: 564*8=4512\n","outputs(x):  $679*9=1162$\n","$\n","wrong  : 679*9=2611\n","correct: 679*9=6111\n","outputs(x):  $932*8=6587$\n","$\n","wrong  : 932*8=7856\n","correct: 932*8=7456\n"," 50% 12/24 [00:00<00:00, 18.45it/s]outputs(x):  $111*4=446$\n","$3\n","wrong  : 111*4=644\n","correct: 111*4=444\n","outputs(x):  $138*3=415$\n","$8\n","wrong  : 138*3=514\n","correct: 138*3=414\n","outputs(x):  $110*1=01$\n","$4*\n","wrong  : 110*1=10\n","correct: 110*1=110\n","outputs(x):  $567*7=9604$\n","$\n","wrong  : 567*7=4069\n","correct: 567*7=3969\n","outputs(x):  $561*3=3871$\n","$\n","wrong  : 561*3=1783\n","correct: 561*3=1683\n","outputs(x):  $856*8=8467$\n","$\n","wrong  : 856*8=7648\n","correct: 856*8=6848\n","outputs(x):  $483*9=7493$\n","$\n","wrong  : 483*9=3947\n","correct: 483*9=4347\n","outputs(x):  $873*8=4856$\n","$\n","wrong  : 873*8=6584\n","correct: 873*8=6984\n","outputs(x):  $276*7=2371$\n","$\n","wrong  : 276*7=1732\n","correct: 276*7=1932\n","outputs(x):  $772*3=6112$\n","$\n","wrong  : 772*3=2116\n","correct: 772*3=2316\n","outputs(x):  $378*7=6432$\n","$\n","wrong  : 378*7=2346\n","correct: 378*7=2646\n","outputs(x):  $959*7=3166$\n","$\n","wrong  : 959*7=6613\n","correct: 959*7=6713\n","outputs(x):  $964*9=6778$\n","$\n","wrong  : 964*9=8776\n","correct: 964*9=8676\n","outputs(x):  $435*9=5124$\n","$\n","wrong  : 435*9=4215\n","correct: 435*9=3915\n","outputs(x):  $962*7=4356$\n","$\n","wrong  : 962*7=6534\n","correct: 962*7=6734\n","outputs(x):  $540*9=0694$\n","$\n","wrong  : 540*9=4960\n","correct: 540*9=4860\n","outputs(x):  $659*9=1385$\n","$\n","wrong  : 659*9=5831\n","correct: 659*9=5931\n","outputs(x):  $925*3=5742$\n","$\n","wrong  : 925*3=2475\n","correct: 925*3=2775\n","outputs(x):  $248*3=4472$\n","$\n","wrong  : 248*3=2744\n","correct: 248*3=744\n","outputs(x):  $323*9=7042$\n","$\n","wrong  : 323*9=2407\n","correct: 323*9=2907\n","outputs(x):  $751*8=8065$\n","$\n","wrong  : 751*8=5608\n","correct: 751*8=6008\n","outputs(x):  $317*3=156$\n","$5\n","wrong  : 317*3=651\n","correct: 317*3=951\n","outputs(x):  $455*6=0392$\n","$\n","wrong  : 455*6=2930\n","correct: 455*6=2730\n","outputs(x):  $404*9=6383$\n","$\n","wrong  : 404*9=3836\n","correct: 404*9=3636\n","outputs(x):  $793*8=4435$\n","$\n","wrong  : 793*8=5344\n","correct: 793*8=6344\n","outputs(x):  $813*8=4036$\n","$\n","wrong  : 813*8=6304\n","correct: 813*8=6504\n","outputs(x):  $774*7=8105$\n","$\n","wrong  : 774*7=5018\n","correct: 774*7=5418\n","outputs(x):  $787*6=2234$\n","$\n","wrong  : 787*6=4322\n","correct: 787*6=4722\n","outputs(x):  $634*9=6085$\n","$\n","wrong  : 634*9=5806\n","correct: 634*9=5706\n","outputs(x):  $609*8=2704$\n","$\n","wrong  : 609*8=4072\n","correct: 609*8=4872\n","outputs(x):  $157*9=3131$\n","$\n","wrong  : 157*9=1313\n","correct: 157*9=1413\n","outputs(x):  $648*3=4422$\n","$\n","wrong  : 648*3=2244\n","correct: 648*3=1944\n","outputs(x):  $442*6=2572$\n","$\n","wrong  : 442*6=2752\n","correct: 442*6=2652\n","outputs(x):  $597*9=3714$\n","$\n","wrong  : 597*9=4173\n","correct: 597*9=5373\n","outputs(x):  $772*9=8486$\n","$\n","wrong  : 772*9=6848\n","correct: 772*9=6948\n","outputs(x):  $963*3=9852$\n","$\n","wrong  : 963*3=2589\n","correct: 963*3=2889\n","outputs(x):  $403*4=2121$\n","$\n","wrong  : 403*4=1212\n","correct: 403*4=1612\n","outputs(x):  $129*4=619$\n","$1\n","wrong  : 129*4=916\n","correct: 129*4=516\n","outputs(x):  $797*7=9774$\n","$\n","wrong  : 797*7=4779\n","correct: 797*7=5579\n","outputs(x):  $879*9=1187$\n","$\n","wrong  : 879*9=7811\n","correct: 879*9=7911\n","outputs(x):  $911*9=9928$\n","$\n","wrong  : 911*9=8299\n","correct: 911*9=8199\n","outputs(x):  $519*9=1753$\n","$\n","wrong  : 519*9=3571\n","correct: 519*9=4671\n","outputs(x):  $137*3=115$\n","$2\n","wrong  : 137*3=511\n","correct: 137*3=411\n","outputs(x):  $330*9=0782$\n","$\n","wrong  : 330*9=2870\n","correct: 330*9=2970\n","outputs(x):  $250*7=0551$\n","$\n","wrong  : 250*7=1550\n","correct: 250*7=1750\n","outputs(x):  $839*6=4335$\n","$\n","wrong  : 839*6=5334\n","correct: 839*6=5034\n","outputs(x):  $249*5=5441$\n","$\n","wrong  : 249*5=1445\n","correct: 249*5=1245\n","outputs(x):  $237*9=3302$\n","$\n","wrong  : 237*9=2033\n","correct: 237*9=2133\n","outputs(x):  $492*7=4493$\n","$\n","wrong  : 492*7=3944\n","correct: 492*7=3444\n","outputs(x):  $891*7=7304$\n","$\n","wrong  : 891*7=4037\n","correct: 891*7=6237\n","outputs(x):  $355*8=0462$\n","$\n","wrong  : 355*8=2640\n","correct: 355*8=2840\n","outputs(x):  $528*7=6953$\n","$\n","wrong  : 528*7=3596\n","correct: 528*7=3696\n","outputs(x):  $699*7=3944$\n","$\n","wrong  : 699*7=4493\n","correct: 699*7=4893\n","outputs(x):  $119*7=3363$\n","$\n","wrong  : 119*7=3633\n","correct: 119*7=833\n","outputs(x):  $708*7=6535$\n","$\n","wrong  : 708*7=5356\n","correct: 708*7=4956\n","outputs(x):  $698*8=4874$\n","$\n","wrong  : 698*8=4784\n","correct: 698*8=5584\n","outputs(x):  $444*8=2533$\n","$\n","wrong  : 444*8=3352\n","correct: 444*8=3552\n","outputs(x):  $234*3=209$\n","$5\n","wrong  : 234*3=902\n","correct: 234*3=702\n","outputs(x):  $798*9=2897$\n","$\n","wrong  : 798*9=7982\n","correct: 798*9=7182\n","outputs(x):  $138*9=2401$\n","$\n","wrong  : 138*9=1042\n","correct: 138*9=1242\n","outputs(x):  $127*9=3461$\n","$\n","wrong  : 127*9=1643\n","correct: 127*9=1143\n","outputs(x):  $806*9=4537$\n","$\n","wrong  : 806*9=7354\n","correct: 806*9=7254\n","outputs(x):  $313*9=7132$\n","$\n","wrong  : 313*9=2317\n","correct: 313*9=2817\n","outputs(x):  $582*8=6544$\n","$\n","wrong  : 582*8=4456\n","correct: 582*8=4656\n","outputs(x):  $746*8=8616$\n","$\n","wrong  : 746*8=6168\n","correct: 746*8=5968\n","outputs(x):  $861*7=7255$\n","$\n","wrong  : 861*7=5527\n","correct: 861*7=6027\n","outputs(x):  $376*4=4041$\n","$\n","wrong  : 376*4=1404\n","correct: 376*4=1504\n"," 58% 14/24 [00:00<00:00, 18.44it/s]outputs(x):  $798*7=6896$\n","$\n","wrong  : 798*7=6986\n","correct: 798*7=5586\n","outputs(x):  $909*7=3656$\n","$\n","wrong  : 909*7=6563\n","correct: 909*7=6363\n","outputs(x):  $142*9=8711$\n","$\n","wrong  : 142*9=1178\n","correct: 142*9=1278\n","outputs(x):  $184*6=4071$\n","$\n","wrong  : 184*6=1704\n","correct: 184*6=1104\n","outputs(x):  $546*7=2293$\n","$\n","wrong  : 546*7=3922\n","correct: 546*7=3822\n","outputs(x):  $849*9=1408$\n","$\n","wrong  : 849*9=8041\n","correct: 849*9=7641\n","outputs(x):  $237*8=6961$\n","$\n","wrong  : 237*8=1696\n","correct: 237*8=1896\n","outputs(x):  $336*8=8652$\n","$\n","wrong  : 336*8=2568\n","correct: 336*8=2688\n","outputs(x):  $179*9=1151$\n","$\n","wrong  : 179*9=1511\n","correct: 179*9=1611\n","outputs(x):  $951*4=4063$\n","$\n","wrong  : 951*4=3604\n","correct: 951*4=3804\n","outputs(x):  $773*3=913$\n","$6\n","wrong  : 773*3=319\n","correct: 773*3=2319\n","outputs(x):  $819*8=2536$\n","$\n","wrong  : 819*8=6352\n","correct: 819*8=6552\n","outputs(x):  $490*7=0392$\n","$\n","wrong  : 490*7=2930\n","correct: 490*7=3430\n","outputs(x):  $742*8=6375$\n","$\n","wrong  : 742*8=5736\n","correct: 742*8=5936\n","outputs(x):  $536*9=4275$\n","$\n","wrong  : 536*9=5724\n","correct: 536*9=4824\n","outputs(x):  $840*7=0865$\n","$\n","wrong  : 840*7=5680\n","correct: 840*7=5880\n","outputs(x):  $319*8=2532$\n","$\n","wrong  : 319*8=2352\n","correct: 319*8=2552\n","outputs(x):  $174*6=446$\n","$3\n","wrong  : 174*6=644\n","correct: 174*6=1044\n","outputs(x):  $581*9=9215$\n","$\n","wrong  : 581*9=5129\n","correct: 581*9=5229\n","outputs(x):  $789*6=4354$\n","$\n","wrong  : 789*6=4534\n","correct: 789*6=4734\n","outputs(x):  $111*7=7753$\n","$\n","wrong  : 111*7=3577\n","correct: 111*7=777\n","outputs(x):  $802*9=8118$\n","$\n","wrong  : 802*9=8118\n","correct: 802*9=7218\n","outputs(x):  $939*6=4355$\n","$\n","wrong  : 939*6=5534\n","correct: 939*6=5634\n","outputs(x):  $109*9=1811$\n","$\n","wrong  : 109*9=1181\n","correct: 109*9=981\n","outputs(x):  $334*8=2782$\n","$\n","wrong  : 334*8=2872\n","correct: 334*8=2672\n","outputs(x):  $731*3=3932$\n","$\n","wrong  : 731*3=2393\n","correct: 731*3=2193\n","outputs(x):  $745*8=0676$\n","$\n","wrong  : 745*8=6760\n","correct: 745*8=5960\n","outputs(x):  $340*7=0862$\n","$\n","wrong  : 340*7=2680\n","correct: 340*7=2380\n","outputs(x):  $923*3=9632$\n","$\n","wrong  : 923*3=2369\n","correct: 923*3=2769\n","outputs(x):  $356*7=2952$\n","$\n","wrong  : 356*7=2592\n","correct: 356*7=2492\n","outputs(x):  $669*8=2515$\n","$\n","wrong  : 669*8=5152\n","correct: 669*8=5352\n","outputs(x):  $933*9=7947$\n","$\n","wrong  : 933*9=7497\n","correct: 933*9=8397\n","outputs(x):  $828*7=6946$\n","$\n","wrong  : 828*7=6496\n","correct: 828*7=5796\n","outputs(x):  $902*6=2155$\n","$\n","wrong  : 902*6=5512\n","correct: 902*6=5412\n","outputs(x):  $299*7=3943$\n","$\n","wrong  : 299*7=3493\n","correct: 299*7=2093\n","outputs(x):  $272*7=4061$\n","$\n","wrong  : 272*7=1604\n","correct: 272*7=1904\n","outputs(x):  $848*9=2357$\n","$\n","wrong  : 848*9=7532\n","correct: 848*9=7632\n","outputs(x):  $565*3=5971$\n","$\n","wrong  : 565*3=1795\n","correct: 565*3=1695\n","outputs(x):  $974*6=4484$\n","$\n","wrong  : 974*6=4844\n","correct: 974*6=5844\n","outputs(x):  $762*9=8546$\n","$\n","wrong  : 762*9=6458\n","correct: 762*9=6858\n","outputs(x):  $311*7=7732$\n","$\n","wrong  : 311*7=2377\n","correct: 311*7=2177\n","outputs(x):  $187*4=8411$\n","$\n","wrong  : 187*4=1148\n","correct: 187*4=748\n","outputs(x):  $788*6=8235$\n","$\n","wrong  : 788*6=5328\n","correct: 788*6=4728\n","outputs(x):  $949*3=7452$\n","$\n","wrong  : 949*3=2547\n","correct: 949*3=2847\n","outputs(x):  $667*7=9624$\n","$\n","wrong  : 667*7=4269\n","correct: 667*7=4669\n","outputs(x):  $774*8=2995$\n","$\n","wrong  : 774*8=5992\n","correct: 774*8=6192\n","outputs(x):  $961*7=7286$\n","$\n","wrong  : 961*7=6827\n","correct: 961*7=6727\n","outputs(x):  $750*7=0595$\n","$\n","wrong  : 750*7=5950\n","correct: 750*7=5250\n","outputs(x):  $440*8=0293$\n","$\n","wrong  : 440*8=3920\n","correct: 440*8=3520\n","outputs(x):  $364*9=6743$\n","$\n","wrong  : 364*9=3476\n","correct: 364*9=3276\n","outputs(x):  $877*8=6187$\n","$\n","wrong  : 877*8=7816\n","correct: 877*8=7016\n","outputs(x):  $376*8=8023$\n","$\n","wrong  : 376*8=3208\n","correct: 376*8=3008\n","outputs(x):  $896*7=2784$\n","$\n","wrong  : 896*7=4872\n","correct: 896*7=6272\n","outputs(x):  $531*8=8404$\n","$\n","wrong  : 531*8=4048\n","correct: 531*8=4248\n","outputs(x):  $770*7=0135$\n","$\n","wrong  : 770*7=5310\n","correct: 770*7=5390\n","outputs(x):  $935*9=5128$\n","$\n","wrong  : 935*9=8215\n","correct: 935*9=8415\n","outputs(x):  $109*8=2721$\n","$\n","wrong  : 109*8=1272\n","correct: 109*8=872\n","outputs(x):  $857*8=6566$\n","$\n","wrong  : 857*8=6656\n","correct: 857*8=6856\n","outputs(x):  $191*7=7351$\n","$\n","wrong  : 191*7=1537\n","correct: 191*7=1337\n","outputs(x):  $225*9=5222$\n","$\n","wrong  : 225*9=2225\n","correct: 225*9=2025\n","outputs(x):  $658*7=6854$\n","$\n","wrong  : 658*7=4586\n","correct: 658*7=4606\n","outputs(x):  $602*7=4164$\n","$\n","wrong  : 602*7=4614\n","correct: 602*7=4214\n","outputs(x):  $411*3=3521$\n","$\n","wrong  : 411*3=1253\n","correct: 411*3=1233\n"," 67% 16/24 [00:00<00:00, 18.75it/s]outputs(x):  $666*7=2634$\n","$\n","wrong  : 666*7=4362\n","correct: 666*7=4662\n","outputs(x):  $707*9=3626$\n","$\n","wrong  : 707*9=6263\n","correct: 707*9=6363\n","outputs(x):  $248*7=6351$\n","$\n","wrong  : 248*7=1536\n","correct: 248*7=1736\n","outputs(x):  $779*9=1196$\n","$\n","wrong  : 779*9=6911\n","correct: 779*9=7011\n","outputs(x):  $751*7=7554$\n","$\n","wrong  : 751*7=4557\n","correct: 751*7=5257\n","outputs(x):  $350*7=0592$\n","$\n","wrong  : 350*7=2950\n","correct: 350*7=2450\n","outputs(x):  $298*7=6891$\n","$\n","wrong  : 298*7=1986\n","correct: 298*7=2086\n","outputs(x):  $783*8=4835$\n","$\n","wrong  : 783*8=5384\n","correct: 783*8=6264\n","outputs(x):  $128*8=428$\n","$3\n","wrong  : 128*8=824\n","correct: 128*8=1024\n","outputs(x):  $470*9=0383$\n","$\n","wrong  : 470*9=3830\n","correct: 470*9=4230\n","outputs(x):  $723*8=4895$\n","$\n","wrong  : 723*8=5984\n","correct: 723*8=5784\n","outputs(x):  $433*6=8972$\n","$\n","wrong  : 433*6=2798\n","correct: 433*6=2598\n","outputs(x):  $741*6=6434$\n","$\n","wrong  : 741*6=4346\n","correct: 741*6=4446\n","outputs(x):  $443*8=4433$\n","$\n","wrong  : 443*8=3344\n","correct: 443*8=3544\n","outputs(x):  $459*7=3113$\n","$\n","wrong  : 459*7=3113\n","correct: 459*7=3213\n","outputs(x):  $560*9=0415$\n","$\n","wrong  : 560*9=5140\n","correct: 560*9=5040\n","outputs(x):  $341*9=9632$\n","$\n","wrong  : 341*9=2369\n","correct: 341*9=3069\n","outputs(x):  $259*9=1362$\n","$\n","wrong  : 259*9=2631\n","correct: 259*9=2331\n","outputs(x):  $866*7=2675$\n","$\n","wrong  : 866*7=5762\n","correct: 866*7=6062\n","outputs(x):  $562*5=0172$\n","$\n","wrong  : 562*5=2710\n","correct: 562*5=2810\n","outputs(x):  $496*7=2793$\n","$\n","wrong  : 496*7=3972\n","correct: 496*7=3472\n","outputs(x):  $465*8=0213$\n","$\n","wrong  : 465*8=3120\n","correct: 465*8=3720\n","outputs(x):  $943*8=4477$\n","$\n","wrong  : 943*8=7744\n","correct: 943*8=7544\n","outputs(x):  $268*7=6752$\n","$\n","wrong  : 268*7=2576\n","correct: 268*7=1876\n","outputs(x):  $549*6=4903$\n","$\n","wrong  : 549*6=3094\n","correct: 549*6=3294\n","outputs(x):  $737*3=1112$\n","$\n","wrong  : 737*3=2111\n","correct: 737*3=2211\n","outputs(x):  $775*9=5786$\n","$\n","wrong  : 775*9=6875\n","correct: 775*9=6975\n","outputs(x):  $924*7=8606$\n","$\n","wrong  : 924*7=6068\n","correct: 924*7=6468\n","outputs(x):  $238*4=2511$\n","$\n","wrong  : 238*4=1152\n","correct: 238*4=952\n","outputs(x):  $266*7=2651$\n","$\n","wrong  : 266*7=1562\n","correct: 266*7=1862\n","outputs(x):  $574*8=2764$\n","$\n","wrong  : 574*8=4672\n","correct: 574*8=4592\n","outputs(x):  $846*7=2275$\n","$\n","wrong  : 846*7=5722\n","correct: 846*7=5922\n","outputs(x):  $482*9=8344$\n","$\n","wrong  : 482*9=4438\n","correct: 482*9=4338\n","outputs(x):  $671*9=9315$\n","$\n","wrong  : 671*9=5139\n","correct: 671*9=6039\n","outputs(x):  $723*7=1615$\n","$\n","wrong  : 723*7=5161\n","correct: 723*7=5061\n","outputs(x):  $859*9=1368$\n","$\n","wrong  : 859*9=8631\n","correct: 859*9=7731\n","outputs(x):  $924*3=2742$\n","$\n","wrong  : 924*3=2472\n","correct: 924*3=2772\n","outputs(x):  $444*7=8092$\n","$\n","wrong  : 444*7=2908\n","correct: 444*7=3108\n","outputs(x):  $909*6=4563$\n","$\n","wrong  : 909*6=3654\n","correct: 909*6=5454\n","outputs(x):  $680*7=0604$\n","$\n","wrong  : 680*7=4060\n","correct: 680*7=4760\n","outputs(x):  $129*8=259$\n","$8\n","wrong  : 129*8=952\n","correct: 129*8=1032\n","outputs(x):  $389*9=1062$\n","$\n","wrong  : 389*9=2601\n","correct: 389*9=3501\n","outputs(x):  $110*9=09$\n","$45\n","wrong  : 110*9=90\n","correct: 110*9=990\n","outputs(x):  $318*9=2603$\n","$\n","wrong  : 318*9=3062\n","correct: 318*9=2862\n","outputs(x):  $379*8=2323$\n","$\n","wrong  : 379*8=3232\n","correct: 379*8=3032\n","outputs(x):  $167*9=3041$\n","$\n","wrong  : 167*9=1403\n","correct: 167*9=1503\n","outputs(x):  $282*7=4771$\n","$\n","wrong  : 282*7=1774\n","correct: 282*7=1974\n","outputs(x):  $114*8=2111$\n","$\n","wrong  : 114*8=1112\n","correct: 114*8=912\n","outputs(x):  $261*9=9412$\n","$\n","wrong  : 261*9=2149\n","correct: 261*9=2349\n","outputs(x):  $429*8=2533$\n","$\n","wrong  : 429*8=3352\n","correct: 429*8=3432\n","outputs(x):  $423*9=7043$\n","$\n","wrong  : 423*9=3407\n","correct: 423*9=3807\n","outputs(x):  $288*3=4611$\n","$\n","wrong  : 288*3=1164\n","correct: 288*3=864\n","outputs(x):  $359*6=4502$\n","$\n","wrong  : 359*6=2054\n","correct: 359*6=2154\n","outputs(x):  $431*9=9773$\n","$\n","wrong  : 431*9=3779\n","correct: 431*9=3879\n","outputs(x):  $777*2=4531$\n","$\n","wrong  : 777*2=1354\n","correct: 777*2=1554\n","outputs(x):  $461*8=8883$\n","$\n","wrong  : 461*8=3888\n","correct: 461*8=3688\n","outputs(x):  $548*3=4471$\n","$\n","wrong  : 548*3=1744\n","correct: 548*3=1644\n","outputs(x):  $686*6=6104$\n","$\n","wrong  : 686*6=4016\n","correct: 686*6=4116\n","outputs(x):  $363*8=4013$\n","$\n","wrong  : 363*8=3104\n","correct: 363*8=2904\n","outputs(x):  $259*6=4541$\n","$\n","wrong  : 259*6=1454\n","correct: 259*6=1554\n","outputs(x):  $585*7=5983$\n","$\n","wrong  : 585*7=3895\n","correct: 585*7=4095\n","outputs(x):  $158*7=6511$\n","$\n","wrong  : 158*7=1156\n","correct: 158*7=1106\n","outputs(x):  $347*7=9272$\n","$\n","wrong  : 347*7=2729\n","correct: 347*7=2429\n","outputs(x):  $107*9=3611$\n","$\n","wrong  : 107*9=1163\n","correct: 107*9=963\n","outputs(x):  $936*7=2546$\n","$\n","wrong  : 936*7=6452\n","correct: 936*7=6552\n","outputs(x):  $215*7=5041$\n","$\n","wrong  : 215*7=1405\n","correct: 215*7=1505\n","outputs(x):  $948*8=4457$\n","$\n","wrong  : 948*8=7544\n","correct: 948*8=7584\n","outputs(x):  $116*4=466$\n","$4\n","wrong  : 116*4=664\n","correct: 116*4=464\n","outputs(x):  $413*7=1923$\n","$\n","wrong  : 413*7=3291\n","correct: 413*7=2891\n","outputs(x):  $259*7=3151$\n","$\n","wrong  : 259*7=1513\n","correct: 259*7=1813\n","outputs(x):  $374*3=2241$\n","$\n","wrong  : 374*3=1422\n","correct: 374*3=1122\n","outputs(x):  $489*3=764$\n","$6\n","wrong  : 489*3=467\n","correct: 489*3=1467\n"," 75% 18/24 [00:00<00:00, 18.78it/s]outputs(x):  $143*7=109$\n","$9\n","wrong  : 143*7=901\n","correct: 143*7=1001\n","outputs(x):  $816*9=4427$\n","$\n","wrong  : 816*9=7244\n","correct: 816*9=7344\n","outputs(x):  $846*9=4177$\n","$\n","wrong  : 846*9=7714\n","correct: 846*9=7614\n","outputs(x):  $747*8=6775$\n","$\n","wrong  : 747*8=5776\n","correct: 747*8=5976\n","outputs(x):  $732*7=4255$\n","$\n","wrong  : 732*7=5524\n","correct: 732*7=5124\n","outputs(x):  $188*7=6141$\n","$\n","wrong  : 188*7=1416\n","correct: 188*7=1316\n","outputs(x):  $622*9=8991$\n","$\n","wrong  : 622*9=1998\n","correct: 622*9=5598\n","outputs(x):  $265*9=5842$\n","$\n","wrong  : 265*9=2485\n","correct: 265*9=2385\n","outputs(x):  $914*9=6273$\n","$\n","wrong  : 914*9=3726\n","correct: 914*9=8226\n","outputs(x):  $719*9=1791$\n","$\n","wrong  : 719*9=1971\n","correct: 719*9=6471\n","outputs(x):  $736*9=4275$\n","$\n","wrong  : 736*9=5724\n","correct: 736*9=6624\n","outputs(x):  $428*4=2153$\n","$\n","wrong  : 428*4=3512\n","correct: 428*4=1712\n","outputs(x):  $720*7=0445$\n","$\n","wrong  : 720*7=5440\n","correct: 720*7=5040\n","outputs(x):  $867*8=6356$\n","$\n","wrong  : 867*8=6536\n","correct: 867*8=6936\n","outputs(x):  $345*9=5023$\n","$\n","wrong  : 345*9=3205\n","correct: 345*9=3105\n","outputs(x):  $876*6=6535$\n","$\n","wrong  : 876*6=5356\n","correct: 876*6=5256\n","outputs(x):  $974*9=6687$\n","$\n","wrong  : 974*9=7866\n","correct: 974*9=8766\n","outputs(x):  $229*8=2512$\n","$\n","wrong  : 229*8=2152\n","correct: 229*8=1832\n","outputs(x):  $409*9=1881$\n","$\n","wrong  : 409*9=1881\n","correct: 409*9=3681\n","outputs(x):  $757*7=9983$\n","$\n","wrong  : 757*7=3899\n","correct: 757*7=5299\n","outputs(x):  $413*9=7193$\n","$\n","wrong  : 413*9=3917\n","correct: 413*9=3717\n","outputs(x):  $886*9=4716$\n","$\n","wrong  : 886*9=6174\n","correct: 886*9=7974\n","outputs(x):  $343*9=7893$\n","$\n","wrong  : 343*9=3987\n","correct: 343*9=3087\n","outputs(x):  $314*3=246$\n","$0\n","wrong  : 314*3=642\n","correct: 314*3=942\n","outputs(x):  $805*8=0427$\n","$\n","wrong  : 805*8=7240\n","correct: 805*8=6440\n","outputs(x):  $237*6=2231$\n","$\n","wrong  : 237*6=1322\n","correct: 237*6=1422\n","outputs(x):  $985*6=0185$\n","$\n","wrong  : 985*6=5810\n","correct: 985*6=5910\n","outputs(x):  $349*9=1403$\n","$\n","wrong  : 349*9=3041\n","correct: 349*9=3141\n","outputs(x):  $747*9=3296$\n","$\n","wrong  : 747*9=6923\n","correct: 747*9=6723\n","outputs(x):  $284*3=256$\n","$2\n","wrong  : 284*3=652\n","correct: 284*3=852\n","outputs(x):  $979*4=6183$\n","$\n","wrong  : 979*4=3816\n","correct: 979*4=3916\n","outputs(x):  $314*9=6272$\n","$\n","wrong  : 314*9=2726\n","correct: 314*9=2826\n","outputs(x):  $366*6=6981$\n","$\n","wrong  : 366*6=1896\n","correct: 366*6=2196\n","outputs(x):  $332*3=696$\n","$9\n","wrong  : 332*3=696\n","correct: 332*3=996\n","outputs(x):  $733*9=7965$\n","$\n","wrong  : 733*9=5697\n","correct: 733*9=6597\n","outputs(x):  $100*9=0018$\n","$\n","wrong  : 100*9=8100\n","correct: 100*9=900\n","outputs(x):  $579*8=2324$\n","$\n","wrong  : 579*8=4232\n","correct: 579*8=4632\n","outputs(x):  $573*9=7505$\n","$\n","wrong  : 573*9=5057\n","correct: 573*9=5157\n","outputs(x):  $522*7=4573$\n","$\n","wrong  : 522*7=3754\n","correct: 522*7=3654\n","outputs(x):  $345*8=0603$\n","$\n","wrong  : 345*8=3060\n","correct: 345*8=2760\n","outputs(x):  $328*4=2151$\n","$\n","wrong  : 328*4=1512\n","correct: 328*4=1312\n","outputs(x):  $332*9=8803$\n","$\n","wrong  : 332*9=3088\n","correct: 332*9=2988\n","outputs(x):  $892*8=6307$\n","$\n","wrong  : 892*8=7036\n","correct: 892*8=7136\n","outputs(x):  $619*7=3363$\n","$\n","wrong  : 619*7=3633\n","correct: 619*7=4333\n","outputs(x):  $972*9=8468$\n","$\n","wrong  : 972*9=8648\n","correct: 972*9=8748\n","outputs(x):  $344*7=8072$\n","$\n","wrong  : 344*7=2708\n","correct: 344*7=2408\n","outputs(x):  $178*9=2051$\n","$\n","wrong  : 178*9=1502\n","correct: 178*9=1602\n","outputs(x):  $483*8=4093$\n","$\n","wrong  : 483*8=3904\n","correct: 483*8=3864\n","outputs(x):  $716*9=4456$\n","$\n","wrong  : 716*9=6544\n","correct: 716*9=6444\n","outputs(x):  $967*7=9686$\n","$\n","wrong  : 967*7=6869\n","correct: 967*7=6769\n","outputs(x):  $273*7=1161$\n","$\n","wrong  : 273*7=1611\n","correct: 273*7=1911\n","outputs(x):  $328*9=2583$\n","$\n","wrong  : 328*9=3852\n","correct: 328*9=2952\n","outputs(x):  $761*9=9406$\n","$\n","wrong  : 761*9=6049\n","correct: 761*9=6849\n","outputs(x):  $740*7=0895$\n","$\n","wrong  : 740*7=5980\n","correct: 740*7=5180\n","outputs(x):  $104*9=6331$\n","$\n","wrong  : 104*9=1336\n","correct: 104*9=936\n","outputs(x):  $869*6=4155$\n","$\n","wrong  : 869*6=5514\n","correct: 869*6=5214\n","outputs(x):  $672*9=8446$\n","$\n","wrong  : 672*9=6448\n","correct: 672*9=6048\n","outputs(x):  $184*4=635$\n","$7\n","wrong  : 184*4=536\n","correct: 184*4=736\n","outputs(x):  $565*5=5232$\n","$\n","wrong  : 565*5=2325\n","correct: 565*5=2825\n","outputs(x):  $608*9=2736$\n","$\n","wrong  : 608*9=6372\n","correct: 608*9=5472\n","outputs(x):  $678*9=2026$\n","$\n","wrong  : 678*9=6202\n","correct: 678*9=6102\n","outputs(x):  $794*8=2595$\n","$\n","wrong  : 794*8=5952\n","correct: 794*8=6352\n","outputs(x):  $486*6=6182$\n","$\n","wrong  : 486*6=2816\n","correct: 486*6=2916\n","outputs(x):  $675*9=5726$\n","$\n","wrong  : 675*9=6275\n","correct: 675*9=6075\n","outputs(x):  $660*9=0415$\n","$\n","wrong  : 660*9=5140\n","correct: 660*9=5940\n","outputs(x):  $236*3=809$\n","$9\n","wrong  : 236*3=908\n","correct: 236*3=708\n","outputs(x):  $787*9=3896$\n","$\n","wrong  : 787*9=6983\n","correct: 787*9=7083\n","outputs(x):  $669*7=3893$\n","$\n","wrong  : 669*7=3983\n","correct: 669*7=4683\n","outputs(x):  $112*9=8091$\n","$\n","wrong  : 112*9=1908\n","correct: 112*9=1008\n","outputs(x):  $750*9=0585$\n","$\n","wrong  : 750*9=5850\n","correct: 750*9=6750\n","outputs(x):  $279*7=3541$\n","$\n","wrong  : 279*7=1453\n","correct: 279*7=1953\n","outputs(x):  $764*6=4834$\n","$\n","wrong  : 764*6=4384\n","correct: 764*6=4584\n","outputs(x):  $310*3=0311$\n","$\n","wrong  : 310*3=1130\n","correct: 310*3=930\n","outputs(x):  $712*8=6984$\n","$\n","wrong  : 712*8=4896\n","correct: 712*8=5696\n","outputs(x):  $353*7=1713$\n","$\n","wrong  : 353*7=3171\n","correct: 353*7=2471\n","outputs(x):  $121*9=9871$\n","$\n","wrong  : 121*9=1789\n","correct: 121*9=1089\n","outputs(x):  $874*8=2707$\n","$\n","wrong  : 874*8=7072\n","correct: 874*8=6992\n","outputs(x):  $284*9=6592$\n","$\n","wrong  : 284*9=2956\n","correct: 284*9=2556\n","outputs(x):  $760*6=0645$\n","$\n","wrong  : 760*6=5460\n","correct: 760*6=4560\n","outputs(x):  $697*9=3716$\n","$\n","wrong  : 697*9=6173\n","correct: 697*9=6273\n","outputs(x):  $391*7=7353$\n","$\n","wrong  : 391*7=3537\n","correct: 391*7=2737\n","outputs(x):  $422*9=8982$\n","$\n","wrong  : 422*9=2898\n","correct: 422*9=3798\n","outputs(x):  $405*9=5423$\n","$\n","wrong  : 405*9=3245\n","correct: 405*9=3645\n","outputs(x):  $317*7=9132$\n","$\n","wrong  : 317*7=2319\n","correct: 317*7=2219\n","outputs(x):  $326*9=4303$\n","$\n","wrong  : 326*9=3034\n","correct: 326*9=2934\n","outputs(x):  $155*7=588$\n","$9\n","wrong  : 155*7=885\n","correct: 155*7=1085\n","outputs(x):  $660*7=0293$\n","$\n","wrong  : 660*7=3920\n","correct: 660*7=4620\n","outputs(x):  $197*8=6731$\n","$\n","wrong  : 197*8=1376\n","correct: 197*8=1576\n","outputs(x):  $765*9=5896$\n","$\n","wrong  : 765*9=6985\n","correct: 765*9=6885\n","outputs(x):  $230*9=0712$\n","$\n","wrong  : 230*9=2170\n","correct: 230*9=2070\n","outputs(x):  $932*9=8818$\n","$\n","wrong  : 932*9=8188\n","correct: 932*9=8388\n","outputs(x):  $176*9=4823$\n","$\n","wrong  : 176*9=3284\n","correct: 176*9=1584\n","outputs(x):  $756*8=8426$\n","$\n","wrong  : 756*8=6248\n","correct: 756*8=6048\n"," 88% 21/24 [00:01<00:00, 19.13it/s]outputs(x):  $266*8=8232$\n","$\n","wrong  : 266*8=2328\n","correct: 266*8=2128\n","outputs(x):  $472*9=8414$\n","$\n","wrong  : 472*9=4148\n","correct: 472*9=4248\n","outputs(x):  $470*4=0861$\n","$\n","wrong  : 470*4=1680\n","correct: 470*4=1880\n","outputs(x):  $923*7=1666$\n","$\n","wrong  : 923*7=6661\n","correct: 923*7=6461\n","outputs(x):  $116*1=61$\n","$28\n","wrong  : 116*1=16\n","correct: 116*1=116\n","outputs(x):  $206*7=2421$\n","$\n","wrong  : 206*7=1242\n","correct: 206*7=1442\n","outputs(x):  $386*9=4753$\n","$\n","wrong  : 386*9=3574\n","correct: 386*9=3474\n","outputs(x):  $521*7=7473$\n","$\n","wrong  : 521*7=3747\n","correct: 521*7=3647\n","outputs(x):  $844*8=2575$\n","$\n","wrong  : 844*8=5752\n","correct: 844*8=6752\n","outputs(x):  $814*7=8941$\n","$\n","wrong  : 814*7=1498\n","correct: 814*7=5698\n","outputs(x):  $137*7=9532$\n","$\n","wrong  : 137*7=2359\n","correct: 137*7=959\n","outputs(x):  $922*7=4556$\n","$\n","wrong  : 922*7=6554\n","correct: 922*7=6454\n","outputs(x):  $390*7=0302$\n","$\n","wrong  : 390*7=2030\n","correct: 390*7=2730\n","outputs(x):  $116*9=4491$\n","$\n","wrong  : 116*9=1944\n","correct: 116*9=1044\n","outputs(x):  $629*4=6192$\n","$\n","wrong  : 629*4=2916\n","correct: 629*4=2516\n","outputs(x):  $798*8=4835$\n","$\n","wrong  : 798*8=5384\n","correct: 798*8=6384\n","outputs(x):  $830*7=0156$\n","$\n","wrong  : 830*7=6510\n","correct: 830*7=5810\n","outputs(x):  $491*3=3941$\n","$\n","wrong  : 491*3=1493\n","correct: 491*3=1473\n","outputs(x):  $471*9=9334$\n","$\n","wrong  : 471*9=4339\n","correct: 471*9=4239\n","outputs(x):  $782*8=6506$\n","$\n","wrong  : 782*8=6056\n","correct: 782*8=6256\n","outputs(x):  $694*7=8484$\n","$\n","wrong  : 694*7=4848\n","correct: 694*7=4858\n","outputs(x):  $317*6=2032$\n","$\n","wrong  : 317*6=2302\n","correct: 317*6=1902\n","outputs(x):  $109*4=636$\n","$2\n","wrong  : 109*4=636\n","correct: 109*4=436\n","outputs(x):  $231*9=9712$\n","$\n","wrong  : 231*9=2179\n","correct: 231*9=2079\n","outputs(x):  $223*7=1611$\n","$\n","wrong  : 223*7=1161\n","correct: 223*7=1561\n","outputs(x):  $936*6=6155$\n","$\n","wrong  : 936*6=5516\n","correct: 936*6=5616\n","outputs(x):  $986*7=2086$\n","$\n","wrong  : 986*7=6802\n","correct: 986*7=6902\n","outputs(x):  $36*6=652$\n","$5\n","wrong  : 36*6=256\n","correct: 36*6=216\n","outputs(x):  $49*7=34$\n","$69\n","wrong  : 49*7=43\n","correct: 49*7=343\n","outputs(x):  $64*8=255$\n","$9\n","wrong  : 64*8=552\n","correct: 64*8=512\n","outputs(x):  $69*9=125$\n","$9\n","wrong  : 69*9=521\n","correct: 69*9=621\n","outputs(x):  $78*7=64$\n","$59\n","wrong  : 78*7=46\n","correct: 78*7=546\n","outputs(x):  $35*9=514$\n","$9\n","wrong  : 35*9=415\n","correct: 35*9=315\n","outputs(x):  $75*9=577$\n","$8\n","wrong  : 75*9=775\n","correct: 75*9=675\n","outputs(x):  $36*8=823$\n","$4\n","wrong  : 36*8=328\n","correct: 36*8=288\n","outputs(x):  $55*7=5883$\n","$\n","wrong  : 55*7=3885\n","correct: 55*7=385\n","outputs(x):  $97*3=191$\n","$2\n","wrong  : 97*3=191\n","correct: 97*3=291\n","outputs(x):  $58*7=654$\n","$9\n","wrong  : 58*7=456\n","correct: 58*7=406\n","outputs(x):  $71*7=7935$\n","$\n","wrong  : 71*7=5397\n","correct: 71*7=497\n","outputs(x):  $29*8=251$\n","$9\n","wrong  : 29*8=152\n","correct: 29*8=232\n","outputs(x):  $62*8=692$\n","$8\n","wrong  : 62*8=296\n","correct: 62*8=496\n","outputs(x):  $99*7=394$\n","$7\n","wrong  : 99*7=493\n","correct: 99*7=693\n","outputs(x):  $53*9=7794$\n","$\n","wrong  : 53*9=4977\n","correct: 53*9=477\n","outputs(x):  $57*7=9983$\n","$\n","wrong  : 57*7=3899\n","correct: 57*7=399\n","outputs(x):  $27*8=616$\n","$1\n","wrong  : 27*8=616\n","correct: 27*8=216\n","outputs(x):  $72*7=402$\n","$9\n","wrong  : 72*7=204\n","correct: 72*7=504\n","outputs(x):  $89*7=325$\n","$4\n","wrong  : 89*7=523\n","correct: 89*7=623\n","outputs(x):  $52*7=461$\n","$2\n","wrong  : 52*7=164\n","correct: 52*7=364\n","outputs(x):  $58*1=855$\n","$4\n","wrong  : 58*1=558\n","correct: 58*1=58\n","outputs(x):  $16*3=841$\n","$9\n","wrong  : 16*3=148\n","correct: 16*3=48\n","outputs(x):  $81*9=928$\n","$6\n","wrong  : 81*9=829\n","correct: 81*9=729\n","outputs(x):  $39*3=75$\n","$79\n","wrong  : 39*3=57\n","correct: 39*3=117\n","outputs(x):  $14*9=623$\n","$6\n","wrong  : 14*9=326\n","correct: 14*9=126\n","outputs(x):  $54*7=874$\n","$8\n","wrong  : 54*7=478\n","correct: 54*7=378\n","outputs(x):  $59*6=454$\n","$7\n","wrong  : 59*6=454\n","correct: 59*6=354\n","outputs(x):  $32*9=881$\n","$3\n","wrong  : 32*9=188\n","correct: 32*9=288\n","outputs(x):  $79*7=354$\n","$4\n","wrong  : 79*7=453\n","correct: 79*7=553\n","outputs(x):  $77*3=13$\n","$94\n","wrong  : 77*3=31\n","correct: 77*3=231\n","100% 24/24 [00:01<00:00, 19.28it/s]\n","accuracy of 3000 examples: 2258/3000 (75.26666666666667%)\n","{'carry0': 89.63668850506254, 'carry1': 55.508830950378474, 'carry2': 68.0672268907563, 'carry3': 92.3076923076923, 'carry4': nan, 'carry5': nan}\n","evaluating addition from: FILE:data/bal/0_to_999_times_1_digit_train_3000.txt\n","Evaluating Addition using test data file: data/bal/0_to_999_times_1_digit_train_3000.txt\n","100% 3000/3000 [00:00<00:00, 20221.28it/s]\n","100% 25/25 [00:00<00:00, 28.01it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 4750: train loss 0.0321, val loss 10.8745\n","iter 4750: loss 0.0338, time 32832.96ms, mfu 6.08%\n","iter 4760: loss 0.0338, time 210.48ms, mfu 6.18%\n","iter 4770: loss 0.0336, time 213.82ms, mfu 6.26%\n","iter 4780: loss 0.0336, time 218.59ms, mfu 6.32%\n","iter 4790: loss 0.0335, time 220.62ms, mfu 6.36%\n","iter 4800: loss 0.0336, time 215.11ms, mfu 6.42%\n","iter 4810: loss 0.0332, time 216.90ms, mfu 6.46%\n","iter 4820: loss 0.0332, time 219.41ms, mfu 6.50%\n","iter 4830: loss 0.0331, time 220.84ms, mfu 6.52%\n","iter 4840: loss 0.0335, time 218.60ms, mfu 6.55%\n","iter 4850: loss 0.0332, time 222.37ms, mfu 6.57%\n","iter 4860: loss 0.0333, time 220.52ms, mfu 6.59%\n","iter 4870: loss 0.0337, time 218.71ms, mfu 6.61%\n","iter 4880: loss 0.0336, time 219.15ms, mfu 6.63%\n","iter 4890: loss 0.0335, time 220.03ms, mfu 6.65%\n","iter 4900: loss 0.0335, time 215.85ms, mfu 6.67%\n","iter 4910: loss 0.0330, time 219.44ms, mfu 6.69%\n","iter 4920: loss 0.0333, time 219.00ms, mfu 6.70%\n","iter 4930: loss 0.0332, time 218.97ms, mfu 6.71%\n","iter 4940: loss 0.0337, time 217.11ms, mfu 6.73%\n","iter 4950: loss 0.0339, time 214.69ms, mfu 6.75%\n","iter 4960: loss 0.0335, time 218.24ms, mfu 6.76%\n","iter 4970: loss 0.0337, time 223.00ms, mfu 6.75%\n","iter 4980: loss 0.0330, time 224.05ms, mfu 6.74%\n","iter 4990: loss 0.0332, time 220.21ms, mfu 6.74%\n","evaluating addition from: FILE:data/bal/0_to_999_times_1_digit_test_3000.txt\n","Evaluating Addition using test data file: data/bal/0_to_999_times_1_digit_test_3000.txt\n","  0% 0/24 [00:00<?, ?it/s]outputs(x):  $786*3=8522$\n","$\n","wrong  : 786*3=2258\n","correct: 786*3=2358\n","outputs(x):  $671*9=9315$\n","$\n","wrong  : 671*9=5139\n","correct: 671*9=6039\n","outputs(x):  $142*9=8711$\n","$\n","wrong  : 142*9=1178\n","correct: 142*9=1278\n","outputs(x):  $296*9=4652$\n","$\n","wrong  : 296*9=2564\n","correct: 296*9=2664\n","outputs(x):  $381*9=9213$\n","$\n","wrong  : 381*9=3129\n","correct: 381*9=3429\n","outputs(x):  $133*8=469$\n","$7\n","wrong  : 133*8=964\n","correct: 133*8=1064\n","outputs(x):  $423*9=7043$\n","$\n","wrong  : 423*9=3407\n","correct: 423*9=3807\n","outputs(x):  $455*6=0392$\n","$\n","wrong  : 455*6=2930\n","correct: 455*6=2730\n","outputs(x):  $111*4=446$\n","$2\n","wrong  : 111*4=644\n","correct: 111*4=444\n","outputs(x):  $789*6=4354$\n","$\n","wrong  : 789*6=4534\n","correct: 789*6=4734\n","outputs(x):  $675*8=0075$\n","$\n","wrong  : 675*8=5700\n","correct: 675*8=5400\n","outputs(x):  $732*9=8846$\n","$\n","wrong  : 732*9=6488\n","correct: 732*9=6588\n","outputs(x):  $647*6=2844$\n","$\n","wrong  : 647*6=4482\n","correct: 647*6=3882\n","outputs(x):  $444*7=8092$\n","$\n","wrong  : 444*7=2908\n","correct: 444*7=3108\n","outputs(x):  $858*3=4762$\n","$\n","wrong  : 858*3=2674\n","correct: 858*3=2574\n","outputs(x):  $867*7=9626$\n","$\n","wrong  : 867*7=6269\n","correct: 867*7=6069\n","outputs(x):  $229*8=2512$\n","$\n","wrong  : 229*8=2152\n","correct: 229*8=1832\n","outputs(x):  $238*4=2511$\n","$\n","wrong  : 238*4=1152\n","correct: 238*4=952\n","outputs(x):  $136*7=2532$\n","$\n","wrong  : 136*7=2352\n","correct: 136*7=952\n","outputs(x):  $314*7=8942$\n","$\n","wrong  : 314*7=2498\n","correct: 314*7=2198\n","outputs(x):  $852*3=6522$\n","$\n","wrong  : 852*3=2256\n","correct: 852*3=2556\n","outputs(x):  $392*9=8263$\n","$\n","wrong  : 392*9=3628\n","correct: 392*9=3528\n","outputs(x):  $240*5=0071$\n","$\n","wrong  : 240*5=1700\n","correct: 240*5=1200\n","outputs(x):  $772*9=8442$\n","$\n","wrong  : 772*9=2448\n","correct: 772*9=6948\n","outputs(x):  $956*3=8652$\n","$\n","wrong  : 956*3=2568\n","correct: 956*3=2868\n","outputs(x):  $784*9=6597$\n","$\n","wrong  : 784*9=7956\n","correct: 784*9=7056\n","outputs(x):  $428*7=6933$\n","$\n","wrong  : 428*7=3396\n","correct: 428*7=2996\n","outputs(x):  $396*8=8653$\n","$\n","wrong  : 396*8=3568\n","correct: 396*8=3168\n","outputs(x):  $222*8=671$\n","$3\n","wrong  : 222*8=176\n","correct: 222*8=1776\n","outputs(x):  $674*8=2705$\n","$\n","wrong  : 674*8=5072\n","correct: 674*8=5392\n","outputs(x):  $856*7=2975$\n","$\n","wrong  : 856*7=5792\n","correct: 856*7=5992\n","outputs(x):  $343*9=7892$\n","$\n","wrong  : 343*9=2987\n","correct: 343*9=3087\n","outputs(x):  $460*7=0213$\n","$\n","wrong  : 460*7=3120\n","correct: 460*7=3220\n","outputs(x):  $311*9=9923$\n","$\n","wrong  : 311*9=3299\n","correct: 311*9=2799\n","outputs(x):  $841*7=7865$\n","$\n","wrong  : 841*7=5687\n","correct: 841*7=5887\n","outputs(x):  $669*8=2515$\n","$\n","wrong  : 669*8=5152\n","correct: 669*8=5352\n","outputs(x):  $435*9=5124$\n","$\n","wrong  : 435*9=4215\n","correct: 435*9=3915\n","outputs(x):  $925*3=5742$\n","$\n","wrong  : 925*3=2475\n","correct: 925*3=2775\n","outputs(x):  $663*8=4094$\n","$\n","wrong  : 663*8=4904\n","correct: 663*8=5304\n","outputs(x):  $711*7=7753$\n","$\n","wrong  : 711*7=3577\n","correct: 711*7=4977\n","outputs(x):  $917*3=1571$\n","$\n","wrong  : 917*3=1751\n","correct: 917*3=2751\n","outputs(x):  $356*8=8442$\n","$\n","wrong  : 356*8=2448\n","correct: 356*8=2848\n","outputs(x):  $364*9=6743$\n","$\n","wrong  : 364*9=3476\n","correct: 364*9=3276\n","outputs(x):  $486*6=6182$\n","$\n","wrong  : 486*6=2816\n","correct: 486*6=2916\n","outputs(x):  $807*7=9436$\n","$\n","wrong  : 807*7=6349\n","correct: 807*7=5649\n","outputs(x):  $466*8=8253$\n","$\n","wrong  : 466*8=3528\n","correct: 466*8=3728\n","outputs(x):  $949*3=7472$\n","$\n","wrong  : 949*3=2747\n","correct: 949*3=2847\n","outputs(x):  $317*6=2031$\n","$\n","wrong  : 317*6=1302\n","correct: 317*6=1902\n","outputs(x):  $986*7=2086$\n","$\n","wrong  : 986*7=6802\n","correct: 986*7=6902\n","outputs(x):  $380*8=0402$\n","$\n","wrong  : 380*8=2040\n","correct: 380*8=3040\n","outputs(x):  $863*8=4076$\n","$\n","wrong  : 863*8=6704\n","correct: 863*8=6904\n","outputs(x):  $284*3=256$\n","$2\n","wrong  : 284*3=652\n","correct: 284*3=852\n","outputs(x):  $606*9=4535$\n","$\n","wrong  : 606*9=5354\n","correct: 606*9=5454\n","outputs(x):  $593*9=7326$\n","$\n","wrong  : 593*9=6237\n","correct: 593*9=5337\n","outputs(x):  $750*9=0586$\n","$\n","wrong  : 750*9=6850\n","correct: 750*9=6750\n","outputs(x):  $174*6=446$\n","$3\n","wrong  : 174*6=644\n","correct: 174*6=1044\n","outputs(x):  $566*8=8234$\n","$\n","wrong  : 566*8=4328\n","correct: 566*8=4528\n","outputs(x):  $459*7=3113$\n","$\n","wrong  : 459*7=3113\n","correct: 459*7=3213\n","outputs(x):  $342*7=4962$\n","$\n","wrong  : 342*7=2694\n","correct: 342*7=2394\n","outputs(x):  $373*9=7553$\n","$\n","wrong  : 373*9=3557\n","correct: 373*9=3357\n","outputs(x):  $137*7=9532$\n","$\n","wrong  : 137*7=2359\n","correct: 137*7=959\n","outputs(x):  $371*7=7932$\n","$\n","wrong  : 371*7=2397\n","correct: 371*7=2597\n","outputs(x):  $267*3=106$\n","$5\n","wrong  : 267*3=601\n","correct: 267*3=801\n","outputs(x):  $854*8=2367$\n","$\n","wrong  : 854*8=7632\n","correct: 854*8=6832\n","  8% 2/24 [00:00<00:01, 18.39it/s]outputs(x):  $846*7=2275$\n","$\n","wrong  : 846*7=5722\n","correct: 846*7=5922\n","outputs(x):  $788*7=6196$\n","$\n","wrong  : 788*7=6916\n","correct: 788*7=5516\n","outputs(x):  $694*7=8484$\n","$\n","wrong  : 694*7=4848\n","correct: 694*7=4858\n","outputs(x):  $569*6=4104$\n","$\n","wrong  : 569*6=4014\n","correct: 569*6=3414\n","outputs(x):  $784*7=8805$\n","$\n","wrong  : 784*7=5088\n","correct: 784*7=5488\n","outputs(x):  $313*7=1922$\n","$\n","wrong  : 313*7=2291\n","correct: 313*7=2191\n","outputs(x):  $892*8=6376$\n","$\n","wrong  : 892*8=6736\n","correct: 892*8=7136\n","outputs(x):  $971*7=7936$\n","$\n","wrong  : 971*7=6397\n","correct: 971*7=6797\n","outputs(x):  $581*9=9215$\n","$\n","wrong  : 581*9=5129\n","correct: 581*9=5229\n","outputs(x):  $363*8=4013$\n","$\n","wrong  : 363*8=3104\n","correct: 363*8=2904\n","outputs(x):  $751*7=7554$\n","$\n","wrong  : 751*7=4557\n","correct: 751*7=5257\n","outputs(x):  $429*9=1663$\n","$\n","wrong  : 429*9=3661\n","correct: 429*9=3861\n","outputs(x):  $566*7=264$\n","$3\n","wrong  : 566*7=462\n","correct: 566*7=3962\n","outputs(x):  $759*6=4544$\n","$\n","wrong  : 759*6=4454\n","correct: 759*6=4554\n","outputs(x):  $936*7=2546$\n","$\n","wrong  : 936*7=6452\n","correct: 936*7=6552\n","outputs(x):  $777*3=1311$\n","$\n","wrong  : 777*3=1131\n","correct: 777*3=2331\n","outputs(x):  $806*7=2416$\n","$\n","wrong  : 806*7=6142\n","correct: 806*7=5642\n","outputs(x):  $444*8=2533$\n","$\n","wrong  : 444*8=3352\n","correct: 444*8=3552\n","outputs(x):  $826*8=8046$\n","$\n","wrong  : 826*8=6408\n","correct: 826*8=6608\n","outputs(x):  $686*7=2044$\n","$\n","wrong  : 686*7=4402\n","correct: 686*7=4802\n","outputs(x):  $391*7=7353$\n","$\n","wrong  : 391*7=3537\n","correct: 391*7=2737\n","outputs(x):  $723*7=1625$\n","$\n","wrong  : 723*7=5261\n","correct: 723*7=5061\n","outputs(x):  $341*9=9602$\n","$\n","wrong  : 341*9=2069\n","correct: 341*9=3069\n","outputs(x):  $688*9=2926$\n","$\n","wrong  : 688*9=6292\n","correct: 688*9=6192\n","outputs(x):  $535*6=0113$\n","$\n","wrong  : 535*6=3110\n","correct: 535*6=3210\n","outputs(x):  $129*4=619$\n","$1\n","wrong  : 129*4=916\n","correct: 129*4=516\n","outputs(x):  $176*9=4824$\n","$\n","wrong  : 176*9=4284\n","correct: 176*9=1584\n","outputs(x):  $857*8=6566$\n","$\n","wrong  : 857*8=6656\n","correct: 857*8=6856\n","outputs(x):  $679*9=1175$\n","$\n","wrong  : 679*9=5711\n","correct: 679*9=6111\n","outputs(x):  $383*9=7453$\n","$\n","wrong  : 383*9=3547\n","correct: 383*9=3447\n","outputs(x):  $874*8=2707$\n","$\n","wrong  : 874*8=7072\n","correct: 874*8=6992\n","outputs(x):  $249*5=5471$\n","$\n","wrong  : 249*5=1745\n","correct: 249*5=1245\n","outputs(x):  $849*9=1408$\n","$\n","wrong  : 849*9=8041\n","correct: 849*9=7641\n","outputs(x):  $403*4=2121$\n","$\n","wrong  : 403*4=1212\n","correct: 403*4=1612\n","outputs(x):  $629*8=2594$\n","$\n","wrong  : 629*8=4952\n","correct: 629*8=5032\n","outputs(x):  $413*9=7153$\n","$\n","wrong  : 413*9=3517\n","correct: 413*9=3717\n","outputs(x):  $347*7=9272$\n","$\n","wrong  : 347*7=2729\n","correct: 347*7=2429\n","outputs(x):  $152*8=6101$\n","$\n","wrong  : 152*8=1016\n","correct: 152*8=1216\n","outputs(x):  $179*9=1151$\n","$\n","wrong  : 179*9=1511\n","correct: 179*9=1611\n","outputs(x):  $376*8=8023$\n","$\n","wrong  : 376*8=3208\n","correct: 376*8=3008\n","outputs(x):  $685*3=5571$\n","$\n","wrong  : 685*3=1755\n","correct: 685*3=2055\n","outputs(x):  $174*8=2741$\n","$\n","wrong  : 174*8=1472\n","correct: 174*8=1392\n","outputs(x):  $310*3=0311$\n","$\n","wrong  : 310*3=1130\n","correct: 310*3=930\n","outputs(x):  $282*7=4771$\n","$\n","wrong  : 282*7=1774\n","correct: 282*7=1974\n","outputs(x):  $448*8=483$\n","$5\n","wrong  : 448*8=384\n","correct: 448*8=3584\n","outputs(x):  $440*8=0293$\n","$\n","wrong  : 440*8=3920\n","correct: 440*8=3520\n","outputs(x):  $385*9=5653$\n","$\n","wrong  : 385*9=3565\n","correct: 385*9=3465\n","outputs(x):  $319*8=2532$\n","$\n","wrong  : 319*8=2352\n","correct: 319*8=2552\n","outputs(x):  $355*8=0462$\n","$\n","wrong  : 355*8=2640\n","correct: 355*8=2840\n","outputs(x):  $366*8=8252$\n","$\n","wrong  : 366*8=2528\n","correct: 366*8=2928\n","outputs(x):  $619*7=3363$\n","$\n","wrong  : 619*7=3633\n","correct: 619*7=4333\n","outputs(x):  $690*8=0215$\n","$\n","wrong  : 690*8=5120\n","correct: 690*8=5520\n","outputs(x):  $422*9=8982$\n","$\n","wrong  : 422*9=2898\n","correct: 422*9=3798\n","outputs(x):  $965*3=5972$\n","$\n","wrong  : 965*3=2795\n","correct: 965*3=2895\n","outputs(x):  $622*9=8991$\n","$\n","wrong  : 622*9=1998\n","correct: 622*9=5598\n","outputs(x):  $889*9=1087$\n","$\n","wrong  : 889*9=7801\n","correct: 889*9=8001\n","outputs(x):  $672*9=8446$\n","$\n","wrong  : 672*9=6448\n","correct: 672*9=6048\n","outputs(x):  $586*7=2073$\n","$\n","wrong  : 586*7=3702\n","correct: 586*7=4102\n","outputs(x):  $761*9=9406$\n","$\n","wrong  : 761*9=6049\n","correct: 761*9=6849\n","outputs(x):  $974*6=4484$\n","$\n","wrong  : 974*6=4844\n","correct: 974*6=5844\n","outputs(x):  $717*6=2073$\n","$\n","wrong  : 717*6=3702\n","correct: 717*6=4302\n","outputs(x):  $901*3=3021$\n","$\n","wrong  : 901*3=1203\n","correct: 901*3=2703\n","outputs(x):  $430*9=0773$\n","$\n","wrong  : 430*9=3770\n","correct: 430*9=3870\n","outputs(x):  $816*9=4427$\n","$\n","wrong  : 816*9=7244\n","correct: 816*9=7344\n","outputs(x):  $634*9=6085$\n","$\n","wrong  : 634*9=5806\n","correct: 634*9=5706\n","outputs(x):  $566*5=0392$\n","$\n","wrong  : 566*5=2930\n","correct: 566*5=2830\n","outputs(x):  $595*9=5534$\n","$\n","wrong  : 595*9=4355\n","correct: 595*9=5355\n","outputs(x):  $772*8=6736$\n","$\n","wrong  : 772*8=6376\n","correct: 772*8=6176\n","outputs(x):  $542*9=8793$\n","$\n","wrong  : 542*9=3978\n","correct: 542*9=4878\n","outputs(x):  $158*7=6511$\n","$\n","wrong  : 158*7=1156\n","correct: 158*7=1106\n","outputs(x):  $230*9=0712$\n","$\n","wrong  : 230*9=2170\n","correct: 230*9=2070\n","outputs(x):  $305*8=0462$\n","$\n","wrong  : 305*8=2640\n","correct: 305*8=2440\n","outputs(x):  $731*3=3932$\n","$\n","wrong  : 731*3=2393\n","correct: 731*3=2193\n","outputs(x):  $926*8=8027$\n","$\n","wrong  : 926*8=7208\n","correct: 926*8=7408\n","outputs(x):  $774*7=8191$\n","$\n","wrong  : 774*7=1918\n","correct: 774*7=5418\n","outputs(x):  $836*7=2515$\n","$\n","wrong  : 836*7=5152\n","correct: 836*7=5852\n","outputs(x):  $249*7=3441$\n","$\n","wrong  : 249*7=1443\n","correct: 249*7=1743\n","outputs(x):  $710*8=0825$\n","$\n","wrong  : 710*8=5280\n","correct: 710*8=5680\n","outputs(x):  $701*6=6063$\n","$\n","wrong  : 701*6=3606\n","correct: 701*6=4206\n","outputs(x):  $178*9=2051$\n","$\n","wrong  : 178*9=1502\n","correct: 178*9=1602\n","outputs(x):  $443*8=4473$\n","$\n","wrong  : 443*8=3744\n","correct: 443*8=3544\n","outputs(x):  $284*9=6592$\n","$\n","wrong  : 284*9=2956\n","correct: 284*9=2556\n","outputs(x):  $666*7=2693$\n","$\n","wrong  : 666*7=3962\n","correct: 666*7=4662\n","outputs(x):  $742*8=6375$\n","$\n","wrong  : 742*8=5736\n","correct: 742*8=5936\n","outputs(x):  $548*3=4471$\n","$\n","wrong  : 548*3=1744\n","correct: 548*3=1644\n","outputs(x):  $764*6=4834$\n","$\n","wrong  : 764*6=4384\n","correct: 764*6=4584\n","outputs(x):  $954*9=6867$\n","$\n","wrong  : 954*9=7686\n","correct: 954*9=8586\n","outputs(x):  $618*4=2782$\n","$\n","wrong  : 618*4=2872\n","correct: 618*4=2472\n","outputs(x):  $469*9=1214$\n","$\n","wrong  : 469*9=4121\n","correct: 469*9=4221\n","outputs(x):  $520*7=0453$\n","$\n","wrong  : 520*7=3540\n","correct: 520*7=3640\n","outputs(x):  $763*6=8793$\n","$\n","wrong  : 763*6=3978\n","correct: 763*6=4578\n","outputs(x):  $309*7=3652$\n","$\n","wrong  : 309*7=2563\n","correct: 309*7=2163\n","outputs(x):  $376*9=4823$\n","$\n","wrong  : 376*9=3284\n","correct: 376*9=3384\n","outputs(x):  $721*7=741$\n","$5\n","wrong  : 721*7=147\n","correct: 721*7=5047\n","outputs(x):  $610*9=0965$\n","$\n","wrong  : 610*9=5690\n","correct: 610*9=5490\n","outputs(x):  $102*9=8118$\n","$\n","wrong  : 102*9=8118\n","correct: 102*9=918\n","outputs(x):  $719*9=1736$\n","$\n","wrong  : 719*9=6371\n","correct: 719*9=6471\n","outputs(x):  $886*9=4716$\n","$\n","wrong  : 886*9=6174\n","correct: 886*9=7974\n"," 21% 5/24 [00:00<00:00, 19.80it/s]outputs(x):  $819*8=2536$\n","$\n","wrong  : 819*8=6352\n","correct: 819*8=6552\n","outputs(x):  $526*8=8044$\n","$\n","wrong  : 526*8=4408\n","correct: 526*8=4208\n","outputs(x):  $439*9=1304$\n","$\n","wrong  : 439*9=4031\n","correct: 439*9=3951\n","outputs(x):  $959*7=3166$\n","$\n","wrong  : 959*7=6613\n","correct: 959*7=6713\n","outputs(x):  $529*8=2704$\n","$\n","wrong  : 529*8=4072\n","correct: 529*8=4232\n","outputs(x):  $332*9=8803$\n","$\n","wrong  : 332*9=3088\n","correct: 332*9=2988\n","outputs(x):  $106*8=8421$\n","$\n","wrong  : 106*8=1248\n","correct: 106*8=848\n","outputs(x):  $756*8=8426$\n","$\n","wrong  : 756*8=6248\n","correct: 756*8=6048\n","outputs(x):  $311*7=7732$\n","$\n","wrong  : 311*7=2377\n","correct: 311*7=2177\n","outputs(x):  $541*9=9694$\n","$\n","wrong  : 541*9=4969\n","correct: 541*9=4869\n","outputs(x):  $379*8=2323$\n","$\n","wrong  : 379*8=3232\n","correct: 379*8=3032\n","outputs(x):  $670*9=0315$\n","$\n","wrong  : 670*9=5130\n","correct: 670*9=6030\n","outputs(x):  $236*3=809$\n","$9\n","wrong  : 236*3=908\n","correct: 236*3=708\n","outputs(x):  $675*9=5726$\n","$\n","wrong  : 675*9=6275\n","correct: 675*9=6075\n","outputs(x):  $741*6=6434$\n","$\n","wrong  : 741*6=4346\n","correct: 741*6=4446\n","outputs(x):  $936*6=6155$\n","$\n","wrong  : 936*6=5516\n","correct: 936*6=5616\n","outputs(x):  $864*7=8416$\n","$\n","wrong  : 864*7=6148\n","correct: 864*7=6048\n","outputs(x):  $924*3=2742$\n","$\n","wrong  : 924*3=2472\n","correct: 924*3=2772\n","outputs(x):  $853*3=9522$\n","$\n","wrong  : 853*3=2259\n","correct: 853*3=2559\n","outputs(x):  $785*3=5522$\n","$\n","wrong  : 785*3=2255\n","correct: 785*3=2355\n","outputs(x):  $798*7=6854$\n","$\n","wrong  : 798*7=4586\n","correct: 798*7=5586\n","outputs(x):  $917*6=2074$\n","$\n","wrong  : 917*6=4702\n","correct: 917*6=5502\n","outputs(x):  $608*9=2736$\n","$\n","wrong  : 608*9=6372\n","correct: 608*9=5472\n","outputs(x):  $316*9=4423$\n","$\n","wrong  : 316*9=3244\n","correct: 316*9=2844\n","outputs(x):  $182*9=8341$\n","$\n","wrong  : 182*9=1438\n","correct: 182*9=1638\n","outputs(x):  $658*7=6554$\n","$\n","wrong  : 658*7=4556\n","correct: 658*7=4606\n","outputs(x):  $427*9=3473$\n","$\n","wrong  : 427*9=3743\n","correct: 427*9=3843\n","outputs(x):  $611*9=9985$\n","$\n","wrong  : 611*9=5899\n","correct: 611*9=5499\n","outputs(x):  $119*7=336$\n","$4\n","wrong  : 119*7=633\n","correct: 119*7=833\n","outputs(x):  $839*7=3775$\n","$\n","wrong  : 839*7=5773\n","correct: 839*7=5873\n","outputs(x):  $191*7=7351$\n","$\n","wrong  : 191*7=1537\n","correct: 191*7=1337\n","outputs(x):  $138*3=415$\n","$8\n","wrong  : 138*3=514\n","correct: 138*3=414\n","outputs(x):  $100*7=006$\n","$1\n","wrong  : 100*7=600\n","correct: 100*7=700\n","outputs(x):  $194*7=8411$\n","$\n","wrong  : 194*7=1148\n","correct: 194*7=1358\n","outputs(x):  $866*7=2675$\n","$\n","wrong  : 866*7=5762\n","correct: 866*7=6062\n","outputs(x):  $259*6=4541$\n","$\n","wrong  : 259*6=1454\n","correct: 259*6=1554\n","outputs(x):  $680*8=0425$\n","$\n","wrong  : 680*8=5240\n","correct: 680*8=5440\n","outputs(x):  $269*9=1251$\n","$\n","wrong  : 269*9=1521\n","correct: 269*9=2421\n","outputs(x):  $287*7=9071$\n","$\n","wrong  : 287*7=1709\n","correct: 287*7=2009\n","outputs(x):  $821*8=8676$\n","$\n","wrong  : 821*8=6768\n","correct: 821*8=6568\n","outputs(x):  $288*3=4611$\n","$\n","wrong  : 288*3=1164\n","correct: 288*3=864\n","outputs(x):  $565*3=5971$\n","$\n","wrong  : 565*3=1795\n","correct: 565*3=1695\n","outputs(x):  $442*6=2522$\n","$\n","wrong  : 442*6=2252\n","correct: 442*6=2652\n","outputs(x):  $907*3=1292$\n","$\n","wrong  : 907*3=2921\n","correct: 907*3=2721\n","outputs(x):  $867*6=2064$\n","$\n","wrong  : 867*6=4602\n","correct: 867*6=5202\n","outputs(x):  $320*9=0892$\n","$\n","wrong  : 320*9=2980\n","correct: 320*9=2880\n","outputs(x):  $405*9=5423$\n","$\n","wrong  : 405*9=3245\n","correct: 405*9=3645\n","outputs(x):  $100*9=0018$\n","$\n","wrong  : 100*9=8100\n","correct: 100*9=900\n","outputs(x):  $914*9=6237$\n","$\n","wrong  : 914*9=7326\n","correct: 914*9=8226\n","outputs(x):  $104*8=2341$\n","$\n","wrong  : 104*8=1432\n","correct: 104*8=832\n","outputs(x):  $363*6=8722$\n","$\n","wrong  : 363*6=2278\n","correct: 363*6=2178\n","outputs(x):  $489*3=764$\n","$6\n","wrong  : 489*3=467\n","correct: 489*3=1467\n","outputs(x):  $943*8=4476$\n","$\n","wrong  : 943*8=6744\n","correct: 943*8=7544\n","outputs(x):  $313*9=7132$\n","$\n","wrong  : 313*9=2317\n","correct: 313*9=2817\n","outputs(x):  $141*8=8251$\n","$\n","wrong  : 141*8=1528\n","correct: 141*8=1128\n","outputs(x):  $639*9=1365$\n","$\n","wrong  : 639*9=5631\n","correct: 639*9=5751\n","outputs(x):  $740*9=0676$\n","$\n","wrong  : 740*9=6760\n","correct: 740*9=6660\n","outputs(x):  $339*7=3772$\n","$\n","wrong  : 339*7=2773\n","correct: 339*7=2373\n","outputs(x):  $300*7=0062$\n","$\n","wrong  : 300*7=2600\n","correct: 300*7=2100\n","outputs(x):  $151*9=959$\n","$8\n","wrong  : 151*9=959\n","correct: 151*9=1359\n","outputs(x):  $319*6=4181$\n","$\n","wrong  : 319*6=1814\n","correct: 319*6=1914\n","outputs(x):  $156*7=2911$\n","$\n","wrong  : 156*7=1192\n","correct: 156*7=1092\n","outputs(x):  $314*9=6201$\n","$\n","wrong  : 314*9=1026\n","correct: 314*9=2826\n","outputs(x):  $322*8=6792$\n","$\n","wrong  : 322*8=2976\n","correct: 322*8=2576\n","outputs(x):  $348*9=2303$\n","$\n","wrong  : 348*9=3032\n","correct: 348*9=3132\n","outputs(x):  $460*8=0823$\n","$\n","wrong  : 460*8=3280\n","correct: 460*8=3680\n","outputs(x):  $667*8=6316$\n","$\n","wrong  : 667*8=6136\n","correct: 667*8=5336\n"," 29% 7/24 [00:00<00:00, 19.81it/s]outputs(x):  $740*7=0895$\n","$\n","wrong  : 740*7=5980\n","correct: 740*7=5180\n","outputs(x):  $502*9=8153$\n","$\n","wrong  : 502*9=3518\n","correct: 502*9=4518\n","outputs(x):  $932*8=6567$\n","$\n","wrong  : 932*8=7656\n","correct: 932*8=7456\n","outputs(x):  $407*9=3624$\n","$\n","wrong  : 407*9=4263\n","correct: 407*9=3663\n","outputs(x):  $109*8=2721$\n","$\n","wrong  : 109*8=1272\n","correct: 109*8=872\n","outputs(x):  $215*9=5381$\n","$\n","wrong  : 215*9=1835\n","correct: 215*9=1935\n","outputs(x):  $694*6=4683$\n","$\n","wrong  : 694*6=3864\n","correct: 694*6=4164\n","outputs(x):  $773*7=1125$\n","$\n","wrong  : 773*7=5211\n","correct: 773*7=5411\n","outputs(x):  $248*7=6351$\n","$\n","wrong  : 248*7=1536\n","correct: 248*7=1736\n","outputs(x):  $167*9=3001$\n","$\n","wrong  : 167*9=1003\n","correct: 167*9=1503\n","outputs(x):  $779*9=117$\n","$9\n","wrong  : 779*9=711\n","correct: 779*9=7011\n","outputs(x):  $898*7=6814$\n","$\n","wrong  : 898*7=4186\n","correct: 898*7=6286\n","outputs(x):  $110*9=09$\n","$45\n","wrong  : 110*9=90\n","correct: 110*9=990\n","outputs(x):  $859*9=1367$\n","$\n","wrong  : 859*9=7631\n","correct: 859*9=7731\n","outputs(x):  $129*8=219$\n","$1\n","wrong  : 129*8=912\n","correct: 129*8=1032\n","outputs(x):  $330*7=0152$\n","$\n","wrong  : 330*7=2510\n","correct: 330*7=2310\n","outputs(x):  $426*8=8023$\n","$\n","wrong  : 426*8=3208\n","correct: 426*8=3408\n","outputs(x):  $629*4=6192$\n","$\n","wrong  : 629*4=2916\n","correct: 629*4=2516\n","outputs(x):  $418*7=6233$\n","$\n","wrong  : 418*7=3326\n","correct: 418*7=2926\n","outputs(x):  $431*9=9773$\n","$\n","wrong  : 431*9=3779\n","correct: 431*9=3879\n","outputs(x):  $396*7=2752$\n","$\n","wrong  : 396*7=2572\n","correct: 396*7=2772\n","outputs(x):  $813*7=1946$\n","$\n","wrong  : 813*7=6491\n","correct: 813*7=5691\n","outputs(x):  $909*6=4565$\n","$\n","wrong  : 909*6=5654\n","correct: 909*6=5454\n","outputs(x):  $419*6=4132$\n","$\n","wrong  : 419*6=2314\n","correct: 419*6=2514\n","outputs(x):  $680*7=0604$\n","$\n","wrong  : 680*7=4060\n","correct: 680*7=4760\n","outputs(x):  $561*3=3871$\n","$\n","wrong  : 561*3=1783\n","correct: 561*3=1683\n","outputs(x):  $970*7=0186$\n","$\n","wrong  : 970*7=6810\n","correct: 970*7=6790\n","outputs(x):  $783*7=1804$\n","$\n","wrong  : 783*7=4081\n","correct: 783*7=5481\n","outputs(x):  $771*8=8675$\n","$\n","wrong  : 771*8=5768\n","correct: 771*8=6168\n","outputs(x):  $157*9=3131$\n","$\n","wrong  : 157*9=1313\n","correct: 157*9=1413\n","outputs(x):  $350*7=0532$\n","$\n","wrong  : 350*7=2350\n","correct: 350*7=2450\n","outputs(x):  $782*7=4735$\n","$\n","wrong  : 782*7=5374\n","correct: 782*7=5474\n","outputs(x):  $669*7=3893$\n","$\n","wrong  : 669*7=3983\n","correct: 669*7=4683\n","outputs(x):  $951*7=7536$\n","$\n","wrong  : 951*7=6357\n","correct: 951*7=6657\n","outputs(x):  $718*8=4494$\n","$\n","wrong  : 718*8=4944\n","correct: 718*8=5744\n","outputs(x):  $274*3=226$\n","$6\n","wrong  : 274*3=622\n","correct: 274*3=822\n","outputs(x):  $117*9=3541$\n","$\n","wrong  : 117*9=1453\n","correct: 117*9=1053\n","outputs(x):  $828*7=6946$\n","$\n","wrong  : 828*7=6496\n","correct: 828*7=5796\n","outputs(x):  $820*8=0676$\n","$\n","wrong  : 820*8=6760\n","correct: 820*8=6560\n","outputs(x):  $261*9=9412$\n","$\n","wrong  : 261*9=2149\n","correct: 261*9=2349\n","outputs(x):  $259*9=1323$\n","$\n","wrong  : 259*9=3231\n","correct: 259*9=2331\n","outputs(x):  $847*7=9266$\n","$\n","wrong  : 847*7=6629\n","correct: 847*7=5929\n","outputs(x):  $676*9=4876$\n","$\n","wrong  : 676*9=6784\n","correct: 676*9=6084\n","outputs(x):  $476*4=4081$\n","$\n","wrong  : 476*4=1804\n","correct: 476*4=1904\n","outputs(x):  $183*9=7451$\n","$\n","wrong  : 183*9=1547\n","correct: 183*9=1647\n","outputs(x):  $374*8=2782$\n","$\n","wrong  : 374*8=2872\n","correct: 374*8=2992\n","outputs(x):  $483*8=4093$\n","$\n","wrong  : 483*8=3904\n","correct: 483*8=3864\n","outputs(x):  $747*9=3226$\n","$\n","wrong  : 747*9=6223\n","correct: 747*9=6723\n","outputs(x):  $994*7=8496$\n","$\n","wrong  : 994*7=6948\n","correct: 994*7=6958\n","outputs(x):  $653*8=4294$\n","$\n","wrong  : 653*8=4924\n","correct: 653*8=5224\n","outputs(x):  $712*8=6984$\n","$\n","wrong  : 712*8=4896\n","correct: 712*8=5696\n","outputs(x):  $881*9=9288$\n","$\n","wrong  : 881*9=8829\n","correct: 881*9=7929\n","outputs(x):  $394*9=6433$\n","$\n","wrong  : 394*9=3346\n","correct: 394*9=3546\n","outputs(x):  $619*9=1791$\n","$\n","wrong  : 619*9=1971\n","correct: 619*9=5571\n","outputs(x):  $180*4=025$\n","$2\n","wrong  : 180*4=520\n","correct: 180*4=720\n","outputs(x):  $848*7=6356$\n","$\n","wrong  : 848*7=6536\n","correct: 848*7=5936\n","outputs(x):  $960*4=0463$\n","$\n","wrong  : 960*4=3640\n","correct: 960*4=3840\n","outputs(x):  $686*6=6174$\n","$\n","wrong  : 686*6=4716\n","correct: 686*6=4116\n","outputs(x):  $963*3=9872$\n","$\n","wrong  : 963*3=2789\n","correct: 963*3=2889\n","outputs(x):  $713*7=1924$\n","$\n","wrong  : 713*7=4291\n","correct: 713*7=4991\n","outputs(x):  $417*9=3533$\n","$\n","wrong  : 417*9=3353\n","correct: 417*9=3753\n","outputs(x):  $841*9=9677$\n","$\n","wrong  : 841*9=7769\n","correct: 841*9=7569\n","outputs(x):  $389*9=1062$\n","$\n","wrong  : 389*9=2601\n","correct: 389*9=3501\n","outputs(x):  $582*7=4774$\n","$\n","wrong  : 582*7=4774\n","correct: 582*7=4074\n","outputs(x):  $310*9=0911$\n","$\n","wrong  : 310*9=1190\n","correct: 310*9=2790\n","outputs(x):  $380*6=0832$\n","$\n","wrong  : 380*6=2380\n","correct: 380*6=2280\n","outputs(x):  $624*8=2975$\n","$\n","wrong  : 624*8=5792\n","correct: 624*8=4992\n","outputs(x):  $957*7=9916$\n","$\n","wrong  : 957*7=6199\n","correct: 957*7=6699\n","outputs(x):  $418*9=2604$\n","$\n","wrong  : 418*9=4062\n","correct: 418*9=3762\n"," 38% 9/24 [00:00<00:00, 19.63it/s]outputs(x):  $783*8=4835$\n","$\n","wrong  : 783*8=5384\n","correct: 783*8=6264\n","outputs(x):  $345*9=5023$\n","$\n","wrong  : 345*9=3205\n","correct: 345*9=3105\n","outputs(x):  $449*7=3466$\n","$\n","wrong  : 449*7=6643\n","correct: 449*7=3143\n","outputs(x):  $962*7=4386$\n","$\n","wrong  : 962*7=6834\n","correct: 962*7=6734\n","outputs(x):  $825*9=527$\n","$5\n","wrong  : 825*9=725\n","correct: 825*9=7425\n","outputs(x):  $879*9=1177$\n","$\n","wrong  : 879*9=7711\n","correct: 879*9=7911\n","outputs(x):  $248*9=2382$\n","$\n","wrong  : 248*9=2832\n","correct: 248*9=2232\n","outputs(x):  $266*8=8231$\n","$\n","wrong  : 266*8=1328\n","correct: 266*8=2128\n","outputs(x):  $328*9=2503$\n","$\n","wrong  : 328*9=3052\n","correct: 328*9=2952\n","outputs(x):  $602*7=4164$\n","$\n","wrong  : 602*7=4614\n","correct: 602*7=4214\n","outputs(x):  $392*8=6392$\n","$\n","wrong  : 392*8=2936\n","correct: 392*8=3136\n","outputs(x):  $787*9=3896$\n","$\n","wrong  : 787*9=6983\n","correct: 787*9=7083\n","outputs(x):  $701*7=7036$\n","$\n","wrong  : 701*7=6307\n","correct: 701*7=4907\n","outputs(x):  $761*8=8826$\n","$\n","wrong  : 761*8=6288\n","correct: 761*8=6088\n","outputs(x):  $748*9=2385$\n","$\n","wrong  : 748*9=5832\n","correct: 748*9=6732\n","outputs(x):  $758*7=6825$\n","$\n","wrong  : 758*7=5286\n","correct: 758*7=5306\n","outputs(x):  $627*6=2634$\n","$\n","wrong  : 627*6=4362\n","correct: 627*6=3762\n","outputs(x):  $567*7=9604$\n","$\n","wrong  : 567*7=4069\n","correct: 567*7=3969\n","outputs(x):  $848*9=2357$\n","$\n","wrong  : 848*9=7532\n","correct: 848*9=7632\n","outputs(x):  $251*9=9591$\n","$\n","wrong  : 251*9=1959\n","correct: 251*9=2259\n","outputs(x):  $699*7=3944$\n","$\n","wrong  : 699*7=4493\n","correct: 699*7=4893\n","outputs(x):  $471*8=8633$\n","$\n","wrong  : 471*8=3368\n","correct: 471*8=3768\n","outputs(x):  $933*9=7947$\n","$\n","wrong  : 933*9=7497\n","correct: 933*9=8397\n","outputs(x):  $353*7=1713$\n","$\n","wrong  : 353*7=3171\n","correct: 353*7=2471\n","outputs(x):  $797*3=193$\n","$6\n","wrong  : 797*3=391\n","correct: 797*3=2391\n","outputs(x):  $628*8=4265$\n","$\n","wrong  : 628*8=5624\n","correct: 628*8=5024\n","outputs(x):  $318*9=2603$\n","$\n","wrong  : 318*9=3062\n","correct: 318*9=2862\n","outputs(x):  $254*9=6832$\n","$\n","wrong  : 254*9=2386\n","correct: 254*9=2286\n","outputs(x):  $863*6=8775$\n","$\n","wrong  : 863*6=5778\n","correct: 863*6=5178\n","outputs(x):  $379*7=3542$\n","$\n","wrong  : 379*7=2453\n","correct: 379*7=2653\n","outputs(x):  $788*6=8234$\n","$\n","wrong  : 788*6=4328\n","correct: 788*6=4728\n","outputs(x):  $198*9=2861$\n","$\n","wrong  : 198*9=1682\n","correct: 198*9=1782\n","outputs(x):  $158*9=2211$\n","$\n","wrong  : 158*9=1122\n","correct: 158*9=1422\n","outputs(x):  $914*7=8946$\n","$\n","wrong  : 914*7=6498\n","correct: 914*7=6398\n","outputs(x):  $375*3=521$\n","$1\n","wrong  : 375*3=125\n","correct: 375*3=1125\n","outputs(x):  $326*9=4303$\n","$\n","wrong  : 326*9=3034\n","correct: 326*9=2934\n","outputs(x):  $442*8=6313$\n","$\n","wrong  : 442*8=3136\n","correct: 442*8=3536\n","outputs(x):  $239*9=1362$\n","$\n","wrong  : 239*9=2631\n","correct: 239*9=2151\n","outputs(x):  $972*9=8448$\n","$\n","wrong  : 972*9=8448\n","correct: 972*9=8748\n","outputs(x):  $188*7=6141$\n","$\n","wrong  : 188*7=1416\n","correct: 188*7=1316\n","outputs(x):  $143*7=109$\n","$9\n","wrong  : 143*7=901\n","correct: 143*7=1001\n","outputs(x):  $116*9=4491$\n","$\n","wrong  : 116*9=1944\n","correct: 116*9=1044\n","outputs(x):  $746*8=8616$\n","$\n","wrong  : 746*8=6168\n","correct: 746*8=5968\n","outputs(x):  $492*7=4496$\n","$\n","wrong  : 492*7=6944\n","correct: 492*7=3444\n","outputs(x):  $940*7=0865$\n","$\n","wrong  : 940*7=5680\n","correct: 940*7=6580\n","outputs(x):  $215*7=5041$\n","$\n","wrong  : 215*7=1405\n","correct: 215*7=1505\n","outputs(x):  $372*9=8443$\n","$\n","wrong  : 372*9=3448\n","correct: 372*9=3348\n","outputs(x):  $952*9=8667$\n","$\n","wrong  : 952*9=7668\n","correct: 952*9=8568\n","outputs(x):  $708*7=6536$\n","$\n","wrong  : 708*7=6356\n","correct: 708*7=4956\n","outputs(x):  $861*7=7216$\n","$\n","wrong  : 861*7=6127\n","correct: 861*7=6027\n","outputs(x):  $152*7=4621$\n","$\n","wrong  : 152*7=1264\n","correct: 152*7=1064\n","outputs(x):  $678*9=2056$\n","$\n","wrong  : 678*9=6502\n","correct: 678*9=6102\n","outputs(x):  $127*6=268$\n","$3\n","wrong  : 127*6=862\n","correct: 127*6=762\n","outputs(x):  $367*9=3003$\n","$\n","wrong  : 367*9=3003\n","correct: 367*9=3303\n","outputs(x):  $592*9=8215$\n","$\n","wrong  : 592*9=5128\n","correct: 592*9=5328\n","outputs(x):  $850*8=0067$\n","$\n","wrong  : 850*8=7600\n","correct: 850*8=6800\n","outputs(x):  $870*7=0396$\n","$\n","wrong  : 870*7=6930\n","correct: 870*7=6090\n","outputs(x):  $830*8=0462$\n","$\n","wrong  : 830*8=2640\n","correct: 830*8=6640\n","outputs(x):  $876*4=4073$\n","$\n","wrong  : 876*4=3704\n","correct: 876*4=3504\n","outputs(x):  $734*9=6086$\n","$\n","wrong  : 734*9=6806\n","correct: 734*9=6606\n","outputs(x):  $574*8=2764$\n","$\n","wrong  : 574*8=4672\n","correct: 574*8=4592\n","outputs(x):  $951*4=4063$\n","$\n","wrong  : 951*4=3604\n","correct: 951*4=3804\n","outputs(x):  $749*7=3455$\n","$\n","wrong  : 749*7=5543\n","correct: 749*7=5243\n","outputs(x):  $574*9=6635$\n","$\n","wrong  : 574*9=5366\n","correct: 574*9=5166\n","outputs(x):  $860*7=0295$\n","$\n","wrong  : 860*7=5920\n","correct: 860*7=6020\n","outputs(x):  $908*9=2736$\n","$\n","wrong  : 908*9=6372\n","correct: 908*9=8172\n","outputs(x):  $604*9=6385$\n","$\n","wrong  : 604*9=5836\n","correct: 604*9=5436\n","outputs(x):  $430*7=0152$\n","$\n","wrong  : 430*7=2510\n","correct: 430*7=3010\n","outputs(x):  $349*9=1403$\n","$\n","wrong  : 349*9=3041\n","correct: 349*9=3141\n","outputs(x):  $925*6=0554$\n","$\n","wrong  : 925*6=4550\n","correct: 925*6=5550\n","outputs(x):  $715*7=5015$\n","$\n","wrong  : 715*7=5105\n","correct: 715*7=5005\n","outputs(x):  $203*7=1261$\n","$\n","wrong  : 203*7=1621\n","correct: 203*7=1421\n","outputs(x):  $473*8=4833$\n","$\n","wrong  : 473*8=3384\n","correct: 473*8=3784\n","outputs(x):  $974*9=6687$\n","$\n","wrong  : 974*9=7866\n","correct: 974*9=8766\n","outputs(x):  $674*9=6624$\n","$\n","wrong  : 674*9=4266\n","correct: 674*9=6066\n","outputs(x):  $971*3=3182$\n","$\n","wrong  : 971*3=2813\n","correct: 971*3=2913\n","outputs(x):  $172*9=8441$\n","$\n","wrong  : 172*9=1448\n","correct: 172*9=1548\n","outputs(x):  $548*9=2385$\n","$\n","wrong  : 548*9=5832\n","correct: 548*9=4932\n","outputs(x):  $112*9=8091$\n","$\n","wrong  : 112*9=1908\n","correct: 112*9=1008\n","outputs(x):  $751*6=6053$\n","$\n","wrong  : 751*6=3506\n","correct: 751*6=4506\n","outputs(x):  $644*3=2381$\n","$\n","wrong  : 644*3=1832\n","correct: 644*3=1932\n","outputs(x):  $905*9=5428$\n","$\n","wrong  : 905*9=8245\n","correct: 905*9=8145\n","outputs(x):  $259*7=3151$\n","$\n","wrong  : 259*7=1513\n","correct: 259*7=1813\n","outputs(x):  $299*7=3943$\n","$\n","wrong  : 299*7=3493\n","correct: 299*7=2093\n","outputs(x):  $886*7=2036$\n","$\n","wrong  : 886*7=6302\n","correct: 886*7=6202\n","outputs(x):  $924*7=8696$\n","$\n","wrong  : 924*7=6968\n","correct: 924*7=6468\n","outputs(x):  $540*9=0694$\n","$\n","wrong  : 540*9=4960\n","correct: 540*9=4860\n","outputs(x):  $733*9=7965$\n","$\n","wrong  : 733*9=5697\n","correct: 733*9=6597\n","outputs(x):  $428*4=2153$\n","$\n","wrong  : 428*4=3512\n","correct: 428*4=1712\n","outputs(x):  $607*8=6565$\n","$\n","wrong  : 607*8=5656\n","correct: 607*8=4856\n","outputs(x):  $271*7=7935$\n","$\n","wrong  : 271*7=5397\n","correct: 271*7=1897\n","outputs(x):  $565*7=5504$\n","$\n","wrong  : 565*7=4055\n","correct: 565*7=3955\n","outputs(x):  $562*5=0183$\n","$\n","wrong  : 562*5=3810\n","correct: 562*5=2810\n","outputs(x):  $751*8=8065$\n","$\n","wrong  : 751*8=5608\n","correct: 751*8=6008\n","outputs(x):  $888*7=6146$\n","$\n","wrong  : 888*7=6416\n","correct: 888*7=6216\n","outputs(x):  $246*9=4132$\n","$\n","wrong  : 246*9=2314\n","correct: 246*9=2214\n","outputs(x):  $772*3=6112$\n","$\n","wrong  : 772*3=2116\n","correct: 772*3=2316\n","outputs(x):  $892*7=4496$\n","$\n","wrong  : 892*7=6944\n","correct: 892*7=6244\n","outputs(x):  $315*8=029$\n","$1\n","wrong  : 315*8=920\n","correct: 315*8=2520\n","outputs(x):  $975*7=5284$\n","$\n","wrong  : 975*7=4825\n","correct: 975*7=6825\n","outputs(x):  $266*7=2651$\n","$\n","wrong  : 266*7=1562\n","correct: 266*7=1862\n","outputs(x):  $964*9=6778$\n","$\n","wrong  : 964*9=8776\n","correct: 964*9=8676\n","outputs(x):  $910*7=0712$\n","$\n","wrong  : 910*7=2170\n","correct: 910*7=6370\n","outputs(x):  $577*9=3906$\n","$\n","wrong  : 577*9=6093\n","correct: 577*9=5193\n","outputs(x):  $229*9=1611$\n","$\n","wrong  : 229*9=1161\n","correct: 229*9=2061\n","outputs(x):  $404*9=6383$\n","$\n","wrong  : 404*9=3836\n","correct: 404*9=3636\n","outputs(x):  $298*7=6871$\n","$\n","wrong  : 298*7=1786\n","correct: 298*7=2086\n"," 50% 12/24 [00:00<00:00, 20.17it/s]outputs(x):  $867*8=6316$\n","$\n","wrong  : 867*8=6136\n","correct: 867*8=6936\n","outputs(x):  $119*9=1791$\n","$\n","wrong  : 119*9=1971\n","correct: 119*9=1071\n","outputs(x):  $519*6=4103$\n","$\n","wrong  : 519*6=3014\n","correct: 519*6=3114\n","outputs(x):  $762*6=2793$\n","$\n","wrong  : 762*6=3972\n","correct: 762*6=4572\n","outputs(x):  $902*6=2155$\n","$\n","wrong  : 902*6=5512\n","correct: 902*6=5412\n","outputs(x):  $264*8=2912$\n","$\n","wrong  : 264*8=2192\n","correct: 264*8=2112\n","outputs(x):  $169*8=2511$\n","$\n","wrong  : 169*8=1152\n","correct: 169*8=1352\n","outputs(x):  $549*3=7471$\n","$\n","wrong  : 549*3=1747\n","correct: 549*3=1647\n","outputs(x):  $482*9=8344$\n","$\n","wrong  : 482*9=4438\n","correct: 482*9=4338\n","outputs(x):  $816*7=2146$\n","$\n","wrong  : 816*7=6412\n","correct: 816*7=5712\n","outputs(x):  $376*4=4041$\n","$\n","wrong  : 376*4=1404\n","correct: 376*4=1504\n","outputs(x):  $923*3=9632$\n","$\n","wrong  : 923*3=2369\n","correct: 923*3=2769\n","outputs(x):  $348*7=6322$\n","$\n","wrong  : 348*7=2236\n","correct: 348*7=2436\n","outputs(x):  $119*4=676$\n","$4\n","wrong  : 119*4=676\n","correct: 119*4=476\n","outputs(x):  $155*7=5841$\n","$\n","wrong  : 155*7=1485\n","correct: 155*7=1085\n","outputs(x):  $813*8=4036$\n","$\n","wrong  : 813*8=6304\n","correct: 813*8=6504\n","outputs(x):  $314*3=246$\n","$0\n","wrong  : 314*3=642\n","correct: 314*3=942\n","outputs(x):  $108*7=6536$\n","$\n","wrong  : 108*7=6356\n","correct: 108*7=756\n","outputs(x):  $648*3=4422$\n","$\n","wrong  : 648*3=2244\n","correct: 648*3=1944\n","outputs(x):  $225*9=5222$\n","$\n","wrong  : 225*9=2225\n","correct: 225*9=2025\n","outputs(x):  $564*3=2971$\n","$\n","wrong  : 564*3=1792\n","correct: 564*3=1692\n","outputs(x):  $437*8=6903$\n","$\n","wrong  : 437*8=3096\n","correct: 437*8=3496\n","outputs(x):  $348*8=4852$\n","$\n","wrong  : 348*8=2584\n","correct: 348*8=2784\n","outputs(x):  $755*9=5967$\n","$\n","wrong  : 755*9=7695\n","correct: 755*9=6795\n","outputs(x):  $737*3=1112$\n","$\n","wrong  : 737*3=2111\n","correct: 737*3=2211\n","outputs(x):  $597*9=3714$\n","$\n","wrong  : 597*9=4173\n","correct: 597*9=5373\n","outputs(x):  $472*9=8444$\n","$\n","wrong  : 472*9=4448\n","correct: 472*9=4248\n","outputs(x):  $667*3=1091$\n","$\n","wrong  : 667*3=1901\n","correct: 667*3=2001\n","outputs(x):  $805*8=0427$\n","$\n","wrong  : 805*8=7240\n","correct: 805*8=6440\n","outputs(x):  $668*4=2703$\n","$\n","wrong  : 668*4=3072\n","correct: 668*4=2672\n","outputs(x):  $462*8=6983$\n","$\n","wrong  : 462*8=3896\n","correct: 462*8=3696\n","outputs(x):  $213*8=4091$\n","$\n","wrong  : 213*8=1904\n","correct: 213*8=1704\n","outputs(x):  $408*7=6536$\n","$\n","wrong  : 408*7=6356\n","correct: 408*7=2856\n","outputs(x):  $794*8=2595$\n","$\n","wrong  : 794*8=5952\n","correct: 794*8=6352\n","outputs(x):  $775*9=5707$\n","$\n","wrong  : 775*9=7075\n","correct: 775*9=6975\n","outputs(x):  $386*7=2032$\n","$\n","wrong  : 386*7=2302\n","correct: 386*7=2702\n","outputs(x):  $792*7=4484$\n","$\n","wrong  : 792*7=4844\n","correct: 792*7=5544\n","outputs(x):  $413*7=1923$\n","$\n","wrong  : 413*7=3291\n","correct: 413*7=2891\n","outputs(x):  $441*9=9673$\n","$\n","wrong  : 441*9=3769\n","correct: 441*9=3969\n","outputs(x):  $615*9=5328$\n","$\n","wrong  : 615*9=8235\n","correct: 615*9=5535\n","outputs(x):  $375*4=0011$\n","$\n","wrong  : 375*4=1100\n","correct: 375*4=1500\n","outputs(x):  $623*7=1624$\n","$\n","wrong  : 623*7=4261\n","correct: 623*7=4361\n","outputs(x):  $877*8=6187$\n","$\n","wrong  : 877*8=7816\n","correct: 877*8=7016\n","outputs(x):  $935*9=5128$\n","$\n","wrong  : 935*9=8215\n","correct: 935*9=8415\n","outputs(x):  $317*7=9132$\n","$\n","wrong  : 317*7=2319\n","correct: 317*7=2219\n","outputs(x):  $644*9=6985$\n","$\n","wrong  : 644*9=5896\n","correct: 644*9=5796\n","outputs(x):  $471*9=9334$\n","$\n","wrong  : 471*9=4339\n","correct: 471*9=4239\n","outputs(x):  $470*8=0693$\n","$\n","wrong  : 470*8=3960\n","correct: 470*8=3760\n","outputs(x):  $842*7=4975$\n","$\n","wrong  : 842*7=5794\n","correct: 842*7=5894\n","outputs(x):  $814*7=8946$\n","$\n","wrong  : 814*7=6498\n","correct: 814*7=5698\n","outputs(x):  $345*8=0603$\n","$\n","wrong  : 345*8=3060\n","correct: 345*8=2760\n","outputs(x):  $657*8=6545$\n","$\n","wrong  : 657*8=5456\n","correct: 657*8=5256\n","outputs(x):  $447*2=496$\n","$5\n","wrong  : 447*2=694\n","correct: 447*2=894\n","outputs(x):  $818*8=4436$\n","$\n","wrong  : 818*8=6344\n","correct: 818*8=6544\n","outputs(x):  $960*9=0478$\n","$\n","wrong  : 960*9=8740\n","correct: 960*9=8640\n","outputs(x):  $567*5=5333$\n","$\n","wrong  : 567*5=3335\n","correct: 567*5=2835\n","outputs(x):  $802*9=8118$\n","$\n","wrong  : 802*9=8118\n","correct: 802*9=7218\n","outputs(x):  $359*6=4502$\n","$\n","wrong  : 359*6=2054\n","correct: 359*6=2154\n","outputs(x):  $667*7=9624$\n","$\n","wrong  : 667*7=4269\n","correct: 667*7=4669\n","outputs(x):  $754*9=6885$\n","$\n","wrong  : 754*9=5886\n","correct: 754*9=6786\n","outputs(x):  $777*8=616$\n","$1\n","wrong  : 777*8=616\n","correct: 777*8=6216\n","outputs(x):  $959*6=4565$\n","$\n","wrong  : 959*6=5654\n","correct: 959*6=5754\n","outputs(x):  $962*9=8578$\n","$\n","wrong  : 962*9=8758\n","correct: 962*9=8658\n","outputs(x):  $802*3=6041$\n","$\n","wrong  : 802*3=1406\n","correct: 802*3=2406\n","outputs(x):  $913*7=1946$\n","$\n","wrong  : 913*7=6491\n","correct: 913*7=6391\n","outputs(x):  $659*9=1385$\n","$\n","wrong  : 659*9=5831\n","correct: 659*9=5931\n","outputs(x):  $522*7=4573$\n","$\n","wrong  : 522*7=3754\n","correct: 522*7=3654\n","outputs(x):  $104*9=6331$\n","$\n","wrong  : 104*9=1336\n","correct: 104*9=936\n","outputs(x):  $437*3=1141$\n","$\n","wrong  : 437*3=1411\n","correct: 437*3=1311\n","outputs(x):  $237*8=6961$\n","$\n","wrong  : 237*8=1696\n","correct: 237*8=1896\n","outputs(x):  $793*9=7326$\n","$\n","wrong  : 793*9=6237\n","correct: 793*9=7137\n","outputs(x):  $940*9=0675$\n","$\n","wrong  : 940*9=5760\n","correct: 940*9=8460\n","outputs(x):  $461*8=8883$\n","$\n","wrong  : 461*8=3888\n","correct: 461*8=3688\n","outputs(x):  $331*9=9723$\n","$\n","wrong  : 331*9=3279\n","correct: 331*9=2979\n","outputs(x):  $248*3=4472$\n","$\n","wrong  : 248*3=2744\n","correct: 248*3=744\n","outputs(x):  $774*8=2975$\n","$\n","wrong  : 774*8=5792\n","correct: 774*8=6192\n","outputs(x):  $765*9=5896$\n","$\n","wrong  : 765*9=6985\n","correct: 765*9=6885\n","outputs(x):  $181*7=7611$\n","$\n","wrong  : 181*7=1167\n","correct: 181*7=1267\n","outputs(x):  $669*9=1285$\n","$\n","wrong  : 669*9=5821\n","correct: 669*9=6021\n","outputs(x):  $137*3=115$\n","$2\n","wrong  : 137*3=511\n","correct: 137*3=411\n","outputs(x):  $411*3=3521$\n","$\n","wrong  : 411*3=1253\n","correct: 411*3=1233\n","outputs(x):  $838*3=4162$\n","$\n","wrong  : 838*3=2614\n","correct: 838*3=2514\n","outputs(x):  $549*6=4903$\n","$\n","wrong  : 549*6=3094\n","correct: 549*6=3294\n","outputs(x):  $929*8=2537$\n","$\n","wrong  : 929*8=7352\n","correct: 929*8=7432\n","outputs(x):  $147*9=3221$\n","$\n","wrong  : 147*9=1223\n","correct: 147*9=1323\n","outputs(x):  $941*9=9628$\n","$\n","wrong  : 941*9=8269\n","correct: 941*9=8469\n","outputs(x):  $962*8=6987$\n","$\n","wrong  : 962*8=7896\n","correct: 962*8=7696\n","outputs(x):  $979*6=4784$\n","$\n","wrong  : 979*6=4874\n","correct: 979*6=5874\n","outputs(x):  $962*3=6872$\n","$\n","wrong  : 962*3=2786\n","correct: 962*3=2886\n","outputs(x):  $536*9=4274$\n","$\n","wrong  : 536*9=4724\n","correct: 536*9=4824\n","outputs(x):  $723*9=7046$\n","$\n","wrong  : 723*9=6407\n","correct: 723*9=6507\n","outputs(x):  $372*7=4032$\n","$\n","wrong  : 372*7=2304\n","correct: 372*7=2604\n","outputs(x):  $795*3=5852$\n","$\n","wrong  : 795*3=2585\n","correct: 795*3=2385\n","outputs(x):  $107*9=3611$\n","$\n","wrong  : 107*9=1163\n","correct: 107*9=963\n"," 62% 15/24 [00:00<00:00, 20.29it/s]outputs(x):  $228*8=4221$\n","$\n","wrong  : 228*8=1224\n","correct: 228*8=1824\n","outputs(x):  $732*7=4225$\n","$\n","wrong  : 732*7=5224\n","correct: 732*7=5124\n","outputs(x):  $914*6=4855$\n","$\n","wrong  : 914*6=5584\n","correct: 914*6=5484\n","outputs(x):  $227*7=9811$\n","$\n","wrong  : 227*7=1189\n","correct: 227*7=1589\n","outputs(x):  $967*7=9686$\n","$\n","wrong  : 967*7=6869\n","correct: 967*7=6769\n","outputs(x):  $523*9=7084$\n","$\n","wrong  : 523*9=4807\n","correct: 523*9=4707\n","outputs(x):  $378*7=6432$\n","$\n","wrong  : 378*7=2346\n","correct: 378*7=2646\n","outputs(x):  $797*7=9726$\n","$\n","wrong  : 797*7=6279\n","correct: 797*7=5579\n","outputs(x):  $741*9=9675$\n","$\n","wrong  : 741*9=5769\n","correct: 741*9=6669\n","outputs(x):  $558*9=2225$\n","$\n","wrong  : 558*9=5222\n","correct: 558*9=5022\n","outputs(x):  $250*7=0551$\n","$\n","wrong  : 250*7=1550\n","correct: 250*7=1750\n","outputs(x):  $197*8=6791$\n","$\n","wrong  : 197*8=1976\n","correct: 197*8=1576\n","outputs(x):  $317*3=156$\n","$5\n","wrong  : 317*3=651\n","correct: 317*3=951\n","outputs(x):  $961*7=7286$\n","$\n","wrong  : 961*7=6827\n","correct: 961*7=6727\n","outputs(x):  $903*5=5104$\n","$\n","wrong  : 903*5=4015\n","correct: 903*5=4515\n","outputs(x):  $808*7=6596$\n","$\n","wrong  : 808*7=6956\n","correct: 808*7=5656\n","outputs(x):  $184*6=407$\n","$9\n","wrong  : 184*6=704\n","correct: 184*6=1104\n","outputs(x):  $103*7=12$\n","$90\n","wrong  : 103*7=21\n","correct: 103*7=721\n","outputs(x):  $231*9=9712$\n","$\n","wrong  : 231*9=2179\n","correct: 231*9=2079\n","outputs(x):  $433*6=8972$\n","$\n","wrong  : 433*6=2798\n","correct: 433*6=2598\n","outputs(x):  $855*8=0467$\n","$\n","wrong  : 855*8=7640\n","correct: 855*8=6840\n","outputs(x):  $110*1=01$\n","$4*\n","wrong  : 110*1=10\n","correct: 110*1=110\n","outputs(x):  $782*8=6506$\n","$\n","wrong  : 782*8=6056\n","correct: 782*8=6256\n","outputs(x):  $109*9=1811$\n","$\n","wrong  : 109*9=1181\n","correct: 109*9=981\n","outputs(x):  $585*7=5983$\n","$\n","wrong  : 585*7=3895\n","correct: 585*7=4095\n","outputs(x):  $368*8=4452$\n","$\n","wrong  : 368*8=2544\n","correct: 368*8=2944\n","outputs(x):  $668*3=4091$\n","$\n","wrong  : 668*3=1904\n","correct: 668*3=2004\n","outputs(x):  $942*7=4966$\n","$\n","wrong  : 942*7=6694\n","correct: 942*7=6594\n","outputs(x):  $300*9=0013$\n","$\n","wrong  : 300*9=3100\n","correct: 300*9=2700\n","outputs(x):  $830*7=0156$\n","$\n","wrong  : 830*7=6510\n","correct: 830*7=5810\n","outputs(x):  $362*8=6923$\n","$\n","wrong  : 362*8=3296\n","correct: 362*8=2896\n","outputs(x):  $147*3=145$\n","$2\n","wrong  : 147*3=541\n","correct: 147*3=441\n","outputs(x):  $511*3=3551$\n","$\n","wrong  : 511*3=1553\n","correct: 511*3=1533\n","outputs(x):  $907*9=3628$\n","$\n","wrong  : 907*9=8263\n","correct: 907*9=8163\n","outputs(x):  $707*9=3656$\n","$\n","wrong  : 707*9=6563\n","correct: 707*9=6363\n","outputs(x):  $709*7=3655$\n","$\n","wrong  : 709*7=5563\n","correct: 709*7=4963\n","outputs(x):  $713*3=936$\n","$2\n","wrong  : 713*3=639\n","correct: 713*3=2139\n","outputs(x):  $103*5=510$\n","$1\n","wrong  : 103*5=15\n","correct: 103*5=515\n","outputs(x):  $896*7=276$\n","$6\n","wrong  : 896*7=672\n","correct: 896*7=6272\n","outputs(x):  $686*8=8805$\n","$\n","wrong  : 686*8=5088\n","correct: 686*8=5488\n","outputs(x):  $519*9=1753$\n","$\n","wrong  : 519*9=3571\n","correct: 519*9=4671\n","outputs(x):  $268*7=6752$\n","$\n","wrong  : 268*7=2576\n","correct: 268*7=1876\n","outputs(x):  $808*4=2343$\n","$\n","wrong  : 808*4=3432\n","correct: 808*4=3232\n","outputs(x):  $869*6=4155$\n","$\n","wrong  : 869*6=5514\n","correct: 869*6=5214\n","outputs(x):  $243*5=5111$\n","$\n","wrong  : 243*5=1115\n","correct: 243*5=1215\n","outputs(x):  $128*8=4261$\n","$\n","wrong  : 128*8=1624\n","correct: 128*8=1024\n","outputs(x):  $375*7=5222$\n","$\n","wrong  : 375*7=2225\n","correct: 375*7=2625\n","outputs(x):  $885*3=5552$\n","$\n","wrong  : 885*3=2555\n","correct: 885*3=2655\n","outputs(x):  $546*7=2293$\n","$\n","wrong  : 546*7=3922\n","correct: 546*7=3822\n","outputs(x):  $698*8=4815$\n","$\n","wrong  : 698*8=5184\n","correct: 698*8=5584\n","outputs(x):  $457*6=2492$\n","$\n","wrong  : 457*6=2942\n","correct: 457*6=2742\n","outputs(x):  $911*9=9927$\n","$\n","wrong  : 911*9=7299\n","correct: 911*9=8199\n","outputs(x):  $617*4=8682$\n","$\n","wrong  : 617*4=2868\n","correct: 617*4=2468\n","outputs(x):  $575*9=5714$\n","$\n","wrong  : 575*9=4175\n","correct: 575*9=5175\n","outputs(x):  $336*8=8652$\n","$\n","wrong  : 336*8=2568\n","correct: 336*8=2688\n","outputs(x):  $258*7=6856$\n","$\n","wrong  : 258*7=6586\n","correct: 258*7=1806\n","outputs(x):  $344*7=8032$\n","$\n","wrong  : 344*7=2308\n","correct: 344*7=2408\n","outputs(x):  $562*7=4325$\n","$\n","wrong  : 562*7=5234\n","correct: 562*7=3934\n","outputs(x):  $292*8=6352$\n","$\n","wrong  : 292*8=2536\n","correct: 292*8=2336\n","outputs(x):  $312*8=6922$\n","$\n","wrong  : 312*8=2296\n","correct: 312*8=2496\n","outputs(x):  $564*8=2554$\n","$\n","wrong  : 564*8=4552\n","correct: 564*8=4512\n","outputs(x):  $386*9=4753$\n","$\n","wrong  : 386*9=3574\n","correct: 386*9=3474\n","outputs(x):  $718*5=0954$\n","$\n","wrong  : 718*5=4590\n","correct: 718*5=3590\n","outputs(x):  $716*9=4436$\n","$\n","wrong  : 716*9=6344\n","correct: 716*9=6444\n","outputs(x):  $856*8=8467$\n","$\n","wrong  : 856*8=7648\n","correct: 856*8=6848\n","outputs(x):  $508*7=6554$\n","$\n","wrong  : 508*7=4556\n","correct: 508*7=3556\n","outputs(x):  $814*8=2176$\n","$\n","wrong  : 814*8=6712\n","correct: 814*8=6512\n","outputs(x):  $105*8=0421$\n","$\n","wrong  : 105*8=1240\n","correct: 105*8=840\n","outputs(x):  $384*9=6593$\n","$\n","wrong  : 384*9=3956\n","correct: 384*9=3456\n","outputs(x):  $775*8=0007$\n","$\n","wrong  : 775*8=7000\n","correct: 775*8=6200\n","outputs(x):  $966*3=8992$\n","$\n","wrong  : 966*3=2998\n","correct: 966*3=2898\n","outputs(x):  $773*3=912$\n","$8\n","wrong  : 773*3=219\n","correct: 773*3=2319\n","outputs(x):  $521*7=7473$\n","$\n","wrong  : 521*7=3747\n","correct: 521*7=3647\n","outputs(x):  $983*8=4097$\n","$\n","wrong  : 983*8=7904\n","correct: 983*8=7864\n","outputs(x):  $110*4=046$\n","$3\n","wrong  : 110*4=640\n","correct: 110*4=440\n","outputs(x):  $845*7=5175$\n","$\n","wrong  : 845*7=5715\n","correct: 845*7=5915\n","outputs(x):  $874*7=8195$\n","$\n","wrong  : 874*7=5918\n","correct: 874*7=6118\n","outputs(x):  $831*7=7156$\n","$\n","wrong  : 831*7=6517\n","correct: 831*7=5817\n","outputs(x):  $116*4=466$\n","$4\n","wrong  : 116*4=664\n","correct: 116*4=464\n","outputs(x):  $985*9=5608$\n","$\n","wrong  : 985*9=8065\n","correct: 985*9=8865\n","outputs(x):  $366*3=8911$\n","$\n","wrong  : 366*3=1198\n","correct: 366*3=1098\n","outputs(x):  $488*8=4053$\n","$\n","wrong  : 488*8=3504\n","correct: 488*8=3904\n","outputs(x):  $654*4=6182$\n","$\n","wrong  : 654*4=2816\n","correct: 654*4=2616\n","outputs(x):  $483*9=7493$\n","$\n","wrong  : 483*9=3947\n","correct: 483*9=4347\n","outputs(x):  $465*4=0661$\n","$\n","wrong  : 465*4=1660\n","correct: 465*4=1860\n","outputs(x):  $272*7=4061$\n","$\n","wrong  : 272*7=1604\n","correct: 272*7=1904\n","outputs(x):  $172*8=679$\n","$2\n","wrong  : 172*8=976\n","correct: 172*8=1376\n","outputs(x):  $750*7=0595$\n","$\n","wrong  : 750*7=5950\n","correct: 750*7=5250\n","outputs(x):  $904*8=2347$\n","$\n","wrong  : 904*8=7432\n","correct: 904*8=7232\n","outputs(x):  $770*7=0155$\n","$\n","wrong  : 770*7=5510\n","correct: 770*7=5390\n","outputs(x):  $932*9=8848$\n","$\n","wrong  : 932*9=8488\n","correct: 932*9=8388\n","outputs(x):  $579*7=3543$\n","$\n","wrong  : 579*7=3453\n","correct: 579*7=4053\n","outputs(x):  $114*8=2111$\n","$\n","wrong  : 114*8=1112\n","correct: 114*8=912\n","outputs(x):  $125*4=009$\n","$4\n","wrong  : 125*4=900\n","correct: 125*4=500\n","outputs(x):  $798*9=2896$\n","$\n","wrong  : 798*9=6982\n","correct: 798*9=7182\n","outputs(x):  $660*7=0293$\n","$\n","wrong  : 660*7=3920\n","correct: 660*7=4620\n","outputs(x):  $311*8=8852$\n","$\n","wrong  : 311*8=2588\n","correct: 311*8=2488\n","outputs(x):  $909*7=3656$\n","$\n","wrong  : 909*7=6563\n","correct: 909*7=6363\n","outputs(x):  $753*9=7758$\n","$\n","wrong  : 753*9=8577\n","correct: 753*9=6777\n","outputs(x):  $533*8=4644$\n","$\n","wrong  : 533*8=4464\n","correct: 533*8=4264\n","outputs(x):  $138*9=2411$\n","$\n","wrong  : 138*9=1142\n","correct: 138*9=1242\n","outputs(x):  $177*7=9351$\n","$\n","wrong  : 177*7=1539\n","correct: 177*7=1239\n","outputs(x):  $840*7=0865$\n","$\n","wrong  : 840*7=5680\n","correct: 840*7=5880\n"," 75% 18/24 [00:00<00:00, 20.21it/s]outputs(x):  $776*7=2374$\n","$\n","wrong  : 776*7=4732\n","correct: 776*7=5432\n","outputs(x):  $425*7=5703$\n","$\n","wrong  : 425*7=3075\n","correct: 425*7=2975\n","outputs(x):  $642*9=8795$\n","$\n","wrong  : 642*9=5978\n","correct: 642*9=5778\n","outputs(x):  $587*7=9073$\n","$\n","wrong  : 587*7=3709\n","correct: 587*7=4109\n","outputs(x):  $234*3=209$\n","$7\n","wrong  : 234*3=902\n","correct: 234*3=702\n","outputs(x):  $306*9=4513$\n","$\n","wrong  : 306*9=3154\n","correct: 306*9=2754\n","outputs(x):  $846*9=4177$\n","$\n","wrong  : 846*9=7714\n","correct: 846*9=7614\n","outputs(x):  $961*3=3852$\n","$\n","wrong  : 961*3=2583\n","correct: 961*3=2883\n","outputs(x):  $111*1=112$\n","$5\n","wrong  : 111*1=211\n","correct: 111*1=111\n","outputs(x):  $827*9=3477$\n","$\n","wrong  : 827*9=7743\n","correct: 827*9=7443\n","outputs(x):  $279*3=736$\n","$5\n","wrong  : 279*3=637\n","correct: 279*3=837\n","outputs(x):  $330*9=0772$\n","$\n","wrong  : 330*9=2770\n","correct: 330*9=2970\n","outputs(x):  $718*6=8074$\n","$\n","wrong  : 718*6=4708\n","correct: 718*6=4308\n","outputs(x):  $272*8=6732$\n","$\n","wrong  : 272*8=2376\n","correct: 272*8=2176\n","outputs(x):  $426*4=4031$\n","$\n","wrong  : 426*4=1304\n","correct: 426*4=1704\n","outputs(x):  $149*7=3483$\n","$\n","wrong  : 149*7=3843\n","correct: 149*7=1043\n","outputs(x):  $144*9=6984$\n","$\n","wrong  : 144*9=4896\n","correct: 144*9=1296\n","outputs(x):  $833*7=1375$\n","$\n","wrong  : 833*7=5731\n","correct: 833*7=5831\n","outputs(x):  $652*7=4684$\n","$\n","wrong  : 652*7=4864\n","correct: 652*7=4564\n","outputs(x):  $116*1=61$\n","$28\n","wrong  : 116*1=16\n","correct: 116*1=116\n","outputs(x):  $224*8=2991$\n","$\n","wrong  : 224*8=1992\n","correct: 224*8=1792\n","outputs(x):  $528*7=6953$\n","$\n","wrong  : 528*7=3596\n","correct: 528*7=3696\n","outputs(x):  $300*8=0062$\n","$\n","wrong  : 300*8=2600\n","correct: 300*8=2400\n","outputs(x):  $367*7=9623$\n","$\n","wrong  : 367*7=3269\n","correct: 367*7=2569\n","outputs(x):  $119*6=415$\n","$4\n","wrong  : 119*6=514\n","correct: 119*6=714\n","outputs(x):  $797*9=3778$\n","$\n","wrong  : 797*9=8773\n","correct: 797*9=7173\n","outputs(x):  $321*8=8692$\n","$\n","wrong  : 321*8=2968\n","correct: 321*8=2568\n","outputs(x):  $915*8=0257$\n","$\n","wrong  : 915*8=7520\n","correct: 915*8=7320\n","outputs(x):  $174*9=6641$\n","$\n","wrong  : 174*9=1466\n","correct: 174*9=1566\n","outputs(x):  $928*3=4842$\n","$\n","wrong  : 928*3=2484\n","correct: 928*3=2784\n","outputs(x):  $227*9=3461$\n","$\n","wrong  : 227*9=1643\n","correct: 227*9=2043\n","outputs(x):  $660*9=0415$\n","$\n","wrong  : 660*9=5140\n","correct: 660*9=5940\n","outputs(x):  $107*4=826$\n","$4\n","wrong  : 107*4=628\n","correct: 107*4=428\n","outputs(x):  $726*9=4346$\n","$\n","wrong  : 726*9=6434\n","correct: 726*9=6534\n","outputs(x):  $206*7=2421$\n","$\n","wrong  : 206*7=1242\n","correct: 206*7=1442\n","outputs(x):  $109*4=636$\n","$2\n","wrong  : 109*4=636\n","correct: 109*4=436\n","outputs(x):  $803*8=4227$\n","$\n","wrong  : 803*8=7224\n","correct: 803*8=6424\n","outputs(x):  $854*7=8725$\n","$\n","wrong  : 854*7=5278\n","correct: 854*7=5978\n","outputs(x):  $693*8=4494$\n","$\n","wrong  : 693*8=4944\n","correct: 693*8=5544\n","outputs(x):  $103*4=218$\n","$1\n","wrong  : 103*4=812\n","correct: 103*4=412\n","outputs(x):  $684*9=6575$\n","$\n","wrong  : 684*9=5756\n","correct: 684*9=6156\n","outputs(x):  $167*3=104$\n","$7\n","wrong  : 167*3=401\n","correct: 167*3=501\n","outputs(x):  $332*3=696$\n","$9\n","wrong  : 332*3=696\n","correct: 332*3=996\n","outputs(x):  $237*9=3303$\n","$\n","wrong  : 237*9=3033\n","correct: 237*9=2133\n","outputs(x):  $438*6=8202$\n","$\n","wrong  : 438*6=2028\n","correct: 438*6=2628\n","outputs(x):  $706*7=2424$\n","$\n","wrong  : 706*7=4242\n","correct: 706*7=4942\n","outputs(x):  $939*6=4355$\n","$\n","wrong  : 939*6=5534\n","correct: 939*6=5634\n","outputs(x):  $490*7=0396$\n","$\n","wrong  : 490*7=6930\n","correct: 490*7=3430\n","outputs(x):  $223*7=1611$\n","$\n","wrong  : 223*7=1161\n","correct: 223*7=1561\n","outputs(x):  $233*9=7912$\n","$\n","wrong  : 233*9=2197\n","correct: 233*9=2097\n","outputs(x):  $608*4=2382$\n","$\n","wrong  : 608*4=2832\n","correct: 608*4=2432\n","outputs(x):  $768*7=6756$\n","$\n","wrong  : 768*7=6576\n","correct: 768*7=5376\n","outputs(x):  $760*6=0645$\n","$\n","wrong  : 760*6=5460\n","correct: 760*6=4560\n","outputs(x):  $643*7=1024$\n","$\n","wrong  : 643*7=4201\n","correct: 643*7=4501\n","outputs(x):  $150*7=0511$\n","$\n","wrong  : 150*7=1150\n","correct: 150*7=1050\n","outputs(x):  $373*8=4852$\n","$\n","wrong  : 373*8=2584\n","correct: 373*8=2984\n","outputs(x):  $279*7=3541$\n","$\n","wrong  : 279*7=1453\n","correct: 279*7=1953\n","outputs(x):  $727*8=6775$\n","$\n","wrong  : 727*8=5776\n","correct: 727*8=5816\n","outputs(x):  $404*4=6121$\n","$\n","wrong  : 404*4=1216\n","correct: 404*4=1616\n","outputs(x):  $447*8=6773$\n","$\n","wrong  : 447*8=3776\n","correct: 447*8=3576\n","outputs(x):  $858*8=4666$\n","$\n","wrong  : 858*8=6664\n","correct: 858*8=6864\n","outputs(x):  $607*4=8282$\n","$\n","wrong  : 607*4=2828\n","correct: 607*4=2428\n","outputs(x):  $220*8=0612$\n","$\n","wrong  : 220*8=2160\n","correct: 220*8=1760\n","outputs(x):  $531*8=8404$\n","$\n","wrong  : 531*8=4048\n","correct: 531*8=4248\n","outputs(x):  $121*6=629$\n","$6\n","wrong  : 121*6=926\n","correct: 121*6=726\n","outputs(x):  $478*9=2224$\n","$\n","wrong  : 478*9=4222\n","correct: 478*9=4302\n","outputs(x):  $653*7=1793$\n","$\n","wrong  : 653*7=3971\n","correct: 653*7=4571\n","outputs(x):  $806*9=4537$\n","$\n","wrong  : 806*9=7354\n","correct: 806*9=7254\n","outputs(x):  $795*9=5526$\n","$\n","wrong  : 795*9=6255\n","correct: 795*9=7155\n","outputs(x):  $395*8=0632$\n","$\n","wrong  : 395*8=2360\n","correct: 395*8=3160\n","outputs(x):  $138*6=8201$\n","$\n","wrong  : 138*6=1028\n","correct: 138*6=828\n","outputs(x):  $745*8=0676$\n","$\n","wrong  : 745*8=6760\n","correct: 745*8=5960\n","outputs(x):  $431*7=7133$\n","$\n","wrong  : 431*7=3317\n","correct: 431*7=3017\n","outputs(x):  $429*8=2533$\n","$\n","wrong  : 429*8=3352\n","correct: 429*8=3432\n","outputs(x):  $762*9=8596$\n","$\n","wrong  : 762*9=6958\n","correct: 762*9=6858\n","outputs(x):  $829*4=6192$\n","$\n","wrong  : 829*4=2916\n","correct: 829*4=3316\n","outputs(x):  $275*9=5732$\n","$\n","wrong  : 275*9=2375\n","correct: 275*9=2475\n","outputs(x):  $525*8=0004$\n","$\n","wrong  : 525*8=4000\n","correct: 525*8=4200\n","outputs(x):  $427*7=9803$\n","$\n","wrong  : 427*7=3089\n","correct: 427*7=2989\n","outputs(x):  $866*8=8217$\n","$\n","wrong  : 866*8=7128\n","correct: 866*8=6928\n","outputs(x):  $885*8=0826$\n","$\n","wrong  : 885*8=6280\n","correct: 885*8=7080\n","outputs(x):  $147*7=9211$\n","$\n","wrong  : 147*7=1129\n","correct: 147*7=1029\n","outputs(x):  $453*9=7793$\n","$\n","wrong  : 453*9=3977\n","correct: 453*9=4077\n","outputs(x):  $390*7=0302$\n","$\n","wrong  : 390*7=2030\n","correct: 390*7=2730\n","outputs(x):  $935*6=0102$\n","$\n","wrong  : 935*6=2010\n","correct: 935*6=5610\n","outputs(x):  $465*8=0214$\n","$\n","wrong  : 465*8=4120\n","correct: 465*8=3720\n","outputs(x):  $760*8=0826$\n","$\n","wrong  : 760*8=6280\n","correct: 760*8=6080\n","outputs(x):  $340*7=0862$\n","$\n","wrong  : 340*7=2680\n","correct: 340*7=2380\n","outputs(x):  $810*4=048$\n","$7\n","wrong  : 810*4=840\n","correct: 810*4=3240\n","outputs(x):  $328*4=2151$\n","$\n","wrong  : 328*4=1512\n","correct: 328*4=1312\n","outputs(x):  $433*8=4663$\n","$\n","wrong  : 433*8=3664\n","correct: 433*8=3464\n","outputs(x):  $111*7=7753$\n","$\n","wrong  : 111*7=3577\n","correct: 111*7=777\n","outputs(x):  $768*8=4436$\n","$\n","wrong  : 768*8=6344\n","correct: 768*8=6144\n","outputs(x):  $470*9=0314$\n","$\n","wrong  : 470*9=4130\n","correct: 470*9=4230\n","outputs(x):  $560*9=0415$\n","$\n","wrong  : 560*9=5140\n","correct: 560*9=5040\n","outputs(x):  $280*3=045$\n","$8\n","wrong  : 280*3=540\n","correct: 280*3=840\n","outputs(x):  $973*8=4837$\n","$\n","wrong  : 973*8=7384\n","correct: 973*8=7784\n","outputs(x):  $419*7=3363$\n","$\n","wrong  : 419*7=3633\n","correct: 419*7=2933\n","outputs(x):  $573*9=7594$\n","$\n","wrong  : 573*9=4957\n","correct: 573*9=5157\n","outputs(x):  $788*9=2996$\n","$\n","wrong  : 788*9=6992\n","correct: 788*9=7092\n"," 88% 21/24 [00:01<00:00, 19.61it/s]outputs(x):  $922*7=4556$\n","$\n","wrong  : 922*7=6554\n","correct: 922*7=6454\n","outputs(x):  $219*8=2594$\n","$\n","wrong  : 219*8=4952\n","correct: 219*8=1752\n","outputs(x):  $110*6=06$\n","$97\n","wrong  : 110*6=60\n","correct: 110*6=660\n","outputs(x):  $245*5=5271$\n","$\n","wrong  : 245*5=1725\n","correct: 245*5=1225\n","outputs(x):  $736*9=4275$\n","$\n","wrong  : 736*9=5724\n","correct: 736*9=6624\n","outputs(x):  $153*7=1711$\n","$\n","wrong  : 153*7=1171\n","correct: 153*7=1071\n","outputs(x):  $720*7=0434$\n","$\n","wrong  : 720*7=4340\n","correct: 720*7=5040\n","outputs(x):  $873*8=4856$\n","$\n","wrong  : 873*8=6584\n","correct: 873*8=6984\n","outputs(x):  $789*9=1076$\n","$\n","wrong  : 789*9=6701\n","correct: 789*9=7101\n","outputs(x):  $620*8=0615$\n","$\n","wrong  : 620*8=5160\n","correct: 620*8=4960\n","outputs(x):  $889*8=2197$\n","$\n","wrong  : 889*8=7912\n","correct: 889*8=7112\n","outputs(x):  $784*8=2706$\n","$\n","wrong  : 784*8=6072\n","correct: 784*8=6272\n","outputs(x):  $876*6=6535$\n","$\n","wrong  : 876*6=5356\n","correct: 876*6=5256\n","outputs(x):  $787*6=2234$\n","$\n","wrong  : 787*6=4322\n","correct: 787*6=4722\n","outputs(x):  $415*7=5013$\n","$\n","wrong  : 415*7=3105\n","correct: 415*7=2905\n","outputs(x):  $183*6=8911$\n","$\n","wrong  : 183*6=1198\n","correct: 183*6=1098\n","outputs(x):  $218*6=8071$\n","$\n","wrong  : 218*6=1708\n","correct: 218*6=1308\n","outputs(x):  $578*9=2215$\n","$\n","wrong  : 578*9=5122\n","correct: 578*9=5202\n","outputs(x):  $624*7=8692$\n","$\n","wrong  : 624*7=2968\n","correct: 624*7=4368\n","outputs(x):  $64*8=255$\n","$9\n","wrong  : 64*8=552\n","correct: 64*8=512\n","outputs(x):  $79*7=354$\n","$4\n","wrong  : 79*7=453\n","correct: 79*7=553\n","outputs(x):  $62*8=692$\n","$8\n","wrong  : 62*8=296\n","correct: 62*8=496\n","outputs(x):  $43*5=511$\n","$5\n","wrong  : 43*5=115\n","correct: 43*5=215\n","outputs(x):  $55*7=5883$\n","$\n","wrong  : 55*7=3885\n","correct: 55*7=385\n","outputs(x):  $57*7=994$\n","$5\n","wrong  : 57*7=499\n","correct: 57*7=399\n","outputs(x):  $39*3=75$\n","$79\n","wrong  : 39*3=57\n","correct: 39*3=117\n","outputs(x):  $89*7=325$\n","$4\n","wrong  : 89*7=523\n","correct: 89*7=623\n","outputs(x):  $16*3=841$\n","$9\n","wrong  : 16*3=148\n","correct: 16*3=48\n","outputs(x):  $73*7=112$\n","$7\n","wrong  : 73*7=211\n","correct: 73*7=511\n","outputs(x):  $74*4=6943$\n","$\n","wrong  : 74*4=3496\n","correct: 74*4=296\n","outputs(x):  $97*3=191$\n","$9\n","wrong  : 97*3=191\n","correct: 97*3=291\n","outputs(x):  $22*9=8991$\n","$\n","wrong  : 22*9=1998\n","correct: 22*9=198\n","outputs(x):  $58*7=654$\n","$9\n","wrong  : 58*7=456\n","correct: 58*7=406\n","outputs(x):  $27*3=181$\n","$1\n","wrong  : 27*3=181\n","correct: 27*3=81\n","outputs(x):  $49*7=34$\n","$69\n","wrong  : 49*7=43\n","correct: 49*7=343\n","outputs(x):  $14*9=623$\n","$6\n","wrong  : 14*9=326\n","correct: 14*9=126\n","outputs(x):  $59*6=454$\n","$7\n","wrong  : 59*6=454\n","correct: 59*6=354\n","outputs(x):  $81*9=928$\n","$6\n","wrong  : 81*9=829\n","correct: 81*9=729\n","outputs(x):  $54*7=874$\n","$8\n","wrong  : 54*7=478\n","correct: 54*7=378\n","outputs(x):  $53*9=7794$\n","$\n","wrong  : 53*9=4977\n","correct: 53*9=477\n","outputs(x):  $69*9=125$\n","$9\n","wrong  : 69*9=521\n","correct: 69*9=621\n","outputs(x):  $36*8=823$\n","$4\n","wrong  : 36*8=328\n","correct: 36*8=288\n","outputs(x):  $63*8=403$\n","$3\n","wrong  : 63*8=304\n","correct: 63*8=504\n","outputs(x):  $52*7=464$\n","$5\n","wrong  : 52*7=464\n","correct: 52*7=364\n","outputs(x):  $35*9=512$\n","$5\n","wrong  : 35*9=215\n","correct: 35*9=315\n","outputs(x):  $93*2=68$\n","$13\n","wrong  : 93*2=86\n","correct: 93*2=186\n","outputs(x):  $72*7=402$\n","$6\n","wrong  : 72*7=204\n","correct: 72*7=504\n","outputs(x):  $92*2=48$\n","$60\n","wrong  : 92*2=84\n","correct: 92*2=184\n","outputs(x):  $93*8=443$\n","$9\n","wrong  : 93*8=344\n","correct: 93*8=744\n","outputs(x):  $65*9=584$\n","$6\n","wrong  : 65*9=485\n","correct: 65*9=585\n","outputs(x):  $29*8=211$\n","$9\n","wrong  : 29*8=112\n","correct: 29*8=232\n","outputs(x):  $71*7=7935$\n","$\n","wrong  : 71*7=5397\n","correct: 71*7=497\n","outputs(x):  $77*3=131$\n","$1\n","wrong  : 77*3=131\n","correct: 77*3=231\n","outputs(x):  $32*9=881$\n","$3\n","wrong  : 32*9=188\n","correct: 32*9=288\n","100% 24/24 [00:01<00:00, 19.99it/s]\n","accuracy of 3000 examples: 2244/3000 (74.8%)\n","{'carry0': 88.74329958308516, 'carry1': 55.340622371740956, 'carry2': 70.58823529411765, 'carry3': 92.3076923076923, 'carry4': nan, 'carry5': nan}\n","evaluating addition from: FILE:data/bal/0_to_999_times_1_digit_train_3000.txt\n","Evaluating Addition using test data file: data/bal/0_to_999_times_1_digit_train_3000.txt\n","100% 3000/3000 [00:00<00:00, 20238.61it/s]\n","100% 25/25 [00:00<00:00, 28.13it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","{'carry0': 100.0, 'carry1': 100.0, 'carry2': 100.0, 'carry3': 100.0, 'carry4': nan, 'carry5': nan}\n","step 5000: train loss 0.0320, val loss 10.8640\n","iter 5000: loss 0.0330, time 32754.37ms, mfu 6.07%\n","saving final checkpoint to out2/multiplication_reverse_0_to_999_times_1_digit_bal\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mreverse_0_to_999_times_1_digit_bal\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/multiplication/runs/2fwslno0\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250629_180152-2fwslno0/logs\u001b[0m\n"]}],"source":["!python train.py config2/multiplication/dollar_reverse/train_addition_dollar_reverse.py"]},{"cell_type":"markdown","metadata":{"id":"pKeAdrmvLFSB"},"source":["## 0_to_99999_times_1_digit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1753557181170,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"Upm1EpciXHfV","outputId":"0a6ee22d-cb44-411f-a643-9ef4da67542b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/addition\n"]}],"source":["%cd /content/drive/MyDrive/addition"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1884,"status":"ok","timestamp":1751502773159,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"1EhtPOruXt6I","outputId":"2585fba6-543e-44b8-e837-8d97bcaa2f59"},"outputs":[{"name":"stdout","output_type":"stream","text":["# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 100\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'multiplication'\n","\n","# to edit: wandb run name\n","wandb_run_name = '0_to_six_digit_times_1_digit_reverse'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'algo_reasoning'\n","data_format='reverse'\n","operator='*'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_plain.pt'\n","\n","# to edit: whether the result is reversed\n","reverse_c = True\n","eval_addition = True\n","\n","analysis = False\n","\n","# to edit: max number of digits in each operand\n","num_digit = 6\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 8\n","\n","# whether need zero‐padding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory\n","out_dir = '/content/drive/MyDrive/addition/0_to_six_digit_times_1_digit/reverse_out'\n","\n","# to edit: training data\n","train_data_path = '/content/drive/MyDrive/addition/data/0_to_six_digit_times_1_digit/0_to_six_digit_times_1_digit_bal_train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","start_train = \"FILE:/content/drive/MyDrive/addition/data/0_to_six_digit_times_1_digit/0_to_six_digit_times_1_digit_bal_train_eval.txt\"\n","\n","# to edit: valuation data\n","val_data_path = '/content/drive/MyDrive/addition/data/0_to_six_digit_times_1_digit/0_to_six_digit_times_1_digit_bal_validation_reverse.txt'\n","\n","# to edit: test data (start is just the test file)\n","start = 'FILE:/content/drive/MyDrive/addition/data/0_to_six_digit_times_1_digit/test/0_to_six_digit_times_1_digit_bal_test.txt'\n","\n","# (optional param) to_edit: whether to enable detailed metric recording at each eval_interval\n","# test_dir: the directory storing test files\n","test_dir = '/content/drive/MyDrive/addition/data/0_to_six_digit_times_1_digit/test'\n","eval_additional_test = True  # whether to evaluate on additional test files \n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval1 = True\n","early_eval_interval1 = 5\n","early_eval_iters1 = 1000\n","\n","more_early_eval2 = False\n","early_eval_interval2 = 5\n","early_eval_iters2 = 750"]}],"source":["%cat ./2_operands_mul_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1881431,"status":"ok","timestamp":1751502626365,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"KWSLVfJBLEnE","outputId":"90cf3295-025d-487d-883a-59c9e18e0089"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n"," 33% 9/27 [00:00<00:00, 21.49it/s]Skipping y_hat=4614088\n","Skipping y_hat=5860067\n","Skipping y_hat=736858\n","Skipping y_hat=4997944\n","Skipping y_hat=3806\n","Skipping y_hat=3119111\n","Skipping y_hat=1705928\n","Skipping y_hat=7221196\n","Skipping y_hat=3202621\n","Skipping y_hat=8519716\n","Skipping y_hat=7762022\n","Skipping y_hat=561017\n","Skipping y_hat=3180976\n","Skipping y_hat=5537022\n","Skipping y_hat=1649596\n"," 44% 12/27 [00:00<00:00, 21.43it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=7798177\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=813264\n","Skipping y_hat=2101484\n","Skipping y_hat=5395396\n","Skipping y_hat=5659273\n","Skipping y_hat=2620358\n","Skipping y_hat=793230\n","Skipping y_hat=2060166\n","Skipping y_hat=8989525\n"," 56% 15/27 [00:00<00:00, 21.63it/s]Skipping y_hat=6252119\n","Skipping y_hat=5070236\n","Skipping y_hat=3271094\n","Skipping y_hat=2469998\n","Skipping y_hat=3597003\n","Skipping y_hat=875801\n","Skipping y_hat=8787691\n","Skipping y_hat=2403988\n","Skipping y_hat=5033699\n","Skipping y_hat=1979936\n","Skipping y_hat=992848\n","Skipping y_hat=853022\n","Skipping y_hat=5010963\n","Skipping y_hat=7753936\n","Skipping y_hat=3984065\n","Skipping y_hat=4441796\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.61it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=676384\n","Skipping y_hat=5745023\n","Skipping y_hat=999050\n","Skipping y_hat=1808602\n","Skipping y_hat=4474440\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=4119886\n","Skipping y_hat=6074984\n","Skipping y_hat=649198\n","Skipping y_hat=996288\n","Skipping y_hat=930898\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.75it/s]Skipping y_hat=7090\n","Skipping y_hat=71554\n","Skipping y_hat=54380\n","Skipping y_hat=94396\n","Skipping y_hat=261199\n","Skipping y_hat=102258\n","Skipping y_hat=795648\n"," 89% 24/27 [00:01<00:00, 22.57it/s]Skipping y_hat=397794\n","Skipping y_hat=271880\n","Skipping y_hat=201248\n","100% 27/27 [00:01<00:00, 22.18it/s]\n","accuracy of 3000 examples: 2887/3000 (96.23333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.46it/s]\n","accuracy of 10000 examples: 9878/10000 (98.78%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904904\n","Skipping y_hat=5905286\n","Skipping y_hat=3972970\n","Skipping y_hat=4601960\n","Skipping y_hat=3220034\n","Skipping y_hat=4683022\n","Skipping y_hat=977077\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=450118\n","Skipping y_hat=5769973\n","Skipping y_hat=3309254\n","Skipping y_hat=6650451\n","Skipping y_hat=1310585\n","Skipping y_hat=714688\n","Skipping y_hat=3430367\n","Skipping y_hat=430981\n","Skipping y_hat=5409963\n","Skipping y_hat=6555818\n"," 11% 3/27 [00:00<00:01, 20.21it/s]Skipping y_hat=3991702\n","Skipping y_hat=8730278\n","Skipping y_hat=3972014\n","Skipping y_hat=3460306\n","Skipping y_hat=837765\n","Skipping y_hat=1881167\n","Skipping y_hat=2899567\n","Skipping y_hat=6919001\n","Skipping y_hat=4902733\n","Skipping y_hat=5321796\n"," 22% 6/27 [00:00<00:01, 20.38it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=2904198\n","Skipping y_hat=5539896\n","Skipping y_hat=668796\n","Skipping y_hat=3526206\n","Skipping y_hat=4903650\n","Skipping y_hat=6097726\n","Skipping y_hat=2002672\n","Skipping y_hat=4893895\n","Skipping y_hat=436954\n"," 33% 9/27 [00:00<00:00, 19.69it/s]Skipping y_hat=1360747\n","Skipping y_hat=7207952\n","Skipping y_hat=5860067\n","Skipping y_hat=736858\n","Skipping y_hat=4997944\n","Skipping y_hat=7560112\n","Skipping y_hat=903806\n","Skipping y_hat=3119111\n","Skipping y_hat=2420136\n","Skipping y_hat=1705928\n","Skipping y_hat=7221196\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=8519716\n"," 41% 11/27 [00:00<00:00, 19.58it/s]Skipping y_hat=1519966\n","Skipping y_hat=4479877\n","Skipping y_hat=5537022\n","Skipping y_hat=1649596\n","Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=7700705\n","Skipping y_hat=1540029\n"," 48% 13/27 [00:00<00:00, 19.68it/s]Skipping y_hat=823204\n","Skipping y_hat=6399424\n","Skipping y_hat=1410433\n","Skipping y_hat=2101484\n","Skipping y_hat=5395396\n","Skipping y_hat=6992847\n","Skipping y_hat=5659273\n","Skipping y_hat=1003526\n","Skipping y_hat=2060166\n","Skipping y_hat=829396\n","Skipping y_hat=649921\n","Skipping y_hat=8989525\n"," 56% 15/27 [00:00<00:00, 19.52it/s]Skipping y_hat=844020\n","Skipping y_hat=912008\n","Skipping y_hat=2173399\n","Skipping y_hat=5070236\n","Skipping y_hat=2469998\n","Skipping y_hat=4531988\n","Skipping y_hat=3597003\n","Skipping y_hat=875801\n","Skipping y_hat=8787691\n","Skipping y_hat=2403988\n","Skipping y_hat=992848\n","Skipping y_hat=853022\n","Skipping y_hat=5010963\n","Skipping y_hat=7753936\n","Skipping y_hat=2400008\n","Skipping y_hat=3984065\n","Skipping y_hat=4441796\n","Skipping y_hat=5819248\n","Skipping y_hat=5080142\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=800340\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 19.73it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=676384\n","Skipping y_hat=5745923\n","Skipping y_hat=999050\n","Skipping y_hat=1065987\n","Skipping y_hat=4077032\n","Skipping y_hat=973096\n","Skipping y_hat=4743160\n","Skipping y_hat=4119886\n","Skipping y_hat=6074984\n","Skipping y_hat=649198\n","Skipping y_hat=740988\n","Skipping y_hat=996288\n","Skipping y_hat=3349777\n","Skipping y_hat=5803490\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:01<00:00, 19.86it/s]Skipping y_hat=7290\n","Skipping y_hat=211040\n"," 89% 24/27 [00:01<00:00, 20.95it/s]Skipping y_hat=397794\n","Skipping y_hat=271880\n","Skipping y_hat=460241\n","100% 27/27 [00:01<00:00, 20.55it/s]\n","accuracy of 3000 examples: 2877/3000 (95.89999999999999%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 95.90%\n","\n","iter 1300: train loss 0.9404, val loss 0.9564\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5318175\n","Skipping y_hat=904004\n","Skipping y_hat=4601960\n","Skipping y_hat=7620016\n","Skipping y_hat=7674016\n","Skipping y_hat=977047\n","Skipping y_hat=2767974\n","Skipping y_hat=5400830\n","Skipping y_hat=984040\n","Skipping y_hat=450108\n","Skipping y_hat=917768\n","Skipping y_hat=4428344\n","Skipping y_hat=5769973\n","Skipping y_hat=714688\n","Skipping y_hat=2160000\n","Skipping y_hat=5070338\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n","Skipping y_hat=3387979\n"," 11% 3/27 [00:00<00:01, 22.45it/s]Skipping y_hat=2229520\n","Skipping y_hat=2991602\n","Skipping y_hat=8730278\n","Skipping y_hat=5550080\n","Skipping y_hat=6102404\n","Skipping y_hat=6929001\n","Skipping y_hat=7079204\n","Skipping y_hat=5321796\n","Skipping y_hat=1343985\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.19it/s]Skipping y_hat=1239956\n","Skipping y_hat=4940672\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=668796\n","Skipping y_hat=5760799\n","Skipping y_hat=6388337\n","Skipping y_hat=4903650\n","Skipping y_hat=928240\n"," 33% 9/27 [00:00<00:00, 20.96it/s]Skipping y_hat=6963320\n","Skipping y_hat=504196\n","Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=7018912\n","Skipping y_hat=3202621\n","Skipping y_hat=5809915\n","Skipping y_hat=1939774\n","Skipping y_hat=562117\n"," 44% 12/27 [00:00<00:00, 20.73it/s]Skipping y_hat=1972808\n","Skipping y_hat=4583004\n","Skipping y_hat=7798177\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=391022\n","Skipping y_hat=6992847\n","Skipping y_hat=2620358\n","Skipping y_hat=3151842\n","Skipping y_hat=8989525\n","Skipping y_hat=3976360\n","Skipping y_hat=4339900\n","Skipping y_hat=6954900\n","Skipping y_hat=4706020\n"," 56% 15/27 [00:00<00:00, 20.87it/s]Skipping y_hat=1015874\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=5280998\n","Skipping y_hat=6033799\n","Skipping y_hat=1980568\n","Skipping y_hat=853022\n","Skipping y_hat=3032423\n","Skipping y_hat=1329472\n","Skipping y_hat=4039658\n","Skipping y_hat=2269972\n","Skipping y_hat=3144086\n","Skipping y_hat=5080142\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n"," 67% 18/27 [00:00<00:00, 21.46it/s]Skipping y_hat=6308567\n","Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=7234642\n","Skipping y_hat=5745023\n","Skipping y_hat=880050\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=4981756\n","Skipping y_hat=740988\n","Skipping y_hat=655904\n","Skipping y_hat=931898\n","Skipping y_hat=2852996\n","Skipping y_hat=6954277\n","Skipping y_hat=6100838\n","Skipping y_hat=808004\n","Skipping y_hat=3500004\n"," 78% 21/27 [00:00<00:00, 21.33it/s]Skipping y_hat=102258\n","Skipping y_hat=544595\n"," 89% 24/27 [00:01<00:00, 22.25it/s]Skipping y_hat=397794\n","Skipping y_hat=460241\n","Skipping y_hat=234404\n","Skipping y_hat=559556\n","Skipping y_hat=81788\n","100% 27/27 [00:01<00:00, 21.90it/s]\n","accuracy of 3000 examples: 2899/3000 (96.63333333333334%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 23.98it/s]\n","accuracy of 10000 examples: 9897/10000 (98.97%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904004\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=3220034\n","Skipping y_hat=7620016\n","Skipping y_hat=7674016\n","Skipping y_hat=1860008\n","Skipping y_hat=977047\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=451118\n","Skipping y_hat=917768\n","Skipping y_hat=4428344\n","Skipping y_hat=5769973\n","Skipping y_hat=3419254\n","Skipping y_hat=714688\n","Skipping y_hat=3430367\n","Skipping y_hat=5070338\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.70it/s]Skipping y_hat=1380578\n","Skipping y_hat=2991602\n","Skipping y_hat=8730278\n","Skipping y_hat=1144058\n","Skipping y_hat=6929001\n","Skipping y_hat=5321796\n","Skipping y_hat=1343985\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:01, 20.78it/s]Skipping y_hat=1239956\n","Skipping y_hat=1752704\n","Skipping y_hat=4940672\n","Skipping y_hat=1004198\n","Skipping y_hat=4700627\n","Skipping y_hat=5539896\n","Skipping y_hat=668796\n","Skipping y_hat=2803670\n","Skipping y_hat=5760799\n","Skipping y_hat=6388337\n","Skipping y_hat=3989568\n"," 33% 9/27 [00:00<00:00, 20.36it/s]Skipping y_hat=6963320\n","Skipping y_hat=504196\n","Skipping y_hat=5860067\n","Skipping y_hat=401088\n","Skipping y_hat=5164144\n","Skipping y_hat=4997944\n","Skipping y_hat=2420136\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5809915\n","Skipping y_hat=1939774\n","Skipping y_hat=1519966\n","Skipping y_hat=3390229\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 20.42it/s]Skipping y_hat=1972808\n","Skipping y_hat=4583004\n","Skipping y_hat=7798177\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=823204\n","Skipping y_hat=6981355\n","Skipping y_hat=391022\n","Skipping y_hat=6992847\n","Skipping y_hat=2620358\n","Skipping y_hat=5102953\n","Skipping y_hat=829396\n","Skipping y_hat=4036004\n"," 56% 15/27 [00:00<00:00, 20.43it/s]Skipping y_hat=2012208\n","Skipping y_hat=780664\n","Skipping y_hat=4531988\n","Skipping y_hat=2303988\n","Skipping y_hat=6033799\n","Skipping y_hat=1980568\n","Skipping y_hat=853022\n","Skipping y_hat=4039658\n","Skipping y_hat=4441796\n","Skipping y_hat=5080142\n","Skipping y_hat=3659828\n"," 67% 18/27 [00:00<00:00, 20.45it/s]Skipping y_hat=1121794\n","Skipping y_hat=6308567\n","Skipping y_hat=5398032\n","Skipping y_hat=910808\n","Skipping y_hat=5996413\n","Skipping y_hat=7234642\n","Skipping y_hat=5745023\n","Skipping y_hat=880050\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=4981756\n","Skipping y_hat=741988\n","Skipping y_hat=2771877\n","Skipping y_hat=1607699\n","Skipping y_hat=2852996\n","Skipping y_hat=6954277\n","Skipping y_hat=6100838\n","Skipping y_hat=808004\n"," 78% 21/27 [00:01<00:00, 20.50it/s]Skipping y_hat=77616\n","Skipping y_hat=94396\n","Skipping y_hat=587816\n","Skipping y_hat=544595\n"," 89% 24/27 [00:01<00:00, 21.37it/s]Skipping y_hat=96674\n","Skipping y_hat=460241\n","Skipping y_hat=162503\n","Skipping y_hat=201248\n","100% 27/27 [00:01<00:00, 21.10it/s]\n","accuracy of 3000 examples: 2897/3000 (96.56666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 96.57%\n","\n","iter 1400: train loss 0.9445, val loss 0.9677\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904004\n","Skipping y_hat=2729906\n","Skipping y_hat=4602960\n","Skipping y_hat=8896420\n","Skipping y_hat=2767974\n","Skipping y_hat=1976611\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=451118\n","Skipping y_hat=5769973\n","Skipping y_hat=715988\n","Skipping y_hat=420981\n","Skipping y_hat=1999009\n","Skipping y_hat=4031198\n","Skipping y_hat=2553994\n","Skipping y_hat=5509963\n","Skipping y_hat=3154304\n"," 11% 3/27 [00:00<00:01, 21.76it/s]Skipping y_hat=5305976\n","Skipping y_hat=2569672\n","Skipping y_hat=3991702\n","Skipping y_hat=838665\n","Skipping y_hat=5803235\n","Skipping y_hat=2899567\n","Skipping y_hat=7231951\n","Skipping y_hat=868954\n","Skipping y_hat=6910001\n","Skipping y_hat=5062951\n","Skipping y_hat=874220\n","Skipping y_hat=3358788\n","Skipping y_hat=1343985\n","Skipping y_hat=1996268\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.83it/s]Skipping y_hat=5758876\n","Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1004198\n","Skipping y_hat=668796\n","Skipping y_hat=5760799\n","Skipping y_hat=1884994\n","Skipping y_hat=436954\n"," 33% 9/27 [00:00<00:00, 22.26it/s]Skipping y_hat=981668\n","Skipping y_hat=1134040\n","Skipping y_hat=5860067\n","Skipping y_hat=1219774\n","Skipping y_hat=2420136\n","Skipping y_hat=5903298\n","Skipping y_hat=7018912\n","Skipping y_hat=6529989\n","Skipping y_hat=4999456\n","Skipping y_hat=1939774\n","Skipping y_hat=7762022\n","Skipping y_hat=3280229\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.89it/s]Skipping y_hat=7499377\n","Skipping y_hat=7267804\n","Skipping y_hat=4583004\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=823204\n","Skipping y_hat=3400316\n","Skipping y_hat=8945713\n","Skipping y_hat=5203953\n","Skipping y_hat=829396\n","Skipping y_hat=8989525\n","Skipping y_hat=7776032\n"," 56% 15/27 [00:00<00:00, 20.94it/s]Skipping y_hat=2660052\n","Skipping y_hat=2469998\n","Skipping y_hat=4531988\n","Skipping y_hat=3598003\n","Skipping y_hat=2403988\n","Skipping y_hat=5280998\n","Skipping y_hat=6033799\n","Skipping y_hat=893848\n","Skipping y_hat=1329472\n","Skipping y_hat=3984065\n","Skipping y_hat=658792\n","Skipping y_hat=4441796\n","Skipping y_hat=5080142\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.00it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=676384\n","Skipping y_hat=880050\n","Skipping y_hat=930160\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=656914\n","Skipping y_hat=5782933\n","Skipping y_hat=931898\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:01<00:00, 20.33it/s]Skipping y_hat=55990\n","Skipping y_hat=7164\n","Skipping y_hat=54380\n","Skipping y_hat=50038\n"," 89% 24/27 [00:01<00:00, 21.53it/s]Skipping y_hat=397794\n","Skipping y_hat=298747\n","Skipping y_hat=559556\n","100% 27/27 [00:01<00:00, 21.74it/s]\n","accuracy of 3000 examples: 2900/3000 (96.66666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.82it/s]\n","accuracy of 10000 examples: 9928/10000 (99.28%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904004\n","Skipping y_hat=2729906\n","Skipping y_hat=4602960\n","Skipping y_hat=8896420\n","Skipping y_hat=1860008\n","Skipping y_hat=977047\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=451118\n","Skipping y_hat=5769973\n","Skipping y_hat=714688\n","Skipping y_hat=3430367\n","Skipping y_hat=420981\n","Skipping y_hat=1999009\n","Skipping y_hat=5599963\n","Skipping y_hat=3297201\n"," 11% 3/27 [00:00<00:01, 21.03it/s]Skipping y_hat=2569672\n","Skipping y_hat=5803235\n","Skipping y_hat=724016\n","Skipping y_hat=7231951\n","Skipping y_hat=6919001\n","Skipping y_hat=5062951\n","Skipping y_hat=1343985\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.00it/s]Skipping y_hat=5758876\n","Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1004198\n","Skipping y_hat=668796\n","Skipping y_hat=5760799\n","Skipping y_hat=6997726\n","Skipping y_hat=1884994\n"," 33% 9/27 [00:00<00:00, 20.97it/s]Skipping y_hat=981668\n","Skipping y_hat=1134040\n","Skipping y_hat=1219774\n","Skipping y_hat=2420136\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=1939774\n","Skipping y_hat=3280229\n","Skipping y_hat=5537022\n","Skipping y_hat=1649596\n","Skipping y_hat=5066974\n"," 44% 12/27 [00:00<00:00, 21.59it/s]Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=5560298\n","Skipping y_hat=3400316\n","Skipping y_hat=8945713\n","Skipping y_hat=5102953\n","Skipping y_hat=829396\n","Skipping y_hat=8989525\n","Skipping y_hat=3976360\n","Skipping y_hat=639998\n"," 56% 15/27 [00:00<00:00, 21.74it/s]Skipping y_hat=2660052\n","Skipping y_hat=2469998\n","Skipping y_hat=4531988\n","Skipping y_hat=3597003\n","Skipping y_hat=2403988\n","Skipping y_hat=5280998\n","Skipping y_hat=6033799\n","Skipping y_hat=4481244\n","Skipping y_hat=993848\n","Skipping y_hat=1329472\n","Skipping y_hat=658792\n","Skipping y_hat=2269972\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.01it/s]Skipping y_hat=5398032\n","Skipping y_hat=676384\n","Skipping y_hat=7234642\n","Skipping y_hat=880050\n","Skipping y_hat=1808602\n","Skipping y_hat=7671604\n","Skipping y_hat=930160\n","Skipping y_hat=1065987\n","Skipping y_hat=982196\n","Skipping y_hat=5571262\n","Skipping y_hat=1733948\n","Skipping y_hat=2762506\n","Skipping y_hat=6074984\n","Skipping y_hat=656914\n","Skipping y_hat=5782933\n","Skipping y_hat=931898\n","Skipping y_hat=6100838\n","Skipping y_hat=7857415\n"," 78% 21/27 [00:00<00:00, 21.97it/s]Skipping y_hat=55990\n","Skipping y_hat=7164\n","Skipping y_hat=27216\n","Skipping y_hat=54380\n","Skipping y_hat=102258\n","Skipping y_hat=889759\n","Skipping y_hat=544595\n"," 89% 24/27 [00:01<00:00, 22.73it/s]Skipping y_hat=519616\n","Skipping y_hat=397794\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.35it/s]\n","accuracy of 3000 examples: 2901/3000 (96.7%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 96.70%\n","\n","iter 1500: train loss 0.9404, val loss 0.9468\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905504\n","Skipping y_hat=4601960\n","Skipping y_hat=8896420\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=451118\n","Skipping y_hat=917768\n","Skipping y_hat=5769973\n","Skipping y_hat=3419254\n","Skipping y_hat=714688\n","  7% 2/27 [00:00<00:01, 19.04it/s]Skipping y_hat=2160000\n","Skipping y_hat=5070338\n","Skipping y_hat=5509963\n","Skipping y_hat=8370769\n","Skipping y_hat=4973014\n","Skipping y_hat=724016\n"," 15% 4/27 [00:00<00:01, 19.35it/s]Skipping y_hat=7231951\n","Skipping y_hat=820913\n","Skipping y_hat=6919001\n","Skipping y_hat=1340932\n","Skipping y_hat=861198\n","Skipping y_hat=6800831\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:01, 19.00it/s]Skipping y_hat=5419931\n","Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n","Skipping y_hat=3207302\n","Skipping y_hat=805498\n","Skipping y_hat=668796\n","Skipping y_hat=5760799\n","Skipping y_hat=4011961\n","Skipping y_hat=6997726\n","Skipping y_hat=1884994\n"," 30% 8/27 [00:00<00:01, 18.47it/s]Skipping y_hat=1960787\n","Skipping y_hat=1960526\n","Skipping y_hat=7207952\n","Skipping y_hat=4969381\n","Skipping y_hat=5860067\n","Skipping y_hat=501188\n","Skipping y_hat=920656\n","Skipping y_hat=4997944\n","Skipping y_hat=1705928\n","Skipping y_hat=7221196\n","Skipping y_hat=7018912\n","Skipping y_hat=887479\n","Skipping y_hat=6529989\n","Skipping y_hat=7018471\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n"," 41% 11/27 [00:00<00:00, 19.85it/s]Skipping y_hat=7762022\n","Skipping y_hat=3280229\n","Skipping y_hat=1649596\n","Skipping y_hat=318490\n","Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=7798177\n","Skipping y_hat=5395396\n","Skipping y_hat=6992847\n","Skipping y_hat=4781992\n"," 52% 14/27 [00:00<00:00, 20.03it/s]Skipping y_hat=829396\n","Skipping y_hat=8989525\n","Skipping y_hat=1976755\n","Skipping y_hat=1099282\n","Skipping y_hat=2469998\n","Skipping y_hat=5849560\n","Skipping y_hat=4531988\n","Skipping y_hat=3497003\n"," 59% 16/27 [00:00<00:00, 19.73it/s]Skipping y_hat=2402088\n","Skipping y_hat=5280998\n","Skipping y_hat=4491244\n","Skipping y_hat=853022\n","Skipping y_hat=1363198\n","Skipping y_hat=7753936\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n","Skipping y_hat=676384\n","Skipping y_hat=899050\n","Skipping y_hat=1065987\n"," 70% 19/27 [00:00<00:00, 20.36it/s]Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=1979681\n","Skipping y_hat=649198\n","Skipping y_hat=751978\n","Skipping y_hat=6802273\n","Skipping y_hat=896298\n","Skipping y_hat=931898\n","Skipping y_hat=1607699\n","Skipping y_hat=3349777\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n","Skipping y_hat=2464996\n"," 81% 22/27 [00:01<00:00, 21.06it/s]Skipping y_hat=94396\n","Skipping y_hat=102258\n","Skipping y_hat=200328\n","Skipping y_hat=96674\n","Skipping y_hat=162503\n","Skipping y_hat=201248\n","100% 27/27 [00:01<00:00, 20.72it/s]\n","accuracy of 3000 examples: 2899/3000 (96.63333333333334%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 23.53it/s]\n","accuracy of 10000 examples: 9921/10000 (99.21%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=5005286\n","Skipping y_hat=4601960\n","Skipping y_hat=8896420\n","Skipping y_hat=7620016\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=451118\n","Skipping y_hat=6074836\n","Skipping y_hat=917768\n","Skipping y_hat=5769973\n","Skipping y_hat=3419254\n","Skipping y_hat=714688\n","Skipping y_hat=2160000\n","Skipping y_hat=5070338\n","Skipping y_hat=5409963\n"," 11% 3/27 [00:00<00:01, 19.71it/s]Skipping y_hat=5305976\n","Skipping y_hat=4973014\n","Skipping y_hat=5803235\n","Skipping y_hat=724016\n","Skipping y_hat=820913\n","Skipping y_hat=6919001\n","Skipping y_hat=5321796\n"," 19% 5/27 [00:00<00:01, 19.51it/s]Skipping y_hat=4049518\n","Skipping y_hat=861098\n","Skipping y_hat=2599385\n","Skipping y_hat=5419931\n","Skipping y_hat=5599882\n","Skipping y_hat=3585952\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n","Skipping y_hat=2216188\n","Skipping y_hat=668796\n","Skipping y_hat=5760799\n","Skipping y_hat=1884994\n"," 30% 8/27 [00:00<00:00, 19.81it/s]Skipping y_hat=4481598\n","Skipping y_hat=1960787\n","Skipping y_hat=2012373\n","Skipping y_hat=606588\n","Skipping y_hat=7207952\n","Skipping y_hat=5860067\n","Skipping y_hat=501188\n","Skipping y_hat=1705928\n","Skipping y_hat=7018912\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=8519716\n"," 41% 11/27 [00:00<00:00, 20.31it/s]Skipping y_hat=3280229\n","Skipping y_hat=550993\n","Skipping y_hat=318490\n","Skipping y_hat=1972808\n","Skipping y_hat=7798177\n","Skipping y_hat=5395396\n","Skipping y_hat=4781992\n"," 52% 14/27 [00:00<00:00, 19.29it/s]Skipping y_hat=829396\n","Skipping y_hat=649921\n","Skipping y_hat=8989525\n","Skipping y_hat=7063976\n","Skipping y_hat=2286590\n","Skipping y_hat=2469998\n","Skipping y_hat=4531988\n"," 59% 16/27 [00:00<00:00, 18.86it/s]Skipping y_hat=2403088\n","Skipping y_hat=5280998\n","Skipping y_hat=4381244\n","Skipping y_hat=861793\n","Skipping y_hat=1363198\n","Skipping y_hat=5010963\n","Skipping y_hat=7753936\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 18.59it/s]Skipping y_hat=5398032\n","Skipping y_hat=676384\n","Skipping y_hat=880050\n","Skipping y_hat=1065987\n","Skipping y_hat=982196\n","Skipping y_hat=6074984\n","Skipping y_hat=1979681\n","Skipping y_hat=649198\n","Skipping y_hat=751988\n","Skipping y_hat=6802273\n"," 74% 20/27 [00:01<00:00, 18.62it/s]Skipping y_hat=1756082\n","Skipping y_hat=931898\n","Skipping y_hat=1607699\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n","Skipping y_hat=2464996\n"," 85% 23/27 [00:01<00:00, 20.25it/s]Skipping y_hat=96674\n","Skipping y_hat=460241\n","Skipping y_hat=162503\n","Skipping y_hat=81788\n","100% 27/27 [00:01<00:00, 19.95it/s]\n","accuracy of 3000 examples: 2907/3000 (96.89999999999999%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 96.90%\n","\n","iter 1600: train loss 0.9277, val loss 0.9676\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904004\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=4683022\n","Skipping y_hat=7674016\n","Skipping y_hat=977087\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=450308\n","Skipping y_hat=6650451\n","Skipping y_hat=714688\n","Skipping y_hat=3430367\n","Skipping y_hat=5599963\n"," 11% 3/27 [00:00<00:01, 21.51it/s]Skipping y_hat=2569672\n","Skipping y_hat=7231951\n","Skipping y_hat=5062951\n","Skipping y_hat=4902733\n","Skipping y_hat=1343985\n","Skipping y_hat=1316528\n"," 22% 6/27 [00:00<00:00, 21.38it/s]Skipping y_hat=5509882\n","Skipping y_hat=1239956\n","Skipping y_hat=2986020\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=5760799\n","Skipping y_hat=4903650\n","Skipping y_hat=927290\n"," 33% 9/27 [00:00<00:00, 21.95it/s]Skipping y_hat=7207952\n","Skipping y_hat=504196\n","Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=3660496\n","Skipping y_hat=2420136\n","Skipping y_hat=3202621\n","Skipping y_hat=1459585\n","Skipping y_hat=5537022\n","Skipping y_hat=1649596\n"," 44% 12/27 [00:00<00:00, 22.04it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=7267804\n","Skipping y_hat=565400\n","Skipping y_hat=1540029\n","Skipping y_hat=6992847\n","Skipping y_hat=1003526\n","Skipping y_hat=828396\n","Skipping y_hat=7089525\n","Skipping y_hat=4339900\n","Skipping y_hat=6954900\n"," 56% 15/27 [00:00<00:00, 22.23it/s]Skipping y_hat=844020\n","Skipping y_hat=2660052\n","Skipping y_hat=2469998\n","Skipping y_hat=4531988\n","Skipping y_hat=2403088\n","Skipping y_hat=4381244\n","Skipping y_hat=993848\n","Skipping y_hat=717478\n","Skipping y_hat=2400008\n","Skipping y_hat=2269972\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.38it/s]Skipping y_hat=5398032\n","Skipping y_hat=676384\n","Skipping y_hat=880050\n","Skipping y_hat=1808602\n","Skipping y_hat=739213\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=649198\n","Skipping y_hat=931898\n","Skipping y_hat=1607699\n","Skipping y_hat=3595792\n","Skipping y_hat=3239777\n","Skipping y_hat=6954277\n"," 78% 21/27 [00:00<00:00, 22.14it/s]Skipping y_hat=60492\n","Skipping y_hat=544595\n"," 89% 24/27 [00:01<00:00, 22.80it/s]Skipping y_hat=298747\n","Skipping y_hat=559556\n","100% 27/27 [00:01<00:00, 22.54it/s]\n","accuracy of 3000 examples: 2921/3000 (97.36666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.38it/s]\n","accuracy of 10000 examples: 9963/10000 (99.63%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904004\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=7674016\n","Skipping y_hat=977087\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=450108\n","Skipping y_hat=6650451\n","Skipping y_hat=714688\n","  7% 2/27 [00:00<00:01, 18.22it/s]Skipping y_hat=3430367\n","Skipping y_hat=2160000\n","Skipping y_hat=430981\n","Skipping y_hat=5599963\n","Skipping y_hat=2569672\n"," 15% 4/27 [00:00<00:01, 18.90it/s]Skipping y_hat=7231951\n","Skipping y_hat=5062951\n","Skipping y_hat=911732\n","Skipping y_hat=996278\n","Skipping y_hat=1316528\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:01, 19.09it/s]Skipping y_hat=5509882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=3560005\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=5760799\n","Skipping y_hat=6307748\n","Skipping y_hat=1613944\n","Skipping y_hat=927290\n"," 33% 9/27 [00:00<00:00, 19.68it/s]Skipping y_hat=7207952\n","Skipping y_hat=504196\n","Skipping y_hat=5869067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=3202621\n","Skipping y_hat=2808196\n","Skipping y_hat=5537022\n","Skipping y_hat=1649596\n"," 44% 12/27 [00:00<00:00, 20.48it/s]Skipping y_hat=7499377\n","Skipping y_hat=7267804\n","Skipping y_hat=565400\n","Skipping y_hat=1540029\n","Skipping y_hat=6992847\n","Skipping y_hat=4781992\n","Skipping y_hat=1003526\n","Skipping y_hat=829396\n","Skipping y_hat=7776032\n","Skipping y_hat=4339900\n","Skipping y_hat=6954900\n"," 56% 15/27 [00:00<00:00, 20.87it/s]Skipping y_hat=4531988\n","Skipping y_hat=6062662\n","Skipping y_hat=2403088\n","Skipping y_hat=993848\n","Skipping y_hat=717478\n","Skipping y_hat=2269972\n","Skipping y_hat=3659828\n"," 67% 18/27 [00:00<00:00, 21.27it/s]Skipping y_hat=5398032\n","Skipping y_hat=676384\n","Skipping y_hat=889050\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=649198\n","Skipping y_hat=996288\n","Skipping y_hat=5782933\n","Skipping y_hat=931898\n","Skipping y_hat=2852996\n","Skipping y_hat=3595792\n","Skipping y_hat=3239777\n","Skipping y_hat=6954277\n","Skipping y_hat=3052032\n","Skipping y_hat=3400104\n"," 78% 21/27 [00:01<00:00, 21.40it/s]Skipping y_hat=60492\n"," 89% 24/27 [00:01<00:00, 22.19it/s]Skipping y_hat=298747\n","Skipping y_hat=559556\n","100% 27/27 [00:01<00:00, 21.35it/s]\n","accuracy of 3000 examples: 2922/3000 (97.39999999999999%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 97.40%\n","\n","iter 1700: train loss 0.9175, val loss 0.9688\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905008\n","Skipping y_hat=2729906\n","Skipping y_hat=5005286\n","Skipping y_hat=4601960\n","Skipping y_hat=8896420\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=451118\n","Skipping y_hat=917768\n","Skipping y_hat=6650451\n","Skipping y_hat=420981\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 22.07it/s]Skipping y_hat=724016\n","Skipping y_hat=6919001\n","Skipping y_hat=861188\n","Skipping y_hat=996278\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.78it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=805498\n","Skipping y_hat=1884994\n"," 33% 9/27 [00:00<00:00, 22.91it/s]Skipping y_hat=981578\n","Skipping y_hat=5860067\n","Skipping y_hat=736858\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=561017\n","Skipping y_hat=1459585\n"," 44% 12/27 [00:00<00:00, 22.91it/s]Skipping y_hat=3197716\n","Skipping y_hat=3392785\n","Skipping y_hat=823204\n","Skipping y_hat=6399424\n","Skipping y_hat=5452876\n","Skipping y_hat=1003526\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 23.02it/s]Skipping y_hat=912008\n","Skipping y_hat=5849560\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=892858\n","Skipping y_hat=717478\n","Skipping y_hat=5010963\n","Skipping y_hat=1329472\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.99it/s]Skipping y_hat=5398032\n","Skipping y_hat=910808\n","Skipping y_hat=5996413\n","Skipping y_hat=2610149\n","Skipping y_hat=5745023\n","Skipping y_hat=982396\n","Skipping y_hat=6250114\n","Skipping y_hat=6074984\n","Skipping y_hat=445517\n","Skipping y_hat=655814\n","Skipping y_hat=8136793\n","Skipping y_hat=5782933\n","Skipping y_hat=931898\n"," 78% 21/27 [00:00<00:00, 23.02it/s]Skipping y_hat=7590\n","Skipping y_hat=94396\n","Skipping y_hat=544595\n","100% 27/27 [00:01<00:00, 23.39it/s]\n","accuracy of 3000 examples: 2932/3000 (97.73333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.07it/s]\n","accuracy of 10000 examples: 9969/10000 (99.69%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904908\n","Skipping y_hat=2729906\n","Skipping y_hat=5005286\n","Skipping y_hat=4601960\n","Skipping y_hat=7674016\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=917768\n","Skipping y_hat=2160000\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 22.09it/s]Skipping y_hat=2899464\n","Skipping y_hat=724016\n","Skipping y_hat=6919001\n","Skipping y_hat=996278\n","Skipping y_hat=6800831\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.24it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=805498\n","Skipping y_hat=1884994\n"," 33% 9/27 [00:00<00:00, 21.59it/s]Skipping y_hat=981578\n","Skipping y_hat=5860067\n","Skipping y_hat=736858\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=1219774\n","Skipping y_hat=787489\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=561017\n","Skipping y_hat=1459585\n"," 44% 12/27 [00:00<00:00, 21.29it/s]Skipping y_hat=7798177\n","Skipping y_hat=3392785\n","Skipping y_hat=823204\n","Skipping y_hat=6399424\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 21.45it/s]Skipping y_hat=912008\n","Skipping y_hat=5849560\n","Skipping y_hat=4531988\n","Skipping y_hat=3598003\n","Skipping y_hat=2403988\n","Skipping y_hat=6033799\n","Skipping y_hat=4381244\n","Skipping y_hat=892858\n","Skipping y_hat=717478\n","Skipping y_hat=5010963\n","Skipping y_hat=1329472\n","Skipping y_hat=3659828\n"," 67% 18/27 [00:00<00:00, 21.25it/s]Skipping y_hat=5398032\n","Skipping y_hat=910808\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=899050\n","Skipping y_hat=982296\n","Skipping y_hat=6074984\n","Skipping y_hat=445517\n","Skipping y_hat=655814\n","Skipping y_hat=931898\n"," 78% 21/27 [00:00<00:00, 20.67it/s]Skipping y_hat=94396\n","Skipping y_hat=544595\n"," 89% 24/27 [00:01<00:00, 21.45it/s]Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 21.62it/s]\n","accuracy of 3000 examples: 2934/3000 (97.8%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 97.80%\n","\n","iter 1800: train loss 0.8885, val loss 0.9854\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4601960\n","Skipping y_hat=5205815\n","Skipping y_hat=977087\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","  7% 2/27 [00:00<00:01, 19.67it/s]Skipping y_hat=420981\n","Skipping y_hat=5409963\n","Skipping y_hat=8959690\n","Skipping y_hat=724016\n","Skipping y_hat=6919001\n","Skipping y_hat=5062951\n"," 19% 5/27 [00:00<00:01, 21.19it/s]Skipping y_hat=996248\n","Skipping y_hat=2599385\n","Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=4940672\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=668796\n","Skipping y_hat=5760799\n"," 30% 8/27 [00:00<00:00, 21.61it/s]Skipping y_hat=2201258\n","Skipping y_hat=3989568\n","Skipping y_hat=981588\n","Skipping y_hat=7207952\n","Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=5201162\n","Skipping y_hat=3202621\n","Skipping y_hat=8519716\n","Skipping y_hat=1939774\n"," 41% 11/27 [00:00<00:00, 21.46it/s]Skipping y_hat=4479877\n","Skipping y_hat=4897063\n","Skipping y_hat=5537022\n","Skipping y_hat=1649596\n","Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=3259071\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=823204\n","Skipping y_hat=6992847\n","Skipping y_hat=654898\n","Skipping y_hat=2620358\n"," 52% 14/27 [00:00<00:00, 20.82it/s]Skipping y_hat=2789735\n","Skipping y_hat=829396\n","Skipping y_hat=8989525\n","Skipping y_hat=3976360\n","Skipping y_hat=912008\n","Skipping y_hat=4531988\n","Skipping y_hat=3497003\n","Skipping y_hat=2403988\n","Skipping y_hat=5280998\n","Skipping y_hat=992848\n","Skipping y_hat=717478\n"," 63% 17/27 [00:00<00:00, 20.91it/s]Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n","Skipping y_hat=5398032\n","Skipping y_hat=910808\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=999050\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=649198\n"," 74% 20/27 [00:00<00:00, 20.79it/s]Skipping y_hat=996288\n","Skipping y_hat=5782933\n","Skipping y_hat=931898\n","Skipping y_hat=6100838\n","Skipping y_hat=7590\n"," 85% 23/27 [00:01<00:00, 20.71it/s]Skipping y_hat=54380\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 21.11it/s]\n","accuracy of 3000 examples: 2923/3000 (97.43333333333334%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 23.76it/s]\n","accuracy of 10000 examples: 9973/10000 (99.72999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4601960\n","Skipping y_hat=5205815\n","Skipping y_hat=977087\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=451008\n","  7% 2/27 [00:00<00:01, 19.03it/s]Skipping y_hat=420981\n","Skipping y_hat=5409963\n","Skipping y_hat=8959690\n","Skipping y_hat=724016\n","Skipping y_hat=6919001\n"," 19% 5/27 [00:00<00:01, 19.94it/s]Skipping y_hat=996248\n","Skipping y_hat=2599385\n","Skipping y_hat=5988070\n","Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=4940672\n","Skipping y_hat=1004198\n"," 26% 7/27 [00:00<00:01, 19.91it/s]Skipping y_hat=5539896\n","Skipping y_hat=2201258\n","Skipping y_hat=3989568\n","Skipping y_hat=981548\n","Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n"," 37% 10/27 [00:00<00:00, 19.96it/s]Skipping y_hat=787489\n","Skipping y_hat=5201162\n","Skipping y_hat=3202621\n","Skipping y_hat=8519716\n","Skipping y_hat=1939774\n","Skipping y_hat=1029934\n","Skipping y_hat=5537022\n","Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=3259071\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n"," 48% 13/27 [00:00<00:00, 20.40it/s]Skipping y_hat=654898\n","Skipping y_hat=2620358\n","Skipping y_hat=2789735\n","Skipping y_hat=828396\n","Skipping y_hat=8989525\n","Skipping y_hat=3976360\n","Skipping y_hat=912008\n","Skipping y_hat=2660052\n","Skipping y_hat=2469998\n","Skipping y_hat=4531988\n","Skipping y_hat=3497003\n"," 59% 16/27 [00:00<00:00, 20.60it/s]Skipping y_hat=2403988\n","Skipping y_hat=992848\n","Skipping y_hat=717478\n","Skipping y_hat=1329472\n","Skipping y_hat=3984065\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n","Skipping y_hat=3115896\n","Skipping y_hat=5398032\n","Skipping y_hat=910808\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=990050\n","Skipping y_hat=1065987\n"," 70% 19/27 [00:00<00:00, 20.63it/s]Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=996288\n","Skipping y_hat=5782933\n","Skipping y_hat=931898\n","Skipping y_hat=6954277\n","Skipping y_hat=6100838\n"," 81% 22/27 [00:01<00:00, 20.78it/s]Skipping y_hat=7590\n","Skipping y_hat=54380\n","Skipping y_hat=200328\n","Skipping y_hat=397794\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 20.89it/s]\n","accuracy of 3000 examples: 2924/3000 (97.46666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 97.47%\n","\n","iter 1900: train loss 0.8524, val loss 1.0060\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4602960\n","Skipping y_hat=977047\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=451118\n","Skipping y_hat=5769973\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 21.74it/s]Skipping y_hat=6919001\n","Skipping y_hat=996278\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.73it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=3982987\n","Skipping y_hat=4903650\n","Skipping y_hat=6997726\n","Skipping y_hat=1884994\n","Skipping y_hat=3989568\n"," 33% 9/27 [00:00<00:00, 21.67it/s]Skipping y_hat=981558\n","Skipping y_hat=504196\n","Skipping y_hat=5860067\n","Skipping y_hat=920656\n","Skipping y_hat=736858\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=2143872\n","Skipping y_hat=3202621\n","Skipping y_hat=3280229\n","Skipping y_hat=562117\n"," 44% 12/27 [00:00<00:00, 21.49it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=2771196\n","Skipping y_hat=829396\n","Skipping y_hat=8989525\n","Skipping y_hat=3976360\n"," 56% 15/27 [00:00<00:00, 21.73it/s]Skipping y_hat=5849560\n","Skipping y_hat=4531988\n","Skipping y_hat=6062662\n","Skipping y_hat=2403988\n","Skipping y_hat=6033799\n","Skipping y_hat=4491244\n","Skipping y_hat=892858\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.92it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=931898\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n"," 78% 21/27 [00:00<00:00, 22.10it/s]Skipping y_hat=94396\n","Skipping y_hat=102258\n"," 89% 24/27 [00:01<00:00, 22.85it/s]Skipping y_hat=96674\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.36it/s]\n","accuracy of 3000 examples: 2940/3000 (98.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.75it/s]\n","accuracy of 10000 examples: 9977/10000 (99.77000000000001%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4602960\n","Skipping y_hat=977047\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=451118\n","Skipping y_hat=5409963\n"," 11% 3/27 [00:00<00:01, 22.93it/s]Skipping y_hat=7231951\n","Skipping y_hat=6919001\n","Skipping y_hat=5062951\n","Skipping y_hat=996278\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.78it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1004198\n","Skipping y_hat=5539996\n","Skipping y_hat=3982987\n","Skipping y_hat=4903650\n","Skipping y_hat=3989568\n"," 33% 9/27 [00:00<00:00, 22.61it/s]Skipping y_hat=981578\n","Skipping y_hat=5860067\n","Skipping y_hat=736858\n","Skipping y_hat=4227403\n","Skipping y_hat=2143872\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=562117\n"," 44% 12/27 [00:00<00:00, 22.59it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=7652111\n","Skipping y_hat=7798177\n","Skipping y_hat=829396\n","Skipping y_hat=8989525\n","Skipping y_hat=3976360\n","Skipping y_hat=7063976\n"," 56% 15/27 [00:00<00:00, 22.74it/s]Skipping y_hat=4531088\n","Skipping y_hat=6062662\n","Skipping y_hat=2403988\n","Skipping y_hat=892858\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.71it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=676384\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982196\n","Skipping y_hat=6111496\n","Skipping y_hat=751988\n","Skipping y_hat=5782933\n","Skipping y_hat=3474196\n","Skipping y_hat=931898\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.62it/s]Skipping y_hat=7590\n","Skipping y_hat=54380\n","Skipping y_hat=94396\n"," 89% 24/27 [00:01<00:00, 23.24it/s]Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.14it/s]\n","accuracy of 3000 examples: 2939/3000 (97.96666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 97.97%\n","\n","iter 2000: train loss 0.8322, val loss 1.0253\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=826640\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=8896420\n","Skipping y_hat=977063\n","Skipping y_hat=2767074\n","Skipping y_hat=984040\n","Skipping y_hat=451118\n","Skipping y_hat=5769973\n","Skipping y_hat=420981\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 21.88it/s]Skipping y_hat=3045878\n","Skipping y_hat=6919001\n","Skipping y_hat=5062951\n","Skipping y_hat=996278\n"," 22% 6/27 [00:00<00:00, 21.79it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=5760799\n","Skipping y_hat=4903650\n","Skipping y_hat=1884994\n"," 33% 9/27 [00:00<00:00, 21.91it/s]Skipping y_hat=981578\n","Skipping y_hat=5860067\n","Skipping y_hat=736858\n","Skipping y_hat=4997944\n","Skipping y_hat=787489\n","Skipping y_hat=6529989\n","Skipping y_hat=5201162\n","Skipping y_hat=3202621\n","Skipping y_hat=8519716\n","Skipping y_hat=6420023\n","Skipping y_hat=1939774\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.59it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=3392785\n","Skipping y_hat=2101484\n","Skipping y_hat=6992847\n"," 56% 15/27 [00:00<00:00, 21.87it/s]Skipping y_hat=2469198\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3984065\n","Skipping y_hat=4441796\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.90it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=990050\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=846247\n","Skipping y_hat=655904\n","Skipping y_hat=5782933\n","Skipping y_hat=3474196\n","Skipping y_hat=931898\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.06it/s]Skipping y_hat=7590\n","Skipping y_hat=94396\n","Skipping y_hat=200328\n"," 89% 24/27 [00:01<00:00, 22.68it/s]Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.53it/s]\n","accuracy of 3000 examples: 2934/3000 (97.8%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.79it/s]\n","accuracy of 10000 examples: 9985/10000 (99.85000000000001%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=8896420\n","Skipping y_hat=5205815\n","Skipping y_hat=977063\n","Skipping y_hat=2767974\n","Skipping y_hat=5400830\n","Skipping y_hat=984040\n","Skipping y_hat=451118\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 21.40it/s]Skipping y_hat=6910001\n","Skipping y_hat=996278\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.76it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=5760799\n","Skipping y_hat=1884994\n"," 33% 9/27 [00:00<00:00, 21.62it/s]Skipping y_hat=981578\n","Skipping y_hat=5860067\n","Skipping y_hat=6499397\n","Skipping y_hat=787489\n","Skipping y_hat=3202621\n"," 44% 12/27 [00:00<00:00, 21.34it/s]Skipping y_hat=7499377\n","Skipping y_hat=7267804\n","Skipping y_hat=1430138\n","Skipping y_hat=2101484\n","Skipping y_hat=6992847\n"," 56% 15/27 [00:00<00:00, 21.40it/s]Skipping y_hat=4531088\n","Skipping y_hat=2403088\n","Skipping y_hat=892858\n","Skipping y_hat=3984065\n","Skipping y_hat=4441796\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.54it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=990050\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=655904\n","Skipping y_hat=5782933\n","Skipping y_hat=3474196\n","Skipping y_hat=931898\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.21it/s]Skipping y_hat=7590\n","Skipping y_hat=94396\n"," 89% 24/27 [00:01<00:00, 22.16it/s]Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 21.95it/s]\n","accuracy of 3000 examples: 2946/3000 (98.2%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.20%\n","\n","iter 2100: train loss 0.7865, val loss 1.0827\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=977087\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5769973\n","Skipping y_hat=420981\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.17it/s]Skipping y_hat=2991602\n","Skipping y_hat=6910001\n","Skipping y_hat=996258\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.52it/s]Skipping y_hat=5758876\n","Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5236703\n","Skipping y_hat=4903650\n"," 33% 9/27 [00:00<00:00, 22.35it/s]Skipping y_hat=981578\n","Skipping y_hat=5860067\n","Skipping y_hat=4227403\n","Skipping y_hat=3202621\n","Skipping y_hat=3390229\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.41it/s]Skipping y_hat=3392785\n","Skipping y_hat=823204\n","Skipping y_hat=828396\n"," 56% 15/27 [00:00<00:00, 21.84it/s]Skipping y_hat=2469998\n","Skipping y_hat=6305786\n","Skipping y_hat=4531988\n","Skipping y_hat=3497003\n","Skipping y_hat=2403988\n","Skipping y_hat=3833906\n","Skipping y_hat=4481244\n","Skipping y_hat=5011963\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.31it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=990050\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=649198\n","Skipping y_hat=6802273\n","Skipping y_hat=3474196\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.52it/s]Skipping y_hat=7590\n","Skipping y_hat=102258\n"," 89% 24/27 [00:01<00:00, 22.51it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.42it/s]\n","accuracy of 3000 examples: 2944/3000 (98.13333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.23it/s]\n","accuracy of 10000 examples: 9990/10000 (99.9%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=3114406\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5769973\n","Skipping y_hat=420981\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.74it/s]Skipping y_hat=2991602\n","Skipping y_hat=724016\n","Skipping y_hat=7231951\n","Skipping y_hat=6910001\n","Skipping y_hat=1343985\n","Skipping y_hat=996288\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.48it/s]Skipping y_hat=5758876\n","Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=5236703\n"," 33% 9/27 [00:00<00:00, 22.26it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=3390229\n"," 44% 12/27 [00:00<00:00, 22.22it/s]Skipping y_hat=1972808\n","Skipping y_hat=7267804\n","Skipping y_hat=3392785\n","Skipping y_hat=1003526\n","Skipping y_hat=2620358\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 22.29it/s]Skipping y_hat=2469998\n","Skipping y_hat=4531988\n","Skipping y_hat=3497003\n","Skipping y_hat=2403988\n","Skipping y_hat=4481244\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.29it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=990050\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=7702703\n","Skipping y_hat=3474196\n"," 78% 21/27 [00:00<00:00, 22.26it/s]Skipping y_hat=7590\n","Skipping y_hat=102258\n"," 89% 24/27 [00:01<00:00, 23.02it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.83it/s]\n","accuracy of 3000 examples: 2942/3000 (98.06666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.07%\n","\n","iter 2200: train loss 0.7395, val loss 1.1353\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=5005286\n","Skipping y_hat=4601960\n","Skipping y_hat=8896420\n","Skipping y_hat=977047\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=451118\n","  7% 2/27 [00:00<00:01, 17.97it/s]Skipping y_hat=3430367\n","Skipping y_hat=5599963\n","Skipping y_hat=3197101\n","Skipping y_hat=3460306\n","Skipping y_hat=6919001\n","Skipping y_hat=4902733\n","Skipping y_hat=921832\n"," 19% 5/27 [00:00<00:01, 20.10it/s]Skipping y_hat=7149970\n","Skipping y_hat=2599385\n","Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=6388337\n","Skipping y_hat=4903650\n"," 30% 8/27 [00:00<00:00, 20.60it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=787489\n","Skipping y_hat=5201162\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n"," 41% 11/27 [00:00<00:00, 20.47it/s]Skipping y_hat=5537022\n","Skipping y_hat=7267804\n","Skipping y_hat=822304\n","Skipping y_hat=655798\n","Skipping y_hat=1003526\n","Skipping y_hat=2620358\n"," 52% 14/27 [00:00<00:00, 20.61it/s]Skipping y_hat=5102953\n","Skipping y_hat=829396\n","Skipping y_hat=649921\n","Skipping y_hat=2660052\n","Skipping y_hat=5849560\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=4491244\n","Skipping y_hat=892858\n"," 63% 17/27 [00:00<00:00, 20.62it/s]Skipping y_hat=5010963\n","Skipping y_hat=4039658\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n","Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=751988\n"," 74% 20/27 [00:00<00:00, 19.97it/s]Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 81% 22/27 [00:01<00:00, 19.79it/s]Skipping y_hat=7590\n","Skipping y_hat=94396\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 20.74it/s]\n","accuracy of 3000 examples: 2937/3000 (97.89999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 23.70it/s]\n","accuracy of 10000 examples: 9993/10000 (99.92999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=5005286\n","Skipping y_hat=4601960\n","Skipping y_hat=8896420\n","Skipping y_hat=4884022\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=3419254\n","Skipping y_hat=5599963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.47it/s]Skipping y_hat=6919001\n","Skipping y_hat=4902733\n","Skipping y_hat=7149970\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.49it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=6388337\n"," 33% 9/27 [00:00<00:00, 21.61it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=1459585\n","Skipping y_hat=5537022\n","Skipping y_hat=318490\n"," 44% 12/27 [00:00<00:00, 21.74it/s]Skipping y_hat=7267804\n","Skipping y_hat=822304\n","Skipping y_hat=5102953\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 21.67it/s]Skipping y_hat=2660052\n","Skipping y_hat=2469998\n","Skipping y_hat=5849560\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=4491244\n","Skipping y_hat=992858\n","Skipping y_hat=5010963\n","Skipping y_hat=7753936\n","Skipping y_hat=1329472\n","Skipping y_hat=4039658\n","Skipping y_hat=8910874\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.35it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=751988\n","Skipping y_hat=655904\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.26it/s]Skipping y_hat=7590\n","Skipping y_hat=94396\n"," 89% 24/27 [00:01<00:00, 22.06it/s]Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 21.99it/s]\n","accuracy of 3000 examples: 2942/3000 (98.06666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.07%\n","\n","iter 2300: train loss 0.6885, val loss 1.1723\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5769973\n","Skipping y_hat=3430367\n","Skipping y_hat=420981\n","Skipping y_hat=5599963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.65it/s]Skipping y_hat=2991602\n","Skipping y_hat=3460306\n","Skipping y_hat=724016\n","Skipping y_hat=6910001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.63it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=668796\n","Skipping y_hat=6937785\n"," 33% 9/27 [00:00<00:00, 21.29it/s]Skipping y_hat=981578\n","Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=2420136\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=3180996\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.04it/s]Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=6399424\n"," 56% 15/27 [00:00<00:00, 20.76it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=5010963\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 20.12it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=3052032\n","Skipping y_hat=2464996\n"," 78% 21/27 [00:01<00:00, 20.11it/s]Skipping y_hat=7590\n","Skipping y_hat=56420\n","Skipping y_hat=200328\n"," 89% 24/27 [00:01<00:00, 20.97it/s]Skipping y_hat=71800\n","Skipping y_hat=96674\n","Skipping y_hat=298747\n","Skipping y_hat=559556\n","100% 27/27 [00:01<00:00, 20.96it/s]\n","accuracy of 3000 examples: 2944/3000 (98.13333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.39it/s]\n","accuracy of 10000 examples: 9996/10000 (99.96000000000001%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=450108\n","Skipping y_hat=5769973\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.69it/s]Skipping y_hat=4973014\n","Skipping y_hat=3460306\n","Skipping y_hat=6910001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.08it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2904198\n","Skipping y_hat=5539896\n","Skipping y_hat=6388337\n","Skipping y_hat=4903650\n","Skipping y_hat=700326\n"," 33% 9/27 [00:00<00:00, 22.05it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=3390229\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.04it/s]Skipping y_hat=7267804\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n"," 56% 15/27 [00:00<00:00, 22.35it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=892858\n","Skipping y_hat=5010963\n","Skipping y_hat=8910874\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.54it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=3474196\n","Skipping y_hat=2852996\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.36it/s]Skipping y_hat=7590\n","Skipping y_hat=56420\n"," 89% 24/27 [00:01<00:00, 23.06it/s]Skipping y_hat=96674\n","Skipping y_hat=298747\n","Skipping y_hat=559556\n","100% 27/27 [00:01<00:00, 22.82it/s]\n","accuracy of 3000 examples: 2947/3000 (98.23333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.23%\n","\n","iter 2400: train loss 0.6492, val loss 1.2573\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=7620016\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5599963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.48it/s]Skipping y_hat=2991602\n","Skipping y_hat=8730278\n","Skipping y_hat=6910001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.77it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=1884994\n","Skipping y_hat=713275\n"," 33% 9/27 [00:00<00:00, 22.26it/s]Skipping y_hat=5860067\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.34it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=822304\n","Skipping y_hat=7089525\n"," 56% 15/27 [00:00<00:00, 22.56it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.50it/s]Skipping y_hat=5996413\n","Skipping y_hat=982096\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.49it/s]Skipping y_hat=7590\n","Skipping y_hat=522804\n","Skipping y_hat=102258\n"," 89% 24/27 [00:01<00:00, 23.08it/s]Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.93it/s]\n","accuracy of 3000 examples: 2961/3000 (98.7%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.56it/s]\n","accuracy of 10000 examples: 9995/10000 (99.95%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904004\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=7620016\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5599963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.54it/s]Skipping y_hat=2991602\n","Skipping y_hat=6910001\n","Skipping y_hat=5062951\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.66it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=2803670\n","Skipping y_hat=2999568\n"," 33% 9/27 [00:00<00:00, 22.52it/s]Skipping y_hat=5860067\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=561017\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.91it/s]Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=822304\n","Skipping y_hat=6399424\n","Skipping y_hat=7089525\n"," 56% 15/27 [00:00<00:00, 21.39it/s]Skipping y_hat=5849560\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=5010963\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.36it/s]Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=3052032\n"," 78% 21/27 [00:00<00:00, 21.41it/s]Skipping y_hat=7590\n","Skipping y_hat=522804\n","Skipping y_hat=102258\n"," 89% 24/27 [00:01<00:00, 22.13it/s]Skipping y_hat=71830\n","Skipping y_hat=460241\n","Skipping y_hat=298747\n","Skipping y_hat=559556\n","100% 27/27 [00:01<00:00, 22.06it/s]\n","accuracy of 3000 examples: 2953/3000 (98.43333333333332%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.43%\n","\n","iter 2500: train loss 0.6174, val loss 1.3084\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=420981\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n","Skipping y_hat=3387979\n","Skipping y_hat=6059212\n"," 11% 3/27 [00:00<00:01, 22.24it/s]Skipping y_hat=2991602\n","Skipping y_hat=6910001\n","Skipping y_hat=5062951\n","Skipping y_hat=4902733\n","Skipping y_hat=7149970\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.29it/s]Skipping y_hat=1239956\n","Skipping y_hat=1004198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n"," 33% 9/27 [00:00<00:00, 22.39it/s]Skipping y_hat=5860067\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=8519716\n","Skipping y_hat=561017\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.31it/s]Skipping y_hat=1405652\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=2620358\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 22.16it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3833906\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.47it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=659098\n","Skipping y_hat=5782933\n"," 78% 21/27 [00:00<00:00, 22.62it/s]Skipping y_hat=7590\n","Skipping y_hat=7144\n","Skipping y_hat=54380\n","Skipping y_hat=94396\n"," 89% 24/27 [00:01<00:00, 23.37it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.06it/s]\n","accuracy of 3000 examples: 2949/3000 (98.3%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.97it/s]\n","accuracy of 10000 examples: 9998/10000 (99.98%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=3430367\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.69it/s]Skipping y_hat=6910001\n","Skipping y_hat=7149970\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.20it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n","Skipping y_hat=713275\n"," 33% 9/27 [00:00<00:00, 22.09it/s]Skipping y_hat=5860067\n","Skipping y_hat=920656\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=3390229\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.05it/s]Skipping y_hat=1972808\n","Skipping y_hat=1405652\n","Skipping y_hat=3240071\n","Skipping y_hat=5706048\n","Skipping y_hat=3392785\n","Skipping y_hat=2620358\n","Skipping y_hat=5102953\n"," 56% 15/27 [00:00<00:00, 22.20it/s]Skipping y_hat=2660052\n","Skipping y_hat=6305786\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=6033799\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.14it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=5782933\n","Skipping y_hat=3474196\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.11it/s]Skipping y_hat=7590\n","Skipping y_hat=7144\n","Skipping y_hat=54380\n","Skipping y_hat=94396\n"," 89% 24/27 [00:01<00:00, 22.71it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.58it/s]\n","accuracy of 3000 examples: 2951/3000 (98.36666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.37%\n","\n","iter 2600: train loss 0.5998, val loss 1.3794\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=3972970\n","Skipping y_hat=4601960\n","Skipping y_hat=3220034\n","Skipping y_hat=6833725\n","Skipping y_hat=977087\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=3430367\n","Skipping y_hat=2160000\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 21.76it/s]Skipping y_hat=724016\n","Skipping y_hat=6919001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.02it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=4903650\n","Skipping y_hat=3989568\n"," 33% 9/27 [00:00<00:00, 22.04it/s]Skipping y_hat=5860067\n","Skipping y_hat=4227403\n","Skipping y_hat=3202621\n","Skipping y_hat=561017\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.98it/s]Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=2101484\n","Skipping y_hat=7584994\n"," 56% 15/27 [00:00<00:00, 21.95it/s]Skipping y_hat=2660052\n","Skipping y_hat=5849560\n","Skipping y_hat=4531988\n","Skipping y_hat=3497003\n","Skipping y_hat=2403988\n","Skipping y_hat=5270898\n","Skipping y_hat=8910874\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.13it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=982096\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n"," 78% 21/27 [00:00<00:00, 22.36it/s]Skipping y_hat=7590\n","Skipping y_hat=7144\n"," 89% 24/27 [00:01<00:00, 23.01it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.66it/s]\n","accuracy of 3000 examples: 2953/3000 (98.43333333333332%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.74it/s]\n","accuracy of 10000 examples: 9998/10000 (99.98%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=3430367\n","Skipping y_hat=2160000\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 21.46it/s]Skipping y_hat=724016\n","Skipping y_hat=6910001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.16it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.34it/s]Skipping y_hat=5860067\n","Skipping y_hat=4227403\n","Skipping y_hat=3202621\n","Skipping y_hat=561017\n","Skipping y_hat=4620158\n","Skipping y_hat=5537022\n","Skipping y_hat=4264046\n"," 44% 12/27 [00:00<00:00, 22.28it/s]Skipping y_hat=7499377\n","Skipping y_hat=2248420\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=2101484\n","Skipping y_hat=2620358\n","Skipping y_hat=829396\n","Skipping y_hat=8989525\n","Skipping y_hat=7776032\n"," 56% 15/27 [00:00<00:00, 21.89it/s]Skipping y_hat=2660052\n","Skipping y_hat=5849560\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=6521026\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.66it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=982096\n","Skipping y_hat=648098\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 20.99it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 21.72it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.02it/s]\n","accuracy of 3000 examples: 2951/3000 (98.36666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.37%\n","\n","iter 2700: train loss 0.5825, val loss 1.4142\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904004\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.20it/s]Skipping y_hat=2991602\n","Skipping y_hat=7231951\n","Skipping y_hat=6910001\n","Skipping y_hat=911732\n","Skipping y_hat=7149970\n","Skipping y_hat=6800831\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.65it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.22it/s]Skipping y_hat=7207952\n","Skipping y_hat=5860067\n","Skipping y_hat=920656\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=3202621\n","Skipping y_hat=8519716\n","Skipping y_hat=561017\n","Skipping y_hat=1459585\n"," 44% 12/27 [00:00<00:00, 21.65it/s]Skipping y_hat=7499377\n","Skipping y_hat=822304\n","Skipping y_hat=6399424\n","Skipping y_hat=5604054\n","Skipping y_hat=7089525\n"," 56% 15/27 [00:00<00:00, 21.42it/s]Skipping y_hat=5849560\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=5270898\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.23it/s]Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.13it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 21.86it/s]Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.03it/s]\n","accuracy of 3000 examples: 2946/3000 (98.2%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.99it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=1860008\n","Skipping y_hat=979067\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=917668\n","Skipping y_hat=3419254\n","Skipping y_hat=5409963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.15it/s]Skipping y_hat=2991602\n","Skipping y_hat=3972014\n","Skipping y_hat=6910001\n","Skipping y_hat=7149970\n","Skipping y_hat=6800831\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:01, 20.90it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 21.27it/s]Skipping y_hat=7207952\n","Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=561017\n","Skipping y_hat=1459585\n"," 44% 12/27 [00:00<00:00, 21.59it/s]Skipping y_hat=7499377\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=6399424\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 22.02it/s]Skipping y_hat=5849560\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=5270898\n","Skipping y_hat=892948\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.18it/s]Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=655814\n","Skipping y_hat=2852996\n","Skipping y_hat=1030136\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.68it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 22.72it/s]Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.38it/s]\n","accuracy of 3000 examples: 2942/3000 (98.06666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.07%\n","\n","iter 2800: train loss 0.5668, val loss 1.4728\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=5409963\n"," 11% 3/27 [00:00<00:01, 22.26it/s]Skipping y_hat=2991602\n","Skipping y_hat=6910001\n"," 22% 6/27 [00:00<00:00, 22.45it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n"," 33% 9/27 [00:00<00:00, 22.05it/s]Skipping y_hat=5860067\n","Skipping y_hat=501098\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.13it/s]Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=1003526\n","Skipping y_hat=2620358\n"," 56% 15/27 [00:00<00:00, 22.39it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=892948\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.50it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=3474196\n","Skipping y_hat=931898\n","Skipping y_hat=2852996\n","Skipping y_hat=3349777\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.54it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.36it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.00it/s]\n","accuracy of 3000 examples: 2952/3000 (98.4%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.01it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=450108\n","Skipping y_hat=917768\n","Skipping y_hat=5409963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.95it/s]Skipping y_hat=2991602\n","Skipping y_hat=6910001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.08it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n","Skipping y_hat=713275\n"," 33% 9/27 [00:00<00:00, 22.10it/s]Skipping y_hat=5860067\n","Skipping y_hat=501098\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.96it/s]Skipping y_hat=7499377\n","Skipping y_hat=7798177\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n"," 56% 15/27 [00:00<00:00, 22.16it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=892948\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.08it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=3474196\n","Skipping y_hat=2852996\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.49it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 22.02it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.10it/s]\n","accuracy of 3000 examples: 2950/3000 (98.33333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.33%\n","\n","iter 2900: train loss 0.5585, val loss 1.5120\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=977867\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=6075936\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n","Skipping y_hat=3387979\n"," 11% 3/27 [00:00<00:01, 21.63it/s]Skipping y_hat=2991602\n","Skipping y_hat=6159430\n","Skipping y_hat=6910001\n","Skipping y_hat=4902733\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.76it/s]Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n","Skipping y_hat=6307748\n","Skipping y_hat=1839849\n","Skipping y_hat=928390\n"," 33% 9/27 [00:00<00:00, 21.84it/s]Skipping y_hat=5860067\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=1649596\n"," 44% 12/27 [00:00<00:00, 22.01it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=5897548\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=6399424\n"," 56% 15/27 [00:00<00:00, 21.92it/s]Skipping y_hat=2660052\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.07it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n"," 89% 24/27 [00:01<00:00, 22.94it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.59it/s]\n","accuracy of 3000 examples: 2952/3000 (98.4%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.53it/s]\n","accuracy of 10000 examples: 9994/10000 (99.94%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=917768\n","Skipping y_hat=3430367\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.50it/s]Skipping y_hat=2991602\n","Skipping y_hat=6159430\n","Skipping y_hat=6910001\n","Skipping y_hat=4902733\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.53it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n","Skipping y_hat=6307748\n"," 33% 9/27 [00:00<00:00, 21.27it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n"," 44% 12/27 [00:00<00:00, 21.55it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=5897548\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=6399424\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 22.01it/s]Skipping y_hat=2660052\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.27it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=1358879\n","Skipping y_hat=6074984\n","Skipping y_hat=1030136\n","Skipping y_hat=3052032\n"," 78% 21/27 [00:00<00:00, 22.17it/s]Skipping y_hat=7590\n","Skipping y_hat=7144\n"," 89% 24/27 [00:01<00:00, 23.05it/s]Skipping y_hat=298747\n","Skipping y_hat=559556\n","100% 27/27 [00:01<00:00, 22.68it/s]\n","accuracy of 3000 examples: 2948/3000 (98.26666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.27%\n","\n","iter 3000: train loss 0.5531, val loss 1.5839\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5769973\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.20it/s]Skipping y_hat=6159430\n","Skipping y_hat=6002504\n","Skipping y_hat=6919001\n","Skipping y_hat=921832\n","Skipping y_hat=1343985\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.07it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.19it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=7018912\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=3180996\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.26it/s]Skipping y_hat=5897548\n","Skipping y_hat=7267804\n","Skipping y_hat=822304\n","Skipping y_hat=829396\n","Skipping y_hat=7089525\n"," 56% 15/27 [00:00<00:00, 22.37it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=1891601\n","Skipping y_hat=6033799\n","Skipping y_hat=4381244\n","Skipping y_hat=892948\n","Skipping y_hat=1329472\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.29it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=4474440\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.31it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.07it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.84it/s]\n","accuracy of 3000 examples: 2949/3000 (98.3%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.15it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984940\n","Skipping y_hat=917768\n","Skipping y_hat=5769973\n","Skipping y_hat=2160000\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n","Skipping y_hat=6069312\n"," 11% 3/27 [00:00<00:01, 22.51it/s]Skipping y_hat=6159430\n","Skipping y_hat=6910001\n","Skipping y_hat=921832\n"," 22% 6/27 [00:00<00:00, 22.67it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.77it/s]Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.42it/s]Skipping y_hat=7499377\n","Skipping y_hat=5897548\n","Skipping y_hat=822304\n"," 56% 15/27 [00:00<00:00, 22.22it/s]Skipping y_hat=2660052\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=6033799\n","Skipping y_hat=892948\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.24it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.30it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.04it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.88it/s]\n","accuracy of 3000 examples: 2955/3000 (98.5%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.50%\n","\n","iter 3100: train loss 0.5467, val loss 1.6251\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904904\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=451008\n","Skipping y_hat=3430367\n","Skipping y_hat=5409963\n","Skipping y_hat=4482073\n"," 11% 3/27 [00:00<00:01, 21.87it/s]Skipping y_hat=6910001\n"," 22% 6/27 [00:00<00:00, 22.29it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n"," 33% 9/27 [00:00<00:00, 22.24it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.11it/s]Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=7584994\n","Skipping y_hat=3492127\n","Skipping y_hat=7089525\n","Skipping y_hat=5035004\n"," 56% 15/27 [00:00<00:00, 22.03it/s]Skipping y_hat=2660052\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=4491244\n","Skipping y_hat=892948\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.79it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3474196\n","Skipping y_hat=6100838\n"," 89% 24/27 [00:01<00:00, 22.48it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.42it/s]\n","accuracy of 3000 examples: 2954/3000 (98.46666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.06it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904904\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=977867\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=451008\n","Skipping y_hat=917768\n","Skipping y_hat=5409963\n","Skipping y_hat=3197101\n","Skipping y_hat=4482073\n"," 11% 3/27 [00:00<00:01, 22.68it/s]Skipping y_hat=6910001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.79it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n","Skipping y_hat=713275\n"," 33% 9/27 [00:00<00:00, 22.80it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.64it/s]Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=7584994\n","Skipping y_hat=3492127\n","Skipping y_hat=7089525\n"," 56% 15/27 [00:00<00:00, 22.74it/s]Skipping y_hat=2660052\n","Skipping y_hat=4531988\n","Skipping y_hat=4091716\n","Skipping y_hat=2403988\n","Skipping y_hat=4491244\n","Skipping y_hat=892948\n","Skipping y_hat=3984065\n","Skipping y_hat=8110874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.43it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3474196\n","Skipping y_hat=2852996\n","Skipping y_hat=6100838\n"," 89% 24/27 [00:01<00:00, 22.62it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","Skipping y_hat=201248\n","100% 27/27 [00:01<00:00, 22.76it/s]\n","accuracy of 3000 examples: 2949/3000 (98.3%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.30%\n","\n","iter 3200: train loss 0.5351, val loss 1.6448\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 20.84it/s]Skipping y_hat=2991602\n","Skipping y_hat=6910001\n","Skipping y_hat=4902733\n","Skipping y_hat=6800831\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:01, 20.79it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n","Skipping y_hat=1884994\n","Skipping y_hat=6307748\n"," 33% 9/27 [00:00<00:00, 21.07it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=2420136\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n"," 44% 12/27 [00:00<00:00, 21.30it/s]Skipping y_hat=7499377\n","Skipping y_hat=5897548\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=8089525\n"," 56% 15/27 [00:00<00:00, 21.16it/s]Skipping y_hat=6305786\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=2112204\n","Skipping y_hat=4491244\n","Skipping y_hat=892948\n","Skipping y_hat=6011963\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 20.95it/s]Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.00it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 22.20it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 21.85it/s]\n","accuracy of 3000 examples: 2949/3000 (98.3%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.31it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=420981\n","Skipping y_hat=5409963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 23.24it/s]Skipping y_hat=2991602\n","Skipping y_hat=6910001\n","Skipping y_hat=4902733\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 23.12it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n","Skipping y_hat=1884994\n","Skipping y_hat=6307748\n"," 33% 9/27 [00:00<00:00, 22.59it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=2420136\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n"," 44% 12/27 [00:00<00:00, 22.67it/s]Skipping y_hat=7499377\n","Skipping y_hat=5897548\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=8089525\n"," 56% 15/27 [00:00<00:00, 22.23it/s]Skipping y_hat=6305786\n","Skipping y_hat=4531988\n","Skipping y_hat=3497003\n","Skipping y_hat=2403988\n","Skipping y_hat=892948\n","Skipping y_hat=717478\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.98it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.73it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 22.60it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.72it/s]\n","accuracy of 3000 examples: 2947/3000 (98.23333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.23%\n","\n","iter 3300: train loss 0.5331, val loss 1.6401\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3972970\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 22.41it/s]Skipping y_hat=6919001\n","Skipping y_hat=7149970\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.37it/s]Skipping y_hat=5599882\n","Skipping y_hat=714379\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=761888\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n","Skipping y_hat=1884994\n"," 33% 9/27 [00:00<00:00, 22.16it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.20it/s]Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=2620358\n","Skipping y_hat=2060166\n"," 56% 15/27 [00:00<00:00, 22.34it/s]Skipping y_hat=2660052\n","Skipping y_hat=4531988\n","Skipping y_hat=3497003\n","Skipping y_hat=2403988\n","Skipping y_hat=892948\n","Skipping y_hat=3201747\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.31it/s]Skipping y_hat=5398032\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=845237\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.06it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 22.86it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.76it/s]\n","accuracy of 3000 examples: 2951/3000 (98.36666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.77it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=3972970\n","Skipping y_hat=4601960\n","Skipping y_hat=977087\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 21.98it/s]Skipping y_hat=2991602\n","Skipping y_hat=6910001\n","Skipping y_hat=4902733\n","Skipping y_hat=7149970\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.37it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=761888\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=6388337\n","Skipping y_hat=4903650\n","Skipping y_hat=713275\n"," 33% 9/27 [00:00<00:00, 22.20it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=3660496\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.75it/s]Skipping y_hat=7499377\n","Skipping y_hat=5897548\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=654898\n","Skipping y_hat=2620358\n"," 56% 15/27 [00:00<00:00, 22.00it/s]Skipping y_hat=2660052\n","Skipping y_hat=4531988\n","Skipping y_hat=3497003\n","Skipping y_hat=2403988\n","Skipping y_hat=892948\n","Skipping y_hat=3201747\n","Skipping y_hat=3659828\n"," 67% 18/27 [00:00<00:00, 22.02it/s]Skipping y_hat=5398032\n","Skipping y_hat=5745023\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=3474196\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.87it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 22.74it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.57it/s]\n","accuracy of 3000 examples: 2947/3000 (98.23333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.23%\n","\n","iter 3400: train loss 0.5358, val loss 1.7095\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5769973\n","Skipping y_hat=420981\n","Skipping y_hat=5409963\n"," 11% 3/27 [00:00<00:01, 20.58it/s]Skipping y_hat=6919001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.62it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=761888\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=6388337\n","Skipping y_hat=1884994\n"," 33% 9/27 [00:00<00:00, 21.87it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n"," 44% 12/27 [00:00<00:00, 22.04it/s]Skipping y_hat=7499377\n","Skipping y_hat=2080322\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=2060166\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 22.03it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3984065\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.14it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.90it/s]Skipping y_hat=7590\n","Skipping y_hat=55990\n"," 89% 24/27 [00:01<00:00, 22.66it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.41it/s]\n","accuracy of 3000 examples: 2954/3000 (98.46666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.69it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3972970\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=420981\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 20.42it/s]Skipping y_hat=6919001\n","Skipping y_hat=2699803\n","Skipping y_hat=4167918\n","Skipping y_hat=7140970\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:01, 20.98it/s]Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=761888\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=6388337\n","Skipping y_hat=4103650\n","Skipping y_hat=1884994\n"," 33% 9/27 [00:00<00:00, 20.77it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n"," 44% 12/27 [00:00<00:00, 20.88it/s]Skipping y_hat=7499377\n","Skipping y_hat=7314688\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=2060166\n"," 56% 15/27 [00:00<00:00, 20.94it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=892948\n","Skipping y_hat=3201747\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.13it/s]Skipping y_hat=5398032\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:01<00:00, 20.87it/s]Skipping y_hat=55990\n"," 89% 24/27 [00:01<00:00, 21.78it/s]Skipping y_hat=96784\n","Skipping y_hat=460241\n","Skipping y_hat=298747\n","Skipping y_hat=201248\n","100% 27/27 [00:01<00:00, 21.54it/s]\n","accuracy of 3000 examples: 2950/3000 (98.33333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.33%\n","\n","iter 3500: train loss 0.5303, val loss 1.7266\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5005286\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=420981\n","Skipping y_hat=5409963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 20.35it/s]Skipping y_hat=6919001\n"," 22% 6/27 [00:00<00:00, 21.38it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=6388337\n"," 33% 9/27 [00:00<00:00, 21.05it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=2420136\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=8519716\n","Skipping y_hat=5537022\n","Skipping y_hat=1649596\n"," 44% 12/27 [00:00<00:00, 21.02it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=4156822\n","Skipping y_hat=5897548\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n","Skipping y_hat=2060166\n"," 56% 15/27 [00:00<00:00, 21.35it/s]Skipping y_hat=4531988\n","Skipping y_hat=8787691\n","Skipping y_hat=2403988\n","Skipping y_hat=892948\n","Skipping y_hat=1329472\n","Skipping y_hat=3984065\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.87it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=655814\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 89% 24/27 [00:01<00:00, 23.03it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.33it/s]\n","accuracy of 3000 examples: 2948/3000 (98.26666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.94it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5409963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 20.42it/s]Skipping y_hat=6919001\n"," 22% 6/27 [00:00<00:00, 21.06it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=6388337\n"," 33% 9/27 [00:00<00:00, 21.94it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=2420136\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.31it/s]Skipping y_hat=7499377\n","Skipping y_hat=4156822\n","Skipping y_hat=5897548\n","Skipping y_hat=7314688\n","Skipping y_hat=1540029\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n","Skipping y_hat=2060166\n"," 56% 15/27 [00:00<00:00, 22.53it/s]Skipping y_hat=4531988\n","Skipping y_hat=3497003\n","Skipping y_hat=2403988\n","Skipping y_hat=892948\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.66it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=655814\n","Skipping y_hat=1030136\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 89% 24/27 [00:01<00:00, 23.35it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.84it/s]\n","accuracy of 3000 examples: 2953/3000 (98.43333333333332%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.43%\n","\n","iter 3600: train loss 0.5254, val loss 1.7513\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=451008\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 22.73it/s]Skipping y_hat=6910001\n","Skipping y_hat=4902733\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.78it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=6388337\n","Skipping y_hat=4903650\n","Skipping y_hat=928390\n"," 33% 9/27 [00:00<00:00, 22.71it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227203\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n","Skipping y_hat=1649596\n"," 44% 12/27 [00:00<00:00, 22.67it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=5897548\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=1003526\n","Skipping y_hat=2060166\n"," 56% 15/27 [00:00<00:00, 22.71it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=1329472\n","Skipping y_hat=598403\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.55it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=655814\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.11it/s]Skipping y_hat=55990\n"," 89% 24/27 [00:01<00:00, 22.72it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.85it/s]\n","accuracy of 3000 examples: 2952/3000 (98.4%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.11it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2710906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 21.48it/s]Skipping y_hat=3460306\n","Skipping y_hat=6910001\n","Skipping y_hat=4902733\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.50it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1004198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n","Skipping y_hat=6388337\n","Skipping y_hat=4903650\n","Skipping y_hat=928390\n"," 33% 9/27 [00:00<00:00, 21.20it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227903\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n"," 44% 12/27 [00:00<00:00, 21.34it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=2060166\n"," 56% 15/27 [00:00<00:00, 21.25it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n"," 67% 18/27 [00:00<00:00, 21.25it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=655814\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.05it/s]Skipping y_hat=7590\n","Skipping y_hat=55990\n"," 89% 24/27 [00:01<00:00, 21.74it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 21.74it/s]\n","accuracy of 3000 examples: 2954/3000 (98.46666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.47%\n","\n","iter 3700: train loss 0.5166, val loss 1.7567\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=5205815\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=451008\n","Skipping y_hat=420981\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.51it/s]Skipping y_hat=6919001\n","Skipping y_hat=7130970\n"," 22% 6/27 [00:00<00:00, 22.72it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=713275\n"," 33% 9/27 [00:00<00:00, 22.41it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=5201162\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=3180996\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.38it/s]Skipping y_hat=7499377\n","Skipping y_hat=779416\n","Skipping y_hat=3392785\n","Skipping y_hat=2060166\n","Skipping y_hat=649921\n"," 56% 15/27 [00:00<00:00, 22.30it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3833906\n","Skipping y_hat=717478\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.20it/s]Skipping y_hat=5398032\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.09it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 22.85it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.80it/s]\n","accuracy of 3000 examples: 2950/3000 (98.33333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.22it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=451008\n","  7% 2/27 [00:00<00:01, 19.58it/s]Skipping y_hat=5509963\n","Skipping y_hat=6919001\n"," 19% 5/27 [00:00<00:01, 20.91it/s]Skipping y_hat=1343985\n","Skipping y_hat=7130970\n","Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n"," 30% 8/27 [00:00<00:00, 21.33it/s]Skipping y_hat=5860067\n","Skipping y_hat=501098\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n"," 41% 11/27 [00:00<00:00, 21.60it/s]Skipping y_hat=5537022\n","Skipping y_hat=7499377\n","Skipping y_hat=779416\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n"," 52% 14/27 [00:00<00:00, 21.77it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3833906\n","Skipping y_hat=717478\n"," 63% 17/27 [00:00<00:00, 21.83it/s]Skipping y_hat=1329472\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n","Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1065987\n","Skipping y_hat=982196\n","Skipping y_hat=6074984\n"," 74% 20/27 [00:00<00:00, 21.91it/s]Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n","Skipping y_hat=7590\n"," 85% 23/27 [00:01<00:00, 22.66it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.22it/s]\n","accuracy of 3000 examples: 2957/3000 (98.56666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.57%\n","\n","iter 3800: train loss 0.5216, val loss 1.7970\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=4601960\n","Skipping y_hat=5205815\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 22.63it/s]Skipping y_hat=6919001\n","Skipping y_hat=6800831\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.42it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=668796\n"," 33% 9/27 [00:00<00:00, 22.04it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=5201162\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.88it/s]Skipping y_hat=7499377\n","Skipping y_hat=5706048\n","Skipping y_hat=822304\n","Skipping y_hat=791240\n","Skipping y_hat=2060166\n"," 56% 15/27 [00:00<00:00, 21.67it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=717478\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.66it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.98it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 22.70it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","Skipping y_hat=559556\n","100% 27/27 [00:01<00:00, 22.52it/s]\n","accuracy of 3000 examples: 2952/3000 (98.4%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.16it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=4601960\n","Skipping y_hat=5205815\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=2160000\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 20.76it/s]Skipping y_hat=2991602\n","Skipping y_hat=8730278\n","Skipping y_hat=6919001\n","Skipping y_hat=4902733\n","Skipping y_hat=6800831\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:01, 20.90it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=1884994\n","Skipping y_hat=713275\n"," 33% 9/27 [00:00<00:00, 21.24it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=5201162\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.56it/s]Skipping y_hat=7499377\n","Skipping y_hat=5706048\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n","Skipping y_hat=2060166\n"," 56% 15/27 [00:00<00:00, 21.84it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=717478\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.00it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845247\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 89% 24/27 [00:01<00:00, 22.69it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.28it/s]\n","accuracy of 3000 examples: 2946/3000 (98.2%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.20%\n","\n","iter 3900: train loss 0.5115, val loss 1.8394\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=4601960\n","Skipping y_hat=979067\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5409963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.27it/s]Skipping y_hat=8730278\n","Skipping y_hat=6910001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.97it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=668796\n","Skipping y_hat=1884994\n","Skipping y_hat=713275\n"," 33% 9/27 [00:00<00:00, 21.05it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=7018912\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 20.91it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=5897548\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 21.12it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.19it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6682970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.56it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 22.48it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.07it/s]\n","accuracy of 3000 examples: 2950/3000 (98.33333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.84it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=977167\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5409963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 20.49it/s]Skipping y_hat=6910001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.14it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=761888\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=1884994\n","Skipping y_hat=713275\n"," 33% 9/27 [00:00<00:00, 20.74it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=7018912\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=3390229\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.41it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 21.83it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3833906\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.42it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=655814\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.51it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 22.42it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.01it/s]\n","accuracy of 3000 examples: 2951/3000 (98.36666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.37%\n","\n","iter 4000: train loss 0.5180, val loss 1.8369\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.45it/s]Skipping y_hat=724016\n","Skipping y_hat=6910001\n","Skipping y_hat=921832\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.41it/s]Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.56it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=1002698\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.59it/s]Skipping y_hat=7499377\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n","Skipping y_hat=7089525\n"," 56% 15/27 [00:00<00:00, 22.65it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=892948\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.66it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.65it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.29it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.11it/s]\n","accuracy of 3000 examples: 2957/3000 (98.56666666666666%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.30it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.79it/s]Skipping y_hat=724016\n","Skipping y_hat=6910001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.79it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=761888\n","Skipping y_hat=1904198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.84it/s]Skipping y_hat=5860067\n","Skipping y_hat=501098\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=1002698\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.72it/s]Skipping y_hat=7499377\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n"," 56% 15/27 [00:00<00:00, 22.64it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.63it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=5745023\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=655814\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.47it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.16it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.11it/s]\n","accuracy of 3000 examples: 2955/3000 (98.5%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.50%\n","\n","iter 4100: train loss 0.5135, val loss 1.8482\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904904\n","Skipping y_hat=2729906\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 22.19it/s]Skipping y_hat=6910001\n","Skipping y_hat=921832\n","Skipping y_hat=7149970\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.49it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=1884994\n"," 33% 9/27 [00:00<00:00, 22.60it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=7018912\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.30it/s]Skipping y_hat=7499377\n","Skipping y_hat=822304\n","Skipping y_hat=6399424\n","Skipping y_hat=5604054\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 22.54it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3833906\n","Skipping y_hat=717478\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.71it/s]Skipping y_hat=5398032\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n","Skipping y_hat=2464996\n"," 78% 21/27 [00:00<00:00, 22.78it/s]Skipping y_hat=7590\n","Skipping y_hat=55990\n"," 89% 24/27 [00:01<00:00, 23.39it/s]Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.13it/s]\n","accuracy of 3000 examples: 2955/3000 (98.5%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.19it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 22.35it/s]Skipping y_hat=6910001\n","Skipping y_hat=7149970\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.58it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n"," 33% 9/27 [00:00<00:00, 22.19it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.34it/s]Skipping y_hat=7499377\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=5395396\n"," 56% 15/27 [00:00<00:00, 22.39it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=717478\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.27it/s]Skipping y_hat=5398032\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n","Skipping y_hat=2464996\n"," 78% 21/27 [00:00<00:00, 22.39it/s]Skipping y_hat=7590\n","Skipping y_hat=55990\n"," 89% 24/27 [00:01<00:00, 23.08it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.86it/s]\n","accuracy of 3000 examples: 2956/3000 (98.53333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.53%\n","\n","iter 4200: train loss 0.5143, val loss 1.9207\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 22.00it/s]Skipping y_hat=2991602\n","Skipping y_hat=724016\n","Skipping y_hat=6919001\n"," 22% 6/27 [00:00<00:00, 21.85it/s]Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 21.61it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 20.72it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=822304\n"," 56% 15/27 [00:00<00:00, 20.49it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 20.18it/s]Skipping y_hat=5398032\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 89% 24/27 [00:01<00:00, 20.78it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 21.13it/s]\n","accuracy of 3000 examples: 2962/3000 (98.73333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.90it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","  7% 2/27 [00:00<00:01, 19.88it/s]Skipping y_hat=5409963\n","Skipping y_hat=724016\n","Skipping y_hat=6919001\n","Skipping y_hat=921832\n"," 19% 5/27 [00:00<00:01, 21.27it/s]Skipping y_hat=4167918\n","Skipping y_hat=7140970\n","Skipping y_hat=2599385\n","Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n"," 30% 8/27 [00:00<00:00, 21.82it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n"," 41% 11/27 [00:00<00:00, 22.20it/s]Skipping y_hat=5537022\n","Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n"," 52% 14/27 [00:00<00:00, 22.21it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n"," 63% 17/27 [00:00<00:00, 21.95it/s]Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n","Skipping y_hat=5398032\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n"," 74% 20/27 [00:00<00:00, 21.30it/s]Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 85% 23/27 [00:01<00:00, 22.09it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.15it/s]\n","accuracy of 3000 examples: 2957/3000 (98.56666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.57%\n","\n","iter 4300: train loss 0.5074, val loss 1.9103\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 21.88it/s]Skipping y_hat=2991602\n","Skipping y_hat=6910001\n","Skipping y_hat=7149970\n","Skipping y_hat=6800831\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.98it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=761888\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.03it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.96it/s]Skipping y_hat=7499377\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n"," 56% 15/27 [00:00<00:00, 22.09it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.09it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.15it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 22.92it/s]Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.56it/s]\n","accuracy of 3000 examples: 2958/3000 (98.6%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.49it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5409963\n"," 11% 3/27 [00:00<00:01, 22.22it/s]Skipping y_hat=2991602\n","Skipping y_hat=6910001\n","Skipping y_hat=5321796\n","Skipping y_hat=6800831\n"," 22% 6/27 [00:00<00:00, 22.58it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1004198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.62it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.63it/s]Skipping y_hat=7499377\n","Skipping y_hat=822304\n","Skipping y_hat=7089525\n"," 56% 15/27 [00:00<00:00, 22.63it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3833906\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.76it/s]Skipping y_hat=5398032\n","Skipping y_hat=5745023\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.73it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.42it/s]Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.23it/s]\n","accuracy of 3000 examples: 2959/3000 (98.63333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.63%\n","\n","iter 4400: train loss 0.4993, val loss 1.8864\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.64it/s]Skipping y_hat=6910001\n","Skipping y_hat=4902733\n","Skipping y_hat=921832\n","Skipping y_hat=4167918\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.71it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.80it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=7018912\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.38it/s]Skipping y_hat=7499377\n","Skipping y_hat=5897548\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n"," 56% 15/27 [00:00<00:00, 22.61it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=5280998\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.48it/s]Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.54it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.33it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.13it/s]\n","accuracy of 3000 examples: 2956/3000 (98.53333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.53it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 20.51it/s]Skipping y_hat=6910001\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.23it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=5539896\n","Skipping y_hat=1884994\n"," 33% 9/27 [00:00<00:00, 20.89it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=7018912\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 20.78it/s]Skipping y_hat=7499377\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n"," 56% 15/27 [00:00<00:00, 20.90it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.04it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:01<00:00, 20.90it/s]Skipping y_hat=7590\n","Skipping y_hat=55990\n"," 89% 24/27 [00:01<00:00, 21.61it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 21.58it/s]\n","accuracy of 3000 examples: 2960/3000 (98.66666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.67%\n","\n","iter 4500: train loss 0.5068, val loss 1.9382\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5409963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.99it/s]Skipping y_hat=2991602\n","Skipping y_hat=6910001\n","Skipping y_hat=921832\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.14it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1004198\n","Skipping y_hat=5539896\n","Skipping y_hat=713275\n"," 33% 9/27 [00:00<00:00, 22.29it/s]Skipping y_hat=5860067\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=3390229\n"," 44% 12/27 [00:00<00:00, 22.40it/s]Skipping y_hat=7499377\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=6399424\n","Skipping y_hat=5395396\n","Skipping y_hat=2620358\n"," 56% 15/27 [00:00<00:00, 22.40it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3833906\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.11it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 89% 24/27 [00:01<00:00, 22.88it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.72it/s]\n","accuracy of 3000 examples: 2957/3000 (98.56666666666666%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.91it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 20.60it/s]Skipping y_hat=2991602\n","Skipping y_hat=6919001\n","Skipping y_hat=921832\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:01, 20.78it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=4903650\n","Skipping y_hat=713275\n"," 33% 9/27 [00:00<00:00, 20.75it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.27it/s]Skipping y_hat=7499377\n","Skipping y_hat=4156822\n","Skipping y_hat=3392785\n","Skipping y_hat=5395396\n"," 56% 15/27 [00:00<00:00, 21.76it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3833906\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.95it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 89% 24/27 [00:01<00:00, 22.83it/s]Skipping y_hat=96784\n","Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.20it/s]\n","accuracy of 3000 examples: 2955/3000 (98.5%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.50%\n","\n","iter 4600: train loss 0.5043, val loss 1.9703\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904904\n","Skipping y_hat=2729906\n","Skipping y_hat=5005286\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.73it/s]Skipping y_hat=6910001\n","Skipping y_hat=921832\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.09it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.40it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=7018912\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.23it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=4156822\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 22.42it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=717478\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.40it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3474196\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.43it/s]Skipping y_hat=55990\n"," 89% 24/27 [00:01<00:00, 23.08it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","Skipping y_hat=559556\n","100% 27/27 [00:01<00:00, 22.85it/s]\n","accuracy of 3000 examples: 2952/3000 (98.4%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.02it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.02it/s]Skipping y_hat=6910001\n","Skipping y_hat=921832\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.30it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.22it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n"," 44% 12/27 [00:00<00:00, 22.30it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=6399424\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 22.08it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.09it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 89% 24/27 [00:01<00:00, 22.89it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.68it/s]\n","accuracy of 3000 examples: 2958/3000 (98.6%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.60%\n","\n","iter 4700: train loss 0.5072, val loss 1.9350\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=6650451\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.05it/s]Skipping y_hat=6919001\n","Skipping y_hat=921832\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.35it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1004198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.32it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=3202621\n","Skipping y_hat=4999456\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.40it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n"," 56% 15/27 [00:00<00:00, 22.40it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.42it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.34it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.09it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.91it/s]\n","accuracy of 3000 examples: 2957/3000 (98.56666666666666%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.93it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5599963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.89it/s]Skipping y_hat=6919001\n","Skipping y_hat=7130970\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.03it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1004198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 21.88it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 21.77it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=779416\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=2620358\n"," 56% 15/27 [00:00<00:00, 21.97it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.06it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 89% 24/27 [00:01<00:00, 22.71it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.46it/s]\n","accuracy of 3000 examples: 2957/3000 (98.56666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.57%\n","\n","iter 4800: train loss 0.5050, val loss 1.9620\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 22.43it/s]Skipping y_hat=8730278\n","Skipping y_hat=6919001\n"," 22% 6/27 [00:00<00:00, 22.71it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=921640\n","Skipping y_hat=4903650\n"," 33% 9/27 [00:00<00:00, 22.74it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=1459585\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.64it/s]Skipping y_hat=7499377\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n"," 56% 15/27 [00:00<00:00, 22.70it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.75it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.49it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.04it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.03it/s]\n","accuracy of 3000 examples: 2958/3000 (98.6%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.31it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=977867\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.83it/s]Skipping y_hat=6919001\n"," 22% 6/27 [00:00<00:00, 22.26it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1004198\n","Skipping y_hat=921640\n","Skipping y_hat=5539896\n","Skipping y_hat=4481598\n"," 33% 9/27 [00:00<00:00, 22.24it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.28it/s]Skipping y_hat=7499377\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n"," 56% 15/27 [00:00<00:00, 22.49it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=1329472\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.57it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1808602\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=844237\n","Skipping y_hat=3052032\n","Skipping y_hat=6681970\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.54it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.13it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.89it/s]\n","accuracy of 3000 examples: 2955/3000 (98.5%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.50%\n","\n","iter 4900: train loss 0.5003, val loss 1.9878\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.11it/s]Skipping y_hat=6919001\n","Skipping y_hat=921832\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.16it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n","Skipping y_hat=4893895\n"," 33% 9/27 [00:00<00:00, 22.26it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=787489\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.32it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n"," 56% 15/27 [00:00<00:00, 22.60it/s]Skipping y_hat=2660052\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3833906\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.61it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.56it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.27it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.03it/s]\n","accuracy of 3000 examples: 2958/3000 (98.6%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.52it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=984040\n","Skipping y_hat=5409963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.66it/s]Skipping y_hat=6919001\n","Skipping y_hat=921832\n"," 22% 6/27 [00:00<00:00, 22.93it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=2996020\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.94it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.91it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=1540029\n","Skipping y_hat=822304\n"," 56% 15/27 [00:00<00:00, 22.97it/s]Skipping y_hat=2660052\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=3833906\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.92it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.86it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.54it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.36it/s]\n","accuracy of 3000 examples: 2962/3000 (98.73333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.73%\n","\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=4601960\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5509963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.57it/s]Skipping y_hat=6919001\n","Skipping y_hat=7079204\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.23it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.53it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.50it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=5897548\n","Skipping y_hat=1540029\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 22.27it/s]Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=4381244\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.51it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.45it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.27it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.04it/s]\n","accuracy of 3000 examples: 2957/3000 (98.56666666666666%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.08it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=905904\n","Skipping y_hat=2729906\n","Skipping y_hat=4601960\n","Skipping y_hat=977967\n","Skipping y_hat=2767974\n","Skipping y_hat=6960120\n","Skipping y_hat=984040\n","Skipping y_hat=5409963\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.18it/s]Skipping y_hat=6919001\n","Skipping y_hat=7149970\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 22.68it/s]Skipping y_hat=5599882\n","Skipping y_hat=1239956\n","Skipping y_hat=1904198\n","Skipping y_hat=5539896\n"," 33% 9/27 [00:00<00:00, 22.78it/s]Skipping y_hat=5860067\n","Skipping y_hat=4997944\n","Skipping y_hat=4227403\n","Skipping y_hat=6529989\n","Skipping y_hat=3202621\n","Skipping y_hat=5537022\n"," 44% 12/27 [00:00<00:00, 22.73it/s]Skipping y_hat=1972808\n","Skipping y_hat=7499377\n","Skipping y_hat=5897548\n","Skipping y_hat=3392785\n","Skipping y_hat=822304\n","Skipping y_hat=2060166\n"," 56% 15/27 [00:00<00:00, 22.72it/s]Skipping y_hat=2660052\n","Skipping y_hat=4531988\n","Skipping y_hat=2403988\n","Skipping y_hat=8910874\n","Skipping y_hat=3659828\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 22.77it/s]Skipping y_hat=5398032\n","Skipping y_hat=5996413\n","Skipping y_hat=1065987\n","Skipping y_hat=982096\n","Skipping y_hat=6074984\n","Skipping y_hat=845237\n","Skipping y_hat=3052032\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.86it/s]Skipping y_hat=7590\n"," 89% 24/27 [00:01<00:00, 23.46it/s]Skipping y_hat=460241\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 23.27it/s]\n","accuracy of 3000 examples: 2955/3000 (98.5%)\n","\n","Final Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.50%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m0_to_six_digit_times_1_digit_plain\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/multiplication/runs/ke7hm1zs\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m0_to_six_digit_times_1_digit/plain_out/wandb/run-20250702_235910-ke7hm1zs/logs\u001b[0m\n"]}],"source":["!python train_end_padding_more_early_eval.py 2_operands_mul_plain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1856915,"status":"ok","timestamp":1751504640292,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"YitJvuwhFTNE","outputId":"8eaeff3b-c047-42f0-8fcf-fdb1480e09ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n"," 56% 15/27 [00:00<00:00, 22.39it/s]Skipping y_hat=7321969\n","Skipping y_hat=6854776\n","Skipping y_hat=6033799\n","Skipping y_hat=716578\n","Skipping y_hat=772714\n","Skipping y_hat=5819248\n"," 67% 18/27 [00:00<00:00, 22.14it/s]Skipping y_hat=5896313\n","Skipping y_hat=5579144\n","Skipping y_hat=6778727\n","Skipping y_hat=1884482\n"," 78% 21/27 [00:00<00:00, 22.11it/s]Skipping y_hat=788759\n"," 89% 24/27 [00:01<00:00, 22.92it/s]Skipping y_hat=865711\n","100% 27/27 [00:01<00:00, 22.75it/s]\n","accuracy of 3000 examples: 2956/3000 (98.53333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.53%\n","\n","iter 635: train loss 0.9523, val loss 0.9454\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3341304\n","Skipping y_hat=1380320\n","Skipping y_hat=2101535\n","Skipping y_hat=6728158\n"," 11% 3/27 [00:00<00:01, 22.07it/s]Skipping y_hat=7639084\n","Skipping y_hat=6148430\n","Skipping y_hat=2556265\n","Skipping y_hat=5903245\n","Skipping y_hat=4393040\n","Skipping y_hat=5877640\n","Skipping y_hat=2996704\n","Skipping y_hat=3535561\n","Skipping y_hat=2599385\n"," 22% 6/27 [00:00<00:00, 21.74it/s]Skipping y_hat=2324143\n","Skipping y_hat=7792111\n","Skipping y_hat=5893795\n"," 33% 9/27 [00:00<00:00, 22.09it/s]Skipping y_hat=6919048\n","Skipping y_hat=5082211\n","Skipping y_hat=7117471\n","Skipping y_hat=1510066\n","Skipping y_hat=1234144\n"," 44% 12/27 [00:00<00:00, 21.98it/s]Skipping y_hat=7323688\n","Skipping y_hat=6757168\n","Skipping y_hat=5617386\n","Skipping y_hat=1990168\n","Skipping y_hat=6731110\n"," 56% 15/27 [00:00<00:00, 22.06it/s]Skipping y_hat=5020236\n","Skipping y_hat=3014153\n","Skipping y_hat=1930568\n"," 67% 18/27 [00:00<00:00, 22.14it/s]Skipping y_hat=1747486\n","Skipping y_hat=8136793\n"," 78% 21/27 [00:00<00:00, 22.25it/s]Skipping y_hat=557551\n","Skipping y_hat=102258\n","Skipping y_hat=277660\n"," 89% 24/27 [00:01<00:00, 22.90it/s]Skipping y_hat=154996\n","100% 27/27 [00:01<00:00, 22.63it/s]\n","accuracy of 3000 examples: 2965/3000 (98.83333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.63it/s]\n","accuracy of 10000 examples: 9893/10000 (98.92999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1481320\n"," 11% 3/27 [00:00<00:01, 21.99it/s]Skipping y_hat=7639084\n","Skipping y_hat=6204871\n","Skipping y_hat=6148430\n","Skipping y_hat=2556265\n","Skipping y_hat=7920001\n","Skipping y_hat=4333040\n","Skipping y_hat=2996704\n","Skipping y_hat=7942987\n","Skipping y_hat=2737898\n"," 22% 6/27 [00:00<00:00, 21.98it/s]Skipping y_hat=4943065\n","Skipping y_hat=4950772\n","Skipping y_hat=5893795\n"," 33% 9/27 [00:00<00:00, 22.07it/s]Skipping y_hat=5082211\n","Skipping y_hat=4227403\n"," 44% 12/27 [00:00<00:00, 22.04it/s]Skipping y_hat=6598447\n","Skipping y_hat=3574486\n","Skipping y_hat=7990525\n","Skipping y_hat=5157820\n","Skipping y_hat=1991168\n"," 56% 15/27 [00:00<00:00, 22.01it/s]Skipping y_hat=5346650\n","Skipping y_hat=3014153\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 22.27it/s]Skipping y_hat=4758449\n","Skipping y_hat=1501984\n","Skipping y_hat=1747486\n","Skipping y_hat=6713877\n","Skipping y_hat=8136793\n","Skipping y_hat=2857572\n","Skipping y_hat=3486884\n"," 78% 21/27 [00:00<00:00, 21.99it/s]Skipping y_hat=511152\n"," 89% 24/27 [00:01<00:00, 22.58it/s]Skipping y_hat=325072\n","Skipping y_hat=658010\n","100% 27/27 [00:01<00:00, 22.43it/s]\n","accuracy of 3000 examples: 2967/3000 (98.9%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.90%\n","\n","iter 640: train loss 0.9622, val loss 0.9481\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6703559\n","Skipping y_hat=2720006\n","Skipping y_hat=2942477\n","Skipping y_hat=976067\n","Skipping y_hat=5515379\n","Skipping y_hat=6650451\n","Skipping y_hat=4031198\n","Skipping y_hat=1663550\n","Skipping y_hat=6555908\n"," 11% 3/27 [00:00<00:01, 22.12it/s]Skipping y_hat=4968919\n","Skipping y_hat=3460306\n","Skipping y_hat=5215664\n","Skipping y_hat=4181804\n","Skipping y_hat=7843087\n","Skipping y_hat=2106476\n","Skipping y_hat=2737898\n","Skipping y_hat=1326076\n"," 22% 6/27 [00:00<00:00, 21.99it/s]Skipping y_hat=2324143\n","Skipping y_hat=6587527\n","Skipping y_hat=5660699\n","Skipping y_hat=5837785\n","Skipping y_hat=7613207\n","Skipping y_hat=6016152\n"," 33% 9/27 [00:00<00:00, 22.28it/s]Skipping y_hat=982568\n","Skipping y_hat=5386787\n","Skipping y_hat=3048470\n","Skipping y_hat=1334537\n","Skipping y_hat=1316870\n","Skipping y_hat=3697776\n","Skipping y_hat=1630596\n"," 44% 12/27 [00:00<00:00, 22.34it/s]Skipping y_hat=5706048\n","Skipping y_hat=2060166\n","Skipping y_hat=5202913\n","Skipping y_hat=8089525\n","Skipping y_hat=2196590\n"," 56% 15/27 [00:00<00:00, 22.41it/s]Skipping y_hat=3014153\n","Skipping y_hat=5746778\n","Skipping y_hat=3001224\n"," 67% 18/27 [00:00<00:00, 22.55it/s]Skipping y_hat=1708702\n","Skipping y_hat=4077032\n","Skipping y_hat=1790944\n","Skipping y_hat=6613777\n","Skipping y_hat=6079984\n","Skipping y_hat=4244533\n","Skipping y_hat=5531867\n"," 78% 21/27 [00:00<00:00, 22.37it/s]Skipping y_hat=10604\n","Skipping y_hat=262112\n","100% 27/27 [00:01<00:00, 22.90it/s]\n","accuracy of 3000 examples: 2953/3000 (98.43333333333332%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.04it/s]\n","accuracy of 10000 examples: 9875/10000 (98.75%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2012686\n","Skipping y_hat=6703559\n","Skipping y_hat=2720006\n","Skipping y_hat=2942477\n","Skipping y_hat=5304815\n","Skipping y_hat=3010656\n","Skipping y_hat=5515379\n","Skipping y_hat=4031208\n","Skipping y_hat=1663550\n","Skipping y_hat=6688240\n"," 11% 3/27 [00:00<00:01, 22.81it/s]Skipping y_hat=4968919\n","Skipping y_hat=6919901\n","Skipping y_hat=4181804\n","Skipping y_hat=3186760\n","Skipping y_hat=1426076\n"," 22% 6/27 [00:00<00:00, 22.79it/s]Skipping y_hat=4927267\n","Skipping y_hat=5625566\n","Skipping y_hat=7144838\n","Skipping y_hat=5608880\n","Skipping y_hat=8387821\n","Skipping y_hat=2933670\n","Skipping y_hat=5996726\n","Skipping y_hat=5936785\n","Skipping y_hat=7613207\n","Skipping y_hat=6016152\n"," 33% 9/27 [00:00<00:00, 22.64it/s]Skipping y_hat=5386787\n","Skipping y_hat=3048470\n","Skipping y_hat=1334537\n","Skipping y_hat=1316870\n","Skipping y_hat=1630596\n"," 44% 12/27 [00:00<00:00, 22.81it/s]Skipping y_hat=6598447\n","Skipping y_hat=3611696\n","Skipping y_hat=5706048\n","Skipping y_hat=3400316\n","Skipping y_hat=2060166\n","Skipping y_hat=2419192\n","Skipping y_hat=4731551\n","Skipping y_hat=2196590\n","Skipping y_hat=5584987\n"," 56% 15/27 [00:00<00:00, 22.58it/s]Skipping y_hat=2015864\n","Skipping y_hat=3014153\n","Skipping y_hat=8776691\n","Skipping y_hat=7136180\n","Skipping y_hat=3932906\n","Skipping y_hat=1501296\n","Skipping y_hat=7331291\n"," 67% 18/27 [00:00<00:00, 22.57it/s]Skipping y_hat=1790944\n","Skipping y_hat=1747396\n","Skipping y_hat=6613777\n","Skipping y_hat=4244533\n","Skipping y_hat=7602803\n","Skipping y_hat=8146250\n"," 78% 21/27 [00:00<00:00, 22.61it/s]Skipping y_hat=10604\n","Skipping y_hat=263099\n","Skipping y_hat=115245\n"," 89% 24/27 [00:01<00:00, 23.21it/s]Skipping y_hat=370230\n","100% 27/27 [00:01<00:00, 23.01it/s]\n","accuracy of 3000 examples: 2943/3000 (98.1%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.10%\n","\n","iter 645: train loss 0.9570, val loss 0.9530\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8417205\n","Skipping y_hat=6254470\n","Skipping y_hat=5090338\n","Skipping y_hat=3288079\n"," 11% 3/27 [00:00<00:01, 22.60it/s]Skipping y_hat=7628184\n","Skipping y_hat=6919901\n","Skipping y_hat=3535561\n","Skipping y_hat=1635427\n"," 22% 6/27 [00:00<00:00, 22.27it/s]Skipping y_hat=3982987\n"," 33% 9/27 [00:00<00:00, 22.46it/s]Skipping y_hat=3757454\n","Skipping y_hat=5082211\n","Skipping y_hat=4227403\n","Skipping y_hat=4807063\n"," 44% 12/27 [00:00<00:00, 22.55it/s]Skipping y_hat=2191484\n","Skipping y_hat=2331820\n"," 56% 15/27 [00:00<00:00, 22.25it/s]Skipping y_hat=5849560\n","Skipping y_hat=3112304\n","Skipping y_hat=2502683\n","Skipping y_hat=2983065\n","Skipping y_hat=3995317\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 21.71it/s]Skipping y_hat=4077032\n","Skipping y_hat=5622759\n"," 89% 24/27 [00:01<00:00, 22.76it/s]Skipping y_hat=162503\n","100% 27/27 [00:01<00:00, 22.67it/s]\n","accuracy of 3000 examples: 2976/3000 (99.2%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.80it/s]\n","accuracy of 10000 examples: 9932/10000 (99.32%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3441716\n"," 11% 3/27 [00:00<00:01, 22.55it/s]Skipping y_hat=2899567\n","Skipping y_hat=6919901\n"," 22% 6/27 [00:00<00:00, 22.34it/s]Skipping y_hat=1506269\n","Skipping y_hat=4943065\n","Skipping y_hat=3982987\n","Skipping y_hat=5750699\n","Skipping y_hat=3912751\n","Skipping y_hat=8789221\n"," 33% 9/27 [00:00<00:00, 22.41it/s]Skipping y_hat=4227403\n","Skipping y_hat=6431023\n"," 44% 12/27 [00:00<00:00, 22.23it/s]Skipping y_hat=5706048\n","Skipping y_hat=2191484\n"," 56% 15/27 [00:00<00:00, 22.10it/s]Skipping y_hat=7006157\n","Skipping y_hat=2035124\n","Skipping y_hat=2983065\n","Skipping y_hat=3995317\n"," 67% 18/27 [00:00<00:00, 21.69it/s]Skipping y_hat=1670201\n","Skipping y_hat=1501984\n","Skipping y_hat=4077032\n","Skipping y_hat=6681070\n"," 78% 21/27 [00:00<00:00, 21.43it/s]Skipping y_hat=8858710\n","Skipping y_hat=2657188\n"," 89% 24/27 [00:01<00:00, 21.84it/s]Skipping y_hat=118205\n","Skipping y_hat=162503\n","100% 27/27 [00:01<00:00, 22.08it/s]\n","accuracy of 3000 examples: 2975/3000 (99.16666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.17%\n","\n","iter 650: train loss 0.9610, val loss 0.9421\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2720006\n","Skipping y_hat=3972070\n","Skipping y_hat=8218205\n","Skipping y_hat=6806852\n","Skipping y_hat=5500840\n","Skipping y_hat=5515379\n","Skipping y_hat=6560451\n","Skipping y_hat=2211535\n","Skipping y_hat=3288079\n"," 11% 3/27 [00:00<00:01, 22.31it/s]Skipping y_hat=4868929\n","Skipping y_hat=8230769\n","Skipping y_hat=2456255\n","Skipping y_hat=2809567\n","Skipping y_hat=6919901\n","Skipping y_hat=3603700\n","Skipping y_hat=1356764\n","Skipping y_hat=6341255\n","Skipping y_hat=5183720\n","Skipping y_hat=7140070\n"," 22% 6/27 [00:00<00:00, 21.93it/s]Skipping y_hat=5509882\n","Skipping y_hat=6587527\n","Skipping y_hat=5750699\n","Skipping y_hat=3526406\n","Skipping y_hat=713255\n"," 33% 9/27 [00:00<00:00, 22.25it/s]Skipping y_hat=4227403\n","Skipping y_hat=5685758\n","Skipping y_hat=6133573\n","Skipping y_hat=4807063\n"," 44% 12/27 [00:00<00:00, 22.30it/s]Skipping y_hat=6598447\n","Skipping y_hat=5707548\n","Skipping y_hat=6800705\n"," 56% 15/27 [00:00<00:00, 22.52it/s]Skipping y_hat=2984055\n","Skipping y_hat=2257972\n","Skipping y_hat=7351291\n","Skipping y_hat=5091924\n"," 67% 18/27 [00:00<00:00, 22.43it/s]Skipping y_hat=1671101\n","Skipping y_hat=6813777\n","Skipping y_hat=8148745\n","Skipping y_hat=6681070\n"," 78% 21/27 [00:00<00:00, 22.17it/s]Skipping y_hat=5828\n","Skipping y_hat=681632\n","Skipping y_hat=251099\n","Skipping y_hat=276560\n","Skipping y_hat=56110\n"," 89% 24/27 [00:01<00:00, 21.12it/s]Skipping y_hat=33076\n","Skipping y_hat=395531\n","Skipping y_hat=658010\n","Skipping y_hat=162503\n","Skipping y_hat=602387\n","Skipping y_hat=555560\n","100% 27/27 [00:01<00:00, 21.60it/s]\n","accuracy of 3000 examples: 2950/3000 (98.33333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.18it/s]\n","accuracy of 10000 examples: 9901/10000 (99.00999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2720006\n","Skipping y_hat=3972070\n","Skipping y_hat=8218205\n","Skipping y_hat=976067\n","Skipping y_hat=6560451\n","Skipping y_hat=6627158\n","Skipping y_hat=2477885\n","Skipping y_hat=3288079\n"," 11% 3/27 [00:00<00:01, 21.54it/s]Skipping y_hat=8230769\n","Skipping y_hat=2456255\n","Skipping y_hat=3873014\n","Skipping y_hat=2809567\n","Skipping y_hat=6919001\n","Skipping y_hat=6341255\n","Skipping y_hat=7140970\n"," 22% 6/27 [00:00<00:00, 21.53it/s]Skipping y_hat=4944055\n","Skipping y_hat=6587527\n","Skipping y_hat=5750699\n","Skipping y_hat=6551757\n","Skipping y_hat=2411100\n","Skipping y_hat=713255\n"," 33% 9/27 [00:00<00:00, 21.61it/s]Skipping y_hat=4227403\n","Skipping y_hat=797479\n","Skipping y_hat=4470877\n","Skipping y_hat=4807063\n"," 44% 12/27 [00:00<00:00, 21.99it/s]Skipping y_hat=6598447\n","Skipping y_hat=3574486\n","Skipping y_hat=6605225\n","Skipping y_hat=2139011\n"," 56% 15/27 [00:00<00:00, 22.36it/s]Skipping y_hat=3833006\n","Skipping y_hat=2984055\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 22.45it/s]Skipping y_hat=1671101\n","Skipping y_hat=4254707\n","Skipping y_hat=3340777\n","Skipping y_hat=5551867\n","Skipping y_hat=6681070\n"," 78% 21/27 [00:00<00:00, 22.57it/s]Skipping y_hat=55090\n","Skipping y_hat=56110\n","Skipping y_hat=242110\n"," 89% 24/27 [00:01<00:00, 23.28it/s]Skipping y_hat=33076\n","Skipping y_hat=361435\n","Skipping y_hat=658010\n","Skipping y_hat=162503\n","Skipping y_hat=602387\n","Skipping y_hat=208747\n","Skipping y_hat=555560\n","100% 27/27 [00:01<00:00, 22.81it/s]\n","accuracy of 3000 examples: 2953/3000 (98.43333333333332%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.43%\n","\n","iter 655: train loss 0.9594, val loss 0.9419\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6833725\n","Skipping y_hat=5515379\n"," 11% 3/27 [00:00<00:01, 22.20it/s]Skipping y_hat=4868929\n","Skipping y_hat=6554556\n","Skipping y_hat=2911223\n","Skipping y_hat=3414796\n"," 22% 6/27 [00:00<00:00, 22.15it/s]Skipping y_hat=4978070\n","Skipping y_hat=2834929\n","Skipping y_hat=5766857\n","Skipping y_hat=5959963\n"," 33% 9/27 [00:00<00:00, 22.17it/s]Skipping y_hat=6908948\n","Skipping y_hat=5071211\n","Skipping y_hat=4227403\n","Skipping y_hat=3670596\n","Skipping y_hat=3180996\n"," 44% 12/27 [00:00<00:00, 21.74it/s]Skipping y_hat=8684029\n","Skipping y_hat=5706048\n","Skipping y_hat=5684329\n","Skipping y_hat=2139011\n","Skipping y_hat=2310290\n","Skipping y_hat=6721120\n"," 56% 15/27 [00:00<00:00, 21.73it/s]Skipping y_hat=6253129\n","Skipping y_hat=2005874\n","Skipping y_hat=3557294\n","Skipping y_hat=1825370\n","Skipping y_hat=4885399\n"," 67% 18/27 [00:00<00:00, 21.82it/s]Skipping y_hat=5796413\n","Skipping y_hat=7407418\n","Skipping y_hat=5602859\n","Skipping y_hat=7939356\n","Skipping y_hat=4244533\n","Skipping y_hat=1774482\n","Skipping y_hat=2852996\n","Skipping y_hat=3770556\n","Skipping y_hat=3444263\n"," 78% 21/27 [00:00<00:00, 21.43it/s]Skipping y_hat=8758720\n","Skipping y_hat=2457088\n","Skipping y_hat=54032\n","Skipping y_hat=251099\n","Skipping y_hat=532331\n"," 89% 24/27 [00:01<00:00, 22.18it/s]Skipping y_hat=678010\n","Skipping y_hat=469241\n","100% 27/27 [00:01<00:00, 22.19it/s]\n","accuracy of 3000 examples: 2958/3000 (98.6%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.54it/s]\n","accuracy of 10000 examples: 9873/10000 (98.72999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4691960\n","Skipping y_hat=3110556\n","Skipping y_hat=5515379\n"," 11% 3/27 [00:00<00:01, 22.52it/s]Skipping y_hat=6554556\n","Skipping y_hat=5551070\n","Skipping y_hat=2557567\n","Skipping y_hat=5061851\n"," 22% 6/27 [00:00<00:00, 22.04it/s]Skipping y_hat=7570304\n","Skipping y_hat=4978070\n","Skipping y_hat=4934065\n","Skipping y_hat=7325029\n","Skipping y_hat=2834929\n","Skipping y_hat=4700627\n","Skipping y_hat=5914504\n","Skipping y_hat=6554831\n","Skipping y_hat=5959963\n"," 33% 9/27 [00:00<00:00, 21.81it/s]Skipping y_hat=5071211\n","Skipping y_hat=4227403\n","Skipping y_hat=3670596\n","Skipping y_hat=3180996\n","Skipping y_hat=4255046\n","Skipping y_hat=6238683\n"," 44% 12/27 [00:00<00:00, 21.75it/s]Skipping y_hat=8684029\n","Skipping y_hat=5684329\n","Skipping y_hat=2139011\n"," 56% 15/27 [00:00<00:00, 21.70it/s]Skipping y_hat=6253129\n","Skipping y_hat=956972\n","Skipping y_hat=7554529\n"," 67% 18/27 [00:00<00:00, 21.76it/s]Skipping y_hat=2610149\n","Skipping y_hat=5781349\n","Skipping y_hat=8147735\n","Skipping y_hat=7939356\n","Skipping y_hat=1774482\n","Skipping y_hat=2852996\n","Skipping y_hat=5541967\n","Skipping y_hat=3770556\n"," 78% 21/27 [00:00<00:00, 21.48it/s]Skipping y_hat=8758720\n","Skipping y_hat=251099\n","Skipping y_hat=546816\n"," 89% 24/27 [00:01<00:00, 22.29it/s]Skipping y_hat=119105\n","Skipping y_hat=159298\n","100% 27/27 [00:01<00:00, 22.25it/s]\n","accuracy of 3000 examples: 2957/3000 (98.56666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.57%\n","\n","iter 660: train loss 0.9479, val loss 0.9442\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6015286\n","Skipping y_hat=8328205\n","Skipping y_hat=4162618\n","Skipping y_hat=5335631\n"," 11% 3/27 [00:00<00:01, 21.60it/s]Skipping y_hat=4171804\n","Skipping y_hat=2630294\n","Skipping y_hat=6900731\n","Skipping y_hat=7371674\n"," 22% 6/27 [00:00<00:00, 21.77it/s]Skipping y_hat=7580204\n","Skipping y_hat=5568950\n","Skipping y_hat=7225019\n","Skipping y_hat=4584844\n","Skipping y_hat=523386\n"," 33% 9/27 [00:00<00:00, 21.80it/s]Skipping y_hat=5387777\n"," 44% 12/27 [00:00<00:00, 22.18it/s]Skipping y_hat=5706048\n","Skipping y_hat=6805225\n","Skipping y_hat=5584987\n","Skipping y_hat=5035004\n"," 56% 15/27 [00:00<00:00, 22.25it/s]Skipping y_hat=3508003\n","Skipping y_hat=2403088\n","Skipping y_hat=956972\n","Skipping y_hat=2511683\n","Skipping y_hat=2400008\n"," 67% 18/27 [00:00<00:00, 22.11it/s]Skipping y_hat=5782933\n","Skipping y_hat=7702703\n","Skipping y_hat=3755196\n"," 78% 21/27 [00:00<00:00, 22.29it/s]Skipping y_hat=29056\n","Skipping y_hat=95245\n","Skipping y_hat=292120\n"," 89% 24/27 [00:01<00:00, 22.94it/s]Skipping y_hat=495531\n","Skipping y_hat=233312\n","100% 27/27 [00:01<00:00, 22.64it/s]\n","accuracy of 3000 examples: 2969/3000 (98.96666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.87it/s]\n","accuracy of 10000 examples: 9917/10000 (99.17%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8328205\n","Skipping y_hat=3942477\n","Skipping y_hat=1837693\n","Skipping y_hat=5097558\n"," 11% 3/27 [00:00<00:01, 22.57it/s]Skipping y_hat=8330869\n","Skipping y_hat=2630294\n","Skipping y_hat=6900731\n","Skipping y_hat=7371674\n","Skipping y_hat=6527404\n"," 22% 6/27 [00:00<00:00, 22.51it/s]Skipping y_hat=4950772\n","Skipping y_hat=7425962\n","Skipping y_hat=4379878\n"," 33% 9/27 [00:00<00:00, 22.32it/s]Skipping y_hat=6928948\n","Skipping y_hat=2567953\n","Skipping y_hat=5860067\n","Skipping y_hat=6131674\n"," 44% 12/27 [00:00<00:00, 22.26it/s]Skipping y_hat=7700705\n","Skipping y_hat=5706048\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5035004\n"," 56% 15/27 [00:00<00:00, 22.33it/s]Skipping y_hat=3272084\n","Skipping y_hat=7404749\n","Skipping y_hat=2403088\n","Skipping y_hat=3008056\n","Skipping y_hat=956972\n","Skipping y_hat=6528430\n","Skipping y_hat=2511683\n","Skipping y_hat=2400008\n","Skipping y_hat=5909248\n","Skipping y_hat=3311461\n","Skipping y_hat=7810874\n"," 67% 18/27 [00:00<00:00, 22.19it/s]Skipping y_hat=4605138\n","Skipping y_hat=5622859\n","Skipping y_hat=3755196\n","Skipping y_hat=8146250\n"," 78% 21/27 [00:00<00:00, 22.09it/s]Skipping y_hat=29056\n"," 89% 24/27 [00:01<00:00, 22.83it/s]Skipping y_hat=495631\n","Skipping y_hat=460241\n","100% 27/27 [00:01<00:00, 22.74it/s]\n","accuracy of 3000 examples: 2962/3000 (98.73333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.73%\n","\n","iter 665: train loss 0.9527, val loss 0.9463\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7740424\n","Skipping y_hat=8757323\n","Skipping y_hat=8318105\n","Skipping y_hat=3220934\n","Skipping y_hat=1837693\n","Skipping y_hat=4430532\n","Skipping y_hat=5515379\n","Skipping y_hat=1310595\n","Skipping y_hat=6627158\n"," 11% 3/27 [00:00<00:01, 21.15it/s]Skipping y_hat=6867124\n","Skipping y_hat=5844599\n","Skipping y_hat=1153058\n","Skipping y_hat=3893212\n"," 22% 6/27 [00:00<00:00, 21.69it/s]Skipping y_hat=5625566\n","Skipping y_hat=4700627\n","Skipping y_hat=5608880\n","Skipping y_hat=4573844\n","Skipping y_hat=5948963\n"," 33% 9/27 [00:00<00:00, 21.83it/s]Skipping y_hat=5071211\n","Skipping y_hat=2053205\n","Skipping y_hat=4514440\n","Skipping y_hat=4741475\n","Skipping y_hat=4468877\n","Skipping y_hat=5632304\n"," 44% 12/27 [00:00<00:00, 21.79it/s]Skipping y_hat=6562214\n","Skipping y_hat=2012810\n","Skipping y_hat=5629884\n","Skipping y_hat=1971168\n"," 56% 15/27 [00:00<00:00, 21.87it/s]Skipping y_hat=1891601\n","Skipping y_hat=4032323\n","Skipping y_hat=6018963\n","Skipping y_hat=5709248\n"," 67% 18/27 [00:00<00:00, 22.05it/s]Skipping y_hat=1110794\n","Skipping y_hat=5896313\n","Skipping y_hat=3997650\n","Skipping y_hat=5780249\n","Skipping y_hat=1884482\n","Skipping y_hat=4713272\n"," 78% 21/27 [00:00<00:00, 21.98it/s]Skipping y_hat=6278572\n","Skipping y_hat=11704\n","Skipping y_hat=252479\n"," 89% 24/27 [00:01<00:00, 22.73it/s]Skipping y_hat=301348\n","100% 27/27 [00:01<00:00, 22.48it/s]\n","accuracy of 3000 examples: 2958/3000 (98.6%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.92it/s]\n","accuracy of 10000 examples: 9896/10000 (98.96000000000001%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8757323\n","Skipping y_hat=3448772\n","Skipping y_hat=3220934\n","Skipping y_hat=5515379\n","Skipping y_hat=2065502\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.25it/s]Skipping y_hat=2990702\n","Skipping y_hat=2010128\n","Skipping y_hat=4181804\n","Skipping y_hat=1706476\n","Skipping y_hat=6900731\n"," 22% 6/27 [00:00<00:00, 22.41it/s]Skipping y_hat=4727267\n","Skipping y_hat=6787527\n","Skipping y_hat=5948963\n"," 33% 9/27 [00:00<00:00, 21.91it/s]Skipping y_hat=2053205\n","Skipping y_hat=4468877\n","Skipping y_hat=4148567\n"," 44% 12/27 [00:00<00:00, 21.78it/s]Skipping y_hat=6562214\n","Skipping y_hat=5569298\n","Skipping y_hat=5892847\n","Skipping y_hat=2629358\n","Skipping y_hat=2231920\n","Skipping y_hat=5083856\n"," 56% 15/27 [00:00<00:00, 21.54it/s]Skipping y_hat=3598003\n","Skipping y_hat=1891601\n","Skipping y_hat=4032323\n"," 67% 18/27 [00:00<00:00, 21.28it/s]Skipping y_hat=4713272\n","Skipping y_hat=1030136\n"," 78% 21/27 [00:00<00:00, 21.53it/s]Skipping y_hat=262489\n"," 89% 24/27 [00:01<00:00, 22.54it/s]Skipping y_hat=755711\n","Skipping y_hat=301348\n","100% 27/27 [00:01<00:00, 22.36it/s]\n","accuracy of 3000 examples: 2969/3000 (98.96666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.97%\n","\n","iter 670: train loss 0.9580, val loss 0.9436\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6960120\n","Skipping y_hat=1720519\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.81it/s]Skipping y_hat=8230769\n","Skipping y_hat=3972014\n"," 22% 6/27 [00:00<00:00, 22.92it/s]Skipping y_hat=2497325\n","Skipping y_hat=7891111\n","Skipping y_hat=5948963\n","Skipping y_hat=6903256\n"," 33% 9/27 [00:00<00:00, 22.64it/s]Skipping y_hat=2053205\n","Skipping y_hat=4227403\n","Skipping y_hat=3748254\n","Skipping y_hat=4620158\n"," 44% 12/27 [00:00<00:00, 22.72it/s]Skipping y_hat=7299377\n","Skipping y_hat=6462214\n","Skipping y_hat=2191484\n","Skipping y_hat=5137820\n"," 56% 15/27 [00:00<00:00, 22.70it/s]Skipping y_hat=7520152\n","Skipping y_hat=8469791\n","Skipping y_hat=8737082\n","Skipping y_hat=7351291\n","Skipping y_hat=4722968\n"," 67% 18/27 [00:00<00:00, 22.68it/s]Skipping y_hat=2622770\n","Skipping y_hat=2761562\n","Skipping y_hat=5330256\n","Skipping y_hat=3239777\n","Skipping y_hat=3760656\n","Skipping y_hat=6847415\n"," 78% 21/27 [00:00<00:00, 22.42it/s]Skipping y_hat=11704\n","Skipping y_hat=565608\n","Skipping y_hat=438308\n"," 89% 24/27 [00:01<00:00, 22.84it/s]Skipping y_hat=159877\n","Skipping y_hat=227690\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.90it/s]\n","accuracy of 3000 examples: 2966/3000 (98.86666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.75it/s]\n","accuracy of 10000 examples: 9883/10000 (98.83%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6703559\n","Skipping y_hat=8218205\n","Skipping y_hat=6961120\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.37it/s]Skipping y_hat=3840506\n","Skipping y_hat=7829084\n","Skipping y_hat=3972014\n","Skipping y_hat=7231951\n","Skipping y_hat=8295245\n","Skipping y_hat=6417404\n"," 22% 6/27 [00:00<00:00, 21.39it/s]Skipping y_hat=2497325\n","Skipping y_hat=1248958\n"," 33% 9/27 [00:00<00:00, 21.63it/s]Skipping y_hat=4227403\n","Skipping y_hat=5695858\n","Skipping y_hat=6421123\n"," 44% 12/27 [00:00<00:00, 21.69it/s]Skipping y_hat=6462314\n","Skipping y_hat=2191484\n","Skipping y_hat=7684894\n","Skipping y_hat=7328672\n","Skipping y_hat=2966360\n"," 56% 15/27 [00:00<00:00, 21.56it/s]Skipping y_hat=7520152\n","Skipping y_hat=7321969\n","Skipping y_hat=3099400\n","Skipping y_hat=8469791\n","Skipping y_hat=8737082\n","Skipping y_hat=4722968\n"," 67% 18/27 [00:00<00:00, 21.52it/s]Skipping y_hat=5330256\n","Skipping y_hat=3239777\n","Skipping y_hat=3760656\n","Skipping y_hat=6847415\n"," 78% 21/27 [00:00<00:00, 21.64it/s]Skipping y_hat=565608\n","Skipping y_hat=438308\n"," 89% 24/27 [00:01<00:00, 22.39it/s]Skipping y_hat=277315\n","100% 27/27 [00:01<00:00, 22.15it/s]\n","accuracy of 3000 examples: 2967/3000 (98.9%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.90%\n","\n","iter 675: train loss 0.9478, val loss 0.9521\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2219770\n","Skipping y_hat=5077796\n","Skipping y_hat=3081032\n","Skipping y_hat=6074836\n","Skipping y_hat=6555708\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 22.58it/s]Skipping y_hat=3972014\n","Skipping y_hat=6373743\n"," 22% 6/27 [00:00<00:00, 22.51it/s]Skipping y_hat=4727267\n","Skipping y_hat=5509882\n","Skipping y_hat=2407325\n","Skipping y_hat=5119604\n","Skipping y_hat=6594243\n"," 33% 9/27 [00:00<00:00, 22.58it/s]Skipping y_hat=4485808\n","Skipping y_hat=2153305\n","Skipping y_hat=6178140\n","Skipping y_hat=3180996\n"," 44% 12/27 [00:00<00:00, 22.60it/s]Skipping y_hat=3202785\n","Skipping y_hat=5620884\n","Skipping y_hat=7427672\n","Skipping y_hat=2417192\n","Skipping y_hat=3089734\n","Skipping y_hat=4916252\n"," 56% 15/27 [00:00<00:00, 22.57it/s]Skipping y_hat=5270898\n","Skipping y_hat=3732906\n","Skipping y_hat=5092729\n","Skipping y_hat=2260972\n","Skipping y_hat=5808248\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 22.42it/s]Skipping y_hat=3150208\n","Skipping y_hat=6083984\n","Skipping y_hat=1774482\n","Skipping y_hat=2852996\n"," 78% 21/27 [00:00<00:00, 22.40it/s]Skipping y_hat=55990\n","Skipping y_hat=515180\n","Skipping y_hat=636823\n","Skipping y_hat=475291\n"," 89% 24/27 [00:01<00:00, 22.83it/s]Skipping y_hat=667010\n","Skipping y_hat=208747\n","100% 27/27 [00:01<00:00, 22.89it/s]\n","accuracy of 3000 examples: 2961/3000 (98.7%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.53it/s]\n","accuracy of 10000 examples: 9923/10000 (99.22999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8154244\n","Skipping y_hat=537181\n","Skipping y_hat=6555708\n","Skipping y_hat=3197101\n"," 11% 3/27 [00:00<00:01, 21.03it/s]Skipping y_hat=4242023\n","Skipping y_hat=6373743\n"," 22% 6/27 [00:00<00:00, 21.29it/s]Skipping y_hat=5509882\n","Skipping y_hat=1995020\n","Skipping y_hat=2316178\n","Skipping y_hat=4370778\n","Skipping y_hat=6594243\n"," 33% 9/27 [00:00<00:00, 21.01it/s]Skipping y_hat=1370847\n","Skipping y_hat=6168140\n","Skipping y_hat=3180996\n"," 44% 12/27 [00:00<00:00, 21.05it/s]Skipping y_hat=6698547\n","Skipping y_hat=2417192\n","Skipping y_hat=3089734\n"," 56% 15/27 [00:00<00:00, 21.16it/s]Skipping y_hat=5838560\n","Skipping y_hat=3497003\n","Skipping y_hat=5270898\n","Skipping y_hat=3732906\n","Skipping y_hat=1511396\n","Skipping y_hat=2037124\n","Skipping y_hat=5092729\n","Skipping y_hat=5808248\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 21.22it/s]Skipping y_hat=5568144\n","Skipping y_hat=3150208\n","Skipping y_hat=1774482\n","Skipping y_hat=2852996\n","Skipping y_hat=3239777\n"," 78% 21/27 [00:00<00:00, 21.48it/s]Skipping y_hat=55990\n","Skipping y_hat=515180\n","Skipping y_hat=270270\n"," 89% 24/27 [00:01<00:00, 22.50it/s]Skipping y_hat=667010\n","Skipping y_hat=208747\n","100% 27/27 [00:01<00:00, 22.03it/s]\n","accuracy of 3000 examples: 2964/3000 (98.8%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.80%\n","\n","iter 680: train loss 0.9548, val loss 0.9343\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7078285\n","Skipping y_hat=3104496\n","Skipping y_hat=2253440\n","Skipping y_hat=6628258\n","Skipping y_hat=899009\n","Skipping y_hat=1673650\n","Skipping y_hat=5070338\n","Skipping y_hat=5382073\n"," 11% 3/27 [00:00<00:01, 22.43it/s]Skipping y_hat=5051951\n","Skipping y_hat=5181904\n","Skipping y_hat=7843987\n"," 22% 6/27 [00:00<00:00, 22.56it/s]Skipping y_hat=8568523\n","Skipping y_hat=8699221\n"," 33% 9/27 [00:00<00:00, 22.68it/s]Skipping y_hat=4605088\n","Skipping y_hat=5370364\n","Skipping y_hat=3818436\n","Skipping y_hat=5287787\n","Skipping y_hat=5071211\n","Skipping y_hat=4630258\n","Skipping y_hat=5055974\n"," 44% 12/27 [00:00<00:00, 22.62it/s]Skipping y_hat=5706048\n","Skipping y_hat=7303688\n","Skipping y_hat=6705235\n","Skipping y_hat=7955713\n","Skipping y_hat=1003526\n","Skipping y_hat=7989535\n"," 56% 15/27 [00:00<00:00, 22.51it/s]Skipping y_hat=3002208\n","Skipping y_hat=5739560\n","Skipping y_hat=875801\n","Skipping y_hat=8669791\n","Skipping y_hat=2409008\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 22.55it/s]Skipping y_hat=2038015\n","Skipping y_hat=7702703\n","Skipping y_hat=2842896\n"," 78% 21/27 [00:00<00:00, 22.58it/s]Skipping y_hat=55990\n","Skipping y_hat=495280\n","Skipping y_hat=789859\n"," 89% 24/27 [00:01<00:00, 23.26it/s]Skipping y_hat=297894\n","100% 27/27 [00:01<00:00, 23.07it/s]\n","accuracy of 3000 examples: 2961/3000 (98.7%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.90it/s]\n","accuracy of 10000 examples: 9899/10000 (98.99%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4430532\n","Skipping y_hat=6627258\n"," 11% 3/27 [00:00<00:01, 22.06it/s]Skipping y_hat=2371706\n","Skipping y_hat=699230\n","Skipping y_hat=3482088\n","Skipping y_hat=4940821\n"," 22% 6/27 [00:00<00:00, 22.06it/s]Skipping y_hat=7581304\n","Skipping y_hat=5236703\n","Skipping y_hat=8568523\n","Skipping y_hat=4993795\n"," 33% 9/27 [00:00<00:00, 22.34it/s]Skipping y_hat=4605088\n","Skipping y_hat=3818436\n","Skipping y_hat=5287787\n","Skipping y_hat=5172211\n","Skipping y_hat=3568357\n","Skipping y_hat=1234034\n","Skipping y_hat=5158567\n"," 44% 12/27 [00:00<00:00, 21.98it/s]Skipping y_hat=7303688\n","Skipping y_hat=7955713\n","Skipping y_hat=1003526\n","Skipping y_hat=6800264\n","Skipping y_hat=2350342\n"," 56% 15/27 [00:00<00:00, 22.15it/s]Skipping y_hat=3002208\n","Skipping y_hat=3597003\n","Skipping y_hat=875801\n","Skipping y_hat=7885592\n","Skipping y_hat=5032423\n","Skipping y_hat=2400008\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 22.14it/s]Skipping y_hat=2038015\n","Skipping y_hat=6485104\n","Skipping y_hat=7357418\n","Skipping y_hat=1401884\n","Skipping y_hat=7702703\n","Skipping y_hat=2842896\n","Skipping y_hat=8246350\n"," 78% 21/27 [00:00<00:00, 22.19it/s]Skipping y_hat=10604\n","Skipping y_hat=495280\n","Skipping y_hat=276760\n","Skipping y_hat=779759\n","Skipping y_hat=532531\n"," 89% 24/27 [00:01<00:00, 22.73it/s]Skipping y_hat=297894\n","100% 27/27 [00:01<00:00, 22.61it/s]\n","accuracy of 3000 examples: 2958/3000 (98.6%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.60%\n","\n","iter 685: train loss 0.9565, val loss 0.9356\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6703559\n","Skipping y_hat=3111535\n","Skipping y_hat=2964241\n","Skipping y_hat=3552994\n","Skipping y_hat=5070338\n","Skipping y_hat=4482073\n"," 11% 3/27 [00:00<00:01, 22.20it/s]Skipping y_hat=5893235\n","Skipping y_hat=5161951\n","Skipping y_hat=4171904\n","Skipping y_hat=3893212\n","Skipping y_hat=1427176\n"," 22% 6/27 [00:00<00:00, 22.00it/s]Skipping y_hat=7581304\n","Skipping y_hat=3172306\n","Skipping y_hat=3323143\n","Skipping y_hat=7425962\n"," 33% 9/27 [00:00<00:00, 21.98it/s]Skipping y_hat=4715088\n","Skipping y_hat=3818436\n","Skipping y_hat=5071211\n","Skipping y_hat=2153305\n","Skipping y_hat=1334537\n","Skipping y_hat=1939774\n","Skipping y_hat=7413504\n","Skipping y_hat=5165974\n"," 44% 12/27 [00:00<00:00, 21.97it/s]Skipping y_hat=8674019\n","Skipping y_hat=3536272\n","Skipping y_hat=7330528\n","Skipping y_hat=5706048\n","Skipping y_hat=5683319\n","Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 21.89it/s]Skipping y_hat=3115874\n","Skipping y_hat=3497903\n","Skipping y_hat=7431463\n","Skipping y_hat=3112304\n","Skipping y_hat=3512683\n","Skipping y_hat=7331291\n"," 67% 18/27 [00:00<00:00, 22.12it/s]Skipping y_hat=5896313\n","Skipping y_hat=3146950\n","Skipping y_hat=1401884\n","Skipping y_hat=2083346\n","Skipping y_hat=3389184\n","Skipping y_hat=8136793\n"," 78% 21/27 [00:00<00:00, 22.06it/s]Skipping y_hat=10604\n","Skipping y_hat=465891\n"," 89% 24/27 [00:01<00:00, 22.77it/s]Skipping y_hat=163603\n","100% 27/27 [00:01<00:00, 22.59it/s]\n","accuracy of 3000 examples: 2956/3000 (98.53333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.63it/s]\n","accuracy of 10000 examples: 9886/10000 (98.86%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6703559\n","Skipping y_hat=2861008\n","Skipping y_hat=3111535\n","Skipping y_hat=6627158\n","Skipping y_hat=3552994\n","Skipping y_hat=5070338\n"," 11% 3/27 [00:00<00:01, 22.36it/s]Skipping y_hat=5893235\n","Skipping y_hat=3882212\n"," 22% 6/27 [00:00<00:00, 21.84it/s]Skipping y_hat=7581304\n","Skipping y_hat=5748976\n","Skipping y_hat=3323143\n","Skipping y_hat=3003650\n","Skipping y_hat=7153240\n","Skipping y_hat=3136663\n"," 33% 9/27 [00:00<00:00, 21.12it/s]Skipping y_hat=4715088\n","Skipping y_hat=5071211\n","Skipping y_hat=4997944\n","Skipping y_hat=3388792\n","Skipping y_hat=7413504\n","Skipping y_hat=5165974\n"," 44% 12/27 [00:00<00:00, 20.82it/s]Skipping y_hat=8674019\n","Skipping y_hat=6662314\n","Skipping y_hat=3536272\n","Skipping y_hat=5706048\n","Skipping y_hat=1050116\n","Skipping y_hat=7957672\n","Skipping y_hat=829396\n","Skipping y_hat=5247820\n","Skipping y_hat=3139111\n"," 56% 15/27 [00:00<00:00, 20.70it/s]Skipping y_hat=2115874\n","Skipping y_hat=5739560\n","Skipping y_hat=875801\n","Skipping y_hat=7431463\n","Skipping y_hat=5281898\n","Skipping y_hat=3112304\n","Skipping y_hat=3144086\n","Skipping y_hat=3391461\n","Skipping y_hat=7331291\n"," 67% 18/27 [00:00<00:00, 20.62it/s]Skipping y_hat=3146950\n","Skipping y_hat=4869488\n"," 78% 21/27 [00:00<00:00, 21.28it/s]Skipping y_hat=10604\n","100% 27/27 [00:01<00:00, 21.79it/s]\n","accuracy of 3000 examples: 2956/3000 (98.53333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.53%\n","\n","iter 690: train loss 0.9571, val loss 0.9378\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3989245\n","Skipping y_hat=1382320\n","Skipping y_hat=1644828\n"," 11% 3/27 [00:00<00:01, 22.66it/s]Skipping y_hat=4367687\n","Skipping y_hat=2145042\n","Skipping y_hat=4366484\n","Skipping y_hat=8395255\n","Skipping y_hat=2630294\n","Skipping y_hat=7468471\n","Skipping y_hat=6299470\n"," 22% 6/27 [00:00<00:00, 22.02it/s]Skipping y_hat=4950772\n","Skipping y_hat=2324143\n"," 33% 9/27 [00:00<00:00, 22.40it/s]Skipping y_hat=3818436\n","Skipping y_hat=6179140\n","Skipping y_hat=6421123\n","Skipping y_hat=1649596\n"," 44% 12/27 [00:00<00:00, 22.61it/s]Skipping y_hat=7798177\n","Skipping y_hat=5685319\n","Skipping y_hat=2779745\n"," 56% 15/27 [00:00<00:00, 22.67it/s]Skipping y_hat=2115874\n","Skipping y_hat=5281898\n","Skipping y_hat=511296\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 22.51it/s]Skipping y_hat=5579144\n"," 78% 21/27 [00:00<00:00, 22.40it/s]Skipping y_hat=568844\n","Skipping y_hat=682732\n","Skipping y_hat=532531\n"," 89% 24/27 [00:01<00:00, 22.76it/s]Skipping y_hat=163603\n","Skipping y_hat=82106\n","100% 27/27 [00:01<00:00, 22.73it/s]\n","accuracy of 3000 examples: 2971/3000 (99.03333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.59it/s]\n","accuracy of 10000 examples: 9904/10000 (99.03999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7088185\n","Skipping y_hat=7721016\n","Skipping y_hat=8072650\n","Skipping y_hat=3187979\n"," 11% 3/27 [00:00<00:01, 21.58it/s]Skipping y_hat=2145042\n","Skipping y_hat=2456275\n","Skipping y_hat=6374843\n","Skipping y_hat=2630294\n"," 22% 6/27 [00:00<00:01, 20.97it/s]Skipping y_hat=2324143\n","Skipping y_hat=2982087\n","Skipping y_hat=7813207\n"," 33% 9/27 [00:00<00:00, 21.26it/s]Skipping y_hat=1516870\n","Skipping y_hat=1324061\n","Skipping y_hat=6179140\n","Skipping y_hat=6176179\n","Skipping y_hat=4501950\n","Skipping y_hat=1649596\n"," 44% 12/27 [00:00<00:00, 21.05it/s]Skipping y_hat=4488787\n","Skipping y_hat=5685319\n","Skipping y_hat=2779745\n","Skipping y_hat=5247820\n","Skipping y_hat=1054542\n"," 56% 15/27 [00:00<00:00, 20.92it/s]Skipping y_hat=3743710\n","Skipping y_hat=1238715\n","Skipping y_hat=5281898\n","Skipping y_hat=2931105\n","Skipping y_hat=2260972\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 20.79it/s]Skipping y_hat=5896513\n","Skipping y_hat=1852644\n","Skipping y_hat=5760232\n","Skipping y_hat=6094381\n","Skipping y_hat=2085346\n","Skipping y_hat=8502436\n"," 78% 21/27 [00:01<00:00, 20.76it/s]Skipping y_hat=286564\n"," 89% 24/27 [00:01<00:00, 21.93it/s]Skipping y_hat=397863\n","Skipping y_hat=405631\n","Skipping y_hat=865711\n","100% 27/27 [00:01<00:00, 21.57it/s]\n","accuracy of 3000 examples: 2962/3000 (98.73333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.73%\n","\n","iter 695: train loss 0.9636, val loss 0.9478\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3727700\n","Skipping y_hat=4448782\n","Skipping y_hat=2211535\n","Skipping y_hat=2065502\n"," 11% 3/27 [00:00<00:01, 20.63it/s]Skipping y_hat=6919901\n","Skipping y_hat=4373040\n","Skipping y_hat=6900731\n"," 22% 6/27 [00:00<00:01, 20.82it/s]Skipping y_hat=6058963\n","Skipping y_hat=5712086\n"," 33% 9/27 [00:00<00:00, 21.12it/s]Skipping y_hat=5271263\n","Skipping y_hat=4227403\n","Skipping y_hat=2316046\n","Skipping y_hat=887479\n","Skipping y_hat=4468877\n","Skipping y_hat=1549585\n"," 44% 12/27 [00:00<00:00, 21.37it/s]Skipping y_hat=7267804\n","Skipping y_hat=7800605\n","Skipping y_hat=1203526\n","Skipping y_hat=7889525\n","Skipping y_hat=1971168\n","Skipping y_hat=3079634\n"," 56% 15/27 [00:00<00:00, 21.12it/s]Skipping y_hat=2563732\n","Skipping y_hat=4430988\n","Skipping y_hat=4091716\n","Skipping y_hat=2260972\n","Skipping y_hat=7351291\n","Skipping y_hat=3749828\n"," 89% 24/27 [00:01<00:00, 22.05it/s]Skipping y_hat=652661\n","100% 27/27 [00:01<00:00, 21.67it/s]\n","accuracy of 3000 examples: 2972/3000 (99.06666666666666%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.43it/s]\n","accuracy of 10000 examples: 9912/10000 (99.11999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3972970\n"," 11% 3/27 [00:00<00:01, 22.01it/s]Skipping y_hat=6719066\n","Skipping y_hat=7628084\n","Skipping y_hat=6919001\n","Skipping y_hat=1654361\n","Skipping y_hat=4181804\n","Skipping y_hat=1635427\n","Skipping y_hat=6851700\n"," 22% 6/27 [00:00<00:00, 22.06it/s]Skipping y_hat=5825566\n","Skipping y_hat=8688221\n","Skipping y_hat=6058963\n","Skipping y_hat=6216152\n"," 33% 9/27 [00:00<00:00, 21.66it/s]Skipping y_hat=4485808\n","Skipping y_hat=4227403\n","Skipping y_hat=3130746\n","Skipping y_hat=1549585\n"," 44% 12/27 [00:00<00:00, 21.79it/s]Skipping y_hat=1078885\n","Skipping y_hat=7889525\n","Skipping y_hat=1971168\n","Skipping y_hat=5784887\n"," 56% 15/27 [00:00<00:00, 22.14it/s]Skipping y_hat=3575714\n","Skipping y_hat=3497003\n","Skipping y_hat=4091716\n","Skipping y_hat=2260972\n","Skipping y_hat=7972152\n","Skipping y_hat=3749828\n"," 67% 18/27 [00:00<00:00, 22.13it/s]Skipping y_hat=2730565\n","Skipping y_hat=6205179\n"," 78% 21/27 [00:00<00:00, 22.35it/s]Skipping y_hat=262489\n"," 89% 24/27 [00:01<00:00, 23.07it/s]Skipping y_hat=652661\n","Skipping y_hat=865711\n","100% 27/27 [00:01<00:00, 22.63it/s]\n","accuracy of 3000 examples: 2969/3000 (98.96666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.97%\n","\n","iter 700: train loss 0.9591, val loss 0.9481\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3161618\n","Skipping y_hat=5205815\n","Skipping y_hat=5335631\n","Skipping y_hat=2211535\n","Skipping y_hat=6057865\n","Skipping y_hat=4031198\n","Skipping y_hat=1663550\n","Skipping y_hat=2553094\n"," 11% 3/27 [00:00<00:01, 22.41it/s]Skipping y_hat=6044599\n","Skipping y_hat=4242023\n","Skipping y_hat=3972914\n","Skipping y_hat=4327588\n","Skipping y_hat=3882212\n"," 22% 6/27 [00:00<00:00, 22.24it/s]Skipping y_hat=3594952\n","Skipping y_hat=1752704\n","Skipping y_hat=7701111\n","Skipping y_hat=3208860\n"," 33% 9/27 [00:00<00:00, 22.37it/s]Skipping y_hat=4227403\n","Skipping y_hat=5201162\n","Skipping y_hat=1549208\n","Skipping y_hat=6168140\n","Skipping y_hat=8509616\n","Skipping y_hat=5\n","Skipping y_hat=6032573\n","Skipping y_hat=3300229\n","Skipping y_hat=4620158\n"," 44% 12/27 [00:00<00:00, 22.18it/s]Skipping y_hat=4049963\n"," 56% 15/27 [00:00<00:00, 22.04it/s]Skipping y_hat=511296\n","Skipping y_hat=7340291\n","100% 27/27 [00:01<00:00, 22.91it/s]\n","accuracy of 3000 examples: 2971/3000 (99.03333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.07it/s]\n","accuracy of 10000 examples: 9933/10000 (99.33%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5205815\n","Skipping y_hat=2211535\n"," 11% 3/27 [00:00<00:01, 22.64it/s]Skipping y_hat=4242023\n","Skipping y_hat=3972914\n","Skipping y_hat=3470206\n","Skipping y_hat=3882212\n","Skipping y_hat=4676590\n","Skipping y_hat=1426076\n"," 22% 6/27 [00:00<00:00, 22.98it/s]Skipping y_hat=3594952\n","Skipping y_hat=63360\n","Skipping y_hat=6058963\n"," 33% 9/27 [00:00<00:00, 23.16it/s]Skipping y_hat=2032091\n","Skipping y_hat=5081111\n","Skipping y_hat=2053205\n","Skipping y_hat=6255442\n","Skipping y_hat=6168140\n","Skipping y_hat=4620158\n","Skipping y_hat=1549585\n"," 44% 12/27 [00:00<00:00, 23.16it/s]Skipping y_hat=6584894\n"," 56% 15/27 [00:00<00:00, 23.24it/s]Skipping y_hat=3014153\n","Skipping y_hat=6146180\n","Skipping y_hat=511296\n","Skipping y_hat=3932423\n","Skipping y_hat=3302461\n"," 67% 18/27 [00:00<00:00, 23.16it/s]Skipping y_hat=7422480\n","Skipping y_hat=6205179\n","100% 27/27 [00:01<00:00, 23.62it/s]\n","accuracy of 3000 examples: 2974/3000 (99.13333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.13%\n","\n","iter 705: train loss 0.9618, val loss 0.9483\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1634756\n","Skipping y_hat=2211535\n","Skipping y_hat=4482073\n"," 11% 3/27 [00:00<00:01, 22.93it/s]Skipping y_hat=7628084\n","Skipping y_hat=8740378\n","Skipping y_hat=5071951\n","Skipping y_hat=3114784\n"," 22% 6/27 [00:00<00:00, 23.01it/s]Skipping y_hat=3594952\n","Skipping y_hat=7425019\n","Skipping y_hat=4950772\n"," 44% 12/27 [00:00<00:00, 22.83it/s]Skipping y_hat=3282785\n","Skipping y_hat=1027488\n","Skipping y_hat=6821110\n"," 56% 15/27 [00:00<00:00, 22.45it/s]Skipping y_hat=4886299\n","Skipping y_hat=6133699\n","Skipping y_hat=2269972\n","Skipping y_hat=3749828\n"," 67% 18/27 [00:00<00:00, 21.86it/s]Skipping y_hat=3146950\n","Skipping y_hat=974987\n","Skipping y_hat=2762506\n","Skipping y_hat=8136683\n","Skipping y_hat=6205179\n"," 78% 21/27 [00:00<00:00, 21.60it/s]Skipping y_hat=532570\n","100% 27/27 [00:01<00:00, 22.70it/s]\n","accuracy of 3000 examples: 2977/3000 (99.23333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.37it/s]\n","accuracy of 10000 examples: 9933/10000 (99.33%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1634756\n","Skipping y_hat=6703559\n","Skipping y_hat=2211535\n"," 11% 3/27 [00:00<00:01, 22.52it/s]Skipping y_hat=2809567\n","Skipping y_hat=3114784\n","Skipping y_hat=3663389\n"," 22% 6/27 [00:00<00:00, 22.52it/s]Skipping y_hat=3594952\n"," 44% 12/27 [00:00<00:00, 22.95it/s]Skipping y_hat=5683319\n","Skipping y_hat=7955713\n","Skipping y_hat=1203526\n"," 56% 15/27 [00:00<00:00, 22.97it/s]Skipping y_hat=4567294\n","Skipping y_hat=6618430\n","Skipping y_hat=2269972\n","Skipping y_hat=3302461\n"," 67% 18/27 [00:00<00:00, 22.89it/s]Skipping y_hat=3146950\n","Skipping y_hat=6094381\n","Skipping y_hat=8136793\n","Skipping y_hat=4823272\n","Skipping y_hat=6205179\n"," 78% 21/27 [00:00<00:00, 22.81it/s]Skipping y_hat=2658088\n","Skipping y_hat=671832\n","100% 27/27 [00:01<00:00, 23.11it/s]\n","accuracy of 3000 examples: 2979/3000 (99.3%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.30%\n","\n","iter 710: train loss 0.9473, val loss 0.9515\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7077185\n","Skipping y_hat=3010656\n","Skipping y_hat=1481320\n","Skipping y_hat=2111545\n","Skipping y_hat=3767475\n"," 11% 3/27 [00:00<00:01, 22.11it/s]Skipping y_hat=7231951\n","Skipping y_hat=5062051\n","Skipping y_hat=6121088\n","Skipping y_hat=7140970\n"," 22% 6/27 [00:00<00:00, 21.94it/s]Skipping y_hat=4711627\n","Skipping y_hat=3082987\n","Skipping y_hat=3012751\n","Skipping y_hat=1630220\n","Skipping y_hat=3117329\n"," 33% 9/27 [00:00<00:00, 22.19it/s]Skipping y_hat=5071211\n","Skipping y_hat=6131674\n","Skipping y_hat=5201162\n"," 44% 12/27 [00:00<00:00, 22.30it/s]Skipping y_hat=8674019\n","Skipping y_hat=4149973\n","Skipping y_hat=5971355\n","Skipping y_hat=7889525\n"," 56% 15/27 [00:00<00:00, 22.35it/s]Skipping y_hat=7321969\n","Skipping y_hat=7351291\n","Skipping y_hat=3749828\n"," 67% 18/27 [00:00<00:00, 22.44it/s]Skipping y_hat=5746123\n","Skipping y_hat=1747486\n","Skipping y_hat=6681070\n"," 78% 21/27 [00:00<00:00, 22.40it/s]Skipping y_hat=671732\n","Skipping y_hat=261199\n","Skipping y_hat=689759\n","Skipping y_hat=475291\n","100% 27/27 [00:01<00:00, 22.86it/s]\n","accuracy of 3000 examples: 2969/3000 (98.96666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.70it/s]\n","accuracy of 10000 examples: 9907/10000 (99.07000000000001%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1481320\n","  7% 2/27 [00:00<00:01, 17.41it/s]Skipping y_hat=5082523\n","Skipping y_hat=4243123\n","Skipping y_hat=5903245\n","Skipping y_hat=8\n"," 15% 4/27 [00:00<00:01, 17.32it/s]Skipping y_hat=1654347\n","Skipping y_hat=4167484\n","Skipping y_hat=7231951\n","Skipping y_hat=3594952\n"," 26% 7/27 [00:00<00:01, 19.16it/s]Skipping y_hat=3082987\n","Skipping y_hat=5938785\n","Skipping y_hat=86150\n","Skipping y_hat=3117329\n","Skipping y_hat=5071211\n"," 37% 10/27 [00:00<00:00, 20.20it/s]Skipping y_hat=3763687\n"," 48% 13/27 [00:00<00:00, 20.93it/s]Skipping y_hat=7845713\n","Skipping y_hat=7889525\n","Skipping y_hat=1594930\n","Skipping y_hat=7321969\n"," 59% 16/27 [00:00<00:00, 19.58it/s]Skipping y_hat=7659604\n","Skipping y_hat=7431463\n","Skipping y_hat=3095317\n","Skipping y_hat=7351291\n","Skipping y_hat=3111747\n","Skipping y_hat=3749828\n"," 70% 19/27 [00:00<00:00, 20.12it/s]Skipping y_hat=1747486\n","Skipping y_hat=946237\n","Skipping y_hat=6681070\n"," 81% 22/27 [00:01<00:00, 20.43it/s]Skipping y_hat=411502\n","100% 27/27 [00:01<00:00, 20.58it/s]\n","accuracy of 3000 examples: 2971/3000 (99.03333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.03%\n","\n","iter 715: train loss 0.9619, val loss 0.9473\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4604841\n","Skipping y_hat=5961298\n","Skipping y_hat=4596571\n","Skipping y_hat=1876511\n","Skipping y_hat=7970120\n","Skipping y_hat=323200\n","  7% 2/27 [00:00<00:01, 19.87it/s]Skipping y_hat=5920984\n","Skipping y_hat=2065502\n","Skipping y_hat=5070338\n","Skipping y_hat=6903235\n"," 19% 5/27 [00:00<00:01, 20.54it/s]Skipping y_hat=5947552\n","Skipping y_hat=1536427\n","Skipping y_hat=6997726\n","Skipping y_hat=7891111\n"," 30% 8/27 [00:00<00:00, 20.71it/s]Skipping y_hat=6963320\n","Skipping y_hat=7918948\n","Skipping y_hat=5071211\n","Skipping y_hat=7155442\n","Skipping y_hat=7131574\n"," 41% 11/27 [00:00<00:00, 18.95it/s]Skipping y_hat=1500966\n","Skipping y_hat=7033573\n","Skipping y_hat=7330528\n","Skipping y_hat=5960752\n"," 48% 13/27 [00:00<00:00, 18.82it/s]Skipping y_hat=6981355\n","Skipping y_hat=5552776\n","Skipping y_hat=7253119\n"," 59% 16/27 [00:00<00:00, 19.55it/s]Skipping y_hat=1066511\n","Skipping y_hat=7136180\n","Skipping y_hat=6809248\n","Skipping y_hat=7351291\n","Skipping y_hat=5991924\n"," 70% 19/27 [00:00<00:00, 20.12it/s]Skipping y_hat=2470496\n","Skipping y_hat=7111396\n","Skipping y_hat=445517\n"," 81% 22/27 [00:01<00:00, 20.97it/s]Skipping y_hat=553348\n","Skipping y_hat=322890\n","Skipping y_hat=475291\n","100% 27/27 [00:01<00:00, 20.83it/s]\n","accuracy of 3000 examples: 2962/3000 (98.73333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.85it/s]\n","accuracy of 10000 examples: 9861/10000 (98.61%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8164234\n","Skipping y_hat=5961298\n","Skipping y_hat=1876511\n","Skipping y_hat=7970120\n","  7% 2/27 [00:00<00:01, 19.77it/s]Skipping y_hat=5920984\n","Skipping y_hat=922732\n"," 19% 5/27 [00:00<00:01, 20.91it/s]Skipping y_hat=5027355\n","Skipping y_hat=5947552\n","Skipping y_hat=1536427\n","Skipping y_hat=7900831\n","Skipping y_hat=1527076\n","Skipping y_hat=5848876\n","Skipping y_hat=5518931\n","Skipping y_hat=1528517\n","Skipping y_hat=2096020\n","Skipping y_hat=4865710\n"," 30% 8/27 [00:00<00:00, 20.67it/s]Skipping y_hat=4389778\n","Skipping y_hat=6958963\n","Skipping y_hat=6918048\n","Skipping y_hat=5071211\n","Skipping y_hat=6165144\n","Skipping y_hat=7155442\n","Skipping y_hat=7131574\n","Skipping y_hat=7169140\n"," 41% 11/27 [00:00<00:00, 20.54it/s]Skipping y_hat=1510966\n","Skipping y_hat=7166179\n","Skipping y_hat=7863022\n","Skipping y_hat=5082267\n","Skipping y_hat=7330528\n","Skipping y_hat=5960752\n","Skipping y_hat=7705225\n","Skipping y_hat=6981355\n","Skipping y_hat=5552776\n"," 52% 14/27 [00:00<00:00, 20.35it/s]Skipping y_hat=1066511\n"," 63% 17/27 [00:00<00:00, 20.99it/s]Skipping y_hat=4541696\n","Skipping y_hat=7351291\n","Skipping y_hat=5991924\n","Skipping y_hat=7029136\n","Skipping y_hat=1681201\n","Skipping y_hat=7111396\n","Skipping y_hat=6074984\n","Skipping y_hat=6320427\n"," 74% 20/27 [00:00<00:00, 20.94it/s]Skipping y_hat=6957415\n"," 85% 23/27 [00:01<00:00, 21.71it/s]Skipping y_hat=262489\n","Skipping y_hat=553348\n","Skipping y_hat=475291\n","100% 27/27 [00:01<00:00, 21.46it/s]\n","accuracy of 3000 examples: 2954/3000 (98.46666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.47%\n","\n","iter 720: train loss 0.9589, val loss 0.9538\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 21.86it/s]Skipping y_hat=6104771\n","Skipping y_hat=4167484\n"," 22% 6/27 [00:00<00:00, 21.61it/s]Skipping y_hat=7425019\n","Skipping y_hat=7153838\n","Skipping y_hat=1375658\n"," 33% 9/27 [00:00<00:00, 21.26it/s]Skipping y_hat=5181211\n","Skipping y_hat=1433537\n","Skipping y_hat=1764244\n"," 44% 12/27 [00:00<00:00, 21.19it/s]Skipping y_hat=5683319\n","Skipping y_hat=4197458\n","Skipping y_hat=8089525\n"," 56% 15/27 [00:00<00:00, 21.34it/s]Skipping y_hat=956972\n","Skipping y_hat=2271529\n","Skipping y_hat=6148288\n","Skipping y_hat=3311461\n"," 67% 18/27 [00:00<00:00, 21.33it/s]Skipping y_hat=1074987\n","Skipping y_hat=2083346\n","Skipping y_hat=1929366\n"," 78% 21/27 [00:00<00:00, 21.19it/s]Skipping y_hat=211040\n","Skipping y_hat=455291\n"," 89% 24/27 [00:01<00:00, 21.84it/s]Skipping y_hat=78968\n","100% 27/27 [00:01<00:00, 21.79it/s]\n","accuracy of 3000 examples: 2979/3000 (99.3%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.71it/s]\n","accuracy of 10000 examples: 9941/10000 (99.41%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8328205\n","Skipping y_hat=4702960\n","Skipping y_hat=6153470\n","Skipping y_hat=1111535\n","Skipping y_hat=189900=\n","Skipping y_hat=189900=\n","Skipping y_hat=189900=\n","Skipping y_hat=189900=\n","Skipping y_hat=189900=\n","Skipping y_hat=189900=\n","Skipping y_hat=189900=\n"," 11% 3/27 [00:00<00:01, 22.91it/s]Skipping y_hat=13=0932\n","Skipping y_hat=13=0932\n","Skipping y_hat=13=0932\n","Skipping y_hat=13=0932\n","Skipping y_hat=13=0932\n","Skipping y_hat=13=0932\n","Skipping y_hat=13=0932\n"," 22% 6/27 [00:00<00:00, 22.59it/s]Skipping y_hat=7680304\n"," 33% 9/27 [00:00<00:00, 22.46it/s]Skipping y_hat=1557953\n","Skipping y_hat=1433537\n"," 44% 12/27 [00:00<00:00, 22.55it/s]Skipping y_hat=8089525\n"," 56% 15/27 [00:00<00:00, 22.60it/s]Skipping y_hat=2271529\n","Skipping y_hat=6148288\n","Skipping y_hat=3749828\n"," 67% 18/27 [00:00<00:00, 22.68it/s]Skipping y_hat=5579144\n","Skipping y_hat=1074987\n","Skipping y_hat=7111396\n"," 78% 21/27 [00:00<00:00, 22.64it/s]Skipping y_hat=95245\n","Skipping y_hat=475291\n","100% 27/27 [00:01<00:00, 23.09it/s]\n","accuracy of 3000 examples: 2981/3000 (99.36666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.37%\n","\n","iter 725: train loss 0.9535, val loss 0.9383\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6243470\n","Skipping y_hat=5515379\n","Skipping y_hat=6157855\n"," 11% 3/27 [00:00<00:01, 22.68it/s]Skipping y_hat=1513864\n","Skipping y_hat=1881167\n"," 22% 6/27 [00:00<00:00, 22.64it/s]Skipping y_hat=1742804\n","Skipping y_hat=7144838\n","Skipping y_hat=8736166\n","Skipping y_hat=1533116\n","Skipping y_hat=8468423\n"," 33% 9/27 [00:00<00:00, 22.31it/s]Skipping y_hat=1433537\n","Skipping y_hat=4227403\n","Skipping y_hat=7117912\n","Skipping y_hat=6168140\n","Skipping y_hat=8409716\n","Skipping y_hat=1519966\n"," 44% 12/27 [00:00<00:00, 22.61it/s]Skipping y_hat=7889525\n"," 56% 15/27 [00:00<00:00, 22.79it/s]Skipping y_hat=6243119\n","Skipping y_hat=6521026\n","Skipping y_hat=2400008\n","Skipping y_hat=3144086\n","Skipping y_hat=7331291\n"," 67% 18/27 [00:00<00:00, 22.85it/s]Skipping y_hat=6250114\n","Skipping y_hat=7824806\n"," 78% 21/27 [00:00<00:00, 22.87it/s]Skipping y_hat=2647088\n","Skipping y_hat=130178\n"," 89% 24/27 [00:01<00:00, 23.25it/s]Skipping y_hat=395531\n","Skipping y_hat=658010\n","100% 27/27 [00:01<00:00, 23.16it/s]\n","accuracy of 3000 examples: 2972/3000 (99.06666666666666%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 23.97it/s]\n","accuracy of 10000 examples: 9920/10000 (99.2%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6015286\n","Skipping y_hat=6243470\n","Skipping y_hat=2942477\n","Skipping y_hat=5345731\n","Skipping y_hat=2552894\n"," 11% 3/27 [00:00<00:01, 21.98it/s]Skipping y_hat=1513864\n","Skipping y_hat=3471306\n","Skipping y_hat=5071951\n"," 22% 6/27 [00:00<00:00, 22.29it/s]Skipping y_hat=1742804\n","Skipping y_hat=651005\n","Skipping y_hat=7144838\n","Skipping y_hat=1215344\n","Skipping y_hat=1533116\n","Skipping y_hat=8468423\n"," 33% 9/27 [00:00<00:00, 22.12it/s]Skipping y_hat=1433537\n","Skipping y_hat=7117912\n","Skipping y_hat=8409716\n","Skipping y_hat=1519966\n"," 56% 15/27 [00:00<00:00, 22.22it/s]Skipping y_hat=6243119\n","Skipping y_hat=2115874\n","Skipping y_hat=3144086\n"," 67% 18/27 [00:00<00:00, 21.37it/s]Skipping y_hat=7824806\n","Skipping y_hat=1919366\n","Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.53it/s]Skipping y_hat=31428\n","Skipping y_hat=135178\n"," 89% 24/27 [00:01<00:00, 22.61it/s]Skipping y_hat=658010\n","100% 27/27 [00:01<00:00, 22.47it/s]\n","accuracy of 3000 examples: 2973/3000 (99.1%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.10%\n","\n","iter 730: train loss 0.9573, val loss 0.9420\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3528604\n"," 11% 3/27 [00:00<00:01, 21.79it/s]Skipping y_hat=3703200\n","Skipping y_hat=3878650\n"," 22% 6/27 [00:00<00:00, 21.51it/s]Skipping y_hat=2416188\n"," 33% 9/27 [00:00<00:00, 21.62it/s]Skipping y_hat=940508\n","Skipping y_hat=5904198\n","Skipping y_hat=1649596\n"," 44% 12/27 [00:00<00:00, 21.46it/s]Skipping y_hat=4754430\n","Skipping y_hat=3574486\n"," 56% 15/27 [00:00<00:00, 21.41it/s]Skipping y_hat=6254119\n","Skipping y_hat=4491244\n","Skipping y_hat=6521026\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 21.63it/s]Skipping y_hat=2957805\n"," 78% 21/27 [00:00<00:00, 21.92it/s]Skipping y_hat=578872\n"," 89% 24/27 [00:01<00:00, 22.77it/s]Skipping y_hat=395531\n","100% 27/27 [00:01<00:00, 22.36it/s]\n","accuracy of 3000 examples: 2984/3000 (99.46666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.18it/s]\n","accuracy of 10000 examples: 9961/10000 (99.61%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3528604\n"," 11% 3/27 [00:00<00:01, 21.72it/s]Skipping y_hat=3308545\n"," 22% 6/27 [00:00<00:00, 21.65it/s]Skipping y_hat=5893795\n"," 33% 9/27 [00:00<00:00, 21.77it/s]Skipping y_hat=6281263\n","Skipping y_hat=5904198\n","Skipping y_hat=1649596\n"," 56% 15/27 [00:00<00:00, 21.72it/s]Skipping y_hat=4491244\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 21.62it/s]Skipping y_hat=5579144\n"," 78% 21/27 [00:00<00:00, 21.60it/s]Skipping y_hat=227040\n"," 89% 24/27 [00:01<00:00, 22.23it/s]Skipping y_hat=111571\n","Skipping y_hat=652661\n","Skipping y_hat=647110\n","100% 27/27 [00:01<00:00, 22.04it/s]\n","accuracy of 3000 examples: 2987/3000 (99.56666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.57%\n","\n","iter 735: train loss 0.9534, val loss 0.9531\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6638158\n","Skipping y_hat=3528604\n"," 11% 3/27 [00:00<00:01, 21.56it/s]Skipping y_hat=3114784\n","Skipping y_hat=7742987\n"," 33% 9/27 [00:00<00:00, 21.88it/s]Skipping y_hat=4227403\n","Skipping y_hat=4479877\n","Skipping y_hat=3390229\n","Skipping y_hat=2982173\n"," 44% 12/27 [00:00<00:00, 20.12it/s]Skipping y_hat=3392785\n"," 56% 15/27 [00:00<00:00, 17.83it/s]Skipping y_hat=2271529\n","Skipping y_hat=4491244\n","Skipping y_hat=7138288\n"," 85% 23/27 [00:01<00:00, 20.03it/s]Skipping y_hat=395531\n","Skipping y_hat=658010\n","100% 27/27 [00:01<00:00, 20.00it/s]\n","accuracy of 3000 examples: 2986/3000 (99.53333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 23.31it/s]\n","accuracy of 10000 examples: 9967/10000 (99.67%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3539594\n","Skipping y_hat=5769973\n","Skipping y_hat=6638158\n","Skipping y_hat=3528604\n"," 22% 6/27 [00:00<00:00, 22.96it/s]Skipping y_hat=4998070\n","Skipping y_hat=5419931\n","Skipping y_hat=781805\n"," 33% 9/27 [00:00<00:00, 23.01it/s]Skipping y_hat=4227403\n","Skipping y_hat=3390229\n"," 56% 15/27 [00:00<00:00, 23.16it/s]Skipping y_hat=4491244\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 23.20it/s]Skipping y_hat=8100343\n"," 78% 21/27 [00:00<00:00, 23.06it/s]Skipping y_hat=536723\n"," 89% 24/27 [00:01<00:00, 23.69it/s]Skipping y_hat=395531\n","100% 27/27 [00:01<00:00, 23.50it/s]\n","accuracy of 3000 examples: 2986/3000 (99.53333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.53%\n","\n","iter 740: train loss 0.9618, val loss 0.9381\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3972970\n","Skipping y_hat=8418205\n","Skipping y_hat=2211535\n"," 11% 3/27 [00:00<00:01, 22.46it/s]Skipping y_hat=8430769\n","Skipping y_hat=2991602\n","Skipping y_hat=6104771\n","Skipping y_hat=2899567\n","Skipping y_hat=6919001\n"," 22% 6/27 [00:00<00:00, 22.36it/s]Skipping y_hat=3172706\n","Skipping y_hat=4993795\n"," 33% 9/27 [00:00<00:00, 22.51it/s]Skipping y_hat=1836750\n","Skipping y_hat=6166279\n"," 44% 12/27 [00:00<00:00, 22.62it/s]Skipping y_hat=5358649\n","Skipping y_hat=3392785\n","Skipping y_hat=7889525\n","Skipping y_hat=7776032\n"," 56% 15/27 [00:00<00:00, 22.63it/s]Skipping y_hat=2115874\n","Skipping y_hat=6148288\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 22.60it/s]Skipping y_hat=4658439\n","Skipping y_hat=5896313\n","Skipping y_hat=1808602\n","Skipping y_hat=5561252\n","Skipping y_hat=2762506\n"," 78% 21/27 [00:00<00:00, 22.52it/s]Skipping y_hat=262489\n","Skipping y_hat=167458\n","Skipping y_hat=148746\n","Skipping y_hat=261199\n","Skipping y_hat=532531\n"," 89% 24/27 [00:01<00:00, 23.12it/s]Skipping y_hat=727327\n","Skipping y_hat=354268\n","Skipping y_hat=154996\n","Skipping y_hat=555760\n","100% 27/27 [00:01<00:00, 22.91it/s]\n","accuracy of 3000 examples: 2967/3000 (98.9%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.22it/s]\n","accuracy of 10000 examples: 9883/10000 (98.83%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3972970\n","Skipping y_hat=2211535\n","Skipping y_hat=3288979\n"," 11% 3/27 [00:00<00:01, 21.75it/s]Skipping y_hat=8430769\n","Skipping y_hat=2991602\n","Skipping y_hat=6919901\n","Skipping y_hat=7149970\n"," 22% 6/27 [00:00<00:00, 22.24it/s]Skipping y_hat=3172706\n","Skipping y_hat=2922751\n","Skipping y_hat=4864610\n"," 33% 9/27 [00:00<00:00, 20.82it/s]Skipping y_hat=1434547\n","Skipping y_hat=4227403\n","Skipping y_hat=6917912\n","Skipping y_hat=6166279\n","Skipping y_hat=3390229\n","Skipping y_hat=4630168\n","Skipping y_hat=4897063\n"," 44% 12/27 [00:00<00:00, 20.53it/s]Skipping y_hat=7800715\n","Skipping y_hat=5358649\n","Skipping y_hat=3392785\n","Skipping y_hat=7584994\n","Skipping y_hat=7889525\n"," 56% 15/27 [00:00<00:00, 20.51it/s]Skipping y_hat=2115874\n","Skipping y_hat=4620562\n","Skipping y_hat=2995327\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 20.67it/s]Skipping y_hat=4658439\n","Skipping y_hat=5896313\n","Skipping y_hat=1808602\n","Skipping y_hat=2145950\n","Skipping y_hat=2633770\n","Skipping y_hat=7090343\n","Skipping y_hat=2762506\n"," 78% 21/27 [00:01<00:00, 20.52it/s]Skipping y_hat=532570\n","Skipping y_hat=262489\n","Skipping y_hat=279687\n"," 89% 24/27 [00:01<00:00, 21.12it/s]Skipping y_hat=727327\n","Skipping y_hat=297894\n","Skipping y_hat=235504\n","Skipping y_hat=154996\n","Skipping y_hat=602387\n","100% 27/27 [00:01<00:00, 21.05it/s]\n","accuracy of 3000 examples: 2959/3000 (98.63333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.63%\n","\n","iter 745: train loss 0.9534, val loss 0.9453\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5097558\n"," 22% 6/27 [00:00<00:00, 22.39it/s]Skipping y_hat=2893670\n","Skipping y_hat=7750171\n","Skipping y_hat=3117329\n"," 33% 9/27 [00:00<00:00, 22.07it/s]Skipping y_hat=5071211\n","Skipping y_hat=1953305\n","Skipping y_hat=6756227\n","Skipping y_hat=3570496\n","Skipping y_hat=6168140\n","Skipping y_hat=6521023\n"," 44% 12/27 [00:00<00:00, 22.06it/s]Skipping y_hat=8674019\n","Skipping y_hat=7438672\n","Skipping y_hat=4173749\n"," 56% 15/27 [00:00<00:00, 22.35it/s]Skipping y_hat=4885399\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 22.34it/s]Skipping y_hat=7223642\n","Skipping y_hat=2633770\n","Skipping y_hat=3521315\n","Skipping y_hat=1774482\n","Skipping y_hat=2771877\n"," 78% 21/27 [00:00<00:00, 22.30it/s]Skipping y_hat=2719429\n","Skipping y_hat=17490\n","Skipping y_hat=17154\n","Skipping y_hat=515180\n","Skipping y_hat=532570\n","Skipping y_hat=223834\n","Skipping y_hat=45842\n","Skipping y_hat=532531\n"," 89% 24/27 [00:01<00:00, 22.85it/s]Skipping y_hat=235504\n","100% 27/27 [00:01<00:00, 22.73it/s]\n","accuracy of 3000 examples: 2971/3000 (99.03333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.54it/s]\n","accuracy of 10000 examples: 9925/10000 (99.25%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5097558\n"," 11% 3/27 [00:00<00:01, 22.24it/s]Skipping y_hat=3973114\n","Skipping y_hat=5893235\n"," 22% 6/27 [00:00<00:00, 22.24it/s]Skipping y_hat=3396470\n","Skipping y_hat=4883795\n"," 33% 9/27 [00:00<00:00, 22.34it/s]Skipping y_hat=1953305\n","Skipping y_hat=3588705\n","Skipping y_hat=4631158\n"," 44% 12/27 [00:00<00:00, 22.29it/s]Skipping y_hat=8674019\n","Skipping y_hat=6320528\n","Skipping y_hat=5991355\n","Skipping y_hat=7438672\n","Skipping y_hat=4339900\n"," 56% 15/27 [00:00<00:00, 22.10it/s]Skipping y_hat=5270898\n","Skipping y_hat=4381244\n","Skipping y_hat=4032323\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 22.19it/s]Skipping y_hat=4658439\n","Skipping y_hat=1774482\n","Skipping y_hat=2852996\n"," 78% 21/27 [00:00<00:00, 22.30it/s]Skipping y_hat=515180\n","Skipping y_hat=532570\n"," 89% 24/27 [00:01<00:00, 22.63it/s]Skipping y_hat=235504\n","100% 27/27 [00:01<00:00, 22.63it/s]\n","accuracy of 3000 examples: 2977/3000 (99.23333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.23%\n","\n","iter 750: train loss 0.9588, val loss 0.9385\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4695571\n","Skipping y_hat=4683922\n","Skipping y_hat=5097558\n","Skipping y_hat=2489501\n"," 11% 3/27 [00:00<00:01, 21.68it/s]Skipping y_hat=5051951\n","Skipping y_hat=5940831\n"," 22% 6/27 [00:00<00:00, 21.93it/s]Skipping y_hat=1289335\n"," 33% 9/27 [00:00<00:00, 22.07it/s]Skipping y_hat=5959381\n","Skipping y_hat=1675052\n","Skipping y_hat=5537922\n"," 44% 12/27 [00:00<00:00, 22.07it/s]Skipping y_hat=2104171\n","Skipping y_hat=7438672\n"," 56% 15/27 [00:00<00:00, 22.06it/s]Skipping y_hat=8785574\n","Skipping y_hat=2035124\n","Skipping y_hat=2359972\n"," 67% 18/27 [00:00<00:00, 22.07it/s]Skipping y_hat=7111396\n","Skipping y_hat=2762506\n","Skipping y_hat=6300838\n"," 78% 21/27 [00:00<00:00, 22.13it/s]Skipping y_hat=380235\n","Skipping y_hat=447908\n"," 89% 24/27 [00:01<00:00, 22.82it/s]Skipping y_hat=388865\n","Skipping y_hat=642561\n","100% 27/27 [00:01<00:00, 22.50it/s]\n","accuracy of 3000 examples: 2977/3000 (99.23333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.73it/s]\n","accuracy of 10000 examples: 9927/10000 (99.27%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4695571\n","Skipping y_hat=5097558\n","Skipping y_hat=2489501\n"," 11% 3/27 [00:00<00:01, 22.11it/s]Skipping y_hat=5051951\n","Skipping y_hat=5940831\n"," 22% 6/27 [00:00<00:00, 22.32it/s]Skipping y_hat=3574135\n"," 33% 9/27 [00:00<00:00, 22.49it/s]Skipping y_hat=5959381\n","Skipping y_hat=5748254\n","Skipping y_hat=7754022\n","Skipping y_hat=1675052\n","Skipping y_hat=5537922\n"," 44% 12/27 [00:00<00:00, 22.41it/s]Skipping y_hat=7303688\n","Skipping y_hat=7438672\n","Skipping y_hat=8089525\n","Skipping y_hat=2350342\n"," 56% 15/27 [00:00<00:00, 22.35it/s]Skipping y_hat=5829560\n","Skipping y_hat=5530988\n","Skipping y_hat=3408003\n","Skipping y_hat=2035124\n"," 67% 18/27 [00:00<00:00, 22.40it/s]Skipping y_hat=2762506\n"," 78% 21/27 [00:00<00:00, 22.47it/s]Skipping y_hat=595180\n","Skipping y_hat=386190\n","Skipping y_hat=87300\n","100% 27/27 [00:01<00:00, 22.85it/s]\n","accuracy of 3000 examples: 2977/3000 (99.23333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.23%\n","\n","iter 755: train loss 0.9581, val loss 0.9430\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4310783\n","Skipping y_hat=4305815\n","Skipping y_hat=4695571\n","Skipping y_hat=3411410\n","Skipping y_hat=6728158\n"," 11% 3/27 [00:00<00:01, 21.81it/s]Skipping y_hat=3740406\n"," 22% 6/27 [00:00<00:00, 21.80it/s]Skipping y_hat=371315\n","Skipping y_hat=928240\n"," 33% 9/27 [00:00<00:00, 21.92it/s]Skipping y_hat=2798762\n","Skipping y_hat=3180996\n"," 44% 12/27 [00:00<00:00, 21.99it/s]Skipping y_hat=7700705\n","Skipping y_hat=4387787\n","Skipping y_hat=8089525\n","Skipping y_hat=2036047\n"," 56% 15/27 [00:00<00:00, 21.86it/s]Skipping y_hat=2464998\n","Skipping y_hat=2271529\n","Skipping y_hat=6521026\n","Skipping y_hat=2260972\n","Skipping y_hat=3995317\n"," 67% 18/27 [00:00<00:00, 22.02it/s]Skipping y_hat=7005280\n","Skipping y_hat=889050\n","Skipping y_hat=1502884\n"," 78% 21/27 [00:00<00:00, 22.13it/s]Skipping y_hat=202268\n","100% 27/27 [00:01<00:00, 22.63it/s]\n","accuracy of 3000 examples: 2977/3000 (99.23333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 23.92it/s]\n","accuracy of 10000 examples: 9924/10000 (99.24%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4305815\n","Skipping y_hat=4695571\n","Skipping y_hat=984040\n","Skipping y_hat=7960690\n"," 11% 3/27 [00:00<00:01, 21.62it/s]Skipping y_hat=4968919\n","Skipping y_hat=3740406\n","Skipping y_hat=3470206\n","Skipping y_hat=5803235\n","Skipping y_hat=6800831\n"," 33% 9/27 [00:00<00:00, 21.80it/s]Skipping y_hat=1334537\n","Skipping y_hat=6255442\n"," 44% 12/27 [00:00<00:00, 22.21it/s]Skipping y_hat=6782776\n","Skipping y_hat=8089525\n","Skipping y_hat=2138111\n"," 56% 15/27 [00:00<00:00, 22.45it/s]Skipping y_hat=3508003\n"," 67% 18/27 [00:00<00:00, 22.46it/s]Skipping y_hat=5029136\n","Skipping y_hat=7005280\n","Skipping y_hat=2430112\n","Skipping y_hat=5531867\n","100% 27/27 [00:01<00:00, 22.70it/s]\n","accuracy of 3000 examples: 2981/3000 (99.36666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.37%\n","\n","iter 760: train loss 0.9553, val loss 0.9426\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4538344\n"," 11% 3/27 [00:00<00:01, 22.36it/s]Skipping y_hat=6851700\n"," 33% 9/27 [00:00<00:00, 22.62it/s]Skipping y_hat=1557953\n","Skipping y_hat=6017912\n","Skipping y_hat=6884055\n","Skipping y_hat=4255046\n"," 44% 12/27 [00:00<00:00, 22.51it/s]Skipping y_hat=829396\n"," 56% 15/27 [00:00<00:00, 22.63it/s]Skipping y_hat=2260972\n"," 67% 18/27 [00:00<00:00, 22.74it/s]Skipping y_hat=4524827\n"," 89% 24/27 [00:01<00:00, 23.37it/s]Skipping y_hat=256716\n","Skipping y_hat=301348\n","100% 27/27 [00:01<00:00, 23.08it/s]\n","accuracy of 3000 examples: 2989/3000 (99.63333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.27it/s]\n","accuracy of 10000 examples: 9953/10000 (99.53%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 23.22it/s]Skipping y_hat=1536427\n","Skipping y_hat=6851700\n"," 22% 6/27 [00:00<00:00, 23.11it/s]Skipping y_hat=7714207\n"," 33% 9/27 [00:00<00:00, 22.80it/s]Skipping y_hat=3138470\n","Skipping y_hat=4538022\n","Skipping y_hat=4255046\n"," 44% 12/27 [00:00<00:00, 22.76it/s]Skipping y_hat=829396\n","Skipping y_hat=6720110\n"," 56% 15/27 [00:00<00:00, 22.81it/s]Skipping y_hat=4091716\n","Skipping y_hat=3959200\n"," 67% 18/27 [00:00<00:00, 22.83it/s]Skipping y_hat=6011396\n","Skipping y_hat=1607699\n"," 78% 21/27 [00:00<00:00, 22.78it/s]Skipping y_hat=689759\n"," 89% 24/27 [00:01<00:00, 23.39it/s]Skipping y_hat=855611\n","Skipping y_hat=301348\n","Skipping y_hat=331264\n","100% 27/27 [00:01<00:00, 23.23it/s]\n","accuracy of 3000 examples: 2984/3000 (99.46666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.47%\n","\n","iter 765: train loss 0.9571, val loss 0.9491\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5515379\n","Skipping y_hat=4538344\n","Skipping y_hat=7665499\n","Skipping y_hat=8959690\n"," 11% 3/27 [00:00<00:01, 20.98it/s]Skipping y_hat=8842987\n","Skipping y_hat=8467471\n","Skipping y_hat=8372674\n"," 22% 6/27 [00:00<00:01, 20.75it/s]Skipping y_hat=8387821\n"," 33% 9/27 [00:00<00:00, 20.40it/s]Skipping y_hat=2361352\n","Skipping y_hat=1510066\n","Skipping y_hat=7851851\n","Skipping y_hat=5066974\n"," 44% 12/27 [00:00<00:00, 20.27it/s]Skipping y_hat=8399377\n","Skipping y_hat=7828336\n","Skipping y_hat=8945713\n","Skipping y_hat=8999525\n","Skipping y_hat=6954900\n"," 56% 15/27 [00:00<00:00, 20.30it/s]Skipping y_hat=3743710\n","Skipping y_hat=2468098\n"," 67% 18/27 [00:00<00:00, 17.86it/s]Skipping y_hat=1502884\n","Skipping y_hat=3894850\n","Skipping y_hat=6011396\n","Skipping y_hat=6074984\n"," 81% 22/27 [00:01<00:00, 16.58it/s]Skipping y_hat=136188\n","Skipping y_hat=885637\n","Skipping y_hat=301348\n","100% 27/27 [00:01<00:00, 19.06it/s]\n","accuracy of 3000 examples: 2974/3000 (99.13333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 23.09it/s]\n","accuracy of 10000 examples: 9935/10000 (99.35000000000001%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8806852\n","Skipping y_hat=4538344\n","Skipping y_hat=7665499\n","Skipping y_hat=8959690\n"," 11% 3/27 [00:00<00:01, 22.61it/s]Skipping y_hat=8629084\n","Skipping y_hat=8467471\n","Skipping y_hat=8139970\n","Skipping y_hat=8372674\n"," 33% 9/27 [00:00<00:00, 22.00it/s]Skipping y_hat=6919048\n","Skipping y_hat=4227403\n","Skipping y_hat=7118912\n"," 44% 12/27 [00:00<00:00, 21.93it/s]Skipping y_hat=8800705\n","Skipping y_hat=3249971\n","Skipping y_hat=8818336\n","Skipping y_hat=8945713\n","Skipping y_hat=829396\n","Skipping y_hat=8999525\n"," 56% 15/27 [00:00<00:00, 22.14it/s]Skipping y_hat=2468098\n","Skipping y_hat=8559604\n"," 67% 18/27 [00:00<00:00, 22.19it/s]Skipping y_hat=1502884\n","Skipping y_hat=2597280\n","Skipping y_hat=8825806\n","Skipping y_hat=6074984\n","Skipping y_hat=2782877\n"," 78% 21/27 [00:00<00:00, 22.34it/s]Skipping y_hat=885637\n","100% 27/27 [00:01<00:00, 22.72it/s]\n","accuracy of 3000 examples: 2975/3000 (99.16666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.17%\n","\n","iter 770: train loss 0.9552, val loss 0.9528\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3441716\n"," 11% 3/27 [00:00<00:01, 22.22it/s]Skipping y_hat=1872167\n"," 33% 9/27 [00:00<00:00, 22.54it/s]Skipping y_hat=4227403\n","Skipping y_hat=4856416\n"," 44% 12/27 [00:00<00:00, 22.63it/s]Skipping y_hat=3249971\n"," 56% 15/27 [00:00<00:00, 22.54it/s]Skipping y_hat=1892848\n"," 89% 24/27 [00:01<00:00, 23.19it/s]Skipping y_hat=233504\n","Skipping y_hat=230220\n","100% 27/27 [00:01<00:00, 22.89it/s]\n","accuracy of 3000 examples: 2989/3000 (99.63333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.25it/s]\n","accuracy of 10000 examples: 9945/10000 (99.45%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.86it/s]Skipping y_hat=1872167\n"," 22% 6/27 [00:00<00:00, 22.57it/s]Skipping y_hat=5893795\n"," 33% 9/27 [00:00<00:00, 22.71it/s]Skipping y_hat=1638670\n"," 44% 12/27 [00:00<00:00, 22.54it/s]Skipping y_hat=1648180\n"," 56% 15/27 [00:00<00:00, 22.55it/s]Skipping y_hat=1892848\n"," 67% 18/27 [00:00<00:00, 22.69it/s]Skipping y_hat=5330256\n","Skipping y_hat=1896288\n"," 89% 24/27 [00:01<00:00, 23.32it/s]Skipping y_hat=235504\n","100% 27/27 [00:01<00:00, 23.06it/s]\n","accuracy of 3000 examples: 2991/3000 (99.7%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.70%\n","\n","iter 775: train loss 0.9560, val loss 0.9478\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7665499\n"," 11% 3/27 [00:00<00:01, 22.46it/s]Skipping y_hat=1881167\n"," 33% 9/27 [00:00<00:00, 22.55it/s]Skipping y_hat=887479\n","Skipping y_hat=3390229\n"," 56% 15/27 [00:00<00:00, 22.45it/s]Skipping y_hat=2260972\n"," 67% 18/27 [00:00<00:00, 22.37it/s]Skipping y_hat=4364440\n"," 89% 24/27 [00:01<00:00, 22.97it/s]Skipping y_hat=865711\n","Skipping y_hat=235504\n","100% 27/27 [00:01<00:00, 22.79it/s]\n","accuracy of 3000 examples: 2992/3000 (99.73333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.52it/s]\n","accuracy of 10000 examples: 9962/10000 (99.62%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4310783\n","Skipping y_hat=7665499\n"," 11% 3/27 [00:00<00:01, 21.53it/s]Skipping y_hat=1881167\n"," 22% 6/27 [00:00<00:00, 21.58it/s]Skipping y_hat=4993795\n"," 33% 9/27 [00:00<00:00, 21.74it/s]Skipping y_hat=3390229\n"," 44% 12/27 [00:00<00:00, 21.74it/s]Skipping y_hat=7798177\n"," 56% 15/27 [00:00<00:00, 21.79it/s]Skipping y_hat=90\n","Skipping y_hat=90\n","Skipping y_hat=2260972\n"," 67% 18/27 [00:00<00:00, 21.54it/s]Skipping y_hat=2735956\n","Skipping y_hat=6074984\n"," 78% 21/27 [00:00<00:00, 21.45it/s]Skipping y_hat=291994\n"," 89% 24/27 [00:01<00:00, 22.48it/s]Skipping y_hat=235504\n","100% 27/27 [00:01<00:00, 22.25it/s]\n","accuracy of 3000 examples: 2988/3000 (99.6%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.60%\n","\n","iter 780: train loss 0.9493, val loss 0.9396\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5088558\n","Skipping y_hat=976067\n"," 11% 3/27 [00:00<00:01, 20.80it/s]Skipping y_hat=4940821\n","Skipping y_hat=6373743\n","Skipping y_hat=2802691\n","Skipping y_hat=3525461\n"," 22% 6/27 [00:00<00:00, 21.03it/s]Skipping y_hat=5409882\n","Skipping y_hat=4307302\n"," 33% 9/27 [00:00<00:00, 21.29it/s]Skipping y_hat=4227403\n","Skipping y_hat=1224380\n"," 44% 12/27 [00:00<00:00, 21.44it/s]Skipping y_hat=8684029\n","Skipping y_hat=5980355\n","Skipping y_hat=7945613\n","Skipping y_hat=5102953\n","Skipping y_hat=8089525\n"," 56% 15/27 [00:00<00:00, 21.21it/s]Skipping y_hat=3124153\n","Skipping y_hat=2260972\n","Skipping y_hat=4301461\n","Skipping y_hat=5991924\n"," 67% 18/27 [00:00<00:00, 21.38it/s]Skipping y_hat=7243642\n","Skipping y_hat=4119886\n","Skipping y_hat=4254707\n"," 78% 21/27 [00:00<00:00, 21.80it/s]Skipping y_hat=138368\n"," 89% 24/27 [00:01<00:00, 19.67it/s]Skipping y_hat=276560\n","100% 27/27 [00:01<00:00, 20.73it/s]\n","accuracy of 3000 examples: 2975/3000 (99.16666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.50it/s]\n","accuracy of 10000 examples: 9922/10000 (99.22%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5335631\n","Skipping y_hat=2909009\n","Skipping y_hat=4131298\n"," 22% 6/27 [00:00<00:00, 22.43it/s]Skipping y_hat=5409882\n","Skipping y_hat=2996020\n","Skipping y_hat=4118329\n"," 33% 9/27 [00:00<00:00, 21.97it/s]Skipping y_hat=5066974\n"," 44% 12/27 [00:00<00:00, 21.91it/s]Skipping y_hat=5102953\n","Skipping y_hat=7889525\n"," 56% 15/27 [00:00<00:00, 21.97it/s]Skipping y_hat=7146280\n","Skipping y_hat=2260972\n"," 67% 18/27 [00:00<00:00, 22.18it/s]Skipping y_hat=4119886\n"," 78% 21/27 [00:00<00:00, 22.09it/s]Skipping y_hat=678844\n","Skipping y_hat=138368\n"," 89% 24/27 [00:01<00:00, 22.97it/s]Skipping y_hat=276560\n","Skipping y_hat=395531\n","Skipping y_hat=845711\n","Skipping y_hat=235504\n","100% 27/27 [00:01<00:00, 22.70it/s]\n","accuracy of 3000 examples: 2982/3000 (99.4%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.40%\n","\n","iter 785: train loss 0.9619, val loss 0.9439\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8418205\n","Skipping y_hat=1310585\n","Skipping y_hat=439981\n","Skipping y_hat=3677475\n"," 11% 3/27 [00:00<00:01, 22.00it/s]Skipping y_hat=4073014\n","Skipping y_hat=1881167\n","Skipping y_hat=3742874\n","Skipping y_hat=4382040\n","Skipping y_hat=5081128\n","Skipping y_hat=8294255\n"," 22% 6/27 [00:00<00:00, 21.56it/s]Skipping y_hat=1995020\n","Skipping y_hat=7891111\n","Skipping y_hat=7813207\n"," 33% 9/27 [00:00<00:00, 21.68it/s]Skipping y_hat=5081111\n","Skipping y_hat=1134040\n","Skipping y_hat=4227403\n","Skipping y_hat=3390229\n","Skipping y_hat=7413504\n"," 44% 12/27 [00:00<00:00, 21.71it/s]Skipping y_hat=7267804\n","Skipping y_hat=7438672\n","Skipping y_hat=783240\n"," 56% 15/27 [00:00<00:00, 21.53it/s]Skipping y_hat=8669791\n","Skipping y_hat=1132003\n"," 67% 18/27 [00:00<00:00, 21.75it/s]Skipping y_hat=3343272\n"," 78% 21/27 [00:00<00:00, 21.55it/s]Skipping y_hat=210348\n","100% 27/27 [00:01<00:00, 21.76it/s]\n","accuracy of 3000 examples: 2975/3000 (99.16666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.54it/s]\n","accuracy of 10000 examples: 9935/10000 (99.35000000000001%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5515379\n"," 11% 3/27 [00:00<00:01, 22.25it/s]Skipping y_hat=4073014\n","Skipping y_hat=5550080\n","Skipping y_hat=1881167\n","Skipping y_hat=4382040\n"," 22% 6/27 [00:00<00:00, 22.23it/s]Skipping y_hat=1995020\n","Skipping y_hat=4950772\n"," 44% 12/27 [00:00<00:00, 22.52it/s]Skipping y_hat=7499377\n","Skipping y_hat=6562324\n","Skipping y_hat=7267804\n","Skipping y_hat=7798177\n","Skipping y_hat=6684894\n","Skipping y_hat=7438672\n","Skipping y_hat=1103516\n","Skipping y_hat=8089525\n","Skipping y_hat=7812768\n"," 56% 15/27 [00:00<00:00, 22.50it/s]Skipping y_hat=3008056\n","Skipping y_hat=1132003\n","Skipping y_hat=7900874\n"," 67% 18/27 [00:00<00:00, 22.58it/s]Skipping y_hat=1681201\n","Skipping y_hat=7802803\n"," 78% 21/27 [00:00<00:00, 22.55it/s]Skipping y_hat=689759\n","100% 27/27 [00:01<00:00, 22.99it/s]\n","accuracy of 3000 examples: 2978/3000 (99.26666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.27%\n","\n","iter 790: train loss 0.9537, val loss 0.9468\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1680935\n","Skipping y_hat=3721000\n","Skipping y_hat=5097558\n","Skipping y_hat=4528334\n"," 11% 3/27 [00:00<00:01, 22.14it/s]Skipping y_hat=1881167\n","Skipping y_hat=2598803\n","Skipping y_hat=3114784\n","Skipping y_hat=6800831\n"," 22% 6/27 [00:00<00:00, 22.04it/s]Skipping y_hat=1752704\n"," 33% 9/27 [00:00<00:00, 22.36it/s]Skipping y_hat=5071211\n","Skipping y_hat=4227403\n"," 44% 12/27 [00:00<00:00, 22.40it/s]Skipping y_hat=6542314\n","Skipping y_hat=3652541\n","Skipping y_hat=1581168\n"," 56% 15/27 [00:00<00:00, 22.61it/s]Skipping y_hat=3144086\n"," 67% 18/27 [00:00<00:00, 22.73it/s]Skipping y_hat=7824806\n"," 89% 24/27 [00:01<00:00, 23.53it/s]Skipping y_hat=163603\n","100% 27/27 [00:01<00:00, 23.16it/s]\n","accuracy of 3000 examples: 2983/3000 (99.43333333333332%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.79it/s]\n","accuracy of 10000 examples: 9951/10000 (99.51%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5097558\n","Skipping y_hat=1310585\n","Skipping y_hat=7665499\n"," 11% 3/27 [00:00<00:01, 21.79it/s]Skipping y_hat=6553656\n","Skipping y_hat=3710802\n","Skipping y_hat=1881167\n","Skipping y_hat=5051951\n","Skipping y_hat=4326488\n"," 22% 6/27 [00:00<00:00, 22.32it/s]Skipping y_hat=4950772\n"," 33% 9/27 [00:00<00:00, 22.24it/s]Skipping y_hat=4227403\n","Skipping y_hat=7220096\n"," 44% 12/27 [00:00<00:00, 22.45it/s]Skipping y_hat=3652541\n"," 56% 15/27 [00:00<00:00, 22.57it/s]Skipping y_hat=1132003\n","Skipping y_hat=3560574\n","Skipping y_hat=3144086\n","Skipping y_hat=3311747\n"," 67% 18/27 [00:00<00:00, 22.40it/s]Skipping y_hat=7824806\n","Skipping y_hat=5531867\n","100% 27/27 [00:01<00:00, 22.87it/s]\n","accuracy of 3000 examples: 2982/3000 (99.4%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.40%\n","\n","iter 795: train loss 0.9494, val loss 0.9383\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6743725\n","Skipping y_hat=5097558\n","Skipping y_hat=6650451\n","Skipping y_hat=1310585\n"," 11% 3/27 [00:00<00:01, 22.67it/s]Skipping y_hat=4167484\n"," 22% 6/27 [00:00<00:00, 22.73it/s]Skipping y_hat=2990568\n","Skipping y_hat=5968963\n"," 33% 9/27 [00:00<00:00, 22.78it/s]Skipping y_hat=5201162\n","Skipping y_hat=8510716\n","Skipping y_hat=3390229\n"," 44% 12/27 [00:00<00:00, 22.48it/s]Skipping y_hat=2536262\n","Skipping y_hat=3240071\n","Skipping y_hat=6731110\n"," 56% 15/27 [00:00<00:00, 22.32it/s]Skipping y_hat=6062662\n","Skipping y_hat=7431463\n","Skipping y_hat=6148288\n","Skipping y_hat=7351291\n"," 67% 18/27 [00:00<00:00, 22.01it/s]Skipping y_hat=5579144\n"," 78% 21/27 [00:00<00:00, 21.86it/s]Skipping y_hat=532570\n"," 89% 24/27 [00:01<00:00, 22.24it/s]Skipping y_hat=198647\n","100% 27/27 [00:01<00:00, 22.44it/s]\n","accuracy of 3000 examples: 2980/3000 (99.33333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.67it/s]\n","accuracy of 10000 examples: 9958/10000 (99.58%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4448762\n","Skipping y_hat=5205815\n","Skipping y_hat=6650451\n","Skipping y_hat=3742388\n"," 11% 3/27 [00:00<00:01, 22.60it/s]Skipping y_hat=4167484\n","Skipping y_hat=3876550\n","Skipping y_hat=2530194\n","Skipping y_hat=6900821\n"," 33% 9/27 [00:00<00:00, 22.43it/s]Skipping y_hat=5201162\n","Skipping y_hat=6431023\n"," 44% 12/27 [00:00<00:00, 22.44it/s]Skipping y_hat=2536262\n","Skipping y_hat=3410306\n","Skipping y_hat=7428662\n","Skipping y_hat=2331820\n"," 56% 15/27 [00:00<00:00, 22.24it/s]Skipping y_hat=3015874\n","Skipping y_hat=7431463\n"," 67% 18/27 [00:00<00:00, 21.87it/s]Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 21.79it/s]Skipping y_hat=532570\n"," 89% 24/27 [00:01<00:00, 21.96it/s]Skipping y_hat=198647\n","100% 27/27 [00:01<00:00, 22.28it/s]\n","accuracy of 3000 examples: 2981/3000 (99.36666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.37%\n","\n","iter 800: train loss 0.9524, val loss 0.9503\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5515379\n","Skipping y_hat=2551376\n","Skipping y_hat=2161100\n"," 22% 6/27 [00:00<00:00, 22.80it/s]Skipping y_hat=4943065\n"," 33% 9/27 [00:00<00:00, 22.57it/s]Skipping y_hat=2754244\n","Skipping y_hat=4255046\n"," 44% 12/27 [00:00<00:00, 22.70it/s]Skipping y_hat=1536272\n","Skipping y_hat=5706048\n","Skipping y_hat=4105576\n","Skipping y_hat=7889525\n","Skipping y_hat=4173749\n"," 67% 18/27 [00:00<00:00, 22.71it/s]Skipping y_hat=4077032\n"," 78% 21/27 [00:00<00:00, 22.56it/s]Skipping y_hat=10604\n"," 89% 24/27 [00:01<00:00, 23.17it/s]Skipping y_hat=490192\n","100% 27/27 [00:01<00:00, 23.01it/s]\n","accuracy of 3000 examples: 2986/3000 (99.53333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.93it/s]\n","accuracy of 10000 examples: 9970/10000 (99.7%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.32it/s]Skipping y_hat=1353145\n"," 33% 9/27 [00:00<00:00, 22.22it/s]Skipping y_hat=4255046\n"," 44% 12/27 [00:00<00:00, 22.30it/s]Skipping y_hat=4105576\n","Skipping y_hat=5102953\n","Skipping y_hat=7889525\n"," 56% 15/27 [00:00<00:00, 22.19it/s]Skipping y_hat=3932423\n","Skipping y_hat=3311747\n"," 67% 18/27 [00:00<00:00, 21.89it/s]Skipping y_hat=5330256\n"," 78% 21/27 [00:00<00:00, 22.11it/s]Skipping y_hat=10604\n","100% 27/27 [00:01<00:00, 22.62it/s]\n","accuracy of 3000 examples: 2991/3000 (99.7%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.70%\n","\n","iter 805: train loss 0.9592, val loss 0.9400\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5266094\n","Skipping y_hat=3441716\n"," 22% 6/27 [00:00<00:00, 22.95it/s]Skipping y_hat=5418911\n"," 33% 9/27 [00:00<00:00, 22.81it/s]Skipping y_hat=3557357\n","Skipping y_hat=1334537\n","Skipping y_hat=4468877\n"," 44% 12/27 [00:00<00:00, 22.42it/s]Skipping y_hat=3410216\n","Skipping y_hat=4105576\n"," 56% 15/27 [00:00<00:00, 22.21it/s]Skipping y_hat=3842906\n","Skipping y_hat=1318472\n"," 67% 18/27 [00:00<00:00, 22.03it/s]Skipping y_hat=6689656\n","Skipping y_hat=4108886\n","Skipping y_hat=8136683\n"," 78% 21/27 [00:00<00:00, 21.80it/s]Skipping y_hat=261199\n","Skipping y_hat=277660\n","100% 27/27 [00:01<00:00, 22.52it/s]\n","accuracy of 3000 examples: 2985/3000 (99.5%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.38it/s]\n","accuracy of 10000 examples: 9973/10000 (99.72999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4711960\n","Skipping y_hat=1017668\n"," 11% 3/27 [00:00<00:01, 21.56it/s]Skipping y_hat=5071951\n"," 22% 6/27 [00:00<00:00, 21.94it/s]Skipping y_hat=3526406\n","Skipping y_hat=1499039\n","Skipping y_hat=6564821\n"," 33% 9/27 [00:00<00:00, 21.82it/s]Skipping y_hat=4468877\n"," 44% 12/27 [00:00<00:00, 21.92it/s]Skipping y_hat=5706048\n","Skipping y_hat=3410216\n","Skipping y_hat=4105576\n","Skipping y_hat=828296\n"," 67% 18/27 [00:00<00:00, 21.95it/s]Skipping y_hat=7005280\n","Skipping y_hat=4108886\n","Skipping y_hat=1606599\n","Skipping y_hat=5540867\n"," 78% 21/27 [00:00<00:00, 21.73it/s]Skipping y_hat=10604\n","Skipping y_hat=261199\n","100% 27/27 [00:01<00:00, 22.21it/s]\n","accuracy of 3000 examples: 2983/3000 (99.43333333333332%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.43%\n","\n","iter 810: train loss 0.9586, val loss 0.9388\n","Using precomputed batches\n","Max number of tokens 8.\n"," 22% 6/27 [00:00<00:00, 22.19it/s]Skipping y_hat=1995020\n"," 67% 18/27 [00:00<00:00, 22.27it/s]Skipping y_hat=6681370\n"," 78% 21/27 [00:00<00:00, 22.20it/s]Skipping y_hat=261199\n","Skipping y_hat=455291\n","100% 27/27 [00:01<00:00, 22.57it/s]\n","accuracy of 3000 examples: 2996/3000 (99.86666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.75it/s]\n","accuracy of 10000 examples: 9974/10000 (99.74%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.86it/s]Skipping y_hat=4166928\n"," 56% 15/27 [00:00<00:00, 22.78it/s]Skipping y_hat=2271529\n"," 78% 21/27 [00:00<00:00, 22.77it/s]Skipping y_hat=455291\n"," 89% 24/27 [00:01<00:00, 23.37it/s]Skipping y_hat=120804\n","Skipping y_hat=222790\n","100% 27/27 [00:01<00:00, 23.23it/s]\n","accuracy of 3000 examples: 2994/3000 (99.8%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.80%\n","\n","iter 815: train loss 0.9521, val loss 0.9553\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2562994\n"," 11% 3/27 [00:00<00:01, 21.52it/s]Skipping y_hat=6044599\n","Skipping y_hat=5071951\n","Skipping y_hat=8928370\n","Skipping y_hat=4676590\n","Skipping y_hat=3525461\n","Skipping y_hat=6090418\n","Skipping y_hat=1536427\n"," 22% 6/27 [00:00<00:01, 20.61it/s]Skipping y_hat=5088070\n","Skipping y_hat=6058963\n"," 33% 9/27 [00:00<00:00, 21.18it/s]Skipping y_hat=887479\n","Skipping y_hat=4909456\n"," 44% 12/27 [00:00<00:00, 21.35it/s]Skipping y_hat=6081355\n","Skipping y_hat=7684894\n","Skipping y_hat=3089734\n"," 56% 15/27 [00:00<00:00, 21.19it/s]Skipping y_hat=2115874\n","Skipping y_hat=7604749\n","Skipping y_hat=7340291\n"," 67% 18/27 [00:00<00:00, 21.27it/s]Skipping y_hat=1500884\n","Skipping y_hat=5440256\n","Skipping y_hat=5712859\n"," 78% 21/27 [00:00<00:00, 21.23it/s]Skipping y_hat=542670\n","Skipping y_hat=568844\n","100% 27/27 [00:01<00:00, 21.85it/s]\n","accuracy of 3000 examples: 2977/3000 (99.23333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.03it/s]\n","accuracy of 10000 examples: 9954/10000 (99.53999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6703559\n","Skipping y_hat=2563094\n","Skipping y_hat=6059212\n"," 11% 3/27 [00:00<00:01, 22.65it/s]Skipping y_hat=6044599\n","Skipping y_hat=5071951\n"," 22% 6/27 [00:00<00:00, 22.24it/s]Skipping y_hat=7680304\n","Skipping y_hat=4370778\n","Skipping y_hat=6058963\n"," 33% 9/27 [00:00<00:00, 22.09it/s]Skipping y_hat=887479\n","Skipping y_hat=4290229\n"," 44% 12/27 [00:00<00:00, 22.21it/s]Skipping y_hat=4155802\n","Skipping y_hat=7684894\n"," 56% 15/27 [00:00<00:00, 22.06it/s]Skipping y_hat=2115874\n","Skipping y_hat=7330463\n","Skipping y_hat=4301461\n"," 67% 18/27 [00:00<00:00, 22.21it/s]Skipping y_hat=5440256\n"," 89% 24/27 [00:01<00:00, 23.17it/s]Skipping y_hat=495631\n","100% 27/27 [00:01<00:00, 22.81it/s]\n","accuracy of 3000 examples: 2983/3000 (99.43333333333332%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.43%\n","\n","iter 820: train loss 0.9573, val loss 0.9447\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5097558\n","Skipping y_hat=1964141\n","Skipping y_hat=3733388\n","Skipping y_hat=4372073\n"," 11% 3/27 [00:00<00:01, 22.50it/s]Skipping y_hat=1373145\n","Skipping y_hat=6104771\n","Skipping y_hat=1881167\n"," 22% 6/27 [00:00<00:00, 22.55it/s]Skipping y_hat=7335019\n"," 33% 9/27 [00:00<00:00, 22.63it/s]Skipping y_hat=6132574\n","Skipping y_hat=6248783\n"," 56% 15/27 [00:00<00:00, 22.58it/s]Skipping y_hat=3932906\n","Skipping y_hat=2262529\n","Skipping y_hat=3512683\n","Skipping y_hat=3391461\n"," 67% 18/27 [00:00<00:00, 22.69it/s]Skipping y_hat=1671101\n","Skipping y_hat=5111396\n","Skipping y_hat=4713272\n"," 89% 24/27 [00:01<00:00, 22.73it/s]Skipping y_hat=395531\n","100% 27/27 [00:01<00:00, 22.84it/s]\n","accuracy of 3000 examples: 2982/3000 (99.4%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.36it/s]\n","accuracy of 10000 examples: 9963/10000 (99.63%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8757323\n","Skipping y_hat=5097558\n","Skipping y_hat=1964141\n"," 11% 3/27 [00:00<00:01, 21.04it/s]Skipping y_hat=8587540\n","Skipping y_hat=6104771\n","Skipping y_hat=1881167\n","Skipping y_hat=6910001\n","Skipping y_hat=8928370\n"," 33% 9/27 [00:00<00:00, 21.59it/s]Skipping y_hat=1316870\n","Skipping y_hat=5131574\n"," 44% 12/27 [00:00<00:00, 21.50it/s]Skipping y_hat=6462314\n","Skipping y_hat=6731110\n"," 56% 15/27 [00:00<00:00, 21.95it/s]Skipping y_hat=5280998\n","Skipping y_hat=7331291\n"," 67% 18/27 [00:00<00:00, 22.09it/s]Skipping y_hat=5029136\n","Skipping y_hat=1500884\n","Skipping y_hat=8592436\n","Skipping y_hat=5531867\n"," 78% 21/27 [00:00<00:00, 22.15it/s]Skipping y_hat=475291\n","100% 27/27 [00:01<00:00, 22.46it/s]\n","accuracy of 3000 examples: 2981/3000 (99.36666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.37%\n","\n","iter 825: train loss 0.9522, val loss 0.9456\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2942477\n"," 11% 3/27 [00:00<00:01, 22.19it/s]Skipping y_hat=4367687\n","Skipping y_hat=3740606\n","Skipping y_hat=1872167\n","Skipping y_hat=1664261\n","Skipping y_hat=5434400\n","Skipping y_hat=6890831\n"," 22% 6/27 [00:00<00:00, 21.10it/s]Skipping y_hat=4888070\n","Skipping y_hat=805388\n"," 33% 9/27 [00:00<00:00, 21.61it/s]Skipping y_hat=5082211\n","Skipping y_hat=5271263\n","Skipping y_hat=1316870\n","Skipping y_hat=4468877\n"," 44% 12/27 [00:00<00:00, 21.78it/s]Skipping y_hat=7808336\n","Skipping y_hat=5247820\n","Skipping y_hat=2186690\n"," 56% 15/27 [00:00<00:00, 22.00it/s]Skipping y_hat=7330463\n","Skipping y_hat=3291461\n"," 67% 18/27 [00:00<00:00, 21.74it/s]Skipping y_hat=2083346\n"," 78% 21/27 [00:00<00:00, 21.96it/s]Skipping y_hat=682732\n","Skipping y_hat=261199\n"," 89% 24/27 [00:01<00:00, 22.47it/s]Skipping y_hat=298794\n","Skipping y_hat=350829\n","Skipping y_hat=154996\n","Skipping y_hat=556660\n","100% 27/27 [00:01<00:00, 22.19it/s]\n","accuracy of 3000 examples: 2975/3000 (99.16666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.37it/s]\n","accuracy of 10000 examples: 9936/10000 (99.36%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.72it/s]Skipping y_hat=8230769\n","Skipping y_hat=4586669\n","Skipping y_hat=2216245\n","Skipping y_hat=796268\n","Skipping y_hat=6890831\n"," 22% 6/27 [00:00<00:00, 21.73it/s]Skipping y_hat=4888070\n","Skipping y_hat=805388\n"," 33% 9/27 [00:00<00:00, 21.58it/s]Skipping y_hat=1334537\n","Skipping y_hat=6155342\n","Skipping y_hat=4856416\n","Skipping y_hat=4468877\n"," 44% 12/27 [00:00<00:00, 21.60it/s]Skipping y_hat=6462314\n","Skipping y_hat=7688177\n","Skipping y_hat=3282785\n","Skipping y_hat=7808336\n","Skipping y_hat=2779745\n","Skipping y_hat=7889525\n","Skipping y_hat=2186690\n"," 56% 15/27 [00:00<00:00, 21.37it/s]Skipping y_hat=3497003\n","Skipping y_hat=1132003\n"," 67% 18/27 [00:00<00:00, 21.57it/s]Skipping y_hat=4108886\n","Skipping y_hat=6320427\n"," 78% 21/27 [00:00<00:00, 21.70it/s]Skipping y_hat=682732\n","Skipping y_hat=261199\n"," 89% 24/27 [00:01<00:00, 22.64it/s]Skipping y_hat=198847\n","Skipping y_hat=556660\n","100% 27/27 [00:01<00:00, 22.32it/s]\n","accuracy of 3000 examples: 2974/3000 (99.13333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.13%\n","\n","iter 830: train loss 0.9561, val loss 0.9459\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=882796\n"," 11% 3/27 [00:00<00:01, 22.85it/s]Skipping y_hat=3740406\n"," 33% 9/27 [00:00<00:00, 22.75it/s]Skipping y_hat=4585908\n","Skipping y_hat=5080211\n"," 44% 12/27 [00:00<00:00, 22.69it/s]Skipping y_hat=4487887\n","Skipping y_hat=1424433\n","Skipping y_hat=5980355\n","Skipping y_hat=4597773\n","Skipping y_hat=3089734\n"," 56% 15/27 [00:00<00:00, 22.49it/s]Skipping y_hat=3842710\n","Skipping y_hat=7530463\n","Skipping y_hat=5873970\n","Skipping y_hat=5747878\n","Skipping y_hat=1132003\n","Skipping y_hat=4541696\n","Skipping y_hat=2995307\n"," 67% 18/27 [00:00<00:00, 22.45it/s]Skipping y_hat=4758449\n","Skipping y_hat=5881933\n"," 78% 21/27 [00:00<00:00, 22.44it/s]Skipping y_hat=14186\n"," 89% 24/27 [00:01<00:00, 23.07it/s]Skipping y_hat=282106\n","100% 27/27 [00:01<00:00, 22.99it/s]\n","accuracy of 3000 examples: 2980/3000 (99.33333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.87it/s]\n","accuracy of 10000 examples: 9939/10000 (99.39%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=882796\n"," 11% 3/27 [00:00<00:01, 22.62it/s]Skipping y_hat=3740406\n"," 33% 9/27 [00:00<00:00, 22.38it/s]Skipping y_hat=5387887\n","Skipping y_hat=6411023\n"," 44% 12/27 [00:00<00:00, 22.19it/s]Skipping y_hat=5897548\n","Skipping y_hat=4487887\n","Skipping y_hat=5980355\n","Skipping y_hat=4597773\n","Skipping y_hat=7089525\n","Skipping y_hat=5247820\n","Skipping y_hat=4183649\n","Skipping y_hat=3089734\n"," 56% 15/27 [00:00<00:00, 22.22it/s]Skipping y_hat=3842710\n","Skipping y_hat=4541696\n"," 67% 18/27 [00:00<00:00, 22.37it/s]Skipping y_hat=4758449\n"," 78% 21/27 [00:00<00:00, 22.29it/s]Skipping y_hat=17264\n"," 89% 24/27 [00:01<00:00, 22.64it/s]Skipping y_hat=282106\n","100% 27/27 [00:01<00:00, 22.71it/s]\n","accuracy of 3000 examples: 2983/3000 (99.43333333333332%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.43%\n","\n","iter 835: train loss 0.9530, val loss 0.9502\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6713569\n","Skipping y_hat=1543545\n","Skipping y_hat=4695571\n","Skipping y_hat=2776974\n","Skipping y_hat=1985040\n","Skipping y_hat=1646630\n"," 11% 3/27 [00:00<00:01, 20.29it/s]Skipping y_hat=5944699\n","Skipping y_hat=4596659\n","Skipping y_hat=2467567\n","Skipping y_hat=1921732\n","Skipping y_hat=2177782\n","Skipping y_hat=1996268\n"," 22% 6/27 [00:00<00:00, 21.06it/s]Skipping y_hat=1551005\n","Skipping y_hat=7153838\n"," 33% 9/27 [00:00<00:00, 21.66it/s]Skipping y_hat=2370747\n","Skipping y_hat=3138470\n","Skipping y_hat=6179140\n","Skipping y_hat=4470877\n"," 44% 12/27 [00:00<00:00, 21.96it/s]Skipping y_hat=1738615\n","Skipping y_hat=1769416\n","Skipping y_hat=4487887\n","Skipping y_hat=6081355\n","Skipping y_hat=1641694\n","Skipping y_hat=1648180\n","Skipping y_hat=1743766\n","Skipping y_hat=7089525\n"," 67% 18/27 [00:00<00:00, 22.43it/s]Skipping y_hat=5579144\n","Skipping y_hat=1890050\n","Skipping y_hat=2739555\n","Skipping y_hat=8158735\n","Skipping y_hat=1655914\n","Skipping y_hat=1681930\n"," 78% 21/27 [00:00<00:00, 22.24it/s]Skipping y_hat=177300\n"," 89% 24/27 [00:01<00:00, 23.05it/s]Skipping y_hat=176074\n","100% 27/27 [00:01<00:00, 22.57it/s]\n","accuracy of 3000 examples: 2966/3000 (98.86666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.99it/s]\n","accuracy of 10000 examples: 9881/10000 (98.81%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1905004\n","Skipping y_hat=1543545\n","Skipping y_hat=2776974\n","Skipping y_hat=1782796\n"," 11% 3/27 [00:00<00:01, 22.87it/s]Skipping y_hat=1921732\n","Skipping y_hat=1925625\n","Skipping y_hat=2177782\n","Skipping y_hat=1996268\n","Skipping y_hat=2535427\n","Skipping y_hat=6527404\n"," 22% 6/27 [00:00<00:00, 22.29it/s]Skipping y_hat=1551005\n","Skipping y_hat=7153838\n","Skipping y_hat=6007726\n","Skipping y_hat=7801111\n","Skipping y_hat=6058963\n"," 33% 9/27 [00:00<00:00, 22.56it/s]Skipping y_hat=1981568\n","Skipping y_hat=3138470\n","Skipping y_hat=1433537\n","Skipping y_hat=1638670\n","Skipping y_hat=6179140\n","Skipping y_hat=1666840\n","Skipping y_hat=1459585\n","Skipping y_hat=5066974\n"," 44% 12/27 [00:00<00:00, 22.53it/s]Skipping y_hat=6798447\n","Skipping y_hat=1925595\n","Skipping y_hat=7798177\n","Skipping y_hat=1769416\n","Skipping y_hat=6081355\n","Skipping y_hat=1641694\n","Skipping y_hat=1654798\n","Skipping y_hat=1793240\n","Skipping y_hat=1743766\n"," 56% 15/27 [00:00<00:00, 22.57it/s]Skipping y_hat=2271529\n","Skipping y_hat=7652936\n","Skipping y_hat=2269972\n"," 67% 18/27 [00:00<00:00, 22.72it/s]Skipping y_hat=5579144\n","Skipping y_hat=1807702\n","Skipping y_hat=2739555\n","Skipping y_hat=8158735\n","Skipping y_hat=1649098\n","Skipping y_hat=1655914\n","Skipping y_hat=1681930\n"," 78% 21/27 [00:00<00:00, 22.64it/s]Skipping y_hat=17490\n","Skipping y_hat=17154\n","Skipping y_hat=187300\n"," 89% 24/27 [00:01<00:00, 23.33it/s]Skipping y_hat=176074\n","Skipping y_hat=191755\n","Skipping y_hat=288785\n","Skipping y_hat=282106\n","Skipping y_hat=298747\n","Skipping y_hat=559556\n","100% 27/27 [00:01<00:00, 23.09it/s]\n","accuracy of 3000 examples: 2949/3000 (98.3%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 98.30%\n","\n","iter 840: train loss 0.9511, val loss 0.9446\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5306815\n","Skipping y_hat=1837583\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 22.93it/s]Skipping y_hat=6805252\n"," 22% 6/27 [00:00<00:00, 23.05it/s]Skipping y_hat=7153838\n"," 33% 9/27 [00:00<00:00, 22.96it/s]Skipping y_hat=6918848\n","Skipping y_hat=3138470\n","Skipping y_hat=6179140\n","Skipping y_hat=4470877\n"," 44% 12/27 [00:00<00:00, 22.54it/s]Skipping y_hat=7803680\n"," 56% 15/27 [00:00<00:00, 22.26it/s]Skipping y_hat=2115874\n","Skipping y_hat=3240261\n","Skipping y_hat=5819248\n"," 67% 18/27 [00:00<00:00, 22.04it/s]Skipping y_hat=2128015\n","Skipping y_hat=7826806\n","Skipping y_hat=4110886\n"," 89% 24/27 [00:01<00:00, 22.69it/s]Skipping y_hat=865711\n","100% 27/27 [00:01<00:00, 22.76it/s]\n","accuracy of 3000 examples: 2983/3000 (99.43333333333332%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.94it/s]\n","accuracy of 10000 examples: 9959/10000 (99.59%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7751215\n","Skipping y_hat=5509963\n"," 22% 6/27 [00:00<00:00, 22.92it/s]Skipping y_hat=7153838\n","Skipping y_hat=1962902\n"," 33% 9/27 [00:00<00:00, 23.13it/s]Skipping y_hat=3138470\n","Skipping y_hat=6179140\n","Skipping y_hat=4470877\n"," 44% 12/27 [00:00<00:00, 23.13it/s]Skipping y_hat=1972918\n","Skipping y_hat=6954900\n"," 56% 15/27 [00:00<00:00, 23.22it/s]Skipping y_hat=2633251\n","Skipping y_hat=2115874\n","Skipping y_hat=7845566\n","Skipping y_hat=3145186\n","Skipping y_hat=5819248\n"," 67% 18/27 [00:00<00:00, 23.21it/s]Skipping y_hat=4758339\n","Skipping y_hat=1084346\n","Skipping y_hat=4110886\n","100% 27/27 [00:01<00:00, 23.33it/s]\n","accuracy of 3000 examples: 2983/3000 (99.43333333333332%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.43%\n","\n","iter 845: train loss 0.9562, val loss 0.9420\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4272836\n","Skipping y_hat=3733388\n","Skipping y_hat=4482073\n"," 11% 3/27 [00:00<00:01, 22.49it/s]Skipping y_hat=6728066\n","Skipping y_hat=7628084\n","Skipping y_hat=5893235\n","Skipping y_hat=5305856\n","Skipping y_hat=2620194\n","Skipping y_hat=6800831\n"," 22% 6/27 [00:00<00:00, 22.50it/s]Skipping y_hat=5290850\n","Skipping y_hat=8836066\n","Skipping y_hat=3526206\n","Skipping y_hat=5686649\n","Skipping y_hat=6407848\n"," 33% 9/27 [00:00<00:00, 22.78it/s]Skipping y_hat=1334537\n","Skipping y_hat=1958336\n"," 44% 12/27 [00:00<00:00, 22.97it/s]Skipping y_hat=3282785\n","Skipping y_hat=3382127\n","Skipping y_hat=8088525\n"," 56% 15/27 [00:00<00:00, 22.43it/s]Skipping y_hat=6906257\n","Skipping y_hat=8469791\n","Skipping y_hat=5431771\n","Skipping y_hat=7331291\n"," 67% 18/27 [00:00<00:00, 22.17it/s]Skipping y_hat=2228531\n","Skipping y_hat=7792803\n","Skipping y_hat=5531867\n"," 89% 24/27 [00:01<00:00, 22.87it/s]Skipping y_hat=158777\n","100% 27/27 [00:01<00:00, 22.82it/s]\n","accuracy of 3000 examples: 2973/3000 (99.1%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.31it/s]\n","accuracy of 10000 examples: 9955/10000 (99.55000000000001%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8191828\n","Skipping y_hat=3733388\n","Skipping y_hat=4482073\n"," 11% 3/27 [00:00<00:01, 21.51it/s]Skipping y_hat=6728066\n","Skipping y_hat=5893235\n","Skipping y_hat=2620194\n","Skipping y_hat=2266685\n","Skipping y_hat=6800831\n","Skipping y_hat=2220104\n","Skipping y_hat=7312674\n"," 22% 6/27 [00:00<00:00, 21.16it/s]Skipping y_hat=8865109\n","Skipping y_hat=4888070\n","Skipping y_hat=8836066\n","Skipping y_hat=5686649\n","Skipping y_hat=6407848\n"," 33% 9/27 [00:00<00:00, 20.81it/s]Skipping y_hat=8481761\n"," 44% 12/27 [00:00<00:00, 21.43it/s]Skipping y_hat=8088525\n"," 56% 15/27 [00:00<00:00, 21.46it/s]Skipping y_hat=8886574\n","Skipping y_hat=6906257\n"," 67% 18/27 [00:00<00:00, 21.51it/s]Skipping y_hat=8136683\n","Skipping y_hat=7792803\n","Skipping y_hat=1030136\n"," 78% 21/27 [00:00<00:00, 21.60it/s]Skipping y_hat=788759\n","Skipping y_hat=532331\n"," 89% 24/27 [00:01<00:00, 22.20it/s]Skipping y_hat=158777\n","100% 27/27 [00:01<00:00, 21.91it/s]\n","accuracy of 3000 examples: 2975/3000 (99.16666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.17%\n","\n","iter 850: train loss 0.9524, val loss 0.9449\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1909009\n","Skipping y_hat=5080328\n","Skipping y_hat=4482073\n"," 11% 3/27 [00:00<00:01, 22.56it/s]Skipping y_hat=1142985\n","Skipping y_hat=2076685\n"," 22% 6/27 [00:00<00:00, 21.96it/s]Skipping y_hat=4865710\n"," 33% 9/27 [00:00<00:00, 21.99it/s]Skipping y_hat=708544\n","Skipping y_hat=1234144\n"," 44% 12/27 [00:00<00:00, 21.84it/s]Skipping y_hat=4512216\n"," 56% 15/27 [00:00<00:00, 21.75it/s]Skipping y_hat=6405886\n","Skipping y_hat=2251529\n"," 67% 18/27 [00:00<00:00, 21.58it/s]Skipping y_hat=1041104\n","Skipping y_hat=9084346\n"," 78% 21/27 [00:00<00:00, 21.59it/s]Skipping y_hat=475291\n","100% 27/27 [00:01<00:00, 22.32it/s]\n","accuracy of 3000 examples: 2986/3000 (99.53333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.81it/s]\n","accuracy of 10000 examples: 9949/10000 (99.49%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3448772\n","Skipping y_hat=8317205\n","Skipping y_hat=4695571\n"," 11% 3/27 [00:00<00:01, 23.12it/s]Skipping y_hat=3972014\n","Skipping y_hat=2276185\n"," 22% 6/27 [00:00<00:00, 22.98it/s]Skipping y_hat=1742804\n","Skipping y_hat=4700627\n","Skipping y_hat=2324143\n"," 33% 9/27 [00:00<00:00, 23.10it/s]Skipping y_hat=8860645\n","Skipping y_hat=6756227\n","Skipping y_hat=4167131\n","Skipping y_hat=7220096\n"," 44% 12/27 [00:00<00:00, 22.70it/s]Skipping y_hat=3282885\n"," 56% 15/27 [00:00<00:00, 22.85it/s]Skipping y_hat=8469791\n"," 67% 18/27 [00:00<00:00, 22.85it/s]Skipping y_hat=4713272\n"," 78% 21/27 [00:00<00:00, 22.88it/s]Skipping y_hat=571608\n","Skipping y_hat=475291\n","100% 27/27 [00:01<00:00, 23.13it/s]\n","accuracy of 3000 examples: 2983/3000 (99.43333333333332%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.43%\n","\n","iter 855: train loss 0.9610, val loss 0.9366\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4404941\n","Skipping y_hat=5311480\n","Skipping y_hat=4482073\n"," 11% 3/27 [00:00<00:01, 21.98it/s]Skipping y_hat=2456275\n"," 22% 6/27 [00:00<00:00, 21.96it/s]Skipping y_hat=3172816\n","Skipping y_hat=5760709\n"," 33% 9/27 [00:00<00:00, 21.88it/s]Skipping y_hat=3818436\n","Skipping y_hat=3671496\n","Skipping y_hat=7764022\n"," 44% 12/27 [00:00<00:00, 21.91it/s]Skipping y_hat=6798447\n"," 56% 15/27 [00:00<00:00, 21.82it/s]Skipping y_hat=12008\n","Skipping y_hat=2984075\n"," 67% 18/27 [00:00<00:00, 22.10it/s]Skipping y_hat=5579144\n","Skipping y_hat=2902273\n","100% 27/27 [00:01<00:00, 22.60it/s]\n","accuracy of 3000 examples: 2986/3000 (99.53333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.78it/s]\n","accuracy of 10000 examples: 9973/10000 (99.72999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5311480\n","Skipping y_hat=5500963\n","Skipping y_hat=4482073\n"," 11% 3/27 [00:00<00:01, 22.45it/s]Skipping y_hat=5683720\n"," 22% 6/27 [00:00<00:00, 22.39it/s]Skipping y_hat=5760709\n"," 44% 12/27 [00:00<00:00, 22.73it/s]Skipping y_hat=5673958\n"," 56% 15/27 [00:00<00:00, 22.43it/s]Skipping y_hat=6033709\n"," 67% 18/27 [00:00<00:00, 22.64it/s]Skipping y_hat=1681201\n","Skipping y_hat=5579144\n","Skipping y_hat=1500884\n","Skipping y_hat=1607609\n"," 89% 24/27 [00:01<00:00, 22.88it/s]Skipping y_hat=865711\n","100% 27/27 [00:01<00:00, 22.87it/s]\n","accuracy of 3000 examples: 2988/3000 (99.6%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.60%\n","\n","iter 860: train loss 0.9534, val loss 0.9475\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7191838\n"," 11% 3/27 [00:00<00:01, 22.47it/s]Skipping y_hat=6919901\n","Skipping y_hat=8395255\n","Skipping y_hat=2213353\n","Skipping y_hat=6517304\n"," 22% 6/27 [00:00<00:00, 22.24it/s]Skipping y_hat=4891795\n"," 44% 12/27 [00:00<00:00, 22.31it/s]Skipping y_hat=6330538\n","Skipping y_hat=839494\n"," 67% 18/27 [00:00<00:00, 22.59it/s]Skipping y_hat=4077032\n","Skipping y_hat=1784472\n"," 78% 21/27 [00:00<00:00, 22.43it/s]Skipping y_hat=55090\n","Skipping y_hat=532331\n"," 89% 24/27 [00:01<00:00, 22.98it/s]Skipping y_hat=462525\n","Skipping y_hat=104752\n","100% 27/27 [00:01<00:00, 22.80it/s]\n","accuracy of 3000 examples: 2986/3000 (99.53333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.77it/s]\n","accuracy of 10000 examples: 9967/10000 (99.67%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8757323\n","Skipping y_hat=7191838\n","Skipping y_hat=1964141\n"," 11% 3/27 [00:00<00:01, 21.86it/s]Skipping y_hat=2456275\n","Skipping y_hat=6919901\n","Skipping y_hat=8395255\n"," 22% 6/27 [00:00<00:00, 22.32it/s]Skipping y_hat=1752704\n"," 44% 12/27 [00:00<00:00, 22.55it/s]Skipping y_hat=1973908\n","Skipping y_hat=6330538\n"," 56% 15/27 [00:00<00:00, 22.46it/s]Skipping y_hat=5421971\n"," 67% 18/27 [00:00<00:00, 22.43it/s]Skipping y_hat=5896313\n","Skipping y_hat=4077032\n"," 89% 24/27 [00:01<00:00, 23.19it/s]Skipping y_hat=652661\n","Skipping y_hat=104752\n","100% 27/27 [00:01<00:00, 22.86it/s]\n","accuracy of 3000 examples: 2985/3000 (99.5%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.50%\n","\n","iter 865: train loss 0.9543, val loss 0.9397\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5325731\n","Skipping y_hat=1663550\n"," 11% 3/27 [00:00<00:01, 21.88it/s]Skipping y_hat=5183720\n"," 22% 6/27 [00:00<00:00, 21.75it/s]Skipping y_hat=2196020\n","Skipping y_hat=5686649\n"," 33% 9/27 [00:00<00:00, 22.08it/s]Skipping y_hat=1433537\n"," 56% 15/27 [00:00<00:00, 22.57it/s]Skipping y_hat=6252119\n","Skipping y_hat=1760944\n","Skipping y_hat=5411771\n"," 67% 18/27 [00:00<00:00, 22.64it/s]Skipping y_hat=4077032\n"," 78% 21/27 [00:00<00:00, 22.40it/s]Skipping y_hat=55090\n"," 89% 24/27 [00:01<00:00, 23.12it/s]Skipping y_hat=395531\n","Skipping y_hat=350719\n","100% 27/27 [00:01<00:00, 22.83it/s]\n","accuracy of 3000 examples: 2987/3000 (99.56666666666666%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.88it/s]\n","accuracy of 10000 examples: 9965/10000 (99.65%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4438772\n","Skipping y_hat=5325731\n"," 11% 3/27 [00:00<00:01, 22.76it/s]Skipping y_hat=8740378\n","Skipping y_hat=5183720\n"," 22% 6/27 [00:00<00:00, 22.91it/s]Skipping y_hat=5686649\n"," 56% 15/27 [00:00<00:00, 22.47it/s]Skipping y_hat=1760944\n","Skipping y_hat=3144086\n","Skipping y_hat=5411771\n"," 67% 18/27 [00:00<00:00, 22.58it/s]Skipping y_hat=1671101\n","Skipping y_hat=2622770\n","Skipping y_hat=4077032\n"," 78% 21/27 [00:00<00:00, 22.33it/s]Skipping y_hat=55090\n"," 89% 24/27 [00:01<00:00, 22.60it/s]Skipping y_hat=395531\n","Skipping y_hat=235504\n","100% 27/27 [00:01<00:00, 22.82it/s]\n","accuracy of 3000 examples: 2986/3000 (99.53333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.53%\n","\n","iter 870: train loss 0.9507, val loss 0.9391\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4481073\n"," 11% 3/27 [00:00<00:01, 21.97it/s]Skipping y_hat=6104771\n","Skipping y_hat=2371706\n","Skipping y_hat=6910001\n"," 22% 6/27 [00:00<00:00, 22.19it/s]Skipping y_hat=7387811\n","Skipping y_hat=8689211\n"," 33% 9/27 [00:00<00:00, 22.11it/s]Skipping y_hat=5091211\n","Skipping y_hat=1334537\n"," 44% 12/27 [00:00<00:00, 21.94it/s]Skipping y_hat=5991355\n","Skipping y_hat=3391127\n"," 56% 15/27 [00:00<00:00, 21.85it/s]Skipping y_hat=6062662\n"," 67% 18/27 [00:00<00:00, 21.90it/s]Skipping y_hat=1671101\n"," 89% 24/27 [00:01<00:00, 22.48it/s]Skipping y_hat=298794\n","Skipping y_hat=159877\n","Skipping y_hat=298747\n","100% 27/27 [00:01<00:00, 22.36it/s]\n","accuracy of 3000 examples: 2985/3000 (99.5%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.45it/s]\n","accuracy of 10000 examples: 9956/10000 (99.56%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1964141\n","Skipping y_hat=4481073\n"," 11% 3/27 [00:00<00:01, 22.07it/s]Skipping y_hat=6104771\n","Skipping y_hat=3751874\n","Skipping y_hat=6910001\n"," 22% 6/27 [00:00<00:00, 21.60it/s]Skipping y_hat=3560005\n","Skipping y_hat=7387811\n","Skipping y_hat=4893895\n","Skipping y_hat=8689211\n"," 33% 9/27 [00:00<00:00, 21.28it/s]Skipping y_hat=5091211\n","Skipping y_hat=4255046\n"," 44% 12/27 [00:00<00:00, 21.14it/s]Skipping y_hat=5991355\n","Skipping y_hat=3391127\n","Skipping y_hat=6954900\n"," 56% 15/27 [00:00<00:00, 21.14it/s]Skipping y_hat=2271529\n","Skipping y_hat=1760944\n","Skipping y_hat=7341191\n"," 67% 18/27 [00:00<00:00, 21.15it/s]Skipping y_hat=1671101\n","Skipping y_hat=6878737\n","Skipping y_hat=6250114\n"," 78% 21/27 [00:00<00:00, 21.06it/s]Skipping y_hat=55090\n","Skipping y_hat=58525\n","Skipping y_hat=475291\n"," 89% 24/27 [00:01<00:00, 21.71it/s]Skipping y_hat=298794\n","100% 27/27 [00:01<00:00, 21.69it/s]\n","accuracy of 3000 examples: 2976/3000 (99.2%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.20%\n","\n","iter 875: train loss 0.9512, val loss 0.9457\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.57it/s]Skipping y_hat=7372684\n"," 44% 12/27 [00:00<00:00, 22.66it/s]Skipping y_hat=3089734\n"," 56% 15/27 [00:00<00:00, 22.58it/s]Skipping y_hat=2271529\n"," 67% 18/27 [00:00<00:00, 22.67it/s]Skipping y_hat=1781877\n"," 78% 21/27 [00:00<00:00, 22.72it/s]Skipping y_hat=279687\n"," 89% 24/27 [00:01<00:00, 23.30it/s]Skipping y_hat=298794\n","Skipping y_hat=549056\n","100% 27/27 [00:01<00:00, 23.12it/s]\n","accuracy of 3000 examples: 2993/3000 (99.76666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.95it/s]\n","accuracy of 10000 examples: 9992/10000 (99.92%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4482073\n"," 11% 3/27 [00:00<00:01, 23.07it/s]Skipping y_hat=2371706\n","Skipping y_hat=7240322\n"," 33% 9/27 [00:00<00:00, 22.82it/s]Skipping y_hat=5201162\n"," 44% 12/27 [00:00<00:00, 22.76it/s]Skipping y_hat=3089734\n"," 56% 15/27 [00:00<00:00, 22.46it/s]Skipping y_hat=2271529\n"," 67% 18/27 [00:00<00:00, 22.44it/s]Skipping y_hat=3474076\n"," 78% 21/27 [00:00<00:00, 22.26it/s]Skipping y_hat=55990\n"," 89% 24/27 [00:01<00:00, 22.84it/s]Skipping y_hat=298794\n","100% 27/27 [00:01<00:00, 22.84it/s]\n","accuracy of 3000 examples: 2991/3000 (99.7%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.70%\n","\n","iter 880: train loss 0.9482, val loss 0.9501\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8786420\n","Skipping y_hat=5500963\n"," 22% 6/27 [00:00<00:00, 21.92it/s]Skipping y_hat=6687427\n"," 44% 12/27 [00:00<00:00, 22.13it/s]Skipping y_hat=5247820\n"," 56% 15/27 [00:00<00:00, 22.02it/s]Skipping y_hat=8847082\n"," 67% 18/27 [00:00<00:00, 22.05it/s]Skipping y_hat=7243642\n","Skipping y_hat=5220427\n"," 78% 21/27 [00:00<00:00, 21.91it/s]Skipping y_hat=682732\n","100% 27/27 [00:01<00:00, 22.48it/s]\n","accuracy of 3000 examples: 2992/3000 (99.73333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.61it/s]\n","accuracy of 10000 examples: 9963/10000 (99.63%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8786420\n"," 11% 3/27 [00:00<00:01, 21.60it/s]Skipping y_hat=4586590\n"," 22% 6/27 [00:00<00:00, 21.68it/s]Skipping y_hat=2397225\n"," 33% 9/27 [00:00<00:00, 21.67it/s]Skipping y_hat=1426870\n","Skipping y_hat=5201162\n"," 44% 12/27 [00:00<00:00, 21.52it/s]Skipping y_hat=5247820\n","Skipping y_hat=3089734\n"," 56% 15/27 [00:00<00:00, 21.55it/s]Skipping y_hat=3832926\n"," 67% 18/27 [00:00<00:00, 21.66it/s]Skipping y_hat=7243642\n","Skipping y_hat=996288\n"," 78% 21/27 [00:00<00:00, 21.57it/s]Skipping y_hat=682732\n"," 89% 24/27 [00:01<00:00, 22.41it/s]Skipping y_hat=556660\n","100% 27/27 [00:01<00:00, 22.16it/s]\n","accuracy of 3000 examples: 2988/3000 (99.6%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.60%\n","\n","iter 885: train loss 0.9563, val loss 0.9475\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.49it/s]Skipping y_hat=1872167\n","Skipping y_hat=8395255\n","Skipping y_hat=3535561\n"," 22% 6/27 [00:00<00:00, 22.34it/s]Skipping y_hat=2423143\n"," 33% 9/27 [00:00<00:00, 22.59it/s]Skipping y_hat=3818436\n","Skipping y_hat=6017912\n","Skipping y_hat=6179140\n"," 44% 12/27 [00:00<00:00, 22.49it/s]Skipping y_hat=7438672\n","Skipping y_hat=5247820\n"," 56% 15/27 [00:00<00:00, 22.45it/s]Skipping y_hat=3008056\n"," 67% 18/27 [00:00<00:00, 22.49it/s]Skipping y_hat=2444578\n"," 78% 21/27 [00:00<00:00, 22.53it/s]Skipping y_hat=10604\n"," 89% 24/27 [00:01<00:00, 23.22it/s]Skipping y_hat=318364\n","Skipping y_hat=280880\n","Skipping y_hat=855611\n","100% 27/27 [00:01<00:00, 22.94it/s]\n","accuracy of 3000 examples: 2985/3000 (99.5%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.77it/s]\n","accuracy of 10000 examples: 9962/10000 (99.62%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6212615\n","Skipping y_hat=1310585\n"," 11% 3/27 [00:00<00:01, 22.59it/s]Skipping y_hat=1872167\n","Skipping y_hat=8395255\n","Skipping y_hat=5937190\n","Skipping y_hat=3535561\n"," 22% 6/27 [00:00<00:00, 22.12it/s]Skipping y_hat=7153838\n","Skipping y_hat=5766857\n"," 33% 9/27 [00:00<00:00, 21.95it/s]Skipping y_hat=3818436\n","Skipping y_hat=6917912\n","Skipping y_hat=6017471\n","Skipping y_hat=6179140\n","Skipping y_hat=7871851\n"," 44% 12/27 [00:00<00:00, 22.16it/s]Skipping y_hat=1973908\n","Skipping y_hat=3249971\n"," 67% 18/27 [00:00<00:00, 22.42it/s]Skipping y_hat=6100343\n","Skipping y_hat=4119886\n"," 78% 21/27 [00:00<00:00, 22.45it/s]Skipping y_hat=220040\n"," 89% 24/27 [00:01<00:00, 23.11it/s]Skipping y_hat=546110\n","Skipping y_hat=238688\n","Skipping y_hat=310502\n","100% 27/27 [00:01<00:00, 22.79it/s]\n","accuracy of 3000 examples: 2979/3000 (99.3%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.30%\n","\n","iter 890: train loss 0.9580, val loss 0.9437\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2340920\n","Skipping y_hat=4308138\n","Skipping y_hat=5325731\n"," 11% 3/27 [00:00<00:01, 22.21it/s]Skipping y_hat=1872167\n"," 22% 6/27 [00:00<00:00, 22.06it/s]Skipping y_hat=3541372\n"," 33% 9/27 [00:00<00:00, 22.46it/s]Skipping y_hat=6017912\n","Skipping y_hat=6917471\n"," 56% 15/27 [00:00<00:00, 22.56it/s]Skipping y_hat=5589434\n","Skipping y_hat=6011963\n"," 67% 18/27 [00:00<00:00, 22.43it/s]Skipping y_hat=1611840\n"," 89% 24/27 [00:01<00:00, 23.05it/s]Skipping y_hat=30076\n","Skipping y_hat=546110\n","100% 27/27 [00:01<00:00, 22.87it/s]\n","accuracy of 3000 examples: 2988/3000 (99.6%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.13it/s]\n","accuracy of 10000 examples: 9977/10000 (99.77000000000001%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4308138\n","Skipping y_hat=3309254\n"," 33% 9/27 [00:00<00:00, 22.06it/s]Skipping y_hat=6017912\n","Skipping y_hat=6917471\n"," 44% 12/27 [00:00<00:00, 21.73it/s]Skipping y_hat=3302918\n"," 78% 21/27 [00:00<00:00, 21.85it/s]Skipping y_hat=10604\n"," 89% 24/27 [00:01<00:00, 22.63it/s]Skipping y_hat=213336\n","Skipping y_hat=546110\n","100% 27/27 [00:01<00:00, 22.37it/s]\n","accuracy of 3000 examples: 2992/3000 (99.73333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.73%\n","\n","iter 895: train loss 0.9466, val loss 0.9427\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6613559\n","Skipping y_hat=1837693\n","Skipping y_hat=7666399\n"," 11% 3/27 [00:00<00:01, 22.65it/s]Skipping y_hat=3526561\n"," 44% 12/27 [00:00<00:00, 22.94it/s]Skipping y_hat=5247820\n","Skipping y_hat=6621110\n"," 89% 24/27 [00:01<00:00, 23.29it/s]Skipping y_hat=865711\n","100% 27/27 [00:01<00:00, 23.13it/s]\n","accuracy of 3000 examples: 2992/3000 (99.73333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.88it/s]\n","accuracy of 10000 examples: 9967/10000 (99.67%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4695571\n","Skipping y_hat=1837693\n","Skipping y_hat=7666399\n","Skipping y_hat=1673650\n"," 22% 6/27 [00:00<00:00, 22.74it/s]Skipping y_hat=1606279\n","Skipping y_hat=6478337\n"," 33% 9/27 [00:00<00:00, 22.50it/s]Skipping y_hat=6919948\n","Skipping y_hat=1969336\n","Skipping y_hat=4255046\n"," 44% 12/27 [00:00<00:00, 22.61it/s]Skipping y_hat=2186690\n"," 67% 18/27 [00:00<00:00, 22.86it/s]Skipping y_hat=7111396\n"," 89% 24/27 [00:01<00:00, 23.35it/s]Skipping y_hat=865711\n","100% 27/27 [00:01<00:00, 23.17it/s]\n","accuracy of 3000 examples: 2987/3000 (99.56666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.57%\n","\n","iter 900: train loss 0.9640, val loss 0.9459\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.80it/s]Skipping y_hat=6104771\n","Skipping y_hat=6910001\n","Skipping y_hat=4002833\n"," 22% 6/27 [00:00<00:00, 22.55it/s]Skipping y_hat=5028813\n","Skipping y_hat=5938785\n","Skipping y_hat=7791011\n"," 33% 9/27 [00:00<00:00, 22.07it/s]Skipping y_hat=4148470\n","Skipping y_hat=7155442\n"," 44% 12/27 [00:00<00:00, 22.36it/s]Skipping y_hat=5991355\n","Skipping y_hat=2139011\n"," 56% 15/27 [00:00<00:00, 22.33it/s]Skipping y_hat=4620562\n","Skipping y_hat=4145086\n","Skipping y_hat=7331291\n"," 67% 18/27 [00:00<00:00, 22.49it/s]Skipping y_hat=1671101\n","100% 27/27 [00:01<00:00, 22.97it/s]\n","accuracy of 3000 examples: 2986/3000 (99.53333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.65it/s]\n","accuracy of 10000 examples: 9961/10000 (99.61%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.71it/s]Skipping y_hat=6919901\n"," 22% 6/27 [00:00<00:00, 22.43it/s]Skipping y_hat=5028813\n","Skipping y_hat=5938785\n","Skipping y_hat=7791011\n"," 33% 9/27 [00:00<00:00, 21.85it/s]Skipping y_hat=4614088\n","Skipping y_hat=4148470\n"," 44% 12/27 [00:00<00:00, 21.57it/s]Skipping y_hat=2139011\n"," 56% 15/27 [00:00<00:00, 21.86it/s]Skipping y_hat=3832806\n","Skipping y_hat=1989568\n","Skipping y_hat=8752936\n","Skipping y_hat=4145086\n","Skipping y_hat=7431291\n"," 67% 18/27 [00:00<00:00, 21.53it/s]Skipping y_hat=1671101\n","Skipping y_hat=5896313\n","Skipping y_hat=1221980\n"," 89% 24/27 [00:01<00:00, 22.17it/s]Skipping y_hat=547010\n","100% 27/27 [00:01<00:00, 22.05it/s]\n","accuracy of 3000 examples: 2984/3000 (99.46666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.47%\n","\n","iter 905: train loss 0.9504, val loss 0.9425\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2553994\n","Skipping y_hat=2824576\n"," 22% 6/27 [00:00<00:00, 21.75it/s]Skipping y_hat=5028813\n"," 33% 9/27 [00:00<00:00, 21.64it/s]Skipping y_hat=632770\n","Skipping y_hat=4227403\n","Skipping y_hat=4479877\n"," 44% 12/27 [00:00<00:00, 21.50it/s]Skipping y_hat=5247820\n"," 56% 15/27 [00:00<00:00, 21.69it/s]Skipping y_hat=7431463\n","Skipping y_hat=2783700\n"," 67% 18/27 [00:00<00:00, 21.72it/s]Skipping y_hat=5896313\n","Skipping y_hat=5542867\n"," 78% 21/27 [00:00<00:00, 21.53it/s]Skipping y_hat=55090\n","Skipping y_hat=475291\n","100% 27/27 [00:01<00:00, 21.97it/s]\n","accuracy of 3000 examples: 2987/3000 (99.56666666666666%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.97it/s]\n","accuracy of 10000 examples: 9946/10000 (99.46000000000001%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4482073\n"," 22% 6/27 [00:00<00:00, 23.02it/s]Skipping y_hat=1538617\n","Skipping y_hat=7723207\n"," 33% 9/27 [00:00<00:00, 22.88it/s]Skipping y_hat=7918948\n","Skipping y_hat=4227403\n","Skipping y_hat=4479877\n"," 44% 12/27 [00:00<00:00, 22.85it/s]Skipping y_hat=7584994\n","Skipping y_hat=5247820\n","Skipping y_hat=6731110\n"," 56% 15/27 [00:00<00:00, 22.91it/s]Skipping y_hat=7431463\n","Skipping y_hat=7441291\n","Skipping y_hat=2783700\n"," 67% 18/27 [00:00<00:00, 22.52it/s]Skipping y_hat=6250114\n","Skipping y_hat=4119886\n","Skipping y_hat=5532867\n"," 78% 21/27 [00:00<00:00, 22.61it/s]Skipping y_hat=55090\n","Skipping y_hat=475291\n","100% 27/27 [00:01<00:00, 23.20it/s]\n","accuracy of 3000 examples: 2983/3000 (99.43333333333332%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.43%\n","\n","iter 910: train loss 0.9534, val loss 0.9410\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4381073\n"," 11% 3/27 [00:00<00:01, 22.62it/s]Skipping y_hat=5944699\n","Skipping y_hat=6104771\n","Skipping y_hat=1881167\n"," 44% 12/27 [00:00<00:00, 23.04it/s]Skipping y_hat=3536272\n","Skipping y_hat=6954900\n"," 67% 18/27 [00:00<00:00, 23.15it/s]Skipping y_hat=6011396\n"," 89% 24/27 [00:01<00:00, 23.52it/s]Skipping y_hat=466300\n","100% 27/27 [00:01<00:00, 23.39it/s]\n","accuracy of 3000 examples: 2992/3000 (99.73333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.77it/s]\n","accuracy of 10000 examples: 9983/10000 (99.83%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5205815\n"," 11% 3/27 [00:00<00:01, 22.08it/s]Skipping y_hat=5944699\n","Skipping y_hat=6104771\n","Skipping y_hat=1881167\n"," 22% 6/27 [00:00<00:00, 21.87it/s]Skipping y_hat=7791011\n"," 33% 9/27 [00:00<00:00, 21.79it/s]Skipping y_hat=4969381\n","Skipping y_hat=4227403\n","100% 27/27 [00:01<00:00, 22.48it/s]\n","accuracy of 3000 examples: 2993/3000 (99.76666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.77%\n","\n","iter 915: train loss 0.9488, val loss 0.9394\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2101535\n"," 11% 3/27 [00:00<00:01, 23.12it/s]Skipping y_hat=7220422\n","Skipping y_hat=1881167\n","Skipping y_hat=6910001\n"," 33% 9/27 [00:00<00:00, 23.05it/s]Skipping y_hat=5869067\n"," 44% 12/27 [00:00<00:00, 22.67it/s]Skipping y_hat=8089525\n"," 56% 15/27 [00:00<00:00, 22.86it/s]Skipping y_hat=5270898\n"," 67% 18/27 [00:00<00:00, 22.77it/s]Skipping y_hat=2762506\n","Skipping y_hat=6205179\n"," 78% 21/27 [00:00<00:00, 22.64it/s]Skipping y_hat=568844\n","100% 27/27 [00:01<00:00, 23.25it/s]\n","accuracy of 3000 examples: 2990/3000 (99.66666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.96it/s]\n","accuracy of 10000 examples: 9975/10000 (99.75%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 23.10it/s]Skipping y_hat=1881167\n","Skipping y_hat=6910001\n"," 22% 6/27 [00:00<00:00, 23.03it/s]Skipping y_hat=3574952\n"," 44% 12/27 [00:00<00:00, 22.91it/s]Skipping y_hat=8089525\n"," 67% 18/27 [00:00<00:00, 22.95it/s]Skipping y_hat=3755196\n"," 78% 21/27 [00:00<00:00, 22.86it/s]Skipping y_hat=568844\n","100% 27/27 [00:01<00:00, 23.33it/s]\n","accuracy of 3000 examples: 2994/3000 (99.8%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.80%\n","\n","iter 920: train loss 0.9541, val loss 0.9515\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5515379\n"," 11% 3/27 [00:00<00:01, 22.55it/s]Skipping y_hat=1881167\n"," 22% 6/27 [00:00<00:00, 21.90it/s]Skipping y_hat=7153838\n"," 56% 15/27 [00:00<00:00, 22.49it/s]Skipping y_hat=4381244\n"," 89% 24/27 [00:01<00:00, 23.38it/s]Skipping y_hat=235504\n","100% 27/27 [00:01<00:00, 23.06it/s]\n","accuracy of 3000 examples: 2995/3000 (99.83333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.81it/s]\n","accuracy of 10000 examples: 9992/10000 (99.92%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.21it/s]Skipping y_hat=1881167\n"," 22% 6/27 [00:00<00:00, 22.42it/s]Skipping y_hat=7153838\n"," 33% 9/27 [00:00<00:00, 22.46it/s]Skipping y_hat=6168140\n"," 89% 24/27 [00:01<00:00, 22.46it/s]Skipping y_hat=466800\n","100% 27/27 [00:01<00:00, 22.47it/s]\n","accuracy of 3000 examples: 2996/3000 (99.86666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.87%\n","\n","iter 925: train loss 0.9480, val loss 0.9487\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=3441216\n"," 11% 3/27 [00:00<00:01, 22.49it/s]Skipping y_hat=6919001\n"," 33% 9/27 [00:00<00:00, 22.41it/s]Skipping y_hat=1433537\n","Skipping y_hat=6248623\n"," 44% 12/27 [00:00<00:00, 22.41it/s]Skipping y_hat=7697177\n"," 56% 15/27 [00:00<00:00, 22.52it/s]Skipping y_hat=4381244\n","Skipping y_hat=2260972\n","Skipping y_hat=7810874\n"," 89% 24/27 [00:01<00:00, 22.65it/s]Skipping y_hat=297894\n","Skipping y_hat=388863\n","Skipping y_hat=104862\n","100% 27/27 [00:01<00:00, 22.51it/s]\n","accuracy of 3000 examples: 2989/3000 (99.63333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.49it/s]\n","accuracy of 10000 examples: 9984/10000 (99.83999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 22% 6/27 [00:00<00:00, 22.05it/s]Skipping y_hat=2497325\n"," 33% 9/27 [00:00<00:00, 21.79it/s]Skipping y_hat=5387887\n","Skipping y_hat=1433537\n","Skipping y_hat=6168140\n"," 44% 12/27 [00:00<00:00, 21.37it/s]Skipping y_hat=2968942\n","Skipping y_hat=5452876\n"," 56% 15/27 [00:00<00:00, 21.29it/s]Skipping y_hat=7810874\n"," 67% 18/27 [00:00<00:00, 21.73it/s]Skipping y_hat=3454253\n"," 89% 24/27 [00:01<00:00, 22.59it/s]Skipping y_hat=104862\n","100% 27/27 [00:01<00:00, 22.27it/s]\n","accuracy of 3000 examples: 2991/3000 (99.7%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.70%\n","\n","iter 930: train loss 0.9566, val loss 0.9406\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7077185\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 20.69it/s]Skipping y_hat=6919901\n","Skipping y_hat=4171904\n"," 22% 6/27 [00:00<00:01, 20.71it/s]Skipping y_hat=3560905\n","Skipping y_hat=3322728\n","Skipping y_hat=4964710\n"," 33% 9/27 [00:00<00:00, 20.75it/s]Skipping y_hat=4997944\n","Skipping y_hat=4999456\n","Skipping y_hat=6168140\n"," 44% 12/27 [00:00<00:00, 20.66it/s]Skipping y_hat=4216222\n","Skipping y_hat=7697177\n","Skipping y_hat=4866672\n","Skipping y_hat=3420316\n","Skipping y_hat=5247820\n"," 56% 15/27 [00:00<00:00, 20.41it/s]Skipping y_hat=7909874\n"," 67% 18/27 [00:00<00:00, 20.66it/s]Skipping y_hat=4119886\n","Skipping y_hat=7712803\n"," 78% 21/27 [00:01<00:00, 20.69it/s]Skipping y_hat=532570\n","100% 27/27 [00:01<00:00, 21.21it/s]\n","accuracy of 3000 examples: 2981/3000 (99.36666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.24it/s]\n","accuracy of 10000 examples: 9981/10000 (99.81%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5298490\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 22.05it/s]Skipping y_hat=6919901\n"," 22% 6/27 [00:00<00:00, 21.92it/s]Skipping y_hat=3561905\n"," 33% 9/27 [00:00<00:00, 21.80it/s]Skipping y_hat=4999456\n","Skipping y_hat=6168140\n"," 44% 12/27 [00:00<00:00, 21.52it/s]Skipping y_hat=4866672\n","Skipping y_hat=5247820\n"," 56% 15/27 [00:00<00:00, 21.52it/s]Skipping y_hat=6148288\n","Skipping y_hat=7846576\n","Skipping y_hat=2260972\n"," 67% 18/27 [00:00<00:00, 21.47it/s]Skipping y_hat=4119886\n"," 78% 21/27 [00:00<00:00, 21.45it/s]Skipping y_hat=532570\n","100% 27/27 [00:01<00:00, 22.02it/s]\n","accuracy of 3000 examples: 2987/3000 (99.56666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.57%\n","\n","iter 935: train loss 0.9481, val loss 0.9376\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5300480\n","Skipping y_hat=5509963\n"," 11% 3/27 [00:00<00:01, 21.58it/s]Skipping y_hat=5300280\n","Skipping y_hat=5550080\n","Skipping y_hat=6910001\n"," 33% 9/27 [00:00<00:00, 21.85it/s]Skipping y_hat=4999456\n","Skipping y_hat=8500716\n","Skipping y_hat=5165974\n"," 44% 12/27 [00:00<00:00, 21.74it/s]Skipping y_hat=4866672\n","Skipping y_hat=5659273\n"," 56% 15/27 [00:00<00:00, 21.66it/s]Skipping y_hat=8669791\n"," 67% 18/27 [00:00<00:00, 21.72it/s]Skipping y_hat=5996413\n","Skipping y_hat=2461496\n","Skipping y_hat=1171747\n","Skipping y_hat=4119886\n"," 78% 21/27 [00:00<00:00, 21.21it/s]Skipping y_hat=532570\n"," 89% 24/27 [00:01<00:00, 22.01it/s]Skipping y_hat=350719\n","Skipping y_hat=865711\n","Skipping y_hat=4762\n","100% 27/27 [00:01<00:00, 21.95it/s]\n","accuracy of 3000 examples: 2980/3000 (99.33333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.75it/s]\n","accuracy of 10000 examples: 9958/10000 (99.58%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5509963\n","Skipping y_hat=6555708\n"," 11% 3/27 [00:00<00:01, 21.88it/s]Skipping y_hat=5300280\n","Skipping y_hat=4171904\n","Skipping y_hat=5284720\n"," 33% 9/27 [00:00<00:00, 22.70it/s]Skipping y_hat=7730416\n","Skipping y_hat=6179140\n","Skipping y_hat=5165974\n"," 44% 12/27 [00:00<00:00, 22.65it/s]Skipping y_hat=7708177\n","Skipping y_hat=4866672\n","Skipping y_hat=5659273\n","Skipping y_hat=2139011\n"," 56% 15/27 [00:00<00:00, 22.70it/s]Skipping y_hat=5270898\n","Skipping y_hat=1329472\n"," 67% 18/27 [00:00<00:00, 22.70it/s]Skipping y_hat=5906413\n","Skipping y_hat=2461496\n","Skipping y_hat=4119886\n"," 89% 24/27 [00:01<00:00, 23.20it/s]Skipping y_hat=350719\n","100% 27/27 [00:01<00:00, 23.05it/s]\n","accuracy of 3000 examples: 2982/3000 (99.4%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.40%\n","\n","iter 940: train loss 0.9579, val loss 0.9515\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904904\n","Skipping y_hat=8328205\n","Skipping y_hat=5694910\n","Skipping y_hat=4695571\n","Skipping y_hat=1999009\n"," 11% 3/27 [00:00<00:01, 22.04it/s]Skipping y_hat=3710802\n","Skipping y_hat=5993235\n"," 22% 6/27 [00:00<00:00, 22.20it/s]Skipping y_hat=5599882\n","Skipping y_hat=4951672\n","Skipping y_hat=2983987\n","Skipping y_hat=5966857\n"," 33% 9/27 [00:00<00:00, 21.94it/s]Skipping y_hat=1459585\n"," 44% 12/27 [00:00<00:00, 22.21it/s]Skipping y_hat=773074\n","Skipping y_hat=2191484\n","Skipping y_hat=7684894\n"," 56% 15/27 [00:00<00:00, 22.16it/s]Skipping y_hat=772714\n","Skipping y_hat=2269972\n"," 67% 18/27 [00:00<00:00, 22.38it/s]Skipping y_hat=8146693\n","Skipping y_hat=3430102\n"," 78% 21/27 [00:00<00:00, 22.43it/s]Skipping y_hat=475291\n"," 89% 24/27 [00:01<00:00, 23.03it/s]Skipping y_hat=71340\n","100% 27/27 [00:01<00:00, 22.74it/s]\n","accuracy of 3000 examples: 2979/3000 (99.3%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.89it/s]\n","accuracy of 10000 examples: 9956/10000 (99.56%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904904\n","Skipping y_hat=4702960\n","Skipping y_hat=3942477\n","Skipping y_hat=4695571\n","Skipping y_hat=2088558\n","Skipping y_hat=1999009\n","Skipping y_hat=6059302\n"," 11% 3/27 [00:00<00:01, 21.74it/s]Skipping y_hat=3710802\n","Skipping y_hat=5993235\n","Skipping y_hat=2206235\n","Skipping y_hat=4427488\n","Skipping y_hat=8395255\n"," 33% 9/27 [00:00<00:00, 22.50it/s]Skipping y_hat=5999915\n","Skipping y_hat=1459585\n"," 44% 12/27 [00:00<00:00, 22.54it/s]Skipping y_hat=1430138\n","Skipping y_hat=2191484\n","Skipping y_hat=7684894\n"," 56% 15/27 [00:00<00:00, 22.61it/s]Skipping y_hat=6528430\n","Skipping y_hat=772714\n","Skipping y_hat=7555519\n"," 67% 18/27 [00:00<00:00, 22.35it/s]Skipping y_hat=4384440\n","Skipping y_hat=2461496\n","Skipping y_hat=3430102\n"," 89% 24/27 [00:01<00:00, 23.03it/s]Skipping y_hat=71340\n","100% 27/27 [00:01<00:00, 22.88it/s]\n","accuracy of 3000 examples: 2976/3000 (99.2%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.20%\n","\n","iter 945: train loss 0.9533, val loss 0.9392\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6613559\n"," 11% 3/27 [00:00<00:01, 22.03it/s]Skipping y_hat=4877740\n"," 22% 6/27 [00:00<00:00, 22.31it/s]Skipping y_hat=5760799\n","Skipping y_hat=5219704\n"," 56% 15/27 [00:00<00:00, 22.60it/s]Skipping y_hat=3955300\n","Skipping y_hat=6033799\n","Skipping y_hat=8669791\n"," 89% 24/27 [00:01<00:00, 22.95it/s]Skipping y_hat=71340\n","Skipping y_hat=865711\n","100% 27/27 [00:01<00:00, 22.84it/s]\n","accuracy of 3000 examples: 2991/3000 (99.7%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.93it/s]\n","accuracy of 10000 examples: 9973/10000 (99.72999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6713659\n","Skipping y_hat=4448762\n"," 22% 6/27 [00:00<00:00, 23.25it/s]Skipping y_hat=5760799\n","Skipping y_hat=5696749\n"," 33% 9/27 [00:00<00:00, 23.30it/s]Skipping y_hat=3757454\n"," 56% 15/27 [00:00<00:00, 22.88it/s]Skipping y_hat=6033799\n","Skipping y_hat=5039758\n"," 67% 18/27 [00:00<00:00, 22.89it/s]Skipping y_hat=1843324\n","Skipping y_hat=5612869\n"," 89% 24/27 [00:01<00:00, 23.45it/s]Skipping y_hat=71340\n","Skipping y_hat=865711\n","100% 27/27 [00:01<00:00, 23.37it/s]\n","accuracy of 3000 examples: 2989/3000 (99.63333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.63%\n","\n","iter 950: train loss 0.9602, val loss 0.9448\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1634756\n","Skipping y_hat=8767423\n"," 11% 3/27 [00:00<00:01, 21.71it/s]Skipping y_hat=4167484\n"," 22% 6/27 [00:00<00:00, 22.00it/s]Skipping y_hat=3594952\n","Skipping y_hat=1239956\n","Skipping y_hat=5968963\n","Skipping y_hat=522120\n"," 33% 9/27 [00:00<00:00, 22.20it/s]Skipping y_hat=1649596\n"," 44% 12/27 [00:00<00:00, 22.38it/s]Skipping y_hat=7787032\n"," 56% 15/27 [00:00<00:00, 22.50it/s]Skipping y_hat=4381244\n","Skipping y_hat=7810874\n"," 67% 18/27 [00:00<00:00, 22.51it/s]Skipping y_hat=5579144\n","Skipping y_hat=4412698\n","Skipping y_hat=1969641\n","Skipping y_hat=3217861\n","Skipping y_hat=4364707\n"," 89% 24/27 [00:01<00:00, 23.05it/s]Skipping y_hat=149530\n","100% 27/27 [00:01<00:00, 22.87it/s]\n","accuracy of 3000 examples: 2983/3000 (99.43333333333332%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.72it/s]\n","accuracy of 10000 examples: 9982/10000 (99.82%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1634756\n","Skipping y_hat=8767423\n"," 11% 3/27 [00:00<00:01, 21.13it/s]Skipping y_hat=3740606\n","Skipping y_hat=1654347\n"," 22% 6/27 [00:00<00:00, 21.23it/s]Skipping y_hat=2497325\n","Skipping y_hat=3594952\n","Skipping y_hat=1239956\n"," 44% 12/27 [00:00<00:00, 21.69it/s]Skipping y_hat=7787032\n"," 56% 15/27 [00:00<00:00, 21.77it/s]Skipping y_hat=1066872\n","Skipping y_hat=4381244\n","Skipping y_hat=7810874\n"," 67% 18/27 [00:00<00:00, 21.39it/s]Skipping y_hat=4364707\n"," 78% 21/27 [00:00<00:00, 21.48it/s]Skipping y_hat=448408\n"," 89% 24/27 [00:01<00:00, 22.08it/s]Skipping y_hat=208747\n","100% 27/27 [00:01<00:00, 21.91it/s]\n","accuracy of 3000 examples: 2986/3000 (99.53333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.53%\n","\n","iter 955: train loss 0.9525, val loss 0.9472\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=8318105\n"," 22% 6/27 [00:00<00:00, 22.66it/s]Skipping y_hat=5509882\n","Skipping y_hat=1239956\n","100% 27/27 [00:01<00:00, 23.14it/s]\n","accuracy of 3000 examples: 2997/3000 (99.9%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.68it/s]\n","accuracy of 10000 examples: 9982/10000 (99.82%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7077185\n","Skipping y_hat=5097558\n"," 22% 6/27 [00:00<00:00, 22.96it/s]Skipping y_hat=1239956\n"," 33% 9/27 [00:00<00:00, 22.70it/s]Skipping y_hat=2448100\n"," 44% 12/27 [00:00<00:00, 22.69it/s]Skipping y_hat=7330528\n"," 56% 15/27 [00:00<00:00, 22.70it/s]Skipping y_hat=4381244\n"," 67% 18/27 [00:00<00:00, 22.55it/s]Skipping y_hat=5896403\n","Skipping y_hat=5747023\n"," 89% 24/27 [00:01<00:00, 23.17it/s]Skipping y_hat=353858\n","100% 27/27 [00:01<00:00, 23.06it/s]\n","accuracy of 3000 examples: 2991/3000 (99.7%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.70%\n","\n","iter 960: train loss 0.9444, val loss 0.9439\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.72it/s]Skipping y_hat=1363245\n"," 44% 12/27 [00:00<00:00, 22.45it/s]Skipping y_hat=5770048\n"," 67% 18/27 [00:00<00:00, 22.69it/s]Skipping y_hat=5747023\n"," 78% 21/27 [00:00<00:00, 22.51it/s]Skipping y_hat=55090\n","Skipping y_hat=358785\n"," 89% 24/27 [00:01<00:00, 23.22it/s]Skipping y_hat=77886\n","100% 27/27 [00:01<00:00, 23.00it/s]\n","accuracy of 3000 examples: 2994/3000 (99.8%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 23.81it/s]\n","accuracy of 10000 examples: 9992/10000 (99.92%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1673650\n"," 22% 6/27 [00:00<00:00, 22.84it/s]Skipping y_hat=3573125\n"," 33% 9/27 [00:00<00:00, 22.80it/s]Skipping y_hat=6132574\n"," 44% 12/27 [00:00<00:00, 22.77it/s]Skipping y_hat=2149111\n"," 67% 18/27 [00:00<00:00, 22.76it/s]Skipping y_hat=3393155\n"," 78% 21/27 [00:00<00:00, 22.73it/s]Skipping y_hat=55090\n","100% 27/27 [00:01<00:00, 23.24it/s]\n","accuracy of 3000 examples: 2994/3000 (99.8%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.80%\n","\n","iter 965: train loss 0.9469, val loss 0.9462\n","Using precomputed batches\n","Max number of tokens 8.\n"," 22% 6/27 [00:00<00:00, 22.71it/s]Skipping y_hat=2987629\n"," 33% 9/27 [00:00<00:00, 22.72it/s]Skipping y_hat=5803198\n"," 56% 15/27 [00:00<00:00, 22.69it/s]Skipping y_hat=7810874\n"," 89% 24/27 [00:01<00:00, 23.38it/s]Skipping y_hat=308274\n","100% 27/27 [00:01<00:00, 23.15it/s]\n","accuracy of 3000 examples: 2996/3000 (99.86666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.93it/s]\n","accuracy of 10000 examples: 9994/10000 (99.94%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 33% 9/27 [00:00<00:00, 22.58it/s]Skipping y_hat=6017471\n","100% 27/27 [00:01<00:00, 23.05it/s]\n","accuracy of 3000 examples: 2999/3000 (99.96666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.97%\n","\n","iter 970: train loss 0.9463, val loss 0.9385\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5410480\n"," 11% 3/27 [00:00<00:01, 22.59it/s]Skipping y_hat=4343023\n"," 22% 6/27 [00:00<00:00, 22.51it/s]Skipping y_hat=8865109\n"," 44% 12/27 [00:00<00:00, 22.58it/s]Skipping y_hat=3249971\n"," 56% 15/27 [00:00<00:00, 22.46it/s]Skipping y_hat=8669791\n"," 67% 18/27 [00:00<00:00, 22.64it/s]Skipping y_hat=6100838\n"," 78% 21/27 [00:00<00:00, 22.63it/s]Skipping y_hat=254930\n","100% 27/27 [00:01<00:00, 23.07it/s]\n","accuracy of 3000 examples: 2993/3000 (99.76666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.87it/s]\n","accuracy of 10000 examples: 9980/10000 (99.8%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 33% 9/27 [00:00<00:00, 22.27it/s]Skipping y_hat=4355046\n"," 44% 12/27 [00:00<00:00, 22.28it/s]Skipping y_hat=3249971\n","Skipping y_hat=6731110\n"," 78% 21/27 [00:00<00:00, 21.88it/s]Skipping y_hat=543570\n","100% 27/27 [00:01<00:00, 22.62it/s]\n","accuracy of 3000 examples: 2996/3000 (99.86666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.87%\n","\n","iter 975: train loss 0.9577, val loss 0.9426\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.69it/s]Skipping y_hat=4822982\n","Skipping y_hat=3742874\n","Skipping y_hat=6900731\n"," 22% 6/27 [00:00<00:00, 22.54it/s]Skipping y_hat=8755109\n"," 33% 9/27 [00:00<00:00, 22.70it/s]Skipping y_hat=6918848\n","Skipping y_hat=1433537\n","Skipping y_hat=7851851\n","Skipping y_hat=4255046\n"," 44% 12/27 [00:00<00:00, 22.77it/s]Skipping y_hat=3382127\n"," 56% 15/27 [00:00<00:00, 22.80it/s]Skipping y_hat=7900874\n"," 67% 18/27 [00:00<00:00, 22.75it/s]Skipping y_hat=4077032\n"," 78% 21/27 [00:00<00:00, 22.55it/s]Skipping y_hat=452348\n","100% 27/27 [00:01<00:00, 23.08it/s]\n","accuracy of 3000 examples: 2988/3000 (99.6%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.04it/s]\n","accuracy of 10000 examples: 9984/10000 (99.83999999999999%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=5325731\n","Skipping y_hat=8052650\n"," 11% 3/27 [00:00<00:01, 22.96it/s]Skipping y_hat=4822982\n","Skipping y_hat=3742874\n"," 22% 6/27 [00:00<00:00, 22.92it/s]Skipping y_hat=8755109\n","Skipping y_hat=2140368\n"," 33% 9/27 [00:00<00:00, 22.99it/s]Skipping y_hat=6918848\n","Skipping y_hat=7851851\n","Skipping y_hat=4255046\n","Skipping y_hat=8571761\n"," 44% 12/27 [00:00<00:00, 22.85it/s]Skipping y_hat=3382127\n"," 56% 15/27 [00:00<00:00, 22.59it/s]Skipping y_hat=7900874\n"," 67% 18/27 [00:00<00:00, 22.64it/s]Skipping y_hat=4077032\n","100% 27/27 [00:01<00:00, 23.22it/s]\n","accuracy of 3000 examples: 2987/3000 (99.56666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.57%\n","\n","iter 980: train loss 0.9557, val loss 0.9465\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6712559\n","Skipping y_hat=3742388\n"," 22% 6/27 [00:00<00:00, 22.73it/s]Skipping y_hat=5673740\n","Skipping y_hat=1535430\n"," 33% 9/27 [00:00<00:00, 22.83it/s]Skipping y_hat=5287787\n","Skipping y_hat=5091211\n","Skipping y_hat=2754244\n"," 67% 18/27 [00:00<00:00, 22.72it/s]Skipping y_hat=5906413\n"," 78% 21/27 [00:00<00:00, 22.70it/s]Skipping y_hat=135178\n","100% 27/27 [00:01<00:00, 23.18it/s]\n","accuracy of 3000 examples: 2991/3000 (99.7%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 22.84it/s]\n","accuracy of 10000 examples: 9979/10000 (99.79%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=1672550\n"," 11% 3/27 [00:00<00:01, 21.93it/s]Skipping y_hat=1185479\n"," 22% 6/27 [00:00<00:00, 21.93it/s]Skipping y_hat=8755109\n","Skipping y_hat=1605269\n"," 33% 9/27 [00:00<00:00, 21.80it/s]Skipping y_hat=5287787\n","Skipping y_hat=5201162\n","Skipping y_hat=4255046\n"," 44% 12/27 [00:00<00:00, 21.70it/s]Skipping y_hat=2967932\n","Skipping y_hat=7708177\n"," 56% 15/27 [00:00<00:00, 21.57it/s]Skipping y_hat=6405776\n","Skipping y_hat=2235227\n"," 67% 18/27 [00:00<00:00, 21.51it/s]Skipping y_hat=5906413\n","Skipping y_hat=2027015\n","100% 27/27 [00:01<00:00, 22.09it/s]\n","accuracy of 3000 examples: 2987/3000 (99.56666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.57%\n","\n","iter 985: train loss 0.9602, val loss 0.9460\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6712559\n","Skipping y_hat=5205815\n","Skipping y_hat=4409138\n","Skipping y_hat=4121198\n","Skipping y_hat=1672550\n","Skipping y_hat=3742388\n"," 11% 3/27 [00:00<00:01, 22.46it/s]Skipping y_hat=5902235\n"," 22% 6/27 [00:00<00:00, 22.67it/s]Skipping y_hat=1147880\n"," 33% 9/27 [00:00<00:00, 22.72it/s]Skipping y_hat=5287787\n","Skipping y_hat=1707692\n","Skipping y_hat=7117912\n","Skipping y_hat=5201162\n","Skipping y_hat=7117471\n"," 44% 12/27 [00:00<00:00, 22.76it/s]Skipping y_hat=972316\n","Skipping y_hat=5026004\n"," 67% 18/27 [00:00<00:00, 22.84it/s]Skipping y_hat=3462072\n"," 89% 24/27 [00:01<00:00, 23.33it/s]Skipping y_hat=466\n","100% 27/27 [00:01<00:00, 23.20it/s]\n","accuracy of 3000 examples: 2983/3000 (99.43333333333332%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.17it/s]\n","accuracy of 10000 examples: 9975/10000 (99.75%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=6712559\n","Skipping y_hat=1672550\n","Skipping y_hat=3742388\n"," 11% 3/27 [00:00<00:01, 22.29it/s]Skipping y_hat=4002753\n"," 33% 9/27 [00:00<00:00, 22.52it/s]Skipping y_hat=4370264\n","Skipping y_hat=5287787\n","Skipping y_hat=7117912\n","Skipping y_hat=5201162\n"," 44% 12/27 [00:00<00:00, 22.45it/s]Skipping y_hat=972316\n","Skipping y_hat=5026004\n"," 67% 18/27 [00:00<00:00, 22.78it/s]Skipping y_hat=972040\n","Skipping y_hat=2762506\n","Skipping y_hat=3462072\n"," 78% 21/27 [00:00<00:00, 22.73it/s]Skipping y_hat=532421\n","100% 27/27 [00:01<00:00, 23.16it/s]\n","accuracy of 3000 examples: 2986/3000 (99.53333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.53%\n","\n","iter 990: train loss 0.9585, val loss 0.9407\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.98it/s]Skipping y_hat=2467567\n","Skipping y_hat=2747798\n"," 22% 6/27 [00:00<00:00, 22.92it/s]Skipping y_hat=6478337\n","Skipping y_hat=7425962\n"," 33% 9/27 [00:00<00:00, 22.76it/s]Skipping y_hat=6179140\n"," 44% 12/27 [00:00<00:00, 22.79it/s]Skipping y_hat=1743766\n"," 56% 15/27 [00:00<00:00, 22.85it/s]Skipping y_hat=7404749\n"," 67% 18/27 [00:00<00:00, 22.91it/s]Skipping y_hat=3462072\n"," 89% 24/27 [00:01<00:00, 23.39it/s]Skipping y_hat=865711\n","100% 27/27 [00:01<00:00, 23.20it/s]\n","accuracy of 3000 examples: 2991/3000 (99.7%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.80it/s]\n","accuracy of 10000 examples: 9970/10000 (99.7%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=2110535\n"," 11% 3/27 [00:00<00:01, 22.94it/s]Skipping y_hat=2467567\n","Skipping y_hat=2747798\n"," 22% 6/27 [00:00<00:00, 22.93it/s]Skipping y_hat=6478337\n"," 33% 9/27 [00:00<00:00, 22.87it/s]Skipping y_hat=1459585\n","Skipping y_hat=5548022\n"," 56% 15/27 [00:00<00:00, 22.87it/s]Skipping y_hat=4567294\n","Skipping y_hat=7404749\n","Skipping y_hat=4393244\n","Skipping y_hat=1877229\n"," 67% 18/27 [00:00<00:00, 22.62it/s]Skipping y_hat=5579144\n","Skipping y_hat=3462072\n"," 78% 21/27 [00:00<00:00, 22.66it/s]Skipping y_hat=148336\n","100% 27/27 [00:01<00:00, 23.16it/s]\n","accuracy of 3000 examples: 2987/3000 (99.56666666666666%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.57%\n","\n","iter 995: train loss 0.9543, val loss 0.9440\n","Using precomputed batches\n","Max number of tokens 8.\n"," 22% 6/27 [00:00<00:00, 22.65it/s]Skipping y_hat=4998070\n"," 33% 9/27 [00:00<00:00, 22.75it/s]Skipping y_hat=6179140\n"," 44% 12/27 [00:00<00:00, 22.76it/s]Skipping y_hat=2139011\n"," 56% 15/27 [00:00<00:00, 22.83it/s]Skipping y_hat=4985299\n"," 67% 18/27 [00:00<00:00, 22.74it/s]Skipping y_hat=118244\n"," 89% 24/27 [00:01<00:00, 23.26it/s]Skipping y_hat=154996\n","100% 27/27 [00:01<00:00, 23.07it/s]\n","accuracy of 3000 examples: 2994/3000 (99.8%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.06it/s]\n","accuracy of 10000 examples: 9981/10000 (99.81%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 23.15it/s]Skipping y_hat=3893212\n"," 22% 6/27 [00:00<00:00, 22.77it/s]Skipping y_hat=4998070\n"," 44% 12/27 [00:00<00:00, 22.78it/s]Skipping y_hat=7989535\n"," 89% 24/27 [00:01<00:00, 23.35it/s]Skipping y_hat=865711\n","100% 27/27 [00:01<00:00, 23.16it/s]\n","accuracy of 3000 examples: 2996/3000 (99.86666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.87%\n","\n","iter 1000: train loss 0.9508, val loss 0.9366\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4595561\n","Skipping y_hat=2206674\n"," 11% 3/27 [00:00<00:01, 21.99it/s]Skipping y_hat=6204761\n","Skipping y_hat=3972014\n","Skipping y_hat=7467461\n"," 22% 6/27 [00:00<00:00, 22.09it/s]Skipping y_hat=3954929\n","Skipping y_hat=7740161\n"," 33% 9/27 [00:00<00:00, 22.28it/s]Skipping y_hat=2053205\n","Skipping y_hat=7017461\n"," 44% 12/27 [00:00<00:00, 22.17it/s]Skipping y_hat=3250061\n","Skipping y_hat=6731110\n"," 56% 15/27 [00:00<00:00, 22.26it/s]Skipping y_hat=8469791\n","Skipping y_hat=1329472\n","Skipping y_hat=8746082\n","Skipping y_hat=5421761\n"," 89% 24/27 [00:01<00:00, 22.67it/s]Skipping y_hat=131804\n","100% 27/27 [00:01<00:00, 22.57it/s]\n","accuracy of 3000 examples: 2984/3000 (99.46666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.19it/s]\n","accuracy of 10000 examples: 9980/10000 (99.8%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=904004\n","Skipping y_hat=4595561\n"," 11% 3/27 [00:00<00:01, 22.30it/s]Skipping y_hat=4242023\n","Skipping y_hat=6204761\n","Skipping y_hat=3972014\n","Skipping y_hat=8295245\n"," 22% 6/27 [00:00<00:00, 22.04it/s]Skipping y_hat=7740161\n"," 44% 12/27 [00:00<00:00, 22.24it/s]Skipping y_hat=2530358\n","Skipping y_hat=5146820\n"," 56% 15/27 [00:00<00:00, 22.29it/s]Skipping y_hat=1329472\n","Skipping y_hat=5421761\n"," 89% 24/27 [00:01<00:00, 23.08it/s]Skipping y_hat=131804\n","100% 27/27 [00:01<00:00, 22.72it/s]\n","accuracy of 3000 examples: 2988/3000 (99.6%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.60%\n","\n","iter 1100: train loss 0.9454, val loss 0.9339\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.49it/s]Skipping y_hat=6910001\n"," 22% 6/27 [00:00<00:00, 22.47it/s]Skipping y_hat=6407848\n"," 44% 12/27 [00:00<00:00, 22.40it/s]Skipping y_hat=3287716\n","100% 27/27 [00:01<00:00, 22.76it/s]\n","accuracy of 3000 examples: 2997/3000 (99.9%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.63it/s]\n","accuracy of 10000 examples: 9983/10000 (99.83%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.83it/s]Skipping y_hat=2878550\n"," 33% 9/27 [00:00<00:00, 22.55it/s]Skipping y_hat=3274234\n","Skipping y_hat=3293420\n"," 67% 18/27 [00:00<00:00, 22.73it/s]Skipping y_hat=1670201\n"," 89% 24/27 [00:01<00:00, 23.41it/s]Skipping y_hat=395531\n","100% 27/27 [00:01<00:00, 23.15it/s]\n","accuracy of 3000 examples: 2995/3000 (99.83333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.83%\n","\n","iter 1200: train loss 0.9548, val loss 0.9413\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7628158\n"," 22% 6/27 [00:00<00:00, 21.94it/s]Skipping y_hat=6407848\n"," 33% 9/27 [00:00<00:00, 21.90it/s]Skipping y_hat=6918048\n"," 44% 12/27 [00:00<00:00, 21.83it/s]Skipping y_hat=8089525\n"," 78% 21/27 [00:00<00:00, 22.03it/s]Skipping y_hat=448327\n","100% 27/27 [00:01<00:00, 22.36it/s]\n","accuracy of 3000 examples: 2995/3000 (99.83333333333333%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.55it/s]\n","accuracy of 10000 examples: 9988/10000 (99.88%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7628158\n"," 22% 6/27 [00:00<00:00, 22.57it/s]Skipping y_hat=6407848\n"," 44% 12/27 [00:00<00:00, 22.47it/s]Skipping y_hat=8089525\n","Skipping y_hat=2231920\n"," 67% 18/27 [00:00<00:00, 22.77it/s]Skipping y_hat=7680970\n","100% 27/27 [00:01<00:00, 23.11it/s]\n","accuracy of 3000 examples: 2995/3000 (99.83333333333333%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.83%\n","\n","iter 1300: train loss 0.9439, val loss 0.9460\n","Using precomputed batches\n","Max number of tokens 8.\n"," 67% 18/27 [00:00<00:00, 22.86it/s]Skipping y_hat=1979681\n"," 89% 24/27 [00:01<00:00, 23.36it/s]Skipping y_hat=737317\n","Skipping y_hat=235504\n","100% 27/27 [00:01<00:00, 23.25it/s]\n","accuracy of 3000 examples: 2997/3000 (99.9%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.91it/s]\n","accuracy of 10000 examples: 9989/10000 (99.89%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 89% 24/27 [00:01<00:00, 22.87it/s]Skipping y_hat=737317\n","Skipping y_hat=557110\n","Skipping y_hat=235504\n","100% 27/27 [00:01<00:00, 22.69it/s]\n","accuracy of 3000 examples: 2997/3000 (99.9%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.90%\n","\n","iter 1400: train loss 0.9485, val loss 0.9532\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=4430832\n"," 89% 24/27 [00:01<00:00, 23.11it/s]Skipping y_hat=338875\n","100% 27/27 [00:01<00:00, 22.85it/s]\n","accuracy of 3000 examples: 2998/3000 (99.93333333333332%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.35it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 56% 15/27 [00:00<00:00, 22.30it/s]Skipping y_hat=4951755\n","100% 27/27 [00:01<00:00, 22.71it/s]\n","accuracy of 3000 examples: 2999/3000 (99.96666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.97%\n","\n","iter 1500: train loss 0.9465, val loss 0.9365\n","Using precomputed batches\n","Max number of tokens 8.\n"," 22% 6/27 [00:00<00:00, 21.61it/s]Skipping y_hat=5509882\n"," 56% 15/27 [00:00<00:00, 22.18it/s]Skipping y_hat=6138228\n"," 67% 18/27 [00:00<00:00, 22.15it/s]Skipping y_hat=3017851\n"," 89% 24/27 [00:01<00:00, 22.96it/s]Skipping y_hat=71830\n","100% 27/27 [00:01<00:00, 22.50it/s]\n","accuracy of 3000 examples: 2996/3000 (99.86666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.74it/s]\n","accuracy of 10000 examples: 9998/10000 (99.98%)\n","Using precomputed batches\n","Max number of tokens 8.\n","  0% 0/27 [00:00<?, ?it/s]Skipping y_hat=7674916\n"," 22% 6/27 [00:00<00:00, 22.78it/s]Skipping y_hat=5509882\n","Skipping y_hat=1604482\n"," 33% 9/27 [00:00<00:00, 22.54it/s]Skipping y_hat=3019111\n","100% 27/27 [00:01<00:00, 23.04it/s]\n","accuracy of 3000 examples: 2996/3000 (99.86666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.87%\n","\n","iter 1600: train loss 0.9350, val loss 0.9451\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.04it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.68it/s]\n","accuracy of 10000 examples: 9998/10000 (99.98%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 89% 24/27 [00:01<00:00, 23.27it/s]Skipping y_hat=652661\n","100% 27/27 [00:01<00:00, 23.10it/s]\n","accuracy of 3000 examples: 2999/3000 (99.96666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.97%\n","\n","iter 1700: train loss 0.9352, val loss 0.9542\n","Using precomputed batches\n","Max number of tokens 8.\n"," 67% 18/27 [00:00<00:00, 23.03it/s]Skipping y_hat=7000343\n","Skipping y_hat=7702703\n","100% 27/27 [00:01<00:00, 23.48it/s]\n","accuracy of 3000 examples: 2998/3000 (99.93333333333332%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.95it/s]\n","accuracy of 10000 examples: 9995/10000 (99.95%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 23.54it/s]Skipping y_hat=6104771\n"," 67% 18/27 [00:00<00:00, 22.87it/s]Skipping y_hat=7702703\n","100% 27/27 [00:01<00:00, 23.54it/s]\n","accuracy of 3000 examples: 2998/3000 (99.93333333333332%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.93%\n","\n","iter 1800: train loss 0.9120, val loss 0.9638\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.16it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.81it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n"," 67% 18/27 [00:00<00:00, 22.74it/s]Skipping y_hat=7702703\n","100% 27/27 [00:01<00:00, 23.15it/s]\n","accuracy of 3000 examples: 2999/3000 (99.96666666666667%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 99.97%\n","\n","iter 1900: train loss 0.8753, val loss 0.9799\n","Using precomputed batches\n","Max number of tokens 8.\n"," 33% 9/27 [00:00<00:00, 22.47it/s]Skipping y_hat=5302162\n"," 89% 24/27 [00:01<00:00, 22.98it/s]Skipping y_hat=395731\n","100% 27/27 [00:01<00:00, 22.78it/s]\n","accuracy of 3000 examples: 2998/3000 (99.93333333333332%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.67it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.99it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 2000: train loss 0.8434, val loss 1.0008\n","Using precomputed batches\n","Max number of tokens 8.\n"," 11% 3/27 [00:00<00:01, 22.30it/s]Skipping y_hat=5550080\n","100% 27/27 [00:01<00:00, 22.73it/s]\n","accuracy of 3000 examples: 2999/3000 (99.96666666666667%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.54it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.20it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 2100: train loss 0.8006, val loss 1.0497\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.67it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.70it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.91it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 2200: train loss 0.7511, val loss 1.1134\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.22it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.56it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.03it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 2300: train loss 0.6873, val loss 1.1674\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.63it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.96it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.12it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 2400: train loss 0.6589, val loss 1.2258\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.86it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.93it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.87it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 2500: train loss 0.6343, val loss 1.3049\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.32it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.01it/s]\n","accuracy of 10000 examples: 9999/10000 (99.99%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.39it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 2600: train loss 0.6120, val loss 1.3794\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.23it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.83it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.19it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 2700: train loss 0.5970, val loss 1.4318\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.11it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.04it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.42it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 2800: train loss 0.5828, val loss 1.4932\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.37it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.31it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.88it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 2900: train loss 0.5731, val loss 1.5381\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.06it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.11it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.81it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 3000: train loss 0.5645, val loss 1.5674\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.85it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.61it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.77it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 3100: train loss 0.5596, val loss 1.6155\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.07it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.79it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.77it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 3200: train loss 0.5488, val loss 1.6822\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.66it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.83it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.06it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 3300: train loss 0.5473, val loss 1.6790\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.25it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.58it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.56it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 3400: train loss 0.5457, val loss 1.7028\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.19it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.92it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.29it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 3500: train loss 0.5362, val loss 1.7503\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.19it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.94it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.11it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 3600: train loss 0.5382, val loss 1.7744\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.83it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.50it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.02it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 3700: train loss 0.5280, val loss 1.7798\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.79it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.96it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.78it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 3800: train loss 0.5301, val loss 1.8114\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 21.96it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 23.63it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 21.98it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 3900: train loss 0.5221, val loss 1.8060\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.86it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.32it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.11it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 4000: train loss 0.5282, val loss 1.8234\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.63it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.50it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.15it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 4100: train loss 0.5233, val loss 1.8726\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.70it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.89it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.43it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 4200: train loss 0.5250, val loss 1.9208\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.00it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.73it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.69it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 4300: train loss 0.5155, val loss 1.9074\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.48it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.89it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.17it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 4400: train loss 0.5061, val loss 1.8892\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.91it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.74it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.13it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 4500: train loss 0.5160, val loss 1.8849\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.86it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.96it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.24it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 4600: train loss 0.5125, val loss 1.9533\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.95it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 25.21it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.06it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 4700: train loss 0.5121, val loss 1.9050\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.02it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.70it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.31it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 4800: train loss 0.5102, val loss 1.9819\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.63it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.69it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.31it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","iter 4900: train loss 0.5081, val loss 1.9816\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.35it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.69it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.05it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 23.61it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 81/81 [00:03<00:00, 24.94it/s]\n","accuracy of 10000 examples: 10000/10000 (100.0%)\n","Using precomputed batches\n","Max number of tokens 8.\n","100% 27/27 [00:01<00:00, 22.94it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Final Test Results:\n","0_to_six_digit_times_1_digit_bal_test: 100.00%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m0_to_six_digit_times_1_digit_reverse\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/multiplication/runs/wppr1iuw\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m0_to_six_digit_times_1_digit/reverse_out/wandb/run-20250703_003311-wppr1iuw/logs\u001b[0m\n"]}],"source":["!python train_end_padding_more_early_eval.py 2_operands_mul_reversed.txt"]},{"cell_type":"markdown","metadata":{"id":"3TKyQakzD06u"},"source":["## 2 Operands 0 to 999 Uniform"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":58,"status":"ok","timestamp":1754833121830,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":240},"id":"zvtg4xABD986","outputId":"bb0ed261-fa6a-4765-a8f6-83e420f7d1c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/addition\n"]}],"source":["%cd /content/drive/MyDrive/addition"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":885,"status":"ok","timestamp":1754833129524,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":240},"id":"TQ8E9emHcorg","outputId":"960dfc11-e4e0-4aae-ea71-373430297c2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 100\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '2_operands_0_to_999_uniform_gold_by_reading_plain'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'algo_reasoning'\n","data_format='plain'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_plain.pt'\n","\n","# to edit: whether the result is reversed\n","reverse_c = False\n","eval_addition = True\n","\n","analysis = False\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# whether need zero‐padding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory\n","out_dir = '/content/drive/MyDrive/addition/2_operands_0_to_999_uniform/gold_by_reading_plain_out'\n","\n","# to edit: training data\n","train_data_path = '/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_reading/train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","start_train = \"/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_reading/train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_path = '/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_reading/val.txt'\n","\n","# to edit: test data; start is just the test file. It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","test_file_path = '/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_reading/test_plain/test.txt'\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval1 = False\n","early_eval_interval1 = 25\n","early_eval_iters1 = 3000\n","\n","more_early_eval2 = False\n","early_eval_interval2 = 5\n","early_eval_iters2 = 750\n","\n","# to edit: whether do mutual information measurement between diffrent pairs of digits (include input-output and output-output)\n","mi_measurement = False\n","\n","# (optional) data for additional statistical measurement\n","stats_measurement_data_file_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/4_operand_addition_stats_measurement_data_reversed.txt'\n","\n","early_mi_measure_border = 200000\n","early_mi_measure_interval = 5000\n","final_mi_measure_interval = 5000"]}],"source":["%cat 2_operands_addition_plain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1145,"status":"ok","timestamp":1754833688723,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":240},"id":"gha9EmVRPPa_","outputId":"a84aeec3-6c4f-4441-d46f-00ec5fb7f3a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["from main_utilities import *\n","from tqdm.auto import tqdm\n","import torch\n","import numpy as np\n","import random\n","import math\n","import os\n","import pandas as pd\n","import csv\n","\n","\n","def get_abc_new(abc: str, zero_pad=False, reverse_c=False, binary=False, mode: str = \"compute_gold\"):\n","    \"\"\"Unified parser: mode='compute_gold' computes the groudtruth on the fly;\n","       mode='read_gold_as_str' reads the groundtruth from the evaluation files (testing, validation) to do string matching.\n","    Returns either\n","      (operands_str, result_int, operation)            # v1\n","    or\n","      (operands_str, result_int, result_str, operation)  # v2\n","    \"\"\"\n","    if '+' in abc:\n","        operation = '+'\n","    elif '-' in abc:\n","        operation = '-' \n","    elif '*' in abc:\n","        operation = '*'\n","    else:\n","        print(f'operation not found, abc: {abc}')\n","        return None, None, None\n","\n","    # Split the input string into parts\n","    parts = abc.split('=')\n","    if len(parts) != 2:\n","        print(f'Invalid format, expected \"a+b+c...=result\", got: {abc}')\n","        return None, None, None\n","\n","    # Get the operands part (before =)\n","    operands_str = parts[0]\n","    if operands_str[0] == '$':\n","        operands_str = operands_str[1:]\n","    if operands_str.startswith('Input:\\n'):\n","        operands_str = operands_str.split('Input:\\n')[-1]\n","    if 'Target' in operands_str:\n","        operands_str = operands_str.split('\\nTarget')[0]\n","\n","    # Split into individual operands\n","    operands = [op.strip() for op in operands_str.split(operation)]\n","    \n","    # Clean up operands\n","    operands = [op.replace(' ', '') for op in operands]\n","    \n","    if binary:\n","        # Convert all operands to binary and sum\n","        result = sum(int(op, 2) for op in operands)\n","        return operands_str, result, operation\n","\n","    if zero_pad:\n","        operands = [remove_zero_pad(op) for op in operands]\n","\n","    # version 1: compute the result\n","    if mode == \"compute_gold\":\n","        if operation == '+':\n","            result = sum(int(op) for op in operands)\n","        elif operation == '-':\n","            result = int(operands[0]) - sum(int(op) for op in operands[1:])\n","        elif operation == '*':\n","            result = 1\n","            for op in operands:\n","                result *= int(op)\n","        else:\n","            raise ValueError(f\"Unsupported operation: {operation}\")\n","\n","        return operands_str, result, operation\n","    # version 2: read the groundtruth from the evaluation files\n","    if mode == \"read_gold_as_str\":\n","        # parts[1] is the result part, which may contain a trailing '$' or newline\n","        result_str = parts[1].strip()\n","        if result_str.endswith('\\n'):\n","            result_str = result_str[:-1].strip()\n","        if result_str.endswith('$'):\n","            result_str = result_str[:-1].strip()\n","        if reverse_c:\n","            result_str = result_str[::-1]  # reverse the result string if needed\n","\n","        return operands_str, result_str, operation\n","\n","_precomputed_batches = {}\n","def prepare_addition_batches(config, encode, num_digit=3, zero_pad=False, reverse_c=False, binary=False,  data_type='binary', \n","                             operator='+', data_format='plain', add_space=False, simple=False, mode: str = \"compute_gold\"):\n","    device = config['device']\n","    test_batch_size = config['test_batch_size'] if 'test_batch_size' in config.keys() else 128\n","    start = config['start'] if 'start' in config.keys() else \"FILE:prompt/prompt_addition_pad_test_0.01.txt\"\n","    print(f\"Preparing batches from: {start}\")\n","    \n","    if start.startswith('FILE:'): # start is just the test file path\n","        with open(start[5:], 'r', encoding='utf-8') as f:\n","            lines = [line.rstrip() for line in f]\n","    else:\n","        lines = start.splitlines()\n","\n","    total = len(lines)\n","    print(f'Preparing batches for {total} examples from: {start}')\n","    \n","    # Process all lines and group by prompt length\n","    prompt_dict = {}\n","    for line in lines:\n","        # split off gold answer\n","        # e.g. line = \"123+456=579\"\n","        prompt_str = line.split('=')[0] + '='      # \"123+456=\"\n","        prompt_ids = encode(prompt_str)\n","        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","        prompt_length = x.size(1)\n","\n","        # parse out gold for evaluation later\n","        operands, result, op = get_abc_new(\n","            line,\n","            zero_pad=zero_pad,\n","            reverse_c=reverse_c,\n","            binary=binary,\n","            mode=mode\n","        )\n","\n","        entry = (x, operands, result)\n","        prompt_dict.setdefault(prompt_length, []).append(entry)\n","\n","    # Construct batches of prompts\n","    batch_list = []\n","    for prompt_length in prompt_dict.keys():\n","        input_tuple_list = prompt_dict[prompt_length]\n","        for batch_idx in range(math.ceil(len(input_tuple_list)/test_batch_size)):\n","            batch_list.append(input_tuple_list[batch_idx*test_batch_size:(batch_idx+1)*test_batch_size])\n","\n","    print(f'Created {len(batch_list)} batches')\n","    \n","    # Cache the batches using a hash of the configuration\n","    config_hash = hash(frozenset({k: str(v) for k, v in config.items() if k != 'device'}.items()))\n","    batch_key = f\"{config_hash}_{data_type}_{operator}_{num_digit}_{zero_pad}_{data_format}_{add_space}\"\n","    _precomputed_batches[batch_key] = (batch_list, total)\n","    \n","    return batch_list, total\n","\n","# Modified evaluation function that uses pre-created batches\n","def evaluate_addition_precomputed(config, model, ctx, decode, batch_list, total,\n","                                  verbose=False, num_digit=3, zero_pad=False, reverse_c=False,\n","                                  add_space=False, operator='+', verbose_correct=False, analyze=False, mode: str = \"compute_gold\"):\n","    model.eval()\n","    device = config['device']\n","    max_new_tokens = config['max_new_tokens'] if 'max_new_tokens' in config.keys() else num_digit+2\n","    temperature = config['temperature'] if 'temperature' in config.keys() else 0.8\n","    top_k = config['top_k'] if 'top_k' in config.keys() else 200\n","\n","    if add_space:\n","        max_new_tokens = 2 * num_digit + 3\n","\n","    correct = 0\n","\n","    if analyze:\n","        # analyze various metrics\n","        error_dict = {'y': [], 'y_hat': [], 'accuracy_eps0': [], 'accuracy_eps5e-4': [],\n","                      'accuracy_eps5e-3': [], 'mse': [], 'normalized_mse': [],\n","                      'digit_wise_difference': [], 'incorrect_digit_count': []}\n","        list_not_num = []\n","        list_outlier_num = []\n","    op = operator\n","    correct_examples = []\n","    incorrect_examples = []\n","    print(f\"Max number of tokens {max_new_tokens}.\")\n","    for batch_idx in tqdm(range(len(batch_list))):\n","        batch = batch_list[batch_idx]\n","        x_list = [input_tuple[0] for input_tuple in batch]\n","        x = torch.cat(x_list, dim=0)\n","\n","        # Run generation\n","        with torch.no_grad():\n","            with ctx:\n","                eos_id = config['eos_id']\n","                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","                outcome_list = [decode(y_i.tolist()) for y_i in y]\n","\n","                for i, outcome in enumerate(outcome_list):\n","                    _, operands, result = batch[i]\n","                    \n","                    if mode == \"compute_gold\":\n","                        c_hat = outcome.split('=')[1].split('$')[0].strip()\n","\n","                        if zero_pad:\n","                            c_hat = remove_zero_pad(c_hat)\n","\n","                        # plain addition\n","                        c_hat = c_hat.split('\\n')[0]\n","\n","                        if reverse_c:\n","                            c_hat = reverse_string(c_hat)\n","\n","                        if add_space:\n","                            c_hat = c_hat.replace(' ', '')\n","\n","                        if is_number(c_hat):\n","                            if '.' in c_hat:\n","                                c_hat = float(c_hat)\n","                            else:\n","                                c_hat = int(c_hat)\n","                        else:  # c_hat is not a number\n","                            result = str(result)\n","\n","                    if mode == \"read_gold_as_str\":\n","                        c_hat = outcome.split('=')[1].split('$')[0].strip()\n","\n","                        if reverse_c:\n","                            c_hat = reverse_string(c_hat)\n","\n","                    # Check correctness\n","                    if op in ['+', '-', '*']:\n","                        if result == c_hat:\n","                            correct += 1\n","                            correct_examples.append((operands, result, outcome, c_hat))\n","                            if verbose_correct:\n","                                print('outputs(o): ', outcome)\n","                                print(f'correct: {operands}={result}')\n","                        else:\n","                            incorrect_examples.append((operands, result, outcome, c_hat))\n","                            if verbose:\n","                                print('outputs(x): ', outcome)\n","                                print(f'wrong  : {operands}={c_hat}')\n","                                print(f'correct: {operands}={result}')\n","                    # Calculate metrics if analyzing\n","                    if analyze:\n","                        error_dict['y'].append(result)\n","                        error_dict['y_hat'].append(c_hat)\n","\n","                        metric_types = ['mse', 'normalized_mse', 'digit_wise_difference', 'incorrect_digit_count']\n","                        for metric_type in metric_types:\n","                            error, list_not_num, list_outlier_num = get_error_metric(result, c_hat, metric_type, eps=config.get('eps', 0),\n","                                                                                    list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                            error_dict[f'{metric_type}'].append(error)\n","\n","                        error, _, _ = get_error_metric(result, c_hat, 'accuracy', eps=0, list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                        error_dict[f'accuracy_eps0'].append(error * 100)\n","                        error, _, _ = get_error_metric(result, c_hat, 'accuracy', eps=5e-4, list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                        error_dict[f'accuracy_eps5e-4'].append(error * 100)\n","                        error, _, _ = get_error_metric(result, c_hat, 'accuracy', eps=5e-3, list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                        error_dict[f'accuracy_eps5e-3'].append(error * 100)\n","\n","    accuracy = correct / total * 100\n","    print(f\"accuracy of {total} examples: {correct}/{total} ({accuracy}%)\")\n","\n","    accuracy_dictionary = {}\n","    if analyze:\n","        error_df = pd.DataFrame(error_dict)\n","        result_dir = config.get('result_dir')\n","        if result_dir is None:\n","            result_dir = get_results_dir(config)\n","        error_df.to_csv(os.path.join(result_dir, 'error_df.csv'), index=False)\n","\n","        error_mean_dict = {\n","            metric_type: np.nanmean(error_dict[f'{metric_type}'])\n","            for metric_type in ['accuracy_eps0', 'accuracy_eps5e-4', 'accuracy_eps5e-3',\n","                               'mse', 'normalized_mse', 'digit_wise_difference', 'incorrect_digit_count']\n","        }\n","        error_mean_dict['num_not_num'] = len(list_not_num) / len(metric_types)\n","        error_mean_dict['num_outlier_num'] = len(list_outlier_num) / len(metric_types)\n","        error_mean_dict['median_mse'] = error_df.mse.median()\n","        error_mean_dict['median_normalized_mse'] = error_df.normalized_mse.median()\n","        accuracy_dictionary.update(error_mean_dict)\n","\n","    model.train()\n","    return accuracy, accuracy_dictionary, correct_examples, incorrect_examples\n","\n","# Keep the original function for backward compatibility, but make it use the new functions\n","def evaluate_addition_batch(config, model, ctx, encode, decode, verbose=False, num_digit=3, zero_pad=False, \n","                          reverse_ab=False, reverse_c=False, data_type='binary', operator='+', \n","                          data_format='plain', add_space=False, verbose_correct=False, analyze=False, mode: str = \"compute_gold\"):\n","    config_hash = hash(frozenset({k: str(v) for k, v in config.items() if k != 'device'}.items()))\n","    batch_key = f\"{config_hash}_{data_type}_{operator}_{num_digit}_{zero_pad}_{reverse_ab}_{data_format}_{add_space}\"\n","    \n","    if batch_key in _precomputed_batches:\n","        print(\"Using precomputed batches\")\n","        batch_list, total = _precomputed_batches[batch_key]\n","    else:\n","        print(\"Creating new batches\")\n","        batch_list, total = prepare_addition_batches(\n","            config, encode, num_digit=num_digit, zero_pad=zero_pad, reverse_c=reverse_c,\n","            data_type=data_type, operator=operator, data_format=data_format, add_space=add_space, mode=mode\n","        )\n","\n","    # Evaluate using the batches\n","    return evaluate_addition_precomputed(\n","        config, model, ctx, decode, batch_list, total, verbose=verbose,\n","        num_digit=num_digit, zero_pad=zero_pad, reverse_c=reverse_c,\n","        add_space=add_space, operator=operator, verbose_correct=verbose_correct, analyze=analyze, mode=mode\n","    )\n","\n","def evaluate_multiple_files(config, model, ctx, encode, decode, test_file, iter_num, result_dir,\n","                          verbose=False, num_digit=3, zero_pad=False, reverse_ab=False, reverse_c=False,\n","                          data_type='binary', operator='+', data_format='plain', add_space=False, analyze=False, mode: str = \"compute_gold\"):\n","    \"\"\"\n","    Evaluate model on multiple test files and store results.\n","    Args:\n","        test_files: List of test file paths\n","        iter_num: Current iteration number\n","        result_dir: Directory to store results\n","    Returns:\n","        dict: Dictionary containing accuracies for each test file\n","    \"\"\"\n","    \n","    # Get test file name without path and extension\n","    test_name = os.path.splitext(os.path.basename(test_file))[0]\n","    \n","    # Set the current test file as start\n","    config['start'] = f\"FILE:{test_file}\"\n","    \n","    # Run evaluation\n","    accuracy, metrics, correct, incorrect = evaluate_addition_batch(\n","        config, model, ctx, encode=encode, decode=decode,\n","        verbose=verbose, num_digit=num_digit, zero_pad=zero_pad,\n","        reverse_ab=reverse_ab, reverse_c=reverse_c,\n","        data_type=data_type, operator=operator,\n","        data_format=data_format, analyze=analyze, mode=mode\n","    )\n","    \n","    # Path for this test file's results\n","    results_file = os.path.join(result_dir, f'{test_name}_results.csv')\n","    \n","    # Combine correct and incorrect examples and sort by operands to maintain consistent order\n","    all_examples = correct + incorrect\n","    all_examples.sort(key=lambda x: x[0])  # Sort by operands\n","    \n","    # Create new DataFrame with operands and actual results\n","    new_df = pd.DataFrame({\n","        'operands': [ex[0] for ex in all_examples],\n","        'actual': [ex[1] for ex in all_examples],\n","        f'pred_iter_{iter_num}': [ex[3] for ex in all_examples]\n","    })\n","    \n","    # Read existing results if file exists and merge\n","    if os.path.exists(results_file):\n","        old_df = pd.read_csv(results_file)\n","        # # Merge based on operands, keeping all predictions\n","        # if 'operands' in old_df.columns:\n","        #     merged_df = pd.merge(old_df, new_df, on=['operands', 'actual'], how='outer')\n","        # else:\n","        #     merged_df = new_df\n","        # ── Normalize keys so they truly match ──\n","        for df in (old_df, new_df):\n","            # strip whitespace from the operands strings\n","            df['operands'] = df['operands'].str.strip()\n","            # ensure actual is an integer\n","            df['actual']   = df['actual'].astype(int)\n","\n","        merged_df = pd.merge(\n","            old_df, new_df,\n","            on=['operands', 'actual'],\n","            how='outer'\n","        )\n","    else:\n","        merged_df = new_df\n","    \n","    # Save results\n","    merged_df.to_csv(results_file, index=False)\n","    \n","    # Save accuracy separately in a summary file\n","    accuracy_file = os.path.join(result_dir, f'{test_name}_accuracy.csv')\n","    if os.path.exists(accuracy_file):\n","        acc_df = pd.read_csv(accuracy_file)\n","    else:\n","        acc_df = pd.DataFrame(columns=['iteration', 'accuracy'])\n","    \n","    # Add new accuracy\n","    new_row = pd.DataFrame({'iteration': [iter_num], 'accuracy': [accuracy]})\n","    acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","    acc_df.to_csv(accuracy_file, index=False)\n","    \n","    return test_name, accuracy, metrics, correct, incorrect"]}],"source":["%cat ./evaluation.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":238,"status":"ok","timestamp":1754833838617,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":240},"id":"PN3gSPS7P0J4","outputId":"6c838575-7524-4765-a709-c0df7fc52c44"},"outputs":[{"name":"stdout","output_type":"stream","text":["import os\n","import pickle\n","import requests\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","import copy\n","import time\n","\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import yaml\n","from torch.utils.data import Dataset, DataLoader\n","import wandb\n","import torch.nn.functional as F\n","import math\n","\n","from model import GPTConfig, GPT\n","from main_utilities import *\n","from evaluation import *\n","from statistical_measurements import *\n","\n","import re\n","\n","def create_meta_for_addition(data):\n","    \"\"\"Create metadata for addition data.\"\"\"\n","    # Define the vocabulary for addition problems\n","    # This includes digits, operators, equals sign, and newline\n","    chars = sorted(list(set(data)))\n","    vocab_size = len(chars)\n","    # Create encoder and decoder dictionaries\n","    stoi = {ch: i for i, ch in enumerate(chars)}\n","    itos = {i: ch for i, ch in enumerate(chars)}\n","    \n","    meta = {\n","        'vocab_size': vocab_size,\n","        'vocab': chars,\n","        'stoi': stoi,\n","        'itos': itos\n","    }\n","    return meta\n","\n","def encode_addition(text, meta):\n","    \"\"\"Encode text to tensor using the metadata.\"\"\"\n","    return torch.tensor([meta['stoi'][c] for c in text], dtype=torch.long)\n","\n","def decode_addition(tensor, meta):\n","    \"\"\"Decode tensor to text using the metadata.\"\"\"\n","    if isinstance(tensor, torch.Tensor):\n","        return ''.join([meta['itos'][i.item()] for i in tensor])\n","    else:\n","        return ''.join([meta['itos'][i] for i in tensor])\n","    \n","def pad_sequence(x: torch.Tensor, length: int, pad_value: int):\n","    if x.size(0) < length:\n","        padding = torch.full((length - x.size(0),), pad_value, dtype=torch.long)\n","        return torch.cat([x, padding], dim=0)\n","    else:\n","        return x\n","\n","class AdditionDataset(Dataset):\n","    def __init__(self, file_path, meta):\n","        self.meta = meta\n","        # Read the text file\n","        with open(file_path, 'r') as f:\n","            self.lines = f.readlines()\n","        # Remove any empty lines and strip whitespace\n","        self.lines = [line.strip() for line in self.lines if line.strip()]\n","        self.block_size = block_size  # from your config\n","        \n","    def __len__(self):\n","        return len(self.lines)\n","    \n","    def __getitem__(self, idx):\n","        line = self.lines[idx]\n","        # Convert the line to tensor using our encoder\n","        raw = encode_addition(line, self.meta)\n","        x = pad_sequence(raw[:-1], self.block_size, pad_value=meta['stoi']['$'])  # all but last char\n","        y = pad_sequence(raw[1:], self.block_size, pad_value=-1)   # all but first char\n","        return x, y\n","\n","# I/O\n","\n","out_dir = '/drive/MyDrive/addition/plain_no_pad/out'\n","resume_dir = None\n","resume_iter = False # if True, resume from saved iter_num, otherwise resume from iter_num 0\n","eval_interval = 2000\n","log_interval = 1\n","eval_iters = 200\n","eval_only = False # if True, script exits right after the first eval\n","always_save_checkpoint = True # if True, always save a checkpoint after each eval\n","init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n","\n","# wandb logging\n","wandb_entity = 'ssdd'\n","wandb_log = False # disabled by default\n","wandb_project = 'owt'\n","wandb_run_name = 'gpt2' # 'run' + str(time.time())\n","exp_name = 'default_exp_name'\n","\n","# data\n","dataset = 'bal'\n","gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n","test_batch_size = 128\n","batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n","block_size = 1024\n","train_data_path = 'train.bin'\n","val_data_path = 'val.bin'\n","multi_digit = False\n","num_digit = 3\n","max_new_tokens = 5\n","binary = False\n","\n","# using two data - data1 = text / data2 = addition\n","train_both = False # use seperate text/add data for train/val (get_batch uses this to sample from two differernt datasets)\n","data_ratio = 0.2 # ratio of data_path2 compared with data_path1\n","train_data_path2 = 'train_addition.bin' # only used when train_both = True\n","val_data_path2 = 'val_addition.bin'\n","\n","# evaluation\n","eval_text = False # if True get perplexity using eval_text_data_path\n","eval_text_data_path = None # directory to text data (.bin file) - ex. 'data/shakespeare_add_ar_mixed/val_text.bin'\n","eval_addition = False # if True compute test accuracy of \"a+b=\"\n","test_file_path = None\n","eval_addition_ar = False\n","start_ar = None\n","eval_other = False # use this to evaluate other operations (ex. train on operator '-' but evaluate on other_operator '+')\n","start_other = None\n","other_operator = '+'\n","eval_addition_train = False\n","start_train = None\n","reverse_ab = False\n","reverse_c = False\n","zero_pad = False\n","algo_reason = False\n","add_space = False\n","analysis = False\n","\n","# model\n","n_layer = 6\n","n_head = 6\n","n_embd = 768\n","dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n","bias = False # do we use bias inside LayerNorm and Linear layers?\n","ckpt_path_name = 'ckpt.pt'\n","save_final = True\n","\n","# adamw optimizer\n","learning_rate = 6e-4 # max learning rate\n","max_iters = 600000 # total number of training iterations\n","weight_decay = 1e-1\n","beta1 = 0.9\n","beta2 = 0.95\n","grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n","# learning rate decay settings\n","decay_lr = True # whether to decay the learning rate\n","warmup_iters = 2000 # how many steps to warm up for\n","lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n","min_lr = None # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n","\n","# DDP settings\n","backend = 'nccl' # 'nccl', 'gloo', etc.\n","# system\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n","dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n","compile = True # use PyTorch 2.0 to compile the model to be faster\n","use_flash = True\n","data_type = 'binary' # 'binary' by default, can be 'text'\n","operator = '+' # can be '+', '-', '*', 'sin', 'sqrt'\n","data_shuffle = True\n","data_format = 'plain' # 'plain' or 'reverse' or 'algo_reasoning'\n","vocabulary = 'all_ascii_chars' # can be 'all_ascii_chars' or 'numbers_only' or 'custom_input_data'\n","meta_path_specified = True # use saved meta_file (False if data_type='text')\n","eps = 0\n","tokenizer = 'char' # by default, use char level tokenizer. but for pretrained models, use openai tokenizer eg: 'gpt2'\n","\n","simple=False\n","random_A=False\n","random_C=False\n","\n","use_lora = False # use lora (from minLoRA)\n","print_interval = 2  # if we're using gpt-2 model, I want to see it prompted on text\n","\n","mode = \"compute_gold\"  # Mode for evaluation: \"compute_gold\" or \"read_gold_as_str\"\n","\n","more_early_eval1 = False # if True, do early, more frequent eval on train and val data\n","early_eval_interval1 = 25\n","early_eval_border1 = 1000\n","\n","more_early_eval2 = False # if True, do even earlier, even more frequent eval on train and val data\n","early_eval_interval2 = 5\n","early_eval_border2 = 500\n","\n","stats_measurement_data_file_path = \"\"\n","\n","drop_leading_digit = False\n","\n","config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str, type(None)))]\n","exec(open('configurator.py').read()) # overrides from command line or config file\n","config = {k: globals()[k] for k in config_keys} # will be useful for logging\n","\n","# additional statistical measurements\n","mi_measurement = False # whether to do mutual information measurement\n","early_mi_measure_border = 20000 # border for early mutual information measurement\n","early_mi_measure_interval = 1000 # interval for early mutual information measurement\n","final_mi_measure_interval = 5000 # interval for final mutual information measurement\n","\n","mi_measure_iters = set(\n","    list(range(0,  early_mi_measure_border, early_mi_measure_interval)) +    # every 20 steps before 200\n","    # list(range(100000, 100000, 20)) +   # every 50 steps from 200 up to 1500\n","    list(range(early_mi_measure_border, max_iters+1, final_mi_measure_interval))  # every 100 steps thereafter\n",")\n","\n","# function to set seed for all random number generators\n","def set_seed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    # to make sure GPU runs are deterministic even if they are slower set this to True\n","    torch.backends.cudnn.deterministic = False\n","    # warning: this causes the code to vary across runs\n","    torch.backends.cudnn.benchmark = True\n","    print(\"Seeded everything: {}\".format(seed))\n","\n","if min_lr == None:\n","    min_lr = learning_rate/10\n","master_process = True\n","seed_offset = 0\n","if master_process:\n","  os.makedirs(out_dir, exist_ok=True)\n","torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","torch.backends.cudnn.benchmark = True # cudnn auto-tuner\n","torch.backends.cudnn.deterministic = False # cudnn auto-tuner\n","# this is probably overkill but seed everything again\n","set_seed(1337 + seed_offset)\n","\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","# note: float16 data type will automatically use a GradScaler\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# Read the data files\n","with open(train_data_path, 'r') as f:\n","    train_data = f.read()\n","with open(val_data_path, 'r') as f:\n","    val_data = f.read()\n","\n","# Create metadata from the combined data\n","all_data = train_data + val_data\n","meta = create_meta_for_addition(train_data)\n","meta_vocab_size = meta['vocab_size']\n","print(f\"Using vocabulary size: {meta_vocab_size}\")\n","\n","config['eos_id'] = meta['stoi']['$']\n","\n","with open(stats_measurement_data_file_path, 'r', encoding='utf-8') as f:\n","    lines = [line.rstrip() for line in f]\n","\n","if drop_leading_digit:\n","        S = num_digit\n","else:\n","    S = num_digit + 1\n","# a simple way to parse test strings\n","padded_lines = [] # add 0 padding, remove $; an example padded_lines[6] is '932+084+230+349=5951'\n","for i in range(len(lines)):\n","    numbers = re.split(r'[+=]', lines[i])\n","    numbers[-1] = numbers[-1][:-1]\n","    for k, number in enumerate(numbers[:-1]):\n","        numbers[k] = '0' * (3-len(number)) + number\n","    numbers[-1] = numbers[-1] + '0' * (S-len(numbers[-1]))\n","    padded_lines.append(\"+\".join(numbers[:-1]) + \"=\" + numbers[-1])\n","\n","stats_measurement_data = torch.cat([encode_addition(padded_lines[i], meta).unsqueeze(0) for i in range(len(padded_lines))], dim=0)\n","\n","# # get 16 different datasets (including the base dataset) by randomizing input/output integers of the base dataset\n","# stats_measurement_dataset_list = gen_randomized_datasets(\n","#     stats_measurement_data,\n","#     meta,\n","#     digits_per_num=num_digit,\n","#     base_seed=2005,\n","#     reverse_input=reverse_ab,\n","#     reverse_output=reverse_c\n","# )\n","\n","# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n","iter_num = 0\n","best_val_loss = 1e9\n","best_perplexity = 1e9 # on text data\n","best_accuracy = -1 # on addition data\n","\n","\n","model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n","                  bias=bias, vocab_size=None, dropout=dropout, use_flash=use_flash) # start with model_args from command line\n","if init_from == 'scratch':\n","    # init a new model from scratch\n","    print(\"Initializing a new model from scratch\")\n","    # determine the vocab size we'll use for from-scratch training\n","    if meta_vocab_size is None:\n","        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n","    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    model.to(device)\n","elif init_from == 'resume':\n","    if resume_dir:\n","        print(f\"Resuming training from {resume_dir}\")\n","        checkpoint = torch.load(resume_dir, map_location=device)\n","    else:\n","        print(f\"Resuming training from {out_dir}\")\n","        # resume training from a checkpoint.\n","        ckpt_path = os.path.join(out_dir, ckpt_path_name)\n","        checkpoint = torch.load(ckpt_path, map_location=device)\n","    checkpoint_model_args = checkpoint['model_args']\n","    # force these config attributes to be equal otherwise we can't even resume training\n","    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n","    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n","        model_args[k] = checkpoint_model_args[k]\n","    # create the model\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    model.to(device)\n","    state_dict = checkpoint['model']\n","    # fix the keys of the state dictionary :(\n","    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)\n","    iter_num = checkpoint['iter_num'] if resume_iter else 0\n","    max_iters += iter_num\n","    best_val_loss = checkpoint['best_val_loss']\n","    if 'best_perplexity' in checkpoint.keys():\n","        best_perplexity = checkpoint['best_perplexity']\n","    if 'best_accuracy' in checkpoint.keys():\n","        best_accuracy = checkpoint['best_accuracy']\n","\n","scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","\n","optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n","\n","# compile the model\n","if compile:\n","    print(\"compiling the model... (takes a ~minute)\")\n","    unoptimized_model = model\n","    model = torch.compile(model) # requires PyTorch 2.0\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        # Get an iterator from the DataLoader\n","        dataloader = train_loader if split == 'train' else val_loader\n","        dataloader_iter = iter(dataloader)\n","\n","        for k in range(eval_iters):\n","            try:\n","                X, Y = next(dataloader_iter)\n","\n","            except StopIteration:\n","                # If we run out of batches, create a new iterator\n","                dataloader_iter = iter(dataloader)\n","                X, Y = next(dataloader_iter)\n","\n","            with ctx:\n","                X, Y = X.to(device), Y.to(device)\n","                logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","\n","def get_lr_for_iter(iter_num):\n","    \"\"\"Calculate learning rate based on iteration number using cosine decay with warmup.\"\"\"\n","    if iter_num < warmup_iters:\n","        return learning_rate * (iter_num + 1) / warmup_iters\n","    \n","    if iter_num >= lr_decay_iters:\n","        return min_lr\n","    \n","    decay_ratio = (iter_num - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n","    return min_lr + coeff * (learning_rate - min_lr)\n","\n","# logging\n","if wandb_log and master_process:\n","    import wandb\n","    wandb.init(project=wandb_project, name=wandb_run_name, config=config, dir = out_dir)\n","\n","\n","\n","\n","train_dataset = AdditionDataset(train_data_path, meta)\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    pin_memory=(device_type=='cuda')\n",")\n","\n","val_dataset = AdditionDataset(val_data_path, meta)\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    pin_memory=(device_type=='cuda')\n",")\n","\n","# encode, decode = get_encode_decode(meta_path, tokenizer=tokenizer)\n","\n","# Initialize result_dict with basic metrics\n","result_dict = {\n","    'iter': [],\n","    'train_loss': [],\n","    'val_loss': [],\n","    'test_acc': [],\n","    'train_acc': []\n","}\n","\n","# Initialize test accuracy keys for all test files\n","result_dict[f'test_acc'] = []\n","\n","result_dir = get_results_dir(config)\n","config['result_dir'] = result_dir\n","with open(os.path.join(result_dir, \"config.yaml\"), \"w\") as yaml_file:\n","    yaml.dump(config, yaml_file, default_flow_style=False)\n","\n","\n","# # build a dict of open file handles, one per dataset\n","# csv_writers = {}\n","# for dataset in stats_measurement_dataset_list:\n","#     name = dataset['name']\n","#     path = os.path.join(result_dir, f\"{name}_stats.csv\")\n","#     f = open(path, 'w', newline='')\n","#     writer = csv.DictWriter(f, fieldnames=[\n","#         'iter',\n","#         'ave_correct_probs',\n","#         'ave_correct_preds',\n","#         'ave_diff_probs_L1',\n","#         'ave_diff_probs_L2',\n","#         'ave_diff_probs_kl',\n","#         'ave_diff_logits_L1',\n","#         'ave_diff_logits_L2',\n","#         'ave_diff_preds',\n","#     ])\n","#     writer.writeheader()\n","#     csv_writers[name] = writer\n","\n","\n","# Initialize additional metrics for statistical measurements\n","stats_oo = [] # output-output mutual information\n","stats_io = [] # input-output mutual information\n","\n","\n","import time\n","t0 = time.time()\n","local_iter_num = 0 # number of iterations in the lifetime of this process\n","raw_model = model\n","running_mfu = -1.0\n","iter_num = 0\n","\n","max_iters = config.get('max_iters', 10000)\n"," # number of epochs to warm up learning rate\n","\n","# Initialize tracking variables\n","iter_num = 0\n","best_val_loss = 1e9\n","best_accuracy = -1\n","running_mfu = -1.0\n","\n","# Create infinite data loader\n","def get_infinite_dataloader(dataloader):\n","    while True:\n","        for batch in dataloader:\n","            yield batch\n","\n","train_loader_iter = get_infinite_dataloader(train_loader)\n","if 'max_new_tokens' in config.keys():\n","    print(f\"max_new_tokens: {config['max_new_tokens']}\")\n","else:\n","    print(f\"max_new_tokens used: {num_digit+2}\")\n","\n","# Training loop - iteration based\n","while iter_num < max_iters:\n","    model.train()\n","    \n","    # Get learning rate for current iteration\n","    if decay_lr:\n","        lr = get_lr_for_iter(iter_num)\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","    \n","    # Get next batch\n","    X, Y = next(train_loader_iter)\n","    X, Y = X.to(device), Y.to(device)\n","    \n","    # Forward pass\n","    with ctx:\n","        logits, loss = model(X, Y)\n","    \n","    # Backward pass\n","    scaler.scale(loss).backward()\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    scaler.step(optimizer)\n","    scaler.update()\n","    optimizer.zero_grad(set_to_none=True)\n","    \n","    # Do additional statistical measurements\n","    if mi_measurement:\n","        if iter_num in mi_measure_iters:\n","            model.eval()\n","            \n","            with torch.no_grad():\n","                # eval_res = eval_model(model, meta, stats_measurement_dataset_list, digits_per_num=num_digit, batch_size=test_batch_size)\n","                mi_stats = calc_model_dataset_mi(\n","                    model = model,\n","                    metadata = meta,\n","                    data = stats_measurement_data,\n","                    digits_per_num = num_digit,\n","                    batch_size = test_batch_size,\n","                    drop_leading_digit = drop_leading_digit\n","                )\n","\n","            # for name, stats in eval_res.items():\n","            #     if name == \"model_embeddings\":\n","            #         continue\n","            #     if name == 'base':\n","            #         row = {\n","            #             'iter': iter_num,\n","            #             'ave_correct_probs': stats['ave_correct_probs'],\n","            #             'ave_correct_preds': stats['ave_correct_preds'],\n","            #         }\n","            #     else:\n","            #         row = {\n","            #             'iter': iter_num,\n","            #             'ave_correct_probs': stats['ave_correct_probs'],\n","            #             'ave_correct_preds': stats['ave_correct_preds'],\n","            #             'ave_diff_probs_L1': stats['ave_diff_probs_L1'],\n","            #             'ave_diff_probs_L2': stats['ave_diff_probs_L2'],\n","            #             'ave_diff_probs_kl': stats['ave_diff_probs_kl'],\n","            #             'ave_diff_logits_L1': stats['ave_diff_logits_L1'],\n","            #             'ave_diff_logits_L2': stats['ave_diff_logits_L2'],\n","            #             'ave_diff_preds': stats['ave_diff_preds'],\n","            #         }\n","            #     # Write to the CSV file for this dataset\n","            #     csv_writers[name].writerow(row)\n","\n","            \n","            # Calculate output-output mutual information\n","            mi_mat = mi_stats['output-output']['mutual_info']\n","            nmi_mat = mi_stats['output-output']['normalized_mutual_info']\n","            for i in range(mi_mat.shape[0]):\n","                for j in range(i, mi_mat.shape[1]):\n","                    stats_oo.append({\n","                        'iter': iter_num,\n","                        'i': i,\n","                        'j': j,\n","                        'mi': mi_mat[i, j].item(),\n","                        'nmi': nmi_mat[i, j].item()\n","                    })\n","\n","            # also calculate input-output mutual information\n","            mi_mat_io = mi_stats['input-output']['mutual_info']\n","            nmi_mat_io = mi_stats['input-output']['normalized_mutual_info']\n","            for i in range(mi_mat_io.shape[0]):\n","                for j in range(mi_mat_io.shape[1]):\n","                    stats_io.append({\n","                        'iter': iter_num,\n","                        'i': i,\n","                        'j': j,\n","                        'mi': mi_mat_io[i, j].item(),\n","                        'nmi': nmi_mat_io[i, j].item()\n","                    })\n","\n","            # **NOW write out the two MI CSVs immediately:**\n","            stats_oo_df = pd.DataFrame(stats_oo)\n","            stats_oo_df.to_csv(os.path.join(result_dir, 'output_output_mi.csv'), index=False)\n","\n","            stats_io_df = pd.DataFrame(stats_io)\n","            stats_io_df.to_csv(os.path.join(result_dir, 'input_output_mi.csv'), index=False)\n","\n","            model.train()\n","        \n","    # Evaluation\n","    if iter_num % eval_interval == 0 or (more_early_eval1 and iter_num <= early_eval_border1 and iter_num % early_eval_interval1 == 0) or (more_early_eval2 and iter_num <= early_eval_border2 and iter_num % early_eval_interval2 == 0):\n","        losses = estimate_loss()\n","        print(f\"iter {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        \n","        # Initialize wandb_dict for this iteration\n","        wandb_dict = {\n","            \"iter\": iter_num,\n","            \"train/loss\": losses['train'],\n","            \"val/loss\": losses['val'],\n","            \"lr\": lr,\n","        }\n","\n","        if losses['val'] < best_val_loss:\n","            best_val_loss = losses['val']\n","        \n","        # Regular test evaluation\n","        test_accuracy = None\n","        if eval_addition:\n","            test_name, test_accuracy, _ , correct, incorrect = evaluate_multiple_files(\n","                config, model, ctx,\n","                encode=lambda x: encode_addition(x, meta),\n","                decode=lambda x: decode_addition(x, meta),\n","                test_file=test_file_path,\n","                iter_num=iter_num,\n","                result_dir=result_dir,\n","                verbose=False,\n","                num_digit=num_digit,\n","                zero_pad=zero_pad,\n","                reverse_ab=reverse_ab,\n","                reverse_c=reverse_c,\n","                data_type=data_type,\n","                operator=operator,\n","                data_format=data_format,\n","                analyze=True,\n","                mode=mode\n","            )\n","\n","            # Log results\n","            print(\"\\nTest Results:\")\n","            print(f\"{test_name}: {test_accuracy:.2f}%\")\n","            # Add accuracy to result_dict (key was initialized at start)\n","            result_dict[f'test_acc'].append(test_accuracy)\n","\n","            print()\n","            \n","            # Add test accuracy to wandb_dict\n","            wandb_dict[\"test/accuracy\"] = test_accuracy\n","            \n","            if test_accuracy > best_accuracy and iter_num % 5 * eval_interval == 0:\n","                best_accuracy = test_accuracy\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'best_accuracy': best_accuracy,\n","                    'config': config,\n","                    'meta': meta,\n","                }\n","                torch.save(checkpoint, os.path.join(out_dir, f'ckpt_iter_{iter_num}_acc.pt'))\n","        \n","        # Training data evaluation\n","        train_accuracy = None\n","        if eval_addition_train:\n","            config['start'] = start_train\n","            train_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","                config, model, ctx, \n","                encode=lambda x: encode_addition(x, meta),\n","                decode=lambda x: decode_addition(x, meta), \n","                verbose=False, \n","                num_digit=num_digit, \n","                zero_pad=zero_pad,\n","                reverse_ab=reverse_ab, \n","                reverse_c=reverse_c,\n","                data_type=data_type, \n","                operator=operator, \n","                data_format=data_format,\n","                mode=mode\n","            )\n","            \n","            # Add train accuracy to wandb_dict\n","            wandb_dict[\"train/accuracy\"] = train_accuracy\n","        \n","        # Update and save basic metrics\n","        result_dict['iter'].append(iter_num)\n","        result_dict['train_loss'].append(losses['train'].item())\n","        result_dict['val_loss'].append(losses['val'].item())\n","        result_dict['test_acc'].append(test_accuracy)\n","        result_dict['train_acc'].append(train_accuracy)\n","        \n","        # Save results to CSV after each evaluation\n","        result_df = pd.DataFrame(result_dict)\n","        result_df.to_csv(os.path.join(result_dir, 'training_metrics.csv'), index=False)\n","        \n","        # Single wandb log per iteration with all metrics\n","        if wandb_log:\n","            wandb.log(wandb_dict)\n","    \n","    iter_num += 1\n","\n","# Save final checkpoint\n","checkpoint = {\n","    'model': raw_model.state_dict(),\n","    'optimizer': optimizer.state_dict(),\n","    'model_args': model_args,\n","    'iter_num': iter_num,\n","    'best_val_loss': best_val_loss,\n","    'best_accuracy': best_accuracy,\n","    'config': config,\n","    'meta': meta,\n","}\n","torch.save(checkpoint, os.path.join(out_dir, f'ckpt_final.pt'))\n","\n","\n","losses = estimate_loss()\n","\n","if eval_addition:\n","    config['start'] = f\"FILE:{test_file_path}\"\n","    test_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","        config, model, ctx, \n","        encode=lambda x: encode_addition(x, meta),\n","        decode=lambda x: decode_addition(x, meta), \n","        verbose=False, \n","        num_digit=num_digit, \n","        zero_pad=zero_pad,\n","        reverse_ab=reverse_ab, \n","        reverse_c=reverse_c,\n","        data_type=data_type, \n","        operator=operator, \n","        data_format=data_format, \n","        analyze=True,\n","        mode=mode\n","    )\n","    import csv\n","    # Save correct examples\n","    correct_path = os.path.join(result_dir, 'correct_examples.csv')\n","    with open(correct_path, 'w', newline='') as csvfile:\n","        fieldnames = ['operands', 'result', 'outcome', 'c_hat2']\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","        writer.writeheader()\n","        for i, nums in enumerate(correct):\n","            operands, result, outcome, c_hat2 = nums\n","            writer.writerow({'operands': operands, 'result': result, 'outcome': outcome, 'c_hat2': c_hat2})\n","    \n","    # Save incorrect examples\n","    incorrect_path = os.path.join(result_dir, 'incorrect_examples.csv')\n","    with open(incorrect_path, 'w', newline='') as csvfile:\n","        fieldnames = ['operands', 'result', 'outcome', 'c_hat2']\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","        writer.writeheader()\n","        for i, nums in enumerate(incorrect):\n","            operands, result, outcome, c_hat2 = nums\n","            writer.writerow({'operands': operands, 'result': result, 'outcome': outcome, 'c_hat2': c_hat2})\n","\n","if eval_addition_train:\n","    config['start'] = start_train\n","    train_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","        config, model, ctx, \n","        encode=lambda x: encode_addition(x, meta),\n","        decode=lambda x: decode_addition(x, meta), \n","        verbose=False, \n","        num_digit=num_digit, \n","        zero_pad=zero_pad,\n","        reverse_ab=reverse_ab, \n","        reverse_c=reverse_c,\n","        data_type=data_type, \n","        operator=operator, \n","        data_format=data_format,\n","        mode=mode\n","    )\n","    \n","    \n","test_name, accuracy, metrics, correct, incorrect = evaluate_multiple_files(\n","    config, model, ctx,\n","    encode=lambda x: encode_addition(x, meta),\n","    decode=lambda x: decode_addition(x, meta),\n","    test_file=test_file_path,\n","    iter_num='final',\n","    result_dir=result_dir,\n","    verbose=False,\n","    num_digit=num_digit,\n","    zero_pad=zero_pad,\n","    reverse_ab=reverse_ab,\n","    reverse_c=reverse_c,\n","    data_type=data_type,\n","    operator=operator,\n","    data_format=data_format,\n","    analyze=True,\n","    mode=mode\n",")\n","\n","print(\"\\nFinal Test Results:\")\n","print(f\"{test_name}: {accuracy:.2f}%\")\n","print()\n","\n","\n","# Final wandb logging\n","if wandb_log:\n","    final_dict = {\n","        \"iter\": iter_num,\n","        \"train/loss\": losses['train'],\n","        \"val/loss\": losses['val'],\n","        \"lr\": lr,\n","        \"test/accuracy\": test_accuracy if eval_addition else None,\n","        \"train/accuracy\": train_accuracy if eval_addition_train else None,\n","    }\n","    final_dict[f\"final_test/accuracy\"] = accuracy\n","    wandb.log(final_dict)\n","\n","# Save final DataFrame\n","result_df = pd.DataFrame(result_dict)\n","result_df.to_csv(os.path.join(result_dir, 'training_metrics.csv'), index=False)\n"]}],"source":["%cat ./train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"zsZrWKHVD7Ii","outputId":"71858713-d883-45ff-d30e-1cab31cd0aaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=888\n","Skipping y_hat=63333\n","Skipping y_hat=63333\n","Skipping y_hat=63333\n","Skipping y_hat=63333\n","Skipping y_hat=63333\n","Skipping y_hat=63333\n","Skipping y_hat=63333\n","Skipping y_hat=88056\n","Skipping y_hat=88056\n","Skipping y_hat=88056\n","Skipping y_hat=88056\n","Skipping y_hat=88056\n","Skipping y_hat=88056\n","Skipping y_hat=88056\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=03633\n","Skipping y_hat=03633\n","Skipping y_hat=03633\n","Skipping y_hat=03633\n","Skipping y_hat=03633\n","Skipping y_hat=03633\n","Skipping y_hat=03633\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=18750\n","Skipping y_hat=18750\n","Skipping y_hat=18750\n","Skipping y_hat=18750\n","Skipping y_hat=18750\n","Skipping y_hat=18750\n","Skipping y_hat=18750\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=5\n","555\n","Skipping y_hat=5\n","555\n","Skipping y_hat=5\n","555\n","Skipping y_hat=5\n","555\n","Skipping y_hat=5\n","555\n","Skipping y_hat=5\n","555\n","Skipping y_hat=5\n","555\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=749++\n","Skipping y_hat=749++\n","Skipping y_hat=749++\n","Skipping y_hat=749++\n","Skipping y_hat=749++\n","Skipping y_hat=749++\n","Skipping y_hat=749++\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=5533\n","Skipping y_hat=5533\n","Skipping y_hat=5533\n","Skipping y_hat=5533\n","Skipping y_hat=5533\n","Skipping y_hat=5533\n","Skipping y_hat=5533\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=0669\n","Skipping y_hat=0669\n","Skipping y_hat=0669\n","Skipping y_hat=0669\n","Skipping y_hat=0669\n","Skipping y_hat=0669\n","Skipping y_hat=0669\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=2\n","4\n","Skipping y_hat=2\n","4\n","Skipping y_hat=2\n","4\n","Skipping y_hat=2\n","4\n","Skipping y_hat=2\n","4\n","Skipping y_hat=2\n","4\n","Skipping y_hat=2\n","4\n","Skipping y_hat=12347\n","Skipping y_hat=12347\n","Skipping y_hat=12347\n","Skipping y_hat=12347\n","Skipping y_hat=12347\n","Skipping y_hat=12347\n","Skipping y_hat=12347\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=7+672\n","Skipping y_hat=7+672\n","Skipping y_hat=7+672\n","Skipping y_hat=7+672\n","Skipping y_hat=7+672\n","Skipping y_hat=7+672\n","Skipping y_hat=7+672\n","Skipping y_hat=66003\n","Skipping y_hat=66003\n","Skipping y_hat=66003\n","Skipping y_hat=66003\n","Skipping y_hat=66003\n","Skipping y_hat=66003\n","Skipping y_hat=66003\n","Skipping y_hat=2840\n","Skipping y_hat=2840\n","Skipping y_hat=2840\n","Skipping y_hat=2840\n","Skipping y_hat=2840\n","Skipping y_hat=2840\n","Skipping y_hat=2840\n","Skipping y_hat=00\n","Skipping y_hat=00\n","Skipping y_hat=00\n","Skipping y_hat=00\n","Skipping y_hat=00\n","Skipping y_hat=00\n","Skipping y_hat=00\n","Skipping y_hat=8\n","007\n","Skipping y_hat=8\n","007\n","Skipping y_hat=8\n","007\n","Skipping y_hat=8\n","007\n","Skipping y_hat=8\n","007\n","Skipping y_hat=8\n","007\n","Skipping y_hat=8\n","007\n","Skipping y_hat=+1888\n","Skipping y_hat=+1888\n","Skipping y_hat=+1888\n","Skipping y_hat=+1888\n","Skipping y_hat=+1888\n","Skipping y_hat=+1888\n","Skipping y_hat=+1888\n","Skipping y_hat=9600\n","Skipping y_hat=9600\n","Skipping y_hat=9600\n","Skipping y_hat=9600\n","Skipping y_hat=9600\n","Skipping y_hat=9600\n","Skipping y_hat=9600\n","Skipping y_hat=699+7\n","Skipping y_hat=699+7\n","Skipping y_hat=699+7\n","Skipping y_hat=699+7\n","Skipping y_hat=699+7\n","Skipping y_hat=699+7\n","Skipping y_hat=699+7\n","Skipping y_hat=53351\n","Skipping y_hat=53351\n","Skipping y_hat=53351\n","Skipping y_hat=53351\n","Skipping y_hat=53351\n","Skipping y_hat=53351\n","Skipping y_hat=53351\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=996+\n","Skipping y_hat=996+\n","Skipping y_hat=996+\n","Skipping y_hat=996+\n","Skipping y_hat=996+\n","Skipping y_hat=996+\n","Skipping y_hat=996+\n","Skipping y_hat=01\n","Skipping y_hat=01\n","Skipping y_hat=01\n","Skipping y_hat=01\n","Skipping y_hat=01\n","Skipping y_hat=01\n","Skipping y_hat=01\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=55332\n","Skipping y_hat=55332\n","Skipping y_hat=55332\n","Skipping y_hat=55332\n","Skipping y_hat=55332\n","Skipping y_hat=55332\n","Skipping y_hat=55332\n","Skipping y_hat=92440\n","Skipping y_hat=92440\n","Skipping y_hat=92440\n","Skipping y_hat=92440\n","Skipping y_hat=92440\n","Skipping y_hat=92440\n","Skipping y_hat=92440\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=40399\n","Skipping y_hat=40399\n","Skipping y_hat=40399\n","Skipping y_hat=40399\n","Skipping y_hat=40399\n","Skipping y_hat=40399\n","Skipping y_hat=40399\n","Skipping y_hat=3339+\n","Skipping y_hat=3339+\n","Skipping y_hat=3339+\n","Skipping y_hat=3339+\n","Skipping y_hat=3339+\n","Skipping y_hat=3339+\n","Skipping y_hat=3339+\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=9++5\n","Skipping y_hat=9++5\n","Skipping y_hat=9++5\n","Skipping y_hat=9++5\n","Skipping y_hat=9++5\n","Skipping y_hat=9++5\n","Skipping y_hat=9++5\n","Skipping y_hat=++508\n","Skipping y_hat=++508\n","Skipping y_hat=++508\n","Skipping y_hat=++508\n","Skipping y_hat=++508\n","Skipping y_hat=++508\n","Skipping y_hat=++508\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=89918\n","Skipping y_hat=89918\n","Skipping y_hat=89918\n","Skipping y_hat=89918\n","Skipping y_hat=89918\n","Skipping y_hat=89918\n","Skipping y_hat=89918\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=7\n","\n","\n","4\n","Skipping y_hat=7\n","\n","\n","4\n","Skipping y_hat=7\n","\n","\n","4\n","Skipping y_hat=7\n","\n","\n","4\n","Skipping y_hat=7\n","\n","\n","4\n","Skipping y_hat=7\n","\n","\n","4\n","Skipping y_hat=7\n","\n","\n","4\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=1++9\n","Skipping y_hat=1++9\n","Skipping y_hat=1++9\n","Skipping y_hat=1++9\n","Skipping y_hat=1++9\n","Skipping y_hat=1++9\n","Skipping y_hat=1++9\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=99844\n","Skipping y_hat=99844\n","Skipping y_hat=99844\n","Skipping y_hat=99844\n","Skipping y_hat=99844\n","Skipping y_hat=99844\n","Skipping y_hat=99844\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=09240\n","Skipping y_hat=09240\n","Skipping y_hat=09240\n","Skipping y_hat=09240\n","Skipping y_hat=09240\n","Skipping y_hat=09240\n","Skipping y_hat=09240\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=90\n","50\n","Skipping y_hat=90\n","50\n","Skipping y_hat=90\n","50\n","Skipping y_hat=90\n","50\n","Skipping y_hat=90\n","50\n","Skipping y_hat=90\n","50\n","Skipping y_hat=90\n","50\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=33046\n","Skipping y_hat=33046\n","Skipping y_hat=33046\n","Skipping y_hat=33046\n","Skipping y_hat=33046\n","Skipping y_hat=33046\n","Skipping y_hat=33046\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=15558\n","Skipping y_hat=15558\n","Skipping y_hat=15558\n","Skipping y_hat=15558\n","Skipping y_hat=15558\n","Skipping y_hat=15558\n","Skipping y_hat=15558\n","Skipping y_hat=88776\n","Skipping y_hat=88776\n","Skipping y_hat=88776\n","Skipping y_hat=88776\n","Skipping y_hat=88776\n","Skipping y_hat=88776\n","Skipping y_hat=88776\n","Skipping y_hat=23394\n","Skipping y_hat=23394\n","Skipping y_hat=23394\n","Skipping y_hat=23394\n","Skipping y_hat=23394\n","Skipping y_hat=23394\n","Skipping y_hat=23394\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=1+93\n","Skipping y_hat=1+93\n","Skipping y_hat=1+93\n","Skipping y_hat=1+93\n","Skipping y_hat=1+93\n","Skipping y_hat=1+93\n","Skipping y_hat=1+93\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=6\n","69\n","Skipping y_hat=6\n","69\n","Skipping y_hat=6\n","69\n","Skipping y_hat=6\n","69\n","Skipping y_hat=6\n","69\n","Skipping y_hat=6\n","69\n","Skipping y_hat=6\n","69\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=65722\n","Skipping y_hat=65722\n","Skipping y_hat=65722\n","Skipping y_hat=65722\n","Skipping y_hat=65722\n","Skipping y_hat=65722\n","Skipping y_hat=65722\n","Skipping y_hat=75559\n","Skipping y_hat=75559\n","Skipping y_hat=75559\n","Skipping y_hat=75559\n","Skipping y_hat=75559\n","Skipping y_hat=75559\n","Skipping y_hat=75559\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=99950\n","Skipping y_hat=99950\n","Skipping y_hat=99950\n","Skipping y_hat=99950\n","Skipping y_hat=99950\n","Skipping y_hat=99950\n","Skipping y_hat=99950\n","Skipping y_hat=0+7\n","Skipping y_hat=0+7\n","Skipping y_hat=0+7\n","Skipping y_hat=0+7\n","Skipping y_hat=0+7\n","Skipping y_hat=0+7\n","Skipping y_hat=0+7\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=+43+4\n","Skipping y_hat=+43+4\n","Skipping y_hat=+43+4\n","Skipping y_hat=+43+4\n","Skipping y_hat=+43+4\n","Skipping y_hat=+43+4\n","Skipping y_hat=+43+4\n","Skipping y_hat=2555\n","Skipping y_hat=2555\n","Skipping y_hat=2555\n","Skipping y_hat=2555\n","Skipping y_hat=2555\n","Skipping y_hat=2555\n","Skipping y_hat=2555\n","Skipping y_hat=16773\n","Skipping y_hat=16773\n","Skipping y_hat=16773\n","Skipping y_hat=16773\n","Skipping y_hat=16773\n","Skipping y_hat=16773\n","Skipping y_hat=16773\n"," 76% 19/25 [00:02<00:00,  8.61it/s]Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=860\n","0\n","Skipping y_hat=860\n","0\n","Skipping y_hat=860\n","0\n","Skipping y_hat=860\n","0\n","Skipping y_hat=860\n","0\n","Skipping y_hat=860\n","0\n","Skipping y_hat=860\n","0\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=7+878\n","Skipping y_hat=7+878\n","Skipping y_hat=7+878\n","Skipping y_hat=7+878\n","Skipping y_hat=7+878\n","Skipping y_hat=7+878\n","Skipping y_hat=7+878\n","Skipping y_hat=7233\n","Skipping y_hat=7233\n","Skipping y_hat=7233\n","Skipping y_hat=7233\n","Skipping y_hat=7233\n","Skipping y_hat=7233\n","Skipping y_hat=7233\n","Skipping y_hat=+2\n","Skipping y_hat=+2\n","Skipping y_hat=+2\n","Skipping y_hat=+2\n","Skipping y_hat=+2\n","Skipping y_hat=+2\n","Skipping y_hat=+2\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=55552\n","Skipping y_hat=55552\n","Skipping y_hat=55552\n","Skipping y_hat=55552\n","Skipping y_hat=55552\n","Skipping y_hat=55552\n","Skipping y_hat=55552\n","Skipping y_hat=882\n","0\n","Skipping y_hat=882\n","0\n","Skipping y_hat=882\n","0\n","Skipping y_hat=882\n","0\n","Skipping y_hat=882\n","0\n","Skipping y_hat=882\n","0\n","Skipping y_hat=882\n","0\n","Skipping y_hat=791+\n","Skipping y_hat=791+\n","Skipping y_hat=791+\n","Skipping y_hat=791+\n","Skipping y_hat=791+\n","Skipping y_hat=791+\n","Skipping y_hat=791+\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=40490\n","Skipping y_hat=40490\n","Skipping y_hat=40490\n","Skipping y_hat=40490\n","Skipping y_hat=40490\n","Skipping y_hat=40490\n","Skipping y_hat=40490\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=70196\n","Skipping y_hat=70196\n","Skipping y_hat=70196\n","Skipping y_hat=70196\n","Skipping y_hat=70196\n","Skipping y_hat=70196\n","Skipping y_hat=70196\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=90\n","Skipping y_hat=90\n","Skipping y_hat=90\n","Skipping y_hat=90\n","Skipping y_hat=90\n","Skipping y_hat=90\n","Skipping y_hat=90\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=89953\n","Skipping y_hat=89953\n","Skipping y_hat=89953\n","Skipping y_hat=89953\n","Skipping y_hat=89953\n","Skipping y_hat=89953\n","Skipping y_hat=89953\n","Skipping y_hat=6+++\n","Skipping y_hat=6+++\n","Skipping y_hat=6+++\n","Skipping y_hat=6+++\n","Skipping y_hat=6+++\n","Skipping y_hat=6+++\n","Skipping y_hat=6+++\n","Skipping y_hat=882+3\n","Skipping y_hat=882+3\n","Skipping y_hat=882+3\n","Skipping y_hat=882+3\n","Skipping y_hat=882+3\n","Skipping y_hat=882+3\n","Skipping y_hat=882+3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=873++\n","Skipping y_hat=873++\n","Skipping y_hat=873++\n","Skipping y_hat=873++\n","Skipping y_hat=873++\n","Skipping y_hat=873++\n","Skipping y_hat=873++\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=31227\n","Skipping y_hat=31227\n","Skipping y_hat=31227\n","Skipping y_hat=31227\n","Skipping y_hat=31227\n","Skipping y_hat=31227\n","Skipping y_hat=31227\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=84218\n","Skipping y_hat=84218\n","Skipping y_hat=84218\n","Skipping y_hat=84218\n","Skipping y_hat=84218\n","Skipping y_hat=84218\n","Skipping y_hat=84218\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=47277\n","Skipping y_hat=47277\n","Skipping y_hat=47277\n","Skipping y_hat=47277\n","Skipping y_hat=47277\n","Skipping y_hat=47277\n","Skipping y_hat=47277\n","Skipping y_hat=69555\n","Skipping y_hat=69555\n","Skipping y_hat=69555\n","Skipping y_hat=69555\n","Skipping y_hat=69555\n","Skipping y_hat=69555\n","Skipping y_hat=69555\n","Skipping y_hat=18\n","Skipping y_hat=18\n","Skipping y_hat=18\n","Skipping y_hat=18\n","Skipping y_hat=18\n","Skipping y_hat=18\n","Skipping y_hat=18\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=88\n","03\n","Skipping y_hat=88\n","03\n","Skipping y_hat=88\n","03\n","Skipping y_hat=88\n","03\n","Skipping y_hat=88\n","03\n","Skipping y_hat=88\n","03\n","Skipping y_hat=88\n","03\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=64\n","25\n","Skipping y_hat=64\n","25\n","Skipping y_hat=64\n","25\n","Skipping y_hat=64\n","25\n","Skipping y_hat=64\n","25\n","Skipping y_hat=64\n","25\n","Skipping y_hat=64\n","25\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=92\n","Skipping y_hat=92\n","Skipping y_hat=92\n","Skipping y_hat=92\n","Skipping y_hat=92\n","Skipping y_hat=92\n","Skipping y_hat=92\n","Skipping y_hat=12992\n","Skipping y_hat=12992\n","Skipping y_hat=12992\n","Skipping y_hat=12992\n","Skipping y_hat=12992\n","Skipping y_hat=12992\n","Skipping y_hat=12992\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=+9192\n","Skipping y_hat=+9192\n","Skipping y_hat=+9192\n","Skipping y_hat=+9192\n","Skipping y_hat=+9192\n","Skipping y_hat=+9192\n","Skipping y_hat=+9192\n","Skipping y_hat=52119\n","Skipping y_hat=52119\n","Skipping y_hat=52119\n","Skipping y_hat=52119\n","Skipping y_hat=52119\n","Skipping y_hat=52119\n","Skipping y_hat=52119\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=53840\n","Skipping y_hat=53840\n","Skipping y_hat=53840\n","Skipping y_hat=53840\n","Skipping y_hat=53840\n","Skipping y_hat=53840\n","Skipping y_hat=53840\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=51551\n","Skipping y_hat=51551\n","Skipping y_hat=51551\n","Skipping y_hat=51551\n","Skipping y_hat=51551\n","Skipping y_hat=51551\n","Skipping y_hat=51551\n","Skipping y_hat=15\n","9+\n","Skipping y_hat=15\n","9+\n","Skipping y_hat=15\n","9+\n","Skipping y_hat=15\n","9+\n","Skipping y_hat=15\n","9+\n","Skipping y_hat=15\n","9+\n","Skipping y_hat=15\n","9+\n","Skipping y_hat=2\n","Skipping y_hat=2\n","Skipping y_hat=2\n","Skipping y_hat=2\n","Skipping y_hat=2\n","Skipping y_hat=2\n","Skipping y_hat=2\n","Skipping y_hat=8+5+2\n","Skipping y_hat=8+5+2\n","Skipping y_hat=8+5+2\n","Skipping y_hat=8+5+2\n","Skipping y_hat=8+5+2\n","Skipping y_hat=8+5+2\n","Skipping y_hat=8+5+2\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=85170\n","Skipping y_hat=85170\n","Skipping y_hat=85170\n","Skipping y_hat=85170\n","Skipping y_hat=85170\n","Skipping y_hat=85170\n","Skipping y_hat=85170\n","Skipping y_hat=38420\n","Skipping y_hat=38420\n","Skipping y_hat=38420\n","Skipping y_hat=38420\n","Skipping y_hat=38420\n","Skipping y_hat=38420\n","Skipping y_hat=38420\n","Skipping y_hat=39733\n","Skipping y_hat=39733\n","Skipping y_hat=39733\n","Skipping y_hat=39733\n","Skipping y_hat=39733\n","Skipping y_hat=39733\n","Skipping y_hat=39733\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=20669\n","Skipping y_hat=20669\n","Skipping y_hat=20669\n","Skipping y_hat=20669\n","Skipping y_hat=20669\n","Skipping y_hat=20669\n","Skipping y_hat=20669\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=15552\n","Skipping y_hat=15552\n","Skipping y_hat=15552\n","Skipping y_hat=15552\n","Skipping y_hat=15552\n","Skipping y_hat=15552\n","Skipping y_hat=15552\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=6095\n","Skipping y_hat=6095\n","Skipping y_hat=6095\n","Skipping y_hat=6095\n","Skipping y_hat=6095\n","Skipping y_hat=6095\n","Skipping y_hat=6095\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=41+93\n","Skipping y_hat=41+93\n","Skipping y_hat=41+93\n","Skipping y_hat=41+93\n","Skipping y_hat=41+93\n","Skipping y_hat=41+93\n","Skipping y_hat=41+93\n","Skipping y_hat=0255\n","Skipping y_hat=0255\n","Skipping y_hat=0255\n","Skipping y_hat=0255\n","Skipping y_hat=0255\n","Skipping y_hat=0255\n","Skipping y_hat=0255\n","Skipping y_hat=57444\n","Skipping y_hat=57444\n","Skipping y_hat=57444\n","Skipping y_hat=57444\n","Skipping y_hat=57444\n","Skipping y_hat=57444\n","Skipping y_hat=57444\n","Skipping y_hat=91107\n","Skipping y_hat=91107\n","Skipping y_hat=91107\n","Skipping y_hat=91107\n","Skipping y_hat=91107\n","Skipping y_hat=91107\n","Skipping y_hat=91107\n","Skipping y_hat=56655\n","Skipping y_hat=56655\n","Skipping y_hat=56655\n","Skipping y_hat=56655\n","Skipping y_hat=56655\n","Skipping y_hat=56655\n","Skipping y_hat=56655\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=26122\n","Skipping y_hat=26122\n","Skipping y_hat=26122\n","Skipping y_hat=26122\n","Skipping y_hat=26122\n","Skipping y_hat=26122\n","Skipping y_hat=26122\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=6\n","395\n","Skipping y_hat=6\n","395\n","Skipping y_hat=6\n","395\n","Skipping y_hat=6\n","395\n","Skipping y_hat=6\n","395\n","Skipping y_hat=6\n","395\n","Skipping y_hat=6\n","395\n","Skipping y_hat=8035+\n","Skipping y_hat=8035+\n","Skipping y_hat=8035+\n","Skipping y_hat=8035+\n","Skipping y_hat=8035+\n","Skipping y_hat=8035+\n","Skipping y_hat=8035+\n","Skipping y_hat=9+611\n","Skipping y_hat=9+611\n","Skipping y_hat=9+611\n","Skipping y_hat=9+611\n","Skipping y_hat=9+611\n","Skipping y_hat=9+611\n","Skipping y_hat=9+611\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=68\n","Skipping y_hat=68\n","Skipping y_hat=68\n","Skipping y_hat=68\n","Skipping y_hat=68\n","Skipping y_hat=68\n","Skipping y_hat=68\n","Skipping y_hat=5++44\n","Skipping y_hat=5++44\n","Skipping y_hat=5++44\n","Skipping y_hat=5++44\n","Skipping y_hat=5++44\n","Skipping y_hat=5++44\n","Skipping y_hat=5++44\n","Skipping y_hat=77211\n","Skipping y_hat=77211\n","Skipping y_hat=77211\n","Skipping y_hat=77211\n","Skipping y_hat=77211\n","Skipping y_hat=77211\n","Skipping y_hat=77211\n","Skipping y_hat=2213\n","Skipping y_hat=2213\n","Skipping y_hat=2213\n","Skipping y_hat=2213\n","Skipping y_hat=2213\n","Skipping y_hat=2213\n","Skipping y_hat=2213\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=59+40\n","Skipping y_hat=59+40\n","Skipping y_hat=59+40\n","Skipping y_hat=59+40\n","Skipping y_hat=59+40\n","Skipping y_hat=59+40\n","Skipping y_hat=59+40\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=67+98\n","Skipping y_hat=67+98\n","Skipping y_hat=67+98\n","Skipping y_hat=67+98\n","Skipping y_hat=67+98\n","Skipping y_hat=67+98\n","Skipping y_hat=67+98\n","Skipping y_hat=76630\n","Skipping y_hat=76630\n","Skipping y_hat=76630\n","Skipping y_hat=76630\n","Skipping y_hat=76630\n","Skipping y_hat=76630\n","Skipping y_hat=76630\n","Skipping y_hat=15222\n","Skipping y_hat=15222\n","Skipping y_hat=15222\n","Skipping y_hat=15222\n","Skipping y_hat=15222\n","Skipping y_hat=15222\n","Skipping y_hat=15222\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=4+33\n","Skipping y_hat=4+33\n","Skipping y_hat=4+33\n","Skipping y_hat=4+33\n","Skipping y_hat=4+33\n","Skipping y_hat=4+33\n","Skipping y_hat=4+33\n","Skipping y_hat=4\n","\n","52\n","Skipping y_hat=4\n","\n","52\n","Skipping y_hat=4\n","\n","52\n","Skipping y_hat=4\n","\n","52\n","Skipping y_hat=4\n","\n","52\n","Skipping y_hat=4\n","\n","52\n","Skipping y_hat=4\n","\n","52\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=+7788\n","Skipping y_hat=+7788\n","Skipping y_hat=+7788\n","Skipping y_hat=+7788\n","Skipping y_hat=+7788\n","Skipping y_hat=+7788\n","Skipping y_hat=+7788\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=2\n","333\n","Skipping y_hat=2\n","333\n","Skipping y_hat=2\n","333\n","Skipping y_hat=2\n","333\n","Skipping y_hat=2\n","333\n","Skipping y_hat=2\n","333\n","Skipping y_hat=2\n","333\n","Skipping y_hat=99++4\n","Skipping y_hat=99++4\n","Skipping y_hat=99++4\n","Skipping y_hat=99++4\n","Skipping y_hat=99++4\n","Skipping y_hat=99++4\n","Skipping y_hat=99++4\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=677+\n","Skipping y_hat=677+\n","Skipping y_hat=677+\n","Skipping y_hat=677+\n","Skipping y_hat=677+\n","Skipping y_hat=677+\n","Skipping y_hat=677+\n","Skipping y_hat=020\n","Skipping y_hat=020\n","Skipping y_hat=020\n","Skipping y_hat=020\n","Skipping y_hat=020\n","Skipping y_hat=020\n","Skipping y_hat=020\n"," 80% 20/25 [00:02<00:00,  8.69it/s]Skipping y_hat=92523\n","Skipping y_hat=92523\n","Skipping y_hat=92523\n","Skipping y_hat=92523\n","Skipping y_hat=92523\n","Skipping y_hat=92523\n","Skipping y_hat=92523\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=09583\n","Skipping y_hat=09583\n","Skipping y_hat=09583\n","Skipping y_hat=09583\n","Skipping y_hat=09583\n","Skipping y_hat=09583\n","Skipping y_hat=09583\n","Skipping y_hat=+995\n","Skipping y_hat=+995\n","Skipping y_hat=+995\n","Skipping y_hat=+995\n","Skipping y_hat=+995\n","Skipping y_hat=+995\n","Skipping y_hat=+995\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=99991\n","Skipping y_hat=99991\n","Skipping y_hat=99991\n","Skipping y_hat=99991\n","Skipping y_hat=99991\n","Skipping y_hat=99991\n","Skipping y_hat=99991\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=85551\n","Skipping y_hat=85551\n","Skipping y_hat=85551\n","Skipping y_hat=85551\n","Skipping y_hat=85551\n","Skipping y_hat=85551\n","Skipping y_hat=85551\n","Skipping y_hat=82\n","\n","0\n","Skipping y_hat=82\n","\n","0\n","Skipping y_hat=82\n","\n","0\n","Skipping y_hat=82\n","\n","0\n","Skipping y_hat=82\n","\n","0\n","Skipping y_hat=82\n","\n","0\n","Skipping y_hat=82\n","\n","0\n","Skipping y_hat=09+3\n","Skipping y_hat=09+3\n","Skipping y_hat=09+3\n","Skipping y_hat=09+3\n","Skipping y_hat=09+3\n","Skipping y_hat=09+3\n","Skipping y_hat=09+3\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=09994\n","Skipping y_hat=09994\n","Skipping y_hat=09994\n","Skipping y_hat=09994\n","Skipping y_hat=09994\n","Skipping y_hat=09994\n","Skipping y_hat=09994\n","Skipping y_hat=0\n","240\n","Skipping y_hat=0\n","240\n","Skipping y_hat=0\n","240\n","Skipping y_hat=0\n","240\n","Skipping y_hat=0\n","240\n","Skipping y_hat=0\n","240\n","Skipping y_hat=0\n","240\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=99+3\n","Skipping y_hat=99+3\n","Skipping y_hat=99+3\n","Skipping y_hat=99+3\n","Skipping y_hat=99+3\n","Skipping y_hat=99+3\n","Skipping y_hat=99+3\n","Skipping y_hat=44822\n","Skipping y_hat=44822\n","Skipping y_hat=44822\n","Skipping y_hat=44822\n","Skipping y_hat=44822\n","Skipping y_hat=44822\n","Skipping y_hat=44822\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=81281\n","Skipping y_hat=81281\n","Skipping y_hat=81281\n","Skipping y_hat=81281\n","Skipping y_hat=81281\n","Skipping y_hat=81281\n","Skipping y_hat=81281\n","Skipping y_hat=4883+\n","Skipping y_hat=4883+\n","Skipping y_hat=4883+\n","Skipping y_hat=4883+\n","Skipping y_hat=4883+\n","Skipping y_hat=4883+\n","Skipping y_hat=4883+\n","Skipping y_hat=44\n","92\n","Skipping y_hat=44\n","92\n","Skipping y_hat=44\n","92\n","Skipping y_hat=44\n","92\n","Skipping y_hat=44\n","92\n","Skipping y_hat=44\n","92\n","Skipping y_hat=44\n","92\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=99930\n","Skipping y_hat=99930\n","Skipping y_hat=99930\n","Skipping y_hat=99930\n","Skipping y_hat=99930\n","Skipping y_hat=99930\n","Skipping y_hat=99930\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=6094\n","Skipping y_hat=6094\n","Skipping y_hat=6094\n","Skipping y_hat=6094\n","Skipping y_hat=6094\n","Skipping y_hat=6094\n","Skipping y_hat=6094\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=33244\n","Skipping y_hat=33244\n","Skipping y_hat=33244\n","Skipping y_hat=33244\n","Skipping y_hat=33244\n","Skipping y_hat=33244\n","Skipping y_hat=33244\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=103\n","Skipping y_hat=103\n","Skipping y_hat=103\n","Skipping y_hat=103\n","Skipping y_hat=103\n","Skipping y_hat=103\n","Skipping y_hat=103\n","Skipping y_hat=40\n","03\n","Skipping y_hat=40\n","03\n","Skipping y_hat=40\n","03\n","Skipping y_hat=40\n","03\n","Skipping y_hat=40\n","03\n","Skipping y_hat=40\n","03\n","Skipping y_hat=40\n","03\n","Skipping y_hat=13364\n","Skipping y_hat=13364\n","Skipping y_hat=13364\n","Skipping y_hat=13364\n","Skipping y_hat=13364\n","Skipping y_hat=13364\n","Skipping y_hat=13364\n","Skipping y_hat=1+88\n","Skipping y_hat=1+88\n","Skipping y_hat=1+88\n","Skipping y_hat=1+88\n","Skipping y_hat=1+88\n","Skipping y_hat=1+88\n","Skipping y_hat=1+88\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=8812+\n","Skipping y_hat=8812+\n","Skipping y_hat=8812+\n","Skipping y_hat=8812+\n","Skipping y_hat=8812+\n","Skipping y_hat=8812+\n","Skipping y_hat=8812+\n","Skipping y_hat=79\n","Skipping y_hat=79\n","Skipping y_hat=79\n","Skipping y_hat=79\n","Skipping y_hat=79\n","Skipping y_hat=79\n","Skipping y_hat=79\n","Skipping y_hat=0760\n","Skipping y_hat=0760\n","Skipping y_hat=0760\n","Skipping y_hat=0760\n","Skipping y_hat=0760\n","Skipping y_hat=0760\n","Skipping y_hat=0760\n","Skipping y_hat=22750\n","Skipping y_hat=22750\n","Skipping y_hat=22750\n","Skipping y_hat=22750\n","Skipping y_hat=22750\n","Skipping y_hat=22750\n","Skipping y_hat=22750\n","Skipping y_hat=59+67\n","Skipping y_hat=59+67\n","Skipping y_hat=59+67\n","Skipping y_hat=59+67\n","Skipping y_hat=59+67\n","Skipping y_hat=59+67\n","Skipping y_hat=59+67\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=25553\n","Skipping y_hat=25553\n","Skipping y_hat=25553\n","Skipping y_hat=25553\n","Skipping y_hat=25553\n","Skipping y_hat=25553\n","Skipping y_hat=25553\n","Skipping y_hat=63250\n","Skipping y_hat=63250\n","Skipping y_hat=63250\n","Skipping y_hat=63250\n","Skipping y_hat=63250\n","Skipping y_hat=63250\n","Skipping y_hat=63250\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=9997+\n","Skipping y_hat=9997+\n","Skipping y_hat=9997+\n","Skipping y_hat=9997+\n","Skipping y_hat=9997+\n","Skipping y_hat=9997+\n","Skipping y_hat=9997+\n","Skipping y_hat=6+994\n","Skipping y_hat=6+994\n","Skipping y_hat=6+994\n","Skipping y_hat=6+994\n","Skipping y_hat=6+994\n","Skipping y_hat=6+994\n","Skipping y_hat=6+994\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=55201\n","Skipping y_hat=55201\n","Skipping y_hat=55201\n","Skipping y_hat=55201\n","Skipping y_hat=55201\n","Skipping y_hat=55201\n","Skipping y_hat=55201\n","Skipping y_hat=58+07\n","Skipping y_hat=58+07\n","Skipping y_hat=58+07\n","Skipping y_hat=58+07\n","Skipping y_hat=58+07\n","Skipping y_hat=58+07\n","Skipping y_hat=58+07\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=1\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=68890\n","Skipping y_hat=68890\n","Skipping y_hat=68890\n","Skipping y_hat=68890\n","Skipping y_hat=68890\n","Skipping y_hat=68890\n","Skipping y_hat=68890\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=9900\n","Skipping y_hat=9900\n","Skipping y_hat=9900\n","Skipping y_hat=9900\n","Skipping y_hat=9900\n","Skipping y_hat=9900\n","Skipping y_hat=9900\n","Skipping y_hat=+9\n","00\n","Skipping y_hat=+9\n","00\n","Skipping y_hat=+9\n","00\n","Skipping y_hat=+9\n","00\n","Skipping y_hat=+9\n","00\n","Skipping y_hat=+9\n","00\n","Skipping y_hat=+9\n","00\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=03\n","Skipping y_hat=03\n","Skipping y_hat=03\n","Skipping y_hat=03\n","Skipping y_hat=03\n","Skipping y_hat=03\n","Skipping y_hat=03\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=2040\n","Skipping y_hat=2040\n","Skipping y_hat=2040\n","Skipping y_hat=2040\n","Skipping y_hat=2040\n","Skipping y_hat=2040\n","Skipping y_hat=2040\n","Skipping y_hat=1066\n","Skipping y_hat=1066\n","Skipping y_hat=1066\n","Skipping y_hat=1066\n","Skipping y_hat=1066\n","Skipping y_hat=1066\n","Skipping y_hat=1066\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=++96\n","Skipping y_hat=++96\n","Skipping y_hat=++96\n","Skipping y_hat=++96\n","Skipping y_hat=++96\n","Skipping y_hat=++96\n","Skipping y_hat=++96\n","Skipping y_hat=86842\n","Skipping y_hat=86842\n","Skipping y_hat=86842\n","Skipping y_hat=86842\n","Skipping y_hat=86842\n","Skipping y_hat=86842\n","Skipping y_hat=86842\n","Skipping y_hat=71+43\n","Skipping y_hat=71+43\n","Skipping y_hat=71+43\n","Skipping y_hat=71+43\n","Skipping y_hat=71+43\n","Skipping y_hat=71+43\n","Skipping y_hat=71+43\n","Skipping y_hat=1+91\n","Skipping y_hat=1+91\n","Skipping y_hat=1+91\n","Skipping y_hat=1+91\n","Skipping y_hat=1+91\n","Skipping y_hat=1+91\n","Skipping y_hat=1+91\n","Skipping y_hat=47++8\n","Skipping y_hat=47++8\n","Skipping y_hat=47++8\n","Skipping y_hat=47++8\n","Skipping y_hat=47++8\n","Skipping y_hat=47++8\n","Skipping y_hat=47++8\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=4+4+5\n","Skipping y_hat=4+4+5\n","Skipping y_hat=4+4+5\n","Skipping y_hat=4+4+5\n","Skipping y_hat=4+4+5\n","Skipping y_hat=4+4+5\n","Skipping y_hat=4+4+5\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=2240\n","Skipping y_hat=2240\n","Skipping y_hat=2240\n","Skipping y_hat=2240\n","Skipping y_hat=2240\n","Skipping y_hat=2240\n","Skipping y_hat=2240\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=4296\n","Skipping y_hat=4296\n","Skipping y_hat=4296\n","Skipping y_hat=4296\n","Skipping y_hat=4296\n","Skipping y_hat=4296\n","Skipping y_hat=4296\n","Skipping y_hat=19188\n","Skipping y_hat=19188\n","Skipping y_hat=19188\n","Skipping y_hat=19188\n","Skipping y_hat=19188\n","Skipping y_hat=19188\n","Skipping y_hat=19188\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=204\n","3\n","Skipping y_hat=204\n","3\n","Skipping y_hat=204\n","3\n","Skipping y_hat=204\n","3\n","Skipping y_hat=204\n","3\n","Skipping y_hat=204\n","3\n","Skipping y_hat=204\n","3\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=01+42\n","Skipping y_hat=01+42\n","Skipping y_hat=01+42\n","Skipping y_hat=01+42\n","Skipping y_hat=01+42\n","Skipping y_hat=01+42\n","Skipping y_hat=01+42\n","Skipping y_hat=+7442\n","Skipping y_hat=+7442\n","Skipping y_hat=+7442\n","Skipping y_hat=+7442\n","Skipping y_hat=+7442\n","Skipping y_hat=+7442\n","Skipping y_hat=+7442\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=8836\n","Skipping y_hat=8836\n","Skipping y_hat=8836\n","Skipping y_hat=8836\n","Skipping y_hat=8836\n","Skipping y_hat=8836\n","Skipping y_hat=8836\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=6200\n","Skipping y_hat=6200\n","Skipping y_hat=6200\n","Skipping y_hat=6200\n","Skipping y_hat=6200\n","Skipping y_hat=6200\n","Skipping y_hat=6200\n","Skipping y_hat=63900\n","Skipping y_hat=63900\n","Skipping y_hat=63900\n","Skipping y_hat=63900\n","Skipping y_hat=63900\n","Skipping y_hat=63900\n","Skipping y_hat=63900\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=4\n","72\n","Skipping y_hat=4\n","72\n","Skipping y_hat=4\n","72\n","Skipping y_hat=4\n","72\n","Skipping y_hat=4\n","72\n","Skipping y_hat=4\n","72\n","Skipping y_hat=4\n","72\n","Skipping y_hat=28888\n","Skipping y_hat=28888\n","Skipping y_hat=28888\n","Skipping y_hat=28888\n","Skipping y_hat=28888\n","Skipping y_hat=28888\n","Skipping y_hat=28888\n","Skipping y_hat=26302\n","Skipping y_hat=26302\n","Skipping y_hat=26302\n","Skipping y_hat=26302\n","Skipping y_hat=26302\n","Skipping y_hat=26302\n","Skipping y_hat=26302\n","Skipping y_hat=+5855\n","Skipping y_hat=+5855\n","Skipping y_hat=+5855\n","Skipping y_hat=+5855\n","Skipping y_hat=+5855\n","Skipping y_hat=+5855\n","Skipping y_hat=+5855\n","Skipping y_hat=06\n","Skipping y_hat=06\n","Skipping y_hat=06\n","Skipping y_hat=06\n","Skipping y_hat=06\n","Skipping y_hat=06\n","Skipping y_hat=06\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=9999+\n","Skipping y_hat=9999+\n","Skipping y_hat=9999+\n","Skipping y_hat=9999+\n","Skipping y_hat=9999+\n","Skipping y_hat=9999+\n","Skipping y_hat=9999+\n","Skipping y_hat=88855\n","Skipping y_hat=88855\n","Skipping y_hat=88855\n","Skipping y_hat=88855\n","Skipping y_hat=88855\n","Skipping y_hat=88855\n","Skipping y_hat=88855\n","Skipping y_hat=77195\n","Skipping y_hat=77195\n","Skipping y_hat=77195\n","Skipping y_hat=77195\n","Skipping y_hat=77195\n","Skipping y_hat=77195\n","Skipping y_hat=77195\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=6030\n","Skipping y_hat=6030\n","Skipping y_hat=6030\n","Skipping y_hat=6030\n","Skipping y_hat=6030\n","Skipping y_hat=6030\n","Skipping y_hat=6030\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=48+\n","Skipping y_hat=48+\n","Skipping y_hat=48+\n","Skipping y_hat=48+\n","Skipping y_hat=48+\n","Skipping y_hat=48+\n","Skipping y_hat=48+\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=7+\n","61\n","Skipping y_hat=7+\n","61\n","Skipping y_hat=7+\n","61\n","Skipping y_hat=7+\n","61\n","Skipping y_hat=7+\n","61\n","Skipping y_hat=7+\n","61\n","Skipping y_hat=7+\n","61\n","Skipping y_hat=9+677\n","Skipping y_hat=9+677\n","Skipping y_hat=9+677\n","Skipping y_hat=9+677\n","Skipping y_hat=9+677\n","Skipping y_hat=9+677\n","Skipping y_hat=9+677\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n"," 84% 21/25 [00:02<00:00,  8.78it/s]Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=88+15\n","Skipping y_hat=88+15\n","Skipping y_hat=88+15\n","Skipping y_hat=88+15\n","Skipping y_hat=88+15\n","Skipping y_hat=88+15\n","Skipping y_hat=88+15\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=41+6\n","Skipping y_hat=41+6\n","Skipping y_hat=41+6\n","Skipping y_hat=41+6\n","Skipping y_hat=41+6\n","Skipping y_hat=41+6\n","Skipping y_hat=41+6\n","Skipping y_hat=77771\n","Skipping y_hat=77771\n","Skipping y_hat=77771\n","Skipping y_hat=77771\n","Skipping y_hat=77771\n","Skipping y_hat=77771\n","Skipping y_hat=77771\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=1\n","520\n","Skipping y_hat=1\n","520\n","Skipping y_hat=1\n","520\n","Skipping y_hat=1\n","520\n","Skipping y_hat=1\n","520\n","Skipping y_hat=1\n","520\n","Skipping y_hat=1\n","520\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=0+++1\n","Skipping y_hat=0+++1\n","Skipping y_hat=0+++1\n","Skipping y_hat=0+++1\n","Skipping y_hat=0+++1\n","Skipping y_hat=0+++1\n","Skipping y_hat=0+++1\n","Skipping y_hat=20201\n","Skipping y_hat=20201\n","Skipping y_hat=20201\n","Skipping y_hat=20201\n","Skipping y_hat=20201\n","Skipping y_hat=20201\n","Skipping y_hat=20201\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=92233\n","Skipping y_hat=92233\n","Skipping y_hat=92233\n","Skipping y_hat=92233\n","Skipping y_hat=92233\n","Skipping y_hat=92233\n","Skipping y_hat=92233\n","Skipping y_hat=22208\n","Skipping y_hat=22208\n","Skipping y_hat=22208\n","Skipping y_hat=22208\n","Skipping y_hat=22208\n","Skipping y_hat=22208\n","Skipping y_hat=22208\n","Skipping y_hat=27\n","Skipping y_hat=27\n","Skipping y_hat=27\n","Skipping y_hat=27\n","Skipping y_hat=27\n","Skipping y_hat=27\n","Skipping y_hat=27\n","Skipping y_hat=66809\n","Skipping y_hat=66809\n","Skipping y_hat=66809\n","Skipping y_hat=66809\n","Skipping y_hat=66809\n","Skipping y_hat=66809\n","Skipping y_hat=66809\n","Skipping y_hat=44+++\n","Skipping y_hat=44+++\n","Skipping y_hat=44+++\n","Skipping y_hat=44+++\n","Skipping y_hat=44+++\n","Skipping y_hat=44+++\n","Skipping y_hat=44+++\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=00227\n","Skipping y_hat=00227\n","Skipping y_hat=00227\n","Skipping y_hat=00227\n","Skipping y_hat=00227\n","Skipping y_hat=00227\n","Skipping y_hat=00227\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=88402\n","Skipping y_hat=88402\n","Skipping y_hat=88402\n","Skipping y_hat=88402\n","Skipping y_hat=88402\n","Skipping y_hat=88402\n","Skipping y_hat=88402\n","Skipping y_hat=9993\n","Skipping y_hat=9993\n","Skipping y_hat=9993\n","Skipping y_hat=9993\n","Skipping y_hat=9993\n","Skipping y_hat=9993\n","Skipping y_hat=9993\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=80+\n","Skipping y_hat=80+\n","Skipping y_hat=80+\n","Skipping y_hat=80+\n","Skipping y_hat=80+\n","Skipping y_hat=80+\n","Skipping y_hat=80+\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=+8\n","3\n","Skipping y_hat=+8\n","3\n","Skipping y_hat=+8\n","3\n","Skipping y_hat=+8\n","3\n","Skipping y_hat=+8\n","3\n","Skipping y_hat=+8\n","3\n","Skipping y_hat=+8\n","3\n","Skipping y_hat=00577\n","Skipping y_hat=00577\n","Skipping y_hat=00577\n","Skipping y_hat=00577\n","Skipping y_hat=00577\n","Skipping y_hat=00577\n","Skipping y_hat=00577\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=77592\n","Skipping y_hat=77592\n","Skipping y_hat=77592\n","Skipping y_hat=77592\n","Skipping y_hat=77592\n","Skipping y_hat=77592\n","Skipping y_hat=77592\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=4\n","355\n","Skipping y_hat=4\n","355\n","Skipping y_hat=4\n","355\n","Skipping y_hat=4\n","355\n","Skipping y_hat=4\n","355\n","Skipping y_hat=4\n","355\n","Skipping y_hat=4\n","355\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=47229\n","Skipping y_hat=47229\n","Skipping y_hat=47229\n","Skipping y_hat=47229\n","Skipping y_hat=47229\n","Skipping y_hat=47229\n","Skipping y_hat=47229\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=222\n","1\n","Skipping y_hat=222\n","1\n","Skipping y_hat=222\n","1\n","Skipping y_hat=222\n","1\n","Skipping y_hat=222\n","1\n","Skipping y_hat=222\n","1\n","Skipping y_hat=222\n","1\n","Skipping y_hat=2++\n","6\n","Skipping y_hat=2++\n","6\n","Skipping y_hat=2++\n","6\n","Skipping y_hat=2++\n","6\n","Skipping y_hat=2++\n","6\n","Skipping y_hat=2++\n","6\n","Skipping y_hat=2++\n","6\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=32272\n","Skipping y_hat=32272\n","Skipping y_hat=32272\n","Skipping y_hat=32272\n","Skipping y_hat=32272\n","Skipping y_hat=32272\n","Skipping y_hat=32272\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=22++4\n","Skipping y_hat=22++4\n","Skipping y_hat=22++4\n","Skipping y_hat=22++4\n","Skipping y_hat=22++4\n","Skipping y_hat=22++4\n","Skipping y_hat=22++4\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=4+838\n","Skipping y_hat=4+838\n","Skipping y_hat=4+838\n","Skipping y_hat=4+838\n","Skipping y_hat=4+838\n","Skipping y_hat=4+838\n","Skipping y_hat=4+838\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=9+05\n","Skipping y_hat=9+05\n","Skipping y_hat=9+05\n","Skipping y_hat=9+05\n","Skipping y_hat=9+05\n","Skipping y_hat=9+05\n","Skipping y_hat=9+05\n","Skipping y_hat=4\n","+9\n","Skipping y_hat=4\n","+9\n","Skipping y_hat=4\n","+9\n","Skipping y_hat=4\n","+9\n","Skipping y_hat=4\n","+9\n","Skipping y_hat=4\n","+9\n","Skipping y_hat=4\n","+9\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=25587\n","Skipping y_hat=25587\n","Skipping y_hat=25587\n","Skipping y_hat=25587\n","Skipping y_hat=25587\n","Skipping y_hat=25587\n","Skipping y_hat=25587\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=4+965\n","Skipping y_hat=4+965\n","Skipping y_hat=4+965\n","Skipping y_hat=4+965\n","Skipping y_hat=4+965\n","Skipping y_hat=4+965\n","Skipping y_hat=4+965\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=0469\n","Skipping y_hat=0469\n","Skipping y_hat=0469\n","Skipping y_hat=0469\n","Skipping y_hat=0469\n","Skipping y_hat=0469\n","Skipping y_hat=0469\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=089\n","Skipping y_hat=089\n","Skipping y_hat=089\n","Skipping y_hat=089\n","Skipping y_hat=089\n","Skipping y_hat=089\n","Skipping y_hat=089\n","Skipping y_hat=4\n","\n","2\n","Skipping y_hat=4\n","\n","2\n","Skipping y_hat=4\n","\n","2\n","Skipping y_hat=4\n","\n","2\n","Skipping y_hat=4\n","\n","2\n","Skipping y_hat=4\n","\n","2\n","Skipping y_hat=4\n","\n","2\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=1144\n","Skipping y_hat=1144\n","Skipping y_hat=1144\n","Skipping y_hat=1144\n","Skipping y_hat=1144\n","Skipping y_hat=1144\n","Skipping y_hat=1144\n","Skipping y_hat=4\n","\n","++\n","Skipping y_hat=4\n","\n","++\n","Skipping y_hat=4\n","\n","++\n","Skipping y_hat=4\n","\n","++\n","Skipping y_hat=4\n","\n","++\n","Skipping y_hat=4\n","\n","++\n","Skipping y_hat=4\n","\n","++\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=27230\n","Skipping y_hat=27230\n","Skipping y_hat=27230\n","Skipping y_hat=27230\n","Skipping y_hat=27230\n","Skipping y_hat=27230\n","Skipping y_hat=27230\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=8661\n","Skipping y_hat=8661\n","Skipping y_hat=8661\n","Skipping y_hat=8661\n","Skipping y_hat=8661\n","Skipping y_hat=8661\n","Skipping y_hat=8661\n","Skipping y_hat=13338\n","Skipping y_hat=13338\n","Skipping y_hat=13338\n","Skipping y_hat=13338\n","Skipping y_hat=13338\n","Skipping y_hat=13338\n","Skipping y_hat=13338\n","Skipping y_hat=5034+\n","Skipping y_hat=5034+\n","Skipping y_hat=5034+\n","Skipping y_hat=5034+\n","Skipping y_hat=5034+\n","Skipping y_hat=5034+\n","Skipping y_hat=5034+\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=4\n","963\n","Skipping y_hat=4\n","963\n","Skipping y_hat=4\n","963\n","Skipping y_hat=4\n","963\n","Skipping y_hat=4\n","963\n","Skipping y_hat=4\n","963\n","Skipping y_hat=4\n","963\n","Skipping y_hat=56\n","35\n","Skipping y_hat=56\n","35\n","Skipping y_hat=56\n","35\n","Skipping y_hat=56\n","35\n","Skipping y_hat=56\n","35\n","Skipping y_hat=56\n","35\n","Skipping y_hat=56\n","35\n","Skipping y_hat=99+1\n","Skipping y_hat=99+1\n","Skipping y_hat=99+1\n","Skipping y_hat=99+1\n","Skipping y_hat=99+1\n","Skipping y_hat=99+1\n","Skipping y_hat=99+1\n","Skipping y_hat=34950\n","Skipping y_hat=34950\n","Skipping y_hat=34950\n","Skipping y_hat=34950\n","Skipping y_hat=34950\n","Skipping y_hat=34950\n","Skipping y_hat=34950\n","Skipping y_hat=19222\n","Skipping y_hat=19222\n","Skipping y_hat=19222\n","Skipping y_hat=19222\n","Skipping y_hat=19222\n","Skipping y_hat=19222\n","Skipping y_hat=19222\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=87+92\n","Skipping y_hat=87+92\n","Skipping y_hat=87+92\n","Skipping y_hat=87+92\n","Skipping y_hat=87+92\n","Skipping y_hat=87+92\n","Skipping y_hat=87+92\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=+\n","Skipping y_hat=25552\n","Skipping y_hat=25552\n","Skipping y_hat=25552\n","Skipping y_hat=25552\n","Skipping y_hat=25552\n","Skipping y_hat=25552\n","Skipping y_hat=25552\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=5++\n","Skipping y_hat=5++\n","Skipping y_hat=5++\n","Skipping y_hat=5++\n","Skipping y_hat=5++\n","Skipping y_hat=5++\n","Skipping y_hat=5++\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=41401\n","Skipping y_hat=41401\n","Skipping y_hat=41401\n","Skipping y_hat=41401\n","Skipping y_hat=41401\n","Skipping y_hat=41401\n","Skipping y_hat=41401\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=48559\n","Skipping y_hat=48559\n","Skipping y_hat=48559\n","Skipping y_hat=48559\n","Skipping y_hat=48559\n","Skipping y_hat=48559\n","Skipping y_hat=48559\n","Skipping y_hat=21536\n","Skipping y_hat=21536\n","Skipping y_hat=21536\n","Skipping y_hat=21536\n","Skipping y_hat=21536\n","Skipping y_hat=21536\n","Skipping y_hat=21536\n","Skipping y_hat=33375\n","Skipping y_hat=33375\n","Skipping y_hat=33375\n","Skipping y_hat=33375\n","Skipping y_hat=33375\n","Skipping y_hat=33375\n","Skipping y_hat=33375\n","Skipping y_hat=6+479\n","Skipping y_hat=6+479\n","Skipping y_hat=6+479\n","Skipping y_hat=6+479\n","Skipping y_hat=6+479\n","Skipping y_hat=6+479\n","Skipping y_hat=6+479\n","Skipping y_hat=13261\n","Skipping y_hat=13261\n","Skipping y_hat=13261\n","Skipping y_hat=13261\n","Skipping y_hat=13261\n","Skipping y_hat=13261\n","Skipping y_hat=13261\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=92554\n","Skipping y_hat=92554\n","Skipping y_hat=92554\n","Skipping y_hat=92554\n","Skipping y_hat=92554\n","Skipping y_hat=92554\n","Skipping y_hat=92554\n","Skipping y_hat=55340\n","Skipping y_hat=55340\n","Skipping y_hat=55340\n","Skipping y_hat=55340\n","Skipping y_hat=55340\n","Skipping y_hat=55340\n","Skipping y_hat=55340\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=85\n","Skipping y_hat=85\n","Skipping y_hat=85\n","Skipping y_hat=85\n","Skipping y_hat=85\n","Skipping y_hat=85\n","Skipping y_hat=85\n","Skipping y_hat=14663\n","Skipping y_hat=14663\n","Skipping y_hat=14663\n","Skipping y_hat=14663\n","Skipping y_hat=14663\n","Skipping y_hat=14663\n","Skipping y_hat=14663\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=19999\n","Skipping y_hat=98802\n","Skipping y_hat=98802\n","Skipping y_hat=98802\n","Skipping y_hat=98802\n","Skipping y_hat=98802\n","Skipping y_hat=98802\n","Skipping y_hat=98802\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=44460\n","Skipping y_hat=44460\n","Skipping y_hat=44460\n","Skipping y_hat=44460\n","Skipping y_hat=44460\n","Skipping y_hat=44460\n","Skipping y_hat=44460\n","Skipping y_hat=+2741\n","Skipping y_hat=+2741\n","Skipping y_hat=+2741\n","Skipping y_hat=+2741\n","Skipping y_hat=+2741\n","Skipping y_hat=+2741\n","Skipping y_hat=+2741\n","Skipping y_hat=+5319\n","Skipping y_hat=+5319\n","Skipping y_hat=+5319\n","Skipping y_hat=+5319\n","Skipping y_hat=+5319\n","Skipping y_hat=+5319\n","Skipping y_hat=+5319\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=55902\n","Skipping y_hat=55902\n","Skipping y_hat=55902\n","Skipping y_hat=55902\n","Skipping y_hat=55902\n","Skipping y_hat=55902\n","Skipping y_hat=55902\n","Skipping y_hat=5599+\n","Skipping y_hat=5599+\n","Skipping y_hat=5599+\n","Skipping y_hat=5599+\n","Skipping y_hat=5599+\n","Skipping y_hat=5599+\n","Skipping y_hat=5599+\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=28+44\n","Skipping y_hat=28+44\n","Skipping y_hat=28+44\n","Skipping y_hat=28+44\n","Skipping y_hat=28+44\n","Skipping y_hat=28+44\n","Skipping y_hat=28+44\n","Skipping y_hat=04499\n","Skipping y_hat=04499\n","Skipping y_hat=04499\n","Skipping y_hat=04499\n","Skipping y_hat=04499\n","Skipping y_hat=04499\n","Skipping y_hat=04499\n"," 88% 22/25 [00:02<00:00,  8.73it/s]Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=5266\n","Skipping y_hat=5266\n","Skipping y_hat=5266\n","Skipping y_hat=5266\n","Skipping y_hat=5266\n","Skipping y_hat=5266\n","Skipping y_hat=5266\n","Skipping y_hat=575\n","Skipping y_hat=575\n","Skipping y_hat=575\n","Skipping y_hat=575\n","Skipping y_hat=575\n","Skipping y_hat=575\n","Skipping y_hat=575\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=8\n","222\n","Skipping y_hat=8\n","222\n","Skipping y_hat=8\n","222\n","Skipping y_hat=8\n","222\n","Skipping y_hat=8\n","222\n","Skipping y_hat=8\n","222\n","Skipping y_hat=8\n","222\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=71221\n","Skipping y_hat=71221\n","Skipping y_hat=71221\n","Skipping y_hat=71221\n","Skipping y_hat=71221\n","Skipping y_hat=71221\n","Skipping y_hat=71221\n","Skipping y_hat=80\n","6+\n","Skipping y_hat=80\n","6+\n","Skipping y_hat=80\n","6+\n","Skipping y_hat=80\n","6+\n","Skipping y_hat=80\n","6+\n","Skipping y_hat=80\n","6+\n","Skipping y_hat=80\n","6+\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=9\n","001\n","Skipping y_hat=9\n","001\n","Skipping y_hat=9\n","001\n","Skipping y_hat=9\n","001\n","Skipping y_hat=9\n","001\n","Skipping y_hat=9\n","001\n","Skipping y_hat=9\n","001\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=8806\n","Skipping y_hat=8806\n","Skipping y_hat=8806\n","Skipping y_hat=8806\n","Skipping y_hat=8806\n","Skipping y_hat=8806\n","Skipping y_hat=8806\n","Skipping y_hat=3\n","336\n","Skipping y_hat=3\n","336\n","Skipping y_hat=3\n","336\n","Skipping y_hat=3\n","336\n","Skipping y_hat=3\n","336\n","Skipping y_hat=3\n","336\n","Skipping y_hat=3\n","336\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=+0440\n","Skipping y_hat=+0440\n","Skipping y_hat=+0440\n","Skipping y_hat=+0440\n","Skipping y_hat=+0440\n","Skipping y_hat=+0440\n","Skipping y_hat=+0440\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=52298\n","Skipping y_hat=52298\n","Skipping y_hat=52298\n","Skipping y_hat=52298\n","Skipping y_hat=52298\n","Skipping y_hat=52298\n","Skipping y_hat=52298\n","Skipping y_hat=58101\n","Skipping y_hat=58101\n","Skipping y_hat=58101\n","Skipping y_hat=58101\n","Skipping y_hat=58101\n","Skipping y_hat=58101\n","Skipping y_hat=58101\n","Skipping y_hat=02847\n","Skipping y_hat=02847\n","Skipping y_hat=02847\n","Skipping y_hat=02847\n","Skipping y_hat=02847\n","Skipping y_hat=02847\n","Skipping y_hat=02847\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=11424\n","Skipping y_hat=11424\n","Skipping y_hat=11424\n","Skipping y_hat=11424\n","Skipping y_hat=11424\n","Skipping y_hat=11424\n","Skipping y_hat=11424\n","Skipping y_hat=97528\n","Skipping y_hat=97528\n","Skipping y_hat=97528\n","Skipping y_hat=97528\n","Skipping y_hat=97528\n","Skipping y_hat=97528\n","Skipping y_hat=97528\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=+62\n","Skipping y_hat=+62\n","Skipping y_hat=+62\n","Skipping y_hat=+62\n","Skipping y_hat=+62\n","Skipping y_hat=+62\n","Skipping y_hat=+62\n","Skipping y_hat=01111\n","Skipping y_hat=01111\n","Skipping y_hat=01111\n","Skipping y_hat=01111\n","Skipping y_hat=01111\n","Skipping y_hat=01111\n","Skipping y_hat=01111\n","Skipping y_hat=8+8\n","Skipping y_hat=8+8\n","Skipping y_hat=8+8\n","Skipping y_hat=8+8\n","Skipping y_hat=8+8\n","Skipping y_hat=8+8\n","Skipping y_hat=8+8\n","Skipping y_hat=72+23\n","Skipping y_hat=72+23\n","Skipping y_hat=72+23\n","Skipping y_hat=72+23\n","Skipping y_hat=72+23\n","Skipping y_hat=72+23\n","Skipping y_hat=72+23\n","Skipping y_hat=1\n","904\n","Skipping y_hat=1\n","904\n","Skipping y_hat=1\n","904\n","Skipping y_hat=1\n","904\n","Skipping y_hat=1\n","904\n","Skipping y_hat=1\n","904\n","Skipping y_hat=1\n","904\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=82663\n","Skipping y_hat=82663\n","Skipping y_hat=82663\n","Skipping y_hat=82663\n","Skipping y_hat=82663\n","Skipping y_hat=82663\n","Skipping y_hat=82663\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=16\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=70661\n","Skipping y_hat=70661\n","Skipping y_hat=70661\n","Skipping y_hat=70661\n","Skipping y_hat=70661\n","Skipping y_hat=70661\n","Skipping y_hat=70661\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=9+220\n","Skipping y_hat=9+220\n","Skipping y_hat=9+220\n","Skipping y_hat=9+220\n","Skipping y_hat=9+220\n","Skipping y_hat=9+220\n","Skipping y_hat=9+220\n","Skipping y_hat=41\n","04\n","Skipping y_hat=41\n","04\n","Skipping y_hat=41\n","04\n","Skipping y_hat=41\n","04\n","Skipping y_hat=41\n","04\n","Skipping y_hat=41\n","04\n","Skipping y_hat=41\n","04\n","Skipping y_hat=8\n","196\n","Skipping y_hat=8\n","196\n","Skipping y_hat=8\n","196\n","Skipping y_hat=8\n","196\n","Skipping y_hat=8\n","196\n","Skipping y_hat=8\n","196\n","Skipping y_hat=8\n","196\n","Skipping y_hat=0\n","\n","40\n","Skipping y_hat=0\n","\n","40\n","Skipping y_hat=0\n","\n","40\n","Skipping y_hat=0\n","\n","40\n","Skipping y_hat=0\n","\n","40\n","Skipping y_hat=0\n","\n","40\n","Skipping y_hat=0\n","\n","40\n","Skipping y_hat=7+901\n","Skipping y_hat=7+901\n","Skipping y_hat=7+901\n","Skipping y_hat=7+901\n","Skipping y_hat=7+901\n","Skipping y_hat=7+901\n","Skipping y_hat=7+901\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=11164\n","Skipping y_hat=11164\n","Skipping y_hat=11164\n","Skipping y_hat=11164\n","Skipping y_hat=11164\n","Skipping y_hat=11164\n","Skipping y_hat=11164\n","Skipping y_hat=69+34\n","Skipping y_hat=69+34\n","Skipping y_hat=69+34\n","Skipping y_hat=69+34\n","Skipping y_hat=69+34\n","Skipping y_hat=69+34\n","Skipping y_hat=69+34\n","Skipping y_hat=404\n","2\n","Skipping y_hat=404\n","2\n","Skipping y_hat=404\n","2\n","Skipping y_hat=404\n","2\n","Skipping y_hat=404\n","2\n","Skipping y_hat=404\n","2\n","Skipping y_hat=404\n","2\n","Skipping y_hat=07704\n","Skipping y_hat=07704\n","Skipping y_hat=07704\n","Skipping y_hat=07704\n","Skipping y_hat=07704\n","Skipping y_hat=07704\n","Skipping y_hat=07704\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=55422\n","Skipping y_hat=55422\n","Skipping y_hat=55422\n","Skipping y_hat=55422\n","Skipping y_hat=55422\n","Skipping y_hat=55422\n","Skipping y_hat=55422\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=0+303\n","Skipping y_hat=0+303\n","Skipping y_hat=0+303\n","Skipping y_hat=0+303\n","Skipping y_hat=0+303\n","Skipping y_hat=0+303\n","Skipping y_hat=0+303\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=37422\n","Skipping y_hat=37422\n","Skipping y_hat=37422\n","Skipping y_hat=37422\n","Skipping y_hat=37422\n","Skipping y_hat=37422\n","Skipping y_hat=37422\n","Skipping y_hat=60072\n","Skipping y_hat=60072\n","Skipping y_hat=60072\n","Skipping y_hat=60072\n","Skipping y_hat=60072\n","Skipping y_hat=60072\n","Skipping y_hat=60072\n","Skipping y_hat=0++2\n","Skipping y_hat=0++2\n","Skipping y_hat=0++2\n","Skipping y_hat=0++2\n","Skipping y_hat=0++2\n","Skipping y_hat=0++2\n","Skipping y_hat=0++2\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=7+\n","Skipping y_hat=7+\n","Skipping y_hat=7+\n","Skipping y_hat=7+\n","Skipping y_hat=7+\n","Skipping y_hat=7+\n","Skipping y_hat=7+\n","Skipping y_hat=5562+\n","Skipping y_hat=5562+\n","Skipping y_hat=5562+\n","Skipping y_hat=5562+\n","Skipping y_hat=5562+\n","Skipping y_hat=5562+\n","Skipping y_hat=5562+\n","Skipping y_hat=1332+\n","Skipping y_hat=1332+\n","Skipping y_hat=1332+\n","Skipping y_hat=1332+\n","Skipping y_hat=1332+\n","Skipping y_hat=1332+\n","Skipping y_hat=1332+\n","Skipping y_hat=76255\n","Skipping y_hat=76255\n","Skipping y_hat=76255\n","Skipping y_hat=76255\n","Skipping y_hat=76255\n","Skipping y_hat=76255\n","Skipping y_hat=76255\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=9+223\n","Skipping y_hat=9+223\n","Skipping y_hat=9+223\n","Skipping y_hat=9+223\n","Skipping y_hat=9+223\n","Skipping y_hat=9+223\n","Skipping y_hat=9+223\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=11655\n","Skipping y_hat=11655\n","Skipping y_hat=11655\n","Skipping y_hat=11655\n","Skipping y_hat=11655\n","Skipping y_hat=11655\n","Skipping y_hat=11655\n","Skipping y_hat=0034\n","Skipping y_hat=0034\n","Skipping y_hat=0034\n","Skipping y_hat=0034\n","Skipping y_hat=0034\n","Skipping y_hat=0034\n","Skipping y_hat=0034\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=2++\n","Skipping y_hat=15016\n","Skipping y_hat=15016\n","Skipping y_hat=15016\n","Skipping y_hat=15016\n","Skipping y_hat=15016\n","Skipping y_hat=15016\n","Skipping y_hat=15016\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=87041\n","Skipping y_hat=87041\n","Skipping y_hat=87041\n","Skipping y_hat=87041\n","Skipping y_hat=87041\n","Skipping y_hat=87041\n","Skipping y_hat=87041\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=82777\n","Skipping y_hat=82777\n","Skipping y_hat=82777\n","Skipping y_hat=82777\n","Skipping y_hat=82777\n","Skipping y_hat=82777\n","Skipping y_hat=82777\n","Skipping y_hat=52\n","07\n","Skipping y_hat=52\n","07\n","Skipping y_hat=52\n","07\n","Skipping y_hat=52\n","07\n","Skipping y_hat=52\n","07\n","Skipping y_hat=52\n","07\n","Skipping y_hat=52\n","07\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=6062\n","Skipping y_hat=6062\n","Skipping y_hat=6062\n","Skipping y_hat=6062\n","Skipping y_hat=6062\n","Skipping y_hat=6062\n","Skipping y_hat=6062\n","Skipping y_hat=0922+\n","Skipping y_hat=0922+\n","Skipping y_hat=0922+\n","Skipping y_hat=0922+\n","Skipping y_hat=0922+\n","Skipping y_hat=0922+\n","Skipping y_hat=0922+\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=22081\n","Skipping y_hat=22081\n","Skipping y_hat=22081\n","Skipping y_hat=22081\n","Skipping y_hat=22081\n","Skipping y_hat=22081\n","Skipping y_hat=22081\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=44046\n","Skipping y_hat=44046\n","Skipping y_hat=44046\n","Skipping y_hat=44046\n","Skipping y_hat=44046\n","Skipping y_hat=44046\n","Skipping y_hat=44046\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=0+33\n","Skipping y_hat=0+33\n","Skipping y_hat=0+33\n","Skipping y_hat=0+33\n","Skipping y_hat=0+33\n","Skipping y_hat=0+33\n","Skipping y_hat=0+33\n","Skipping y_hat=8+00\n","Skipping y_hat=8+00\n","Skipping y_hat=8+00\n","Skipping y_hat=8+00\n","Skipping y_hat=8+00\n","Skipping y_hat=8+00\n","Skipping y_hat=8+00\n","Skipping y_hat=119+2\n","Skipping y_hat=119+2\n","Skipping y_hat=119+2\n","Skipping y_hat=119+2\n","Skipping y_hat=119+2\n","Skipping y_hat=119+2\n","Skipping y_hat=119+2\n","Skipping y_hat=0997\n","Skipping y_hat=0997\n","Skipping y_hat=0997\n","Skipping y_hat=0997\n","Skipping y_hat=0997\n","Skipping y_hat=0997\n","Skipping y_hat=0997\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=77229\n","Skipping y_hat=77229\n","Skipping y_hat=77229\n","Skipping y_hat=77229\n","Skipping y_hat=77229\n","Skipping y_hat=77229\n","Skipping y_hat=77229\n","Skipping y_hat=13333\n","Skipping y_hat=13333\n","Skipping y_hat=13333\n","Skipping y_hat=13333\n","Skipping y_hat=13333\n","Skipping y_hat=13333\n","Skipping y_hat=13333\n","Skipping y_hat=6\n","9\n","8\n","Skipping y_hat=6\n","9\n","8\n","Skipping y_hat=6\n","9\n","8\n","Skipping y_hat=6\n","9\n","8\n","Skipping y_hat=6\n","9\n","8\n","Skipping y_hat=6\n","9\n","8\n","Skipping y_hat=6\n","9\n","8\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=6036\n","Skipping y_hat=6036\n","Skipping y_hat=6036\n","Skipping y_hat=6036\n","Skipping y_hat=6036\n","Skipping y_hat=6036\n","Skipping y_hat=6036\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=94766\n","Skipping y_hat=94766\n","Skipping y_hat=94766\n","Skipping y_hat=94766\n","Skipping y_hat=94766\n","Skipping y_hat=94766\n","Skipping y_hat=94766\n","Skipping y_hat=6891+\n","Skipping y_hat=6891+\n","Skipping y_hat=6891+\n","Skipping y_hat=6891+\n","Skipping y_hat=6891+\n","Skipping y_hat=6891+\n","Skipping y_hat=6891+\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=27742\n","Skipping y_hat=27742\n","Skipping y_hat=27742\n","Skipping y_hat=27742\n","Skipping y_hat=27742\n","Skipping y_hat=27742\n","Skipping y_hat=27742\n","Skipping y_hat=53004\n","Skipping y_hat=53004\n","Skipping y_hat=53004\n","Skipping y_hat=53004\n","Skipping y_hat=53004\n","Skipping y_hat=53004\n","Skipping y_hat=53004\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=99339\n","Skipping y_hat=99339\n","Skipping y_hat=99339\n","Skipping y_hat=99339\n","Skipping y_hat=99339\n","Skipping y_hat=99339\n","Skipping y_hat=99339\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=4404\n","Skipping y_hat=4404\n","Skipping y_hat=4404\n","Skipping y_hat=4404\n","Skipping y_hat=4404\n","Skipping y_hat=4404\n","Skipping y_hat=4404\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=0022\n","Skipping y_hat=0022\n","Skipping y_hat=0022\n","Skipping y_hat=0022\n","Skipping y_hat=0022\n","Skipping y_hat=0022\n","Skipping y_hat=0022\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=40134\n","Skipping y_hat=40134\n","Skipping y_hat=40134\n","Skipping y_hat=40134\n","Skipping y_hat=40134\n","Skipping y_hat=40134\n","Skipping y_hat=40134\n","Skipping y_hat=487+0\n","Skipping y_hat=487+0\n","Skipping y_hat=487+0\n","Skipping y_hat=487+0\n","Skipping y_hat=487+0\n","Skipping y_hat=487+0\n","Skipping y_hat=487+0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=0\n","Skipping y_hat=960\n","Skipping y_hat=960\n","Skipping y_hat=960\n","Skipping y_hat=960\n","Skipping y_hat=960\n","Skipping y_hat=960\n","Skipping y_hat=960\n","Skipping y_hat=63929\n","Skipping y_hat=63929\n","Skipping y_hat=63929\n","Skipping y_hat=63929\n","Skipping y_hat=63929\n","Skipping y_hat=63929\n","Skipping y_hat=63929\n","Skipping y_hat=+22\n","9\n","Skipping y_hat=+22\n","9\n","Skipping y_hat=+22\n","9\n","Skipping y_hat=+22\n","9\n","Skipping y_hat=+22\n","9\n","Skipping y_hat=+22\n","9\n","Skipping y_hat=+22\n","9\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=3\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=18959\n","Skipping y_hat=18959\n","Skipping y_hat=18959\n","Skipping y_hat=18959\n","Skipping y_hat=18959\n","Skipping y_hat=18959\n","Skipping y_hat=18959\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=88016\n","Skipping y_hat=88016\n","Skipping y_hat=88016\n","Skipping y_hat=88016\n","Skipping y_hat=88016\n","Skipping y_hat=88016\n","Skipping y_hat=88016\n","Skipping y_hat=24796\n","Skipping y_hat=24796\n","Skipping y_hat=24796\n","Skipping y_hat=24796\n","Skipping y_hat=24796\n","Skipping y_hat=24796\n","Skipping y_hat=24796\n","Skipping y_hat=41888\n","Skipping y_hat=41888\n","Skipping y_hat=41888\n","Skipping y_hat=41888\n","Skipping y_hat=41888\n","Skipping y_hat=41888\n","Skipping y_hat=41888\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=55011\n","Skipping y_hat=55011\n","Skipping y_hat=55011\n","Skipping y_hat=55011\n","Skipping y_hat=55011\n","Skipping y_hat=55011\n","Skipping y_hat=55011\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=59990\n","Skipping y_hat=59990\n","Skipping y_hat=59990\n","Skipping y_hat=59990\n","Skipping y_hat=59990\n","Skipping y_hat=59990\n","Skipping y_hat=59990\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=10210\n","Skipping y_hat=10210\n","Skipping y_hat=10210\n","Skipping y_hat=10210\n","Skipping y_hat=10210\n","Skipping y_hat=10210\n","Skipping y_hat=10210\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=4+155\n","Skipping y_hat=4+155\n","Skipping y_hat=4+155\n","Skipping y_hat=4+155\n","Skipping y_hat=4+155\n","Skipping y_hat=4+155\n","Skipping y_hat=4+155\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=07\n","\n","6\n","Skipping y_hat=07\n","\n","6\n","Skipping y_hat=07\n","\n","6\n","Skipping y_hat=07\n","\n","6\n","Skipping y_hat=07\n","\n","6\n","Skipping y_hat=07\n","\n","6\n","Skipping y_hat=07\n","\n","6\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=111\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=55544\n","Skipping y_hat=55544\n","Skipping y_hat=55544\n","Skipping y_hat=55544\n","Skipping y_hat=55544\n","Skipping y_hat=55544\n","Skipping y_hat=55544\n","Skipping y_hat=04\n","Skipping y_hat=04\n","Skipping y_hat=04\n","Skipping y_hat=04\n","Skipping y_hat=04\n","Skipping y_hat=04\n","Skipping y_hat=04\n","Skipping y_hat=4\n","59\n","Skipping y_hat=4\n","59\n","Skipping y_hat=4\n","59\n","Skipping y_hat=4\n","59\n","Skipping y_hat=4\n","59\n","Skipping y_hat=4\n","59\n","Skipping y_hat=4\n","59\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=80+33\n","Skipping y_hat=80+33\n","Skipping y_hat=80+33\n","Skipping y_hat=80+33\n","Skipping y_hat=80+33\n","Skipping y_hat=80+33\n","Skipping y_hat=80+33\n"," 96% 24/25 [00:02<00:00, 10.19it/s]Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=3662\n","Skipping y_hat=3662\n","Skipping y_hat=3662\n","Skipping y_hat=3662\n","Skipping y_hat=3662\n","Skipping y_hat=3662\n","Skipping y_hat=3662\n","Skipping y_hat=3093\n","Skipping y_hat=3093\n","Skipping y_hat=3093\n","Skipping y_hat=3093\n","Skipping y_hat=3093\n","Skipping y_hat=3093\n","Skipping y_hat=3093\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","Skipping y_hat=\n","100% 25/25 [00:02<00:00,  8.46it/s]\n","accuracy of 3000 examples: 0/3000 (0.0%)\n","/content/drive/MyDrive/addition/evaluation.py:255: RuntimeWarning: Mean of empty slice\n","  metric_type: np.nanmean(error_dict[f'{metric_type}'])\n","/content/drive/MyDrive/addition/evaluation.py:369: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test: 0.00%\n","\n","Creating new batches\n","Preparing batches from: /content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_reading/train_eval.txt\n","Preparing batches for 1 examples from: /content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_reading/train_eval.txt\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/addition/train.py\", line 665, in <module>\n","    train_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","                                             ^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/addition/evaluation.py\", line 280, in evaluate_addition_batch\n","    batch_list, total = prepare_addition_batches(\n","                        ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/addition/evaluation.py\", line 109, in prepare_addition_batches\n","    prompt_ids = encode(prompt_str)\n","                 ^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/addition/train.py\", line 667, in <lambda>\n","    encode=lambda x: encode_addition(x, meta),\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/addition/train.py\", line 48, in encode_addition\n","    return torch.tensor([meta['stoi'][c] for c in text], dtype=torch.long)\n","                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/addition/train.py\", line 48, in <listcomp>\n","    return torch.tensor([meta['stoi'][c] for c in text], dtype=torch.long)\n","                         ~~~~~~~~~~~~^^^\n","KeyError: '/'\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m2_operands_0_to_999_uniform_gold_by_reading_plain\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/2w5li0mn\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m2_operands_0_to_999_uniform/gold_by_reading_plain_out/wandb/run-20250810_140357-2w5li0mn/logs\u001b[0m\n"]}],"source":["!python train.py 2_operands_addition_plain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":270656,"status":"ok","timestamp":1754687136472,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":240},"id":"t34_PVKFJfuU","outputId":"cfe4d0ae-bb86-46ad-dbec-af14f48a6f79"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=616\n","Skipping y_hat=420\n","Skipping y_hat=689\n","Skipping y_hat=822\n","Skipping y_hat=182\n","Skipping y_hat=196\n","Skipping y_hat=732\n","Skipping y_hat=750\n","Skipping y_hat=229\n","Skipping y_hat=189\n","Skipping y_hat=178\n","Skipping y_hat=748\n","Skipping y_hat=86\n","Skipping y_hat=320\n","Skipping y_hat=340\n","Skipping y_hat=760\n","Skipping y_hat=133\n","Skipping y_hat=200\n","Skipping y_hat=862\n","Skipping y_hat=3\n","Skipping y_hat=411\n","Skipping y_hat=639\n","Skipping y_hat=742\n","Skipping y_hat=174\n","Skipping y_hat=804\n","Skipping y_hat=89\n","Skipping y_hat=722\n","Skipping y_hat=170\n","Skipping y_hat=676\n","Skipping y_hat=41\n","Skipping y_hat=145\n","Skipping y_hat=706\n","Skipping y_hat=132\n","Skipping y_hat=138\n","Skipping y_hat=248\n","Skipping y_hat=40\n","Skipping y_hat=722\n","Skipping y_hat=91\n","Skipping y_hat=774\n","Skipping y_hat=77\n","Skipping y_hat=185\n","Skipping y_hat=285\n","Skipping y_hat=882\n","Skipping y_hat=73\n","Skipping y_hat=78\n","Skipping y_hat=91\n","Skipping y_hat=74\n","100% 25/25 [00:00<00:00, 30.26it/s]\n","accuracy of 3000 examples: 2916/3000 (97.2%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 34.83it/s]\n","accuracy of 3000 examples: 2920/3000 (97.33333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n","  0% 0/25 [00:00<?, ?it/s]Skipping y_hat=622\n"," 68% 17/25 [00:00<00:00, 29.91it/s]Skipping y_hat=693\n"," 80% 20/25 [00:00<00:00, 29.93it/s]Skipping y_hat=453\n"," 92% 23/25 [00:00<00:00, 29.90it/s]Skipping y_hat=485\n","Skipping y_hat=788\n","Skipping y_hat=416\n","Skipping y_hat=675\n","Skipping y_hat=322\n","Skipping y_hat=77\n","Skipping y_hat=530\n","Skipping y_hat=889\n","Skipping y_hat=541\n","Skipping y_hat=319\n","Skipping y_hat=346\n","Skipping y_hat=689\n","Skipping y_hat=848\n","Skipping y_hat=322\n","Skipping y_hat=199\n","Skipping y_hat=140\n","Skipping y_hat=286\n","Skipping y_hat=992\n","Skipping y_hat=698\n","Skipping y_hat=821\n","Skipping y_hat=813\n","Skipping y_hat=615\n","Skipping y_hat=776\n","Skipping y_hat=193\n","Skipping y_hat=602\n","Skipping y_hat=717\n","Skipping y_hat=885\n","Skipping y_hat=408\n","Skipping y_hat=446\n","Skipping y_hat=187\n","Skipping y_hat=266\n","Skipping y_hat=430\n","Skipping y_hat=866\n","Skipping y_hat=790\n","Skipping y_hat=569\n","Skipping y_hat=822\n","Skipping y_hat=316\n","Skipping y_hat=82\n","Skipping y_hat=106\n","Skipping y_hat=642\n","Skipping y_hat=700\n","Skipping y_hat=269\n","Skipping y_hat=199\n","Skipping y_hat=198\n","Skipping y_hat=778\n","Skipping y_hat=126\n","Skipping y_hat=230\n","Skipping y_hat=870\n","Skipping y_hat=130\n","Skipping y_hat=183\n","Skipping y_hat=200\n","Skipping y_hat=582\n","Skipping y_hat=3\n","Skipping y_hat=431\n","Skipping y_hat=669\n","Skipping y_hat=712\n","Skipping y_hat=174\n","Skipping y_hat=764\n","Skipping y_hat=109\n","Skipping y_hat=512\n","Skipping y_hat=150\n","Skipping y_hat=506\n","Skipping y_hat=41\n","Skipping y_hat=155\n","Skipping y_hat=876\n","Skipping y_hat=222\n","Skipping y_hat=321\n","Skipping y_hat=178\n","Skipping y_hat=10\n","Skipping y_hat=722\n","Skipping y_hat=11\n","Skipping y_hat=354\n","Skipping y_hat=87\n","Skipping y_hat=185\n","Skipping y_hat=955\n","Skipping y_hat=92\n","Skipping y_hat=23\n","Skipping y_hat=88\n","Skipping y_hat=74\n","Skipping y_hat=44\n","Skipping y_hat=9\n","100% 25/25 [00:00<00:00, 29.17it/s]\n","accuracy of 3000 examples: 2916/3000 (97.2%)\n","\n","Test Results:\n","test: 97.20%\n","\n","iter 2300: train loss 1.0378, val loss 1.0374\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 92% 23/25 [00:00<00:00, 32.12it/s]Skipping y_hat=445\n","Skipping y_hat=378\n","Skipping y_hat=446\n","Skipping y_hat=895\n","Skipping y_hat=322\n","Skipping y_hat=87\n","Skipping y_hat=560\n","Skipping y_hat=136\n","Skipping y_hat=469\n","Skipping y_hat=501\n","Skipping y_hat=789\n","Skipping y_hat=489\n","Skipping y_hat=948\n","Skipping y_hat=812\n","Skipping y_hat=169\n","Skipping y_hat=120\n","Skipping y_hat=656\n","Skipping y_hat=882\n","Skipping y_hat=238\n","Skipping y_hat=821\n","Skipping y_hat=883\n","Skipping y_hat=795\n","Skipping y_hat=256\n","Skipping y_hat=83\n","Skipping y_hat=682\n","Skipping y_hat=767\n","Skipping y_hat=295\n","Skipping y_hat=728\n","Skipping y_hat=776\n","Skipping y_hat=87\n","Skipping y_hat=226\n","Skipping y_hat=770\n","Skipping y_hat=796\n","Skipping y_hat=740\n","Skipping y_hat=899\n","Skipping y_hat=862\n","Skipping y_hat=316\n","Skipping y_hat=22\n","Skipping y_hat=106\n","Skipping y_hat=622\n","Skipping y_hat=720\n","Skipping y_hat=249\n","Skipping y_hat=129\n","Skipping y_hat=98\n","Skipping y_hat=388\n","Skipping y_hat=6\n","Skipping y_hat=440\n","Skipping y_hat=730\n","Skipping y_hat=160\n","Skipping y_hat=133\n","Skipping y_hat=260\n","Skipping y_hat=472\n","Skipping y_hat=83\n","Skipping y_hat=431\n","Skipping y_hat=669\n","Skipping y_hat=712\n","Skipping y_hat=164\n","Skipping y_hat=704\n","Skipping y_hat=39\n","Skipping y_hat=992\n","Skipping y_hat=110\n","Skipping y_hat=206\n","Skipping y_hat=91\n","Skipping y_hat=145\n","Skipping y_hat=796\n","Skipping y_hat=222\n","Skipping y_hat=301\n","Skipping y_hat=188\n","Skipping y_hat=248\n","Skipping y_hat=0\n","Skipping y_hat=712\n","Skipping y_hat=11\n","Skipping y_hat=874\n","Skipping y_hat=105\n","Skipping y_hat=705\n","Skipping y_hat=882\n","Skipping y_hat=7\n","Skipping y_hat=42\n","Skipping y_hat=26\n","Skipping y_hat=13\n","Skipping y_hat=23\n","100% 25/25 [00:00<00:00, 31.12it/s]\n","accuracy of 3000 examples: 2919/3000 (97.3%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.90it/s]\n","accuracy of 3000 examples: 2917/3000 (97.23333333333333%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.83it/s]Skipping y_hat=477\n","Skipping y_hat=875\n","Skipping y_hat=348\n","Skipping y_hat=446\n","Skipping y_hat=835\n","Skipping y_hat=312\n","Skipping y_hat=107\n","Skipping y_hat=540\n","Skipping y_hat=146\n","Skipping y_hat=349\n","Skipping y_hat=581\n","Skipping y_hat=49\n","Skipping y_hat=476\n","Skipping y_hat=599\n","Skipping y_hat=998\n","Skipping y_hat=802\n","Skipping y_hat=49\n","Skipping y_hat=110\n","Skipping y_hat=776\n","Skipping y_hat=742\n","Skipping y_hat=668\n","Skipping y_hat=861\n","Skipping y_hat=843\n","Skipping y_hat=285\n","Skipping y_hat=346\n","Skipping y_hat=43\n","Skipping y_hat=642\n","Skipping y_hat=787\n","Skipping y_hat=755\n","Skipping y_hat=748\n","Skipping y_hat=556\n","Skipping y_hat=97\n","Skipping y_hat=440\n","Skipping y_hat=626\n","Skipping y_hat=110\n","Skipping y_hat=789\n","Skipping y_hat=822\n","Skipping y_hat=306\n","Skipping y_hat=92\n","Skipping y_hat=76\n","Skipping y_hat=382\n","Skipping y_hat=770\n","Skipping y_hat=199\n","Skipping y_hat=88\n","Skipping y_hat=108\n","Skipping y_hat=16\n","Skipping y_hat=490\n","Skipping y_hat=780\n","Skipping y_hat=640\n","Skipping y_hat=163\n","Skipping y_hat=290\n","Skipping y_hat=232\n","Skipping y_hat=83\n","Skipping y_hat=891\n","Skipping y_hat=649\n","Skipping y_hat=772\n","Skipping y_hat=24\n","Skipping y_hat=814\n","Skipping y_hat=39\n","Skipping y_hat=392\n","Skipping y_hat=160\n","Skipping y_hat=316\n","Skipping y_hat=11\n","Skipping y_hat=135\n","Skipping y_hat=386\n","Skipping y_hat=162\n","Skipping y_hat=321\n","Skipping y_hat=148\n","Skipping y_hat=278\n","Skipping y_hat=10\n","Skipping y_hat=752\n","Skipping y_hat=11\n","Skipping y_hat=274\n","Skipping y_hat=57\n","Skipping y_hat=55\n","Skipping y_hat=582\n"," 96% 24/25 [00:00<00:00, 31.16it/s]Skipping y_hat=36\n","Skipping y_hat=85\n","Skipping y_hat=32\n","100% 25/25 [00:00<00:00, 31.39it/s]\n","accuracy of 3000 examples: 2920/3000 (97.33333333333334%)\n","\n","Test Results:\n","test: 97.33%\n","\n","iter 2400: train loss 1.0348, val loss 1.0455\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 72% 18/25 [00:00<00:00, 26.95it/s]Skipping y_hat=233\n"," 84% 21/25 [00:00<00:00, 27.60it/s]Skipping y_hat=205\n","Skipping y_hat=648\n","Skipping y_hat=446\n","Skipping y_hat=815\n","Skipping y_hat=392\n","Skipping y_hat=127\n","Skipping y_hat=520\n","Skipping y_hat=196\n","Skipping y_hat=839\n","Skipping y_hat=521\n","Skipping y_hat=609\n","Skipping y_hat=316\n","Skipping y_hat=849\n","Skipping y_hat=948\n","Skipping y_hat=872\n","Skipping y_hat=89\n","Skipping y_hat=150\n","Skipping y_hat=876\n","Skipping y_hat=672\n","Skipping y_hat=128\n","Skipping y_hat=841\n","Skipping y_hat=893\n","Skipping y_hat=515\n","Skipping y_hat=406\n","Skipping y_hat=73\n","Skipping y_hat=672\n","Skipping y_hat=757\n","Skipping y_hat=795\n","Skipping y_hat=988\n","Skipping y_hat=316\n","Skipping y_hat=117\n","Skipping y_hat=246\n","Skipping y_hat=170\n","Skipping y_hat=426\n","Skipping y_hat=240\n","Skipping y_hat=999\n","Skipping y_hat=832\n","Skipping y_hat=316\n","Skipping y_hat=92\n","Skipping y_hat=146\n","Skipping y_hat=342\n","Skipping y_hat=740\n","Skipping y_hat=269\n","Skipping y_hat=118\n","Skipping y_hat=798\n","Skipping y_hat=46\n","Skipping y_hat=840\n","Skipping y_hat=640\n","Skipping y_hat=740\n","Skipping y_hat=173\n","Skipping y_hat=200\n","Skipping y_hat=782\n","Skipping y_hat=73\n","Skipping y_hat=631\n","Skipping y_hat=699\n","Skipping y_hat=762\n","Skipping y_hat=114\n","Skipping y_hat=214\n","Skipping y_hat=39\n","Skipping y_hat=572\n","Skipping y_hat=80\n","Skipping y_hat=716\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=786\n","Skipping y_hat=122\n","Skipping y_hat=341\n","Skipping y_hat=178\n","Skipping y_hat=248\n","Skipping y_hat=180\n","Skipping y_hat=782\n","Skipping y_hat=161\n","Skipping y_hat=744\n","Skipping y_hat=27\n","Skipping y_hat=195\n","Skipping y_hat=875\n","Skipping y_hat=442\n"," 96% 24/25 [00:00<00:00, 27.93it/s]Skipping y_hat=78\n","Skipping y_hat=95\n","Skipping y_hat=96\n","Skipping y_hat=52\n","Skipping y_hat=85\n","100% 25/25 [00:00<00:00, 28.18it/s]\n","accuracy of 3000 examples: 2917/3000 (97.23333333333333%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.33it/s]\n","accuracy of 3000 examples: 2918/3000 (97.26666666666667%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 32% 8/25 [00:00<00:00, 30.73it/s]Skipping y_hat=1457\n"," 80% 20/25 [00:00<00:00, 30.91it/s]Skipping y_hat=485\n","Skipping y_hat=878\n","Skipping y_hat=496\n","Skipping y_hat=805\n","Skipping y_hat=302\n","Skipping y_hat=177\n","Skipping y_hat=530\n","Skipping y_hat=146\n","Skipping y_hat=509\n","Skipping y_hat=581\n","Skipping y_hat=449\n","Skipping y_hat=426\n","Skipping y_hat=789\n","Skipping y_hat=928\n","Skipping y_hat=282\n","Skipping y_hat=99\n","Skipping y_hat=140\n","Skipping y_hat=546\n","Skipping y_hat=582\n","Skipping y_hat=158\n","Skipping y_hat=821\n","Skipping y_hat=873\n","Skipping y_hat=755\n","Skipping y_hat=686\n","Skipping y_hat=133\n","Skipping y_hat=632\n","Skipping y_hat=777\n","Skipping y_hat=375\n","Skipping y_hat=708\n","Skipping y_hat=466\n","Skipping y_hat=77\n","Skipping y_hat=246\n","Skipping y_hat=630\n","Skipping y_hat=826\n","Skipping y_hat=730\n","Skipping y_hat=789\n","Skipping y_hat=882\n","Skipping y_hat=316\n","Skipping y_hat=142\n","Skipping y_hat=136\n","Skipping y_hat=482\n","Skipping y_hat=750\n","Skipping y_hat=259\n","Skipping y_hat=119\n","Skipping y_hat=198\n","Skipping y_hat=278\n","Skipping y_hat=86\n","Skipping y_hat=170\n","Skipping y_hat=520\n","Skipping y_hat=740\n","Skipping y_hat=183\n","Skipping y_hat=240\n","Skipping y_hat=742\n","Skipping y_hat=73\n","Skipping y_hat=721\n","Skipping y_hat=649\n","Skipping y_hat=742\n","Skipping y_hat=84\n","Skipping y_hat=384\n","Skipping y_hat=29\n","Skipping y_hat=762\n","Skipping y_hat=170\n","Skipping y_hat=616\n","Skipping y_hat=71\n","Skipping y_hat=145\n","Skipping y_hat=716\n","Skipping y_hat=102\n","Skipping y_hat=148\n","Skipping y_hat=20\n","Skipping y_hat=742\n","Skipping y_hat=71\n","Skipping y_hat=784\n","Skipping y_hat=27\n","Skipping y_hat=445\n","Skipping y_hat=492\n"," 96% 24/25 [00:00<00:00, 30.28it/s]Skipping y_hat=95\n","Skipping y_hat=42\n","Skipping y_hat=67\n","Skipping y_hat=51\n","Skipping y_hat=14\n","100% 25/25 [00:00<00:00, 30.70it/s]\n","accuracy of 3000 examples: 2919/3000 (97.3%)\n","\n","Test Results:\n","test: 97.30%\n","\n","iter 2500: train loss 1.0322, val loss 1.0389\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.47it/s]Skipping y_hat=489\n","Skipping y_hat=785\n","Skipping y_hat=858\n","Skipping y_hat=416\n","Skipping y_hat=525\n","Skipping y_hat=372\n","Skipping y_hat=187\n","Skipping y_hat=520\n","Skipping y_hat=186\n","Skipping y_hat=899\n","Skipping y_hat=591\n","Skipping y_hat=769\n","Skipping y_hat=426\n","Skipping y_hat=859\n","Skipping y_hat=958\n","Skipping y_hat=722\n","Skipping y_hat=109\n","Skipping y_hat=170\n","Skipping y_hat=166\n","Skipping y_hat=682\n","Skipping y_hat=668\n","Skipping y_hat=841\n","Skipping y_hat=843\n","Skipping y_hat=585\n","Skipping y_hat=276\n","Skipping y_hat=173\n","Skipping y_hat=632\n","Skipping y_hat=767\n","Skipping y_hat=825\n","Skipping y_hat=958\n","Skipping y_hat=306\n","Skipping y_hat=107\n","Skipping y_hat=246\n","Skipping y_hat=400\n","Skipping y_hat=716\n","Skipping y_hat=530\n","Skipping y_hat=409\n","Skipping y_hat=842\n","Skipping y_hat=376\n","Skipping y_hat=92\n","Skipping y_hat=196\n","Skipping y_hat=732\n","Skipping y_hat=179\n","Skipping y_hat=188\n","Skipping y_hat=838\n","Skipping y_hat=26\n","Skipping y_hat=840\n","Skipping y_hat=420\n","Skipping y_hat=640\n","Skipping y_hat=183\n","Skipping y_hat=210\n","Skipping y_hat=592\n","Skipping y_hat=93\n","Skipping y_hat=241\n","Skipping y_hat=629\n","Skipping y_hat=732\n","Skipping y_hat=174\n","Skipping y_hat=874\n","Skipping y_hat=872\n","Skipping y_hat=160\n","Skipping y_hat=286\n","Skipping y_hat=11\n","Skipping y_hat=115\n","Skipping y_hat=906\n","Skipping y_hat=132\n","Skipping y_hat=198\n","Skipping y_hat=248\n","Skipping y_hat=50\n","Skipping y_hat=702\n","Skipping y_hat=894\n","Skipping y_hat=37\n","Skipping y_hat=195\n","Skipping y_hat=845\n","Skipping y_hat=882\n"," 96% 24/25 [00:00<00:00, 30.97it/s]Skipping y_hat=7\n","Skipping y_hat=37\n","Skipping y_hat=63\n","Skipping y_hat=45\n","Skipping y_hat=77\n","100% 25/25 [00:00<00:00, 31.38it/s]\n","accuracy of 3000 examples: 2921/3000 (97.36666666666667%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.61it/s]\n","accuracy of 3000 examples: 2921/3000 (97.36666666666667%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.32it/s]Skipping y_hat=489\n","Skipping y_hat=595\n","Skipping y_hat=358\n","Skipping y_hat=496\n","Skipping y_hat=695\n","Skipping y_hat=382\n","Skipping y_hat=87\n","Skipping y_hat=530\n","Skipping y_hat=136\n","Skipping y_hat=929\n","Skipping y_hat=561\n","Skipping y_hat=559\n","Skipping y_hat=796\n","Skipping y_hat=729\n","Skipping y_hat=938\n","Skipping y_hat=112\n","Skipping y_hat=199\n","Skipping y_hat=100\n","Skipping y_hat=776\n","Skipping y_hat=982\n","Skipping y_hat=328\n","Skipping y_hat=843\n","Skipping y_hat=485\n","Skipping y_hat=676\n","Skipping y_hat=113\n","Skipping y_hat=672\n","Skipping y_hat=727\n","Skipping y_hat=795\n","Skipping y_hat=748\n","Skipping y_hat=856\n","Skipping y_hat=197\n","Skipping y_hat=246\n","Skipping y_hat=800\n","Skipping y_hat=586\n","Skipping y_hat=730\n","Skipping y_hat=619\n","Skipping y_hat=832\n","Skipping y_hat=376\n","Skipping y_hat=192\n","Skipping y_hat=146\n","Skipping y_hat=582\n","Skipping y_hat=720\n","Skipping y_hat=149\n","Skipping y_hat=198\n","Skipping y_hat=778\n","Skipping y_hat=86\n","Skipping y_hat=340\n","Skipping y_hat=530\n","Skipping y_hat=750\n","Skipping y_hat=123\n","Skipping y_hat=290\n","Skipping y_hat=382\n","Skipping y_hat=93\n","Skipping y_hat=731\n","Skipping y_hat=629\n","Skipping y_hat=702\n","Skipping y_hat=154\n","Skipping y_hat=644\n","Skipping y_hat=29\n","Skipping y_hat=772\n","Skipping y_hat=100\n","Skipping y_hat=566\n","Skipping y_hat=11\n","Skipping y_hat=796\n","Skipping y_hat=182\n","Skipping y_hat=158\n","Skipping y_hat=278\n","Skipping y_hat=10\n","Skipping y_hat=722\n","Skipping y_hat=121\n","Skipping y_hat=784\n","Skipping y_hat=97\n","Skipping y_hat=105\n","Skipping y_hat=795\n","Skipping y_hat=712\n"," 96% 24/25 [00:00<00:00, 30.96it/s]Skipping y_hat=29\n","Skipping y_hat=32\n","Skipping y_hat=68\n","Skipping y_hat=53\n","Skipping y_hat=28\n","100% 25/25 [00:00<00:00, 31.27it/s]\n","accuracy of 3000 examples: 2920/3000 (97.33333333333334%)\n","\n","Test Results:\n","test: 97.33%\n","\n","iter 2600: train loss 1.0315, val loss 1.0418\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 88% 22/25 [00:00<00:00, 31.26it/s]Skipping y_hat=865\n","Skipping y_hat=398\n","Skipping y_hat=426\n","Skipping y_hat=495\n","Skipping y_hat=372\n","Skipping y_hat=187\n","Skipping y_hat=500\n","Skipping y_hat=146\n","Skipping y_hat=929\n","Skipping y_hat=561\n","Skipping y_hat=669\n","Skipping y_hat=786\n","Skipping y_hat=709\n","Skipping y_hat=988\n","Skipping y_hat=592\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=976\n","Skipping y_hat=762\n","Skipping y_hat=248\n","Skipping y_hat=873\n","Skipping y_hat=675\n","Skipping y_hat=846\n","Skipping y_hat=183\n","Skipping y_hat=747\n","Skipping y_hat=785\n","Skipping y_hat=908\n","Skipping y_hat=446\n","Skipping y_hat=187\n","Skipping y_hat=286\n","Skipping y_hat=220\n","Skipping y_hat=706\n","Skipping y_hat=730\n","Skipping y_hat=289\n","Skipping y_hat=116\n","Skipping y_hat=472\n","Skipping y_hat=720\n","Skipping y_hat=149\n","Skipping y_hat=198\n","Skipping y_hat=468\n","Skipping y_hat=96\n","Skipping y_hat=930\n","Skipping y_hat=890\n","Skipping y_hat=430\n","Skipping y_hat=133\n","Skipping y_hat=210\n","Skipping y_hat=432\n","Skipping y_hat=103\n","Skipping y_hat=131\n","Skipping y_hat=629\n","Skipping y_hat=84\n","Skipping y_hat=554\n","Skipping y_hat=29\n","Skipping y_hat=982\n","Skipping y_hat=170\n","Skipping y_hat=296\n","Skipping y_hat=71\n","Skipping y_hat=145\n","Skipping y_hat=346\n","Skipping y_hat=192\n","Skipping y_hat=108\n","Skipping y_hat=218\n","Skipping y_hat=80\n","Skipping y_hat=712\n","Skipping y_hat=111\n","Skipping y_hat=774\n","Skipping y_hat=67\n","Skipping y_hat=95\n","Skipping y_hat=495\n","Skipping y_hat=122\n","Skipping y_hat=76\n","Skipping y_hat=19\n","Skipping y_hat=67\n","Skipping y_hat=77\n","Skipping y_hat=14\n","100% 25/25 [00:00<00:00, 30.42it/s]\n","accuracy of 3000 examples: 2925/3000 (97.5%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 33.27it/s]\n","accuracy of 3000 examples: 2919/3000 (97.3%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n","  0% 0/25 [00:00<?, ?it/s]Skipping y_hat=1575\n"," 80% 20/25 [00:00<00:00, 28.91it/s]Skipping y_hat=295\n","Skipping y_hat=778\n","Skipping y_hat=486\n","Skipping y_hat=795\n","Skipping y_hat=322\n","Skipping y_hat=187\n","Skipping y_hat=540\n","Skipping y_hat=136\n","Skipping y_hat=989\n","Skipping y_hat=551\n","Skipping y_hat=449\n","Skipping y_hat=566\n","Skipping y_hat=599\n","Skipping y_hat=988\n","Skipping y_hat=482\n","Skipping y_hat=169\n","Skipping y_hat=120\n","Skipping y_hat=796\n","Skipping y_hat=672\n","Skipping y_hat=768\n","Skipping y_hat=871\n","Skipping y_hat=873\n","Skipping y_hat=885\n","Skipping y_hat=976\n","Skipping y_hat=602\n","Skipping y_hat=727\n","Skipping y_hat=795\n","Skipping y_hat=278\n","Skipping y_hat=846\n","Skipping y_hat=177\n","Skipping y_hat=226\n","Skipping y_hat=420\n","Skipping y_hat=706\n","Skipping y_hat=140\n","Skipping y_hat=919\n","Skipping y_hat=872\n","Skipping y_hat=396\n","Skipping y_hat=182\n","Skipping y_hat=156\n","Skipping y_hat=692\n","Skipping y_hat=790\n","Skipping y_hat=99\n","Skipping y_hat=198\n","Skipping y_hat=748\n","Skipping y_hat=16\n","Skipping y_hat=440\n","Skipping y_hat=490\n","Skipping y_hat=540\n","Skipping y_hat=133\n","Skipping y_hat=230\n","Skipping y_hat=822\n","Skipping y_hat=93\n","Skipping y_hat=311\n","Skipping y_hat=669\n","Skipping y_hat=712\n","Skipping y_hat=174\n","Skipping y_hat=944\n","Skipping y_hat=69\n","Skipping y_hat=472\n","Skipping y_hat=180\n","Skipping y_hat=556\n","Skipping y_hat=41\n","Skipping y_hat=185\n","Skipping y_hat=476\n","Skipping y_hat=192\n","Skipping y_hat=128\n","Skipping y_hat=20\n","Skipping y_hat=712\n","Skipping y_hat=21\n","Skipping y_hat=764\n","Skipping y_hat=177\n","Skipping y_hat=95\n","Skipping y_hat=405\n","Skipping y_hat=732\n"," 96% 24/25 [00:00<00:00, 29.13it/s]Skipping y_hat=74\n","Skipping y_hat=86\n","Skipping y_hat=40\n","Skipping y_hat=65\n","Skipping y_hat=52\n","100% 25/25 [00:00<00:00, 28.96it/s]\n","accuracy of 3000 examples: 2920/3000 (97.33333333333334%)\n","\n","Test Results:\n","test: 97.33%\n","\n","iter 2700: train loss 1.0285, val loss 1.0450\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.96it/s]Skipping y_hat=795\n","Skipping y_hat=418\n","Skipping y_hat=446\n","Skipping y_hat=895\n","Skipping y_hat=372\n","Skipping y_hat=77\n","Skipping y_hat=590\n","Skipping y_hat=229\n","Skipping y_hat=669\n","Skipping y_hat=976\n","Skipping y_hat=609\n","Skipping y_hat=968\n","Skipping y_hat=572\n","Skipping y_hat=129\n","Skipping y_hat=120\n","Skipping y_hat=876\n","Skipping y_hat=822\n","Skipping y_hat=878\n","Skipping y_hat=821\n","Skipping y_hat=813\n","Skipping y_hat=485\n","Skipping y_hat=776\n","Skipping y_hat=43\n","Skipping y_hat=602\n","Skipping y_hat=747\n","Skipping y_hat=265\n","Skipping y_hat=388\n","Skipping y_hat=396\n","Skipping y_hat=97\n","Skipping y_hat=276\n","Skipping y_hat=830\n","Skipping y_hat=496\n","Skipping y_hat=240\n","Skipping y_hat=789\n","Skipping y_hat=346\n","Skipping y_hat=146\n","Skipping y_hat=512\n","Skipping y_hat=720\n","Skipping y_hat=309\n","Skipping y_hat=79\n","Skipping y_hat=88\n","Skipping y_hat=328\n","Skipping y_hat=86\n","Skipping y_hat=820\n","Skipping y_hat=130\n","Skipping y_hat=740\n","Skipping y_hat=133\n","Skipping y_hat=240\n","Skipping y_hat=122\n","Skipping y_hat=73\n","Skipping y_hat=742\n","Skipping y_hat=104\n","Skipping y_hat=284\n","Skipping y_hat=89\n","Skipping y_hat=132\n","Skipping y_hat=80\n","Skipping y_hat=446\n","Skipping y_hat=71\n","Skipping y_hat=155\n","Skipping y_hat=476\n","Skipping y_hat=122\n","Skipping y_hat=321\n","Skipping y_hat=178\n","Skipping y_hat=288\n","Skipping y_hat=80\n","Skipping y_hat=732\n","Skipping y_hat=81\n","Skipping y_hat=774\n","Skipping y_hat=27\n","Skipping y_hat=85\n","Skipping y_hat=775\n","Skipping y_hat=882\n"," 96% 24/25 [00:00<00:00, 31.04it/s]Skipping y_hat=3\n","Skipping y_hat=91\n","Skipping y_hat=71\n","Skipping y_hat=14\n","100% 25/25 [00:00<00:00, 31.70it/s]\n","accuracy of 3000 examples: 2923/3000 (97.43333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.37it/s]\n","accuracy of 3000 examples: 2919/3000 (97.3%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 84% 21/25 [00:00<00:00, 30.45it/s]Skipping y_hat=855\n","Skipping y_hat=648\n","Skipping y_hat=446\n","Skipping y_hat=495\n","Skipping y_hat=302\n","Skipping y_hat=77\n","Skipping y_hat=500\n","Skipping y_hat=146\n","Skipping y_hat=729\n","Skipping y_hat=809\n","Skipping y_hat=976\n","Skipping y_hat=449\n","Skipping y_hat=908\n","Skipping y_hat=882\n","Skipping y_hat=169\n","Skipping y_hat=180\n","Skipping y_hat=286\n","Skipping y_hat=882\n","Skipping y_hat=828\n","Skipping y_hat=831\n","Skipping y_hat=873\n","Skipping y_hat=585\n","Skipping y_hat=946\n","Skipping y_hat=83\n","Skipping y_hat=612\n","Skipping y_hat=747\n","Skipping y_hat=195\n","Skipping y_hat=278\n","Skipping y_hat=126\n","Skipping y_hat=107\n","Skipping y_hat=246\n","Skipping y_hat=340\n","Skipping y_hat=616\n","Skipping y_hat=749\n","Skipping y_hat=862\n","Skipping y_hat=346\n","Skipping y_hat=156\n","Skipping y_hat=942\n","Skipping y_hat=780\n","Skipping y_hat=89\n","Skipping y_hat=88\n","Skipping y_hat=748\n","Skipping y_hat=86\n","Skipping y_hat=440\n","Skipping y_hat=920\n","Skipping y_hat=210\n","Skipping y_hat=193\n","Skipping y_hat=240\n","Skipping y_hat=532\n","Skipping y_hat=103\n","Skipping y_hat=671\n","Skipping y_hat=619\n","Skipping y_hat=712\n","Skipping y_hat=154\n","Skipping y_hat=294\n","Skipping y_hat=29\n","Skipping y_hat=582\n","Skipping y_hat=986\n","Skipping y_hat=51\n","Skipping y_hat=145\n","Skipping y_hat=836\n","Skipping y_hat=132\n","Skipping y_hat=351\n","Skipping y_hat=108\n","Skipping y_hat=110\n","Skipping y_hat=782\n","Skipping y_hat=51\n","Skipping y_hat=294\n","Skipping y_hat=7\n","Skipping y_hat=195\n","Skipping y_hat=902\n","Skipping y_hat=40\n","Skipping y_hat=77\n","Skipping y_hat=48\n","Skipping y_hat=66\n","Skipping y_hat=28\n","100% 25/25 [00:00<00:00, 29.66it/s]\n","accuracy of 3000 examples: 2924/3000 (97.46666666666667%)\n","\n","Test Results:\n","test: 97.47%\n","\n","iter 2800: train loss 1.0405, val loss 1.0391\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 32.05it/s]Skipping y_hat=835\n","Skipping y_hat=288\n","Skipping y_hat=486\n","Skipping y_hat=985\n","Skipping y_hat=372\n","Skipping y_hat=97\n","Skipping y_hat=809\n","Skipping y_hat=799\n","Skipping y_hat=776\n","Skipping y_hat=809\n","Skipping y_hat=948\n","Skipping y_hat=582\n","Skipping y_hat=159\n","Skipping y_hat=180\n","Skipping y_hat=776\n","Skipping y_hat=372\n","Skipping y_hat=608\n","Skipping y_hat=841\n","Skipping y_hat=843\n","Skipping y_hat=885\n","Skipping y_hat=576\n","Skipping y_hat=133\n","Skipping y_hat=652\n","Skipping y_hat=777\n","Skipping y_hat=285\n","Skipping y_hat=728\n","Skipping y_hat=246\n","Skipping y_hat=97\n","Skipping y_hat=216\n","Skipping y_hat=590\n","Skipping y_hat=686\n","Skipping y_hat=250\n","Skipping y_hat=999\n","Skipping y_hat=346\n","Skipping y_hat=106\n","Skipping y_hat=322\n","Skipping y_hat=99\n","Skipping y_hat=98\n","Skipping y_hat=448\n","Skipping y_hat=6\n","Skipping y_hat=750\n","Skipping y_hat=720\n","Skipping y_hat=740\n","Skipping y_hat=113\n","Skipping y_hat=220\n","Skipping y_hat=212\n","Skipping y_hat=13\n","Skipping y_hat=321\n","Skipping y_hat=619\n","Skipping y_hat=772\n","Skipping y_hat=44\n","Skipping y_hat=394\n","Skipping y_hat=182\n","Skipping y_hat=170\n","Skipping y_hat=306\n","Skipping y_hat=21\n","Skipping y_hat=226\n","Skipping y_hat=381\n","Skipping y_hat=128\n","Skipping y_hat=80\n","Skipping y_hat=722\n","Skipping y_hat=31\n","Skipping y_hat=784\n","Skipping y_hat=195\n","Skipping y_hat=485\n","Skipping y_hat=812\n"," 96% 24/25 [00:00<00:00, 31.99it/s]Skipping y_hat=44\n","Skipping y_hat=99\n","Skipping y_hat=77\n","Skipping y_hat=80\n","Skipping y_hat=50\n","100% 25/25 [00:00<00:00, 31.93it/s]\n","accuracy of 3000 examples: 2929/3000 (97.63333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.87it/s]\n","accuracy of 3000 examples: 2927/3000 (97.56666666666666%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 32.52it/s]Skipping y_hat=455\n","Skipping y_hat=426\n","Skipping y_hat=745\n","Skipping y_hat=382\n","Skipping y_hat=87\n","Skipping y_hat=500\n","Skipping y_hat=146\n","Skipping y_hat=209\n","Skipping y_hat=501\n","Skipping y_hat=369\n","Skipping y_hat=786\n","Skipping y_hat=419\n","Skipping y_hat=908\n","Skipping y_hat=822\n","Skipping y_hat=149\n","Skipping y_hat=140\n","Skipping y_hat=876\n","Skipping y_hat=762\n","Skipping y_hat=448\n","Skipping y_hat=831\n","Skipping y_hat=873\n","Skipping y_hat=775\n","Skipping y_hat=176\n","Skipping y_hat=83\n","Skipping y_hat=682\n","Skipping y_hat=727\n","Skipping y_hat=275\n","Skipping y_hat=628\n","Skipping y_hat=136\n","Skipping y_hat=97\n","Skipping y_hat=246\n","Skipping y_hat=340\n","Skipping y_hat=696\n","Skipping y_hat=630\n","Skipping y_hat=259\n","Skipping y_hat=346\n","Skipping y_hat=92\n","Skipping y_hat=116\n","Skipping y_hat=672\n","Skipping y_hat=770\n","Skipping y_hat=179\n","Skipping y_hat=98\n","Skipping y_hat=788\n","Skipping y_hat=46\n","Skipping y_hat=440\n","Skipping y_hat=720\n","Skipping y_hat=540\n","Skipping y_hat=183\n","Skipping y_hat=200\n","Skipping y_hat=712\n","Skipping y_hat=93\n","Skipping y_hat=521\n","Skipping y_hat=669\n","Skipping y_hat=702\n","Skipping y_hat=74\n","Skipping y_hat=294\n","Skipping y_hat=69\n","Skipping y_hat=100\n","Skipping y_hat=176\n","Skipping y_hat=21\n","Skipping y_hat=145\n","Skipping y_hat=186\n","Skipping y_hat=341\n","Skipping y_hat=148\n","Skipping y_hat=248\n","Skipping y_hat=10\n","Skipping y_hat=722\n","Skipping y_hat=21\n","Skipping y_hat=784\n","Skipping y_hat=87\n","Skipping y_hat=115\n","Skipping y_hat=445\n","Skipping y_hat=482\n"," 96% 24/25 [00:00<00:00, 31.64it/s]Skipping y_hat=72\n","Skipping y_hat=74\n","Skipping y_hat=91\n","Skipping y_hat=78\n","100% 25/25 [00:00<00:00, 32.01it/s]\n","accuracy of 3000 examples: 2923/3000 (97.43333333333334%)\n","\n","Test Results:\n","test: 97.43%\n","\n","iter 2900: train loss 1.0394, val loss 1.0412\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 64% 16/25 [00:00<00:00, 30.81it/s]Skipping y_hat=1364\n"," 80% 20/25 [00:00<00:00, 31.34it/s]Skipping y_hat=475\n","Skipping y_hat=568\n","Skipping y_hat=446\n","Skipping y_hat=585\n","Skipping y_hat=392\n","Skipping y_hat=97\n","Skipping y_hat=146\n","Skipping y_hat=559\n","Skipping y_hat=501\n","Skipping y_hat=629\n","Skipping y_hat=276\n","Skipping y_hat=159\n","Skipping y_hat=122\n","Skipping y_hat=199\n","Skipping y_hat=120\n","Skipping y_hat=236\n","Skipping y_hat=722\n","Skipping y_hat=208\n","Skipping y_hat=841\n","Skipping y_hat=863\n","Skipping y_hat=685\n","Skipping y_hat=746\n","Skipping y_hat=193\n","Skipping y_hat=662\n","Skipping y_hat=727\n","Skipping y_hat=775\n","Skipping y_hat=538\n","Skipping y_hat=776\n","Skipping y_hat=87\n","Skipping y_hat=226\n","Skipping y_hat=530\n","Skipping y_hat=586\n","Skipping y_hat=210\n","Skipping y_hat=989\n","Skipping y_hat=882\n","Skipping y_hat=346\n","Skipping y_hat=106\n","Skipping y_hat=362\n","Skipping y_hat=780\n","Skipping y_hat=309\n","Skipping y_hat=108\n","Skipping y_hat=288\n","Skipping y_hat=86\n","Skipping y_hat=260\n","Skipping y_hat=240\n","Skipping y_hat=530\n","Skipping y_hat=173\n","Skipping y_hat=230\n","Skipping y_hat=522\n","Skipping y_hat=193\n","Skipping y_hat=831\n","Skipping y_hat=629\n","Skipping y_hat=712\n","Skipping y_hat=104\n","Skipping y_hat=574\n","Skipping y_hat=972\n","Skipping y_hat=180\n","Skipping y_hat=786\n","Skipping y_hat=11\n","Skipping y_hat=145\n","Skipping y_hat=626\n","Skipping y_hat=182\n","Skipping y_hat=331\n","Skipping y_hat=108\n","Skipping y_hat=80\n","Skipping y_hat=712\n","Skipping y_hat=21\n","Skipping y_hat=774\n","Skipping y_hat=87\n","Skipping y_hat=195\n","Skipping y_hat=745\n","Skipping y_hat=122\n"," 96% 24/25 [00:00<00:00, 30.85it/s]Skipping y_hat=25\n","Skipping y_hat=68\n","Skipping y_hat=48\n","Skipping y_hat=15\n","100% 25/25 [00:00<00:00, 30.89it/s]\n","accuracy of 3000 examples: 2923/3000 (97.43333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.09it/s]\n","accuracy of 3000 examples: 2923/3000 (97.43333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 64% 16/25 [00:00<00:00, 30.45it/s]Skipping y_hat=422\n"," 80% 20/25 [00:00<00:00, 30.78it/s]Skipping y_hat=775\n","Skipping y_hat=118\n","Skipping y_hat=486\n","Skipping y_hat=495\n","Skipping y_hat=322\n","Skipping y_hat=77\n","Skipping y_hat=176\n","Skipping y_hat=989\n","Skipping y_hat=511\n","Skipping y_hat=269\n","Skipping y_hat=476\n","Skipping y_hat=319\n","Skipping y_hat=928\n","Skipping y_hat=712\n","Skipping y_hat=189\n","Skipping y_hat=100\n","Skipping y_hat=976\n","Skipping y_hat=922\n","Skipping y_hat=738\n","Skipping y_hat=821\n","Skipping y_hat=813\n","Skipping y_hat=265\n","Skipping y_hat=666\n","Skipping y_hat=113\n","Skipping y_hat=727\n","Skipping y_hat=515\n","Skipping y_hat=988\n","Skipping y_hat=876\n","Skipping y_hat=107\n","Skipping y_hat=246\n","Skipping y_hat=430\n","Skipping y_hat=776\n","Skipping y_hat=420\n","Skipping y_hat=189\n","Skipping y_hat=872\n","Skipping y_hat=126\n","Skipping y_hat=242\n","Skipping y_hat=710\n","Skipping y_hat=309\n","Skipping y_hat=119\n","Skipping y_hat=198\n","Skipping y_hat=218\n","Skipping y_hat=86\n","Skipping y_hat=230\n","Skipping y_hat=920\n","Skipping y_hat=740\n","Skipping y_hat=123\n","Skipping y_hat=280\n","Skipping y_hat=822\n","Skipping y_hat=103\n","Skipping y_hat=781\n","Skipping y_hat=689\n","Skipping y_hat=752\n","Skipping y_hat=164\n","Skipping y_hat=444\n","Skipping y_hat=129\n","Skipping y_hat=222\n","Skipping y_hat=190\n","Skipping y_hat=466\n","Skipping y_hat=1\n","Skipping y_hat=185\n","Skipping y_hat=876\n","Skipping y_hat=142\n","Skipping y_hat=341\n","Skipping y_hat=268\n","Skipping y_hat=20\n","Skipping y_hat=732\n","Skipping y_hat=854\n","Skipping y_hat=87\n","Skipping y_hat=195\n","Skipping y_hat=345\n","Skipping y_hat=582\n"," 96% 24/25 [00:00<00:00, 28.80it/s]Skipping y_hat=17\n","Skipping y_hat=8\n","Skipping y_hat=20\n","Skipping y_hat=37\n","Skipping y_hat=76\n","100% 25/25 [00:00<00:00, 29.83it/s]\n","accuracy of 3000 examples: 2922/3000 (97.39999999999999%)\n","\n","Test Results:\n","test: 97.40%\n","\n","iter 3000: train loss 1.0238, val loss 1.0423\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 64% 16/25 [00:00<00:00, 31.91it/s]Skipping y_hat=819\n"," 80% 20/25 [00:00<00:00, 32.39it/s]Skipping y_hat=745\n","Skipping y_hat=268\n","Skipping y_hat=446\n","Skipping y_hat=875\n","Skipping y_hat=302\n","Skipping y_hat=107\n","Skipping y_hat=590\n","Skipping y_hat=176\n","Skipping y_hat=199\n","Skipping y_hat=591\n","Skipping y_hat=419\n","Skipping y_hat=376\n","Skipping y_hat=629\n","Skipping y_hat=968\n","Skipping y_hat=782\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=976\n","Skipping y_hat=882\n","Skipping y_hat=859\n","Skipping y_hat=893\n","Skipping y_hat=895\n","Skipping y_hat=656\n","Skipping y_hat=103\n","Skipping y_hat=602\n","Skipping y_hat=777\n","Skipping y_hat=795\n","Skipping y_hat=308\n","Skipping y_hat=246\n","Skipping y_hat=197\n","Skipping y_hat=226\n","Skipping y_hat=310\n","Skipping y_hat=716\n","Skipping y_hat=310\n","Skipping y_hat=209\n","Skipping y_hat=346\n","Skipping y_hat=82\n","Skipping y_hat=126\n","Skipping y_hat=932\n","Skipping y_hat=750\n","Skipping y_hat=269\n","Skipping y_hat=109\n","Skipping y_hat=198\n","Skipping y_hat=708\n","Skipping y_hat=86\n","Skipping y_hat=460\n","Skipping y_hat=550\n","Skipping y_hat=520\n","Skipping y_hat=183\n","Skipping y_hat=240\n","Skipping y_hat=682\n","Skipping y_hat=103\n","Skipping y_hat=431\n","Skipping y_hat=669\n","Skipping y_hat=742\n","Skipping y_hat=194\n","Skipping y_hat=594\n","Skipping y_hat=89\n","Skipping y_hat=422\n","Skipping y_hat=100\n","Skipping y_hat=606\n","Skipping y_hat=71\n","Skipping y_hat=145\n","Skipping y_hat=516\n","Skipping y_hat=331\n","Skipping y_hat=88\n","Skipping y_hat=218\n","Skipping y_hat=20\n","Skipping y_hat=712\n","Skipping y_hat=31\n","Skipping y_hat=794\n","Skipping y_hat=57\n","Skipping y_hat=185\n","Skipping y_hat=835\n","Skipping y_hat=682\n"," 96% 24/25 [00:00<00:00, 31.90it/s]Skipping y_hat=72\n","Skipping y_hat=95\n","Skipping y_hat=44\n","Skipping y_hat=78\n","Skipping y_hat=15\n","100% 25/25 [00:00<00:00, 32.14it/s]\n","accuracy of 3000 examples: 2919/3000 (97.3%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.34it/s]\n","accuracy of 3000 examples: 2932/3000 (97.73333333333333%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n","  0% 0/25 [00:00<?, ?it/s]Skipping y_hat=1221\n"," 80% 20/25 [00:00<00:00, 30.95it/s]Skipping y_hat=255\n","Skipping y_hat=416\n","Skipping y_hat=875\n","Skipping y_hat=392\n","Skipping y_hat=197\n","Skipping y_hat=500\n","Skipping y_hat=146\n","Skipping y_hat=499\n","Skipping y_hat=591\n","Skipping y_hat=109\n","Skipping y_hat=266\n","Skipping y_hat=709\n","Skipping y_hat=988\n","Skipping y_hat=892\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=776\n","Skipping y_hat=482\n","Skipping y_hat=938\n","Skipping y_hat=813\n","Skipping y_hat=575\n","Skipping y_hat=626\n","Skipping y_hat=103\n","Skipping y_hat=757\n","Skipping y_hat=615\n","Skipping y_hat=478\n","Skipping y_hat=446\n","Skipping y_hat=197\n","Skipping y_hat=246\n","Skipping y_hat=570\n","Skipping y_hat=696\n","Skipping y_hat=820\n","Skipping y_hat=489\n","Skipping y_hat=336\n","Skipping y_hat=146\n","Skipping y_hat=582\n","Skipping y_hat=710\n","Skipping y_hat=109\n","Skipping y_hat=108\n","Skipping y_hat=768\n","Skipping y_hat=96\n","Skipping y_hat=740\n","Skipping y_hat=800\n","Skipping y_hat=330\n","Skipping y_hat=183\n","Skipping y_hat=220\n","Skipping y_hat=782\n","Skipping y_hat=103\n","Skipping y_hat=431\n","Skipping y_hat=689\n","Skipping y_hat=712\n","Skipping y_hat=104\n","Skipping y_hat=754\n","Skipping y_hat=79\n","Skipping y_hat=752\n","Skipping y_hat=90\n","Skipping y_hat=176\n","Skipping y_hat=71\n","Skipping y_hat=155\n","Skipping y_hat=666\n","Skipping y_hat=182\n","Skipping y_hat=341\n","Skipping y_hat=128\n","Skipping y_hat=20\n","Skipping y_hat=712\n","Skipping y_hat=51\n","Skipping y_hat=594\n","Skipping y_hat=87\n","Skipping y_hat=195\n","Skipping y_hat=455\n","Skipping y_hat=482\n"," 96% 24/25 [00:00<00:00, 29.88it/s]Skipping y_hat=16\n","Skipping y_hat=1\n","100% 25/25 [00:00<00:00, 30.51it/s]\n","accuracy of 3000 examples: 2924/3000 (97.46666666666667%)\n","\n","Test Results:\n","test: 97.47%\n","\n","iter 3100: train loss 1.0272, val loss 1.0431\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 92% 23/25 [00:00<00:00, 29.60it/s]Skipping y_hat=685\n","Skipping y_hat=858\n","Skipping y_hat=446\n","Skipping y_hat=675\n","Skipping y_hat=322\n","Skipping y_hat=97\n","Skipping y_hat=590\n","Skipping y_hat=126\n","Skipping y_hat=909\n","Skipping y_hat=559\n","Skipping y_hat=266\n","Skipping y_hat=749\n","Skipping y_hat=998\n","Skipping y_hat=682\n","Skipping y_hat=109\n","Skipping y_hat=140\n","Skipping y_hat=806\n","Skipping y_hat=672\n","Skipping y_hat=468\n","Skipping y_hat=813\n","Skipping y_hat=495\n","Skipping y_hat=676\n","Skipping y_hat=747\n","Skipping y_hat=775\n","Skipping y_hat=528\n","Skipping y_hat=476\n","Skipping y_hat=97\n","Skipping y_hat=740\n","Skipping y_hat=776\n","Skipping y_hat=430\n","Skipping y_hat=759\n","Skipping y_hat=872\n","Skipping y_hat=156\n","Skipping y_hat=382\n","Skipping y_hat=770\n","Skipping y_hat=109\n","Skipping y_hat=88\n","Skipping y_hat=828\n","Skipping y_hat=96\n","Skipping y_hat=370\n","Skipping y_hat=700\n","Skipping y_hat=460\n","Skipping y_hat=183\n","Skipping y_hat=310\n","Skipping y_hat=382\n","Skipping y_hat=103\n","Skipping y_hat=211\n","Skipping y_hat=669\n","Skipping y_hat=742\n","Skipping y_hat=94\n","Skipping y_hat=954\n","Skipping y_hat=29\n","Skipping y_hat=722\n","Skipping y_hat=180\n","Skipping y_hat=306\n","Skipping y_hat=21\n","Skipping y_hat=115\n","Skipping y_hat=256\n","Skipping y_hat=321\n","Skipping y_hat=178\n","Skipping y_hat=50\n","Skipping y_hat=772\n","Skipping y_hat=111\n","Skipping y_hat=694\n","Skipping y_hat=17\n","Skipping y_hat=95\n","Skipping y_hat=415\n","Skipping y_hat=892\n","Skipping y_hat=67\n","Skipping y_hat=99\n","Skipping y_hat=44\n","Skipping y_hat=46\n","Skipping y_hat=82\n","100% 25/25 [00:00<00:00, 29.68it/s]\n","accuracy of 3000 examples: 2927/3000 (97.56666666666666%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 35.95it/s]\n","accuracy of 3000 examples: 2936/3000 (97.86666666666667%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.29it/s]Skipping y_hat=585\n","Skipping y_hat=278\n","Skipping y_hat=446\n","Skipping y_hat=495\n","Skipping y_hat=322\n","Skipping y_hat=107\n","Skipping y_hat=590\n","Skipping y_hat=709\n","Skipping y_hat=561\n","Skipping y_hat=799\n","Skipping y_hat=776\n","Skipping y_hat=709\n","Skipping y_hat=968\n","Skipping y_hat=792\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=666\n","Skipping y_hat=762\n","Skipping y_hat=348\n","Skipping y_hat=843\n","Skipping y_hat=195\n","Skipping y_hat=446\n","Skipping y_hat=173\n","Skipping y_hat=747\n","Skipping y_hat=985\n","Skipping y_hat=218\n","Skipping y_hat=856\n","Skipping y_hat=17\n","Skipping y_hat=510\n","Skipping y_hat=986\n","Skipping y_hat=460\n","Skipping y_hat=809\n","Skipping y_hat=872\n","Skipping y_hat=82\n","Skipping y_hat=146\n","Skipping y_hat=772\n","Skipping y_hat=790\n","Skipping y_hat=39\n","Skipping y_hat=88\n","Skipping y_hat=278\n","Skipping y_hat=86\n","Skipping y_hat=930\n","Skipping y_hat=250\n","Skipping y_hat=530\n","Skipping y_hat=173\n","Skipping y_hat=210\n","Skipping y_hat=722\n","Skipping y_hat=103\n","Skipping y_hat=631\n","Skipping y_hat=732\n","Skipping y_hat=164\n","Skipping y_hat=194\n","Skipping y_hat=69\n","Skipping y_hat=742\n","Skipping y_hat=180\n","Skipping y_hat=726\n","Skipping y_hat=21\n","Skipping y_hat=145\n","Skipping y_hat=456\n","Skipping y_hat=98\n","Skipping y_hat=80\n","Skipping y_hat=722\n","Skipping y_hat=71\n","Skipping y_hat=594\n","Skipping y_hat=7\n","Skipping y_hat=845\n","Skipping y_hat=472\n"," 96% 24/25 [00:00<00:00, 30.95it/s]Skipping y_hat=70\n","Skipping y_hat=88\n","Skipping y_hat=25\n","Skipping y_hat=40\n","Skipping y_hat=87\n","100% 25/25 [00:00<00:00, 31.13it/s]\n","accuracy of 3000 examples: 2928/3000 (97.6%)\n","\n","Test Results:\n","test: 97.60%\n","\n","iter 3200: train loss 1.0290, val loss 1.0459\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.14it/s]Skipping y_hat=475\n","Skipping y_hat=848\n","Skipping y_hat=446\n","Skipping y_hat=875\n","Skipping y_hat=322\n","Skipping y_hat=187\n","Skipping y_hat=590\n","Skipping y_hat=146\n","Skipping y_hat=359\n","Skipping y_hat=769\n","Skipping y_hat=276\n","Skipping y_hat=409\n","Skipping y_hat=968\n","Skipping y_hat=782\n","Skipping y_hat=149\n","Skipping y_hat=180\n","Skipping y_hat=876\n","Skipping y_hat=772\n","Skipping y_hat=238\n","Skipping y_hat=195\n","Skipping y_hat=476\n","Skipping y_hat=103\n","Skipping y_hat=602\n","Skipping y_hat=775\n","Skipping y_hat=448\n","Skipping y_hat=436\n","Skipping y_hat=77\n","Skipping y_hat=246\n","Skipping y_hat=830\n","Skipping y_hat=506\n","Skipping y_hat=810\n","Skipping y_hat=659\n","Skipping y_hat=346\n","Skipping y_hat=126\n","Skipping y_hat=282\n","Skipping y_hat=770\n","Skipping y_hat=309\n","Skipping y_hat=119\n","Skipping y_hat=198\n","Skipping y_hat=678\n","Skipping y_hat=86\n","Skipping y_hat=230\n","Skipping y_hat=440\n","Skipping y_hat=450\n","Skipping y_hat=173\n","Skipping y_hat=322\n","Skipping y_hat=3\n","Skipping y_hat=411\n","Skipping y_hat=669\n","Skipping y_hat=84\n","Skipping y_hat=464\n","Skipping y_hat=29\n","Skipping y_hat=662\n","Skipping y_hat=180\n","Skipping y_hat=136\n","Skipping y_hat=21\n","Skipping y_hat=135\n","Skipping y_hat=746\n","Skipping y_hat=152\n","Skipping y_hat=128\n","Skipping y_hat=40\n","Skipping y_hat=722\n","Skipping y_hat=484\n","Skipping y_hat=27\n","Skipping y_hat=445\n","Skipping y_hat=182\n"," 96% 24/25 [00:00<00:00, 30.70it/s]Skipping y_hat=47\n","Skipping y_hat=66\n","Skipping y_hat=79\n","100% 25/25 [00:00<00:00, 31.21it/s]\n","accuracy of 3000 examples: 2929/3000 (97.63333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.45it/s]\n","accuracy of 3000 examples: 2938/3000 (97.93333333333332%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.65it/s]Skipping y_hat=675\n","Skipping y_hat=448\n","Skipping y_hat=446\n","Skipping y_hat=295\n","Skipping y_hat=342\n","Skipping y_hat=187\n","Skipping y_hat=560\n","Skipping y_hat=639\n","Skipping y_hat=409\n","Skipping y_hat=276\n","Skipping y_hat=889\n","Skipping y_hat=968\n","Skipping y_hat=162\n","Skipping y_hat=169\n","Skipping y_hat=150\n","Skipping y_hat=756\n","Skipping y_hat=432\n","Skipping y_hat=128\n","Skipping y_hat=875\n","Skipping y_hat=736\n","Skipping y_hat=153\n","Skipping y_hat=662\n","Skipping y_hat=747\n","Skipping y_hat=675\n","Skipping y_hat=448\n","Skipping y_hat=776\n","Skipping y_hat=197\n","Skipping y_hat=226\n","Skipping y_hat=630\n","Skipping y_hat=686\n","Skipping y_hat=430\n","Skipping y_hat=409\n","Skipping y_hat=336\n","Skipping y_hat=126\n","Skipping y_hat=122\n","Skipping y_hat=740\n","Skipping y_hat=289\n","Skipping y_hat=169\n","Skipping y_hat=108\n","Skipping y_hat=778\n","Skipping y_hat=86\n","Skipping y_hat=430\n","Skipping y_hat=430\n","Skipping y_hat=183\n","Skipping y_hat=260\n","Skipping y_hat=522\n","Skipping y_hat=3\n","Skipping y_hat=611\n","Skipping y_hat=669\n","Skipping y_hat=712\n","Skipping y_hat=114\n","Skipping y_hat=454\n","Skipping y_hat=69\n","Skipping y_hat=752\n","Skipping y_hat=100\n","Skipping y_hat=866\n","Skipping y_hat=41\n","Skipping y_hat=766\n","Skipping y_hat=162\n","Skipping y_hat=128\n","Skipping y_hat=20\n","Skipping y_hat=742\n","Skipping y_hat=774\n","Skipping y_hat=155\n","Skipping y_hat=315\n","Skipping y_hat=752\n"," 96% 24/25 [00:00<00:00, 31.15it/s]Skipping y_hat=73\n","Skipping y_hat=19\n","Skipping y_hat=86\n","Skipping y_hat=44\n","Skipping y_hat=82\n","100% 25/25 [00:00<00:00, 31.53it/s]\n","accuracy of 3000 examples: 2929/3000 (97.63333333333334%)\n","\n","Test Results:\n","test: 97.63%\n","\n","iter 3300: train loss 1.0339, val loss 1.0485\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 32.32it/s]Skipping y_hat=195\n","Skipping y_hat=608\n","Skipping y_hat=426\n","Skipping y_hat=575\n","Skipping y_hat=342\n","Skipping y_hat=107\n","Skipping y_hat=590\n","Skipping y_hat=126\n","Skipping y_hat=789\n","Skipping y_hat=889\n","Skipping y_hat=426\n","Skipping y_hat=709\n","Skipping y_hat=988\n","Skipping y_hat=762\n","Skipping y_hat=129\n","Skipping y_hat=140\n","Skipping y_hat=146\n","Skipping y_hat=442\n","Skipping y_hat=528\n","Skipping y_hat=813\n","Skipping y_hat=875\n","Skipping y_hat=416\n","Skipping y_hat=103\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=555\n","Skipping y_hat=858\n","Skipping y_hat=456\n","Skipping y_hat=197\n","Skipping y_hat=226\n","Skipping y_hat=696\n","Skipping y_hat=230\n","Skipping y_hat=959\n","Skipping y_hat=862\n","Skipping y_hat=192\n","Skipping y_hat=146\n","Skipping y_hat=832\n","Skipping y_hat=750\n","Skipping y_hat=169\n","Skipping y_hat=198\n","Skipping y_hat=788\n","Skipping y_hat=410\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=113\n","Skipping y_hat=240\n","Skipping y_hat=422\n","Skipping y_hat=103\n","Skipping y_hat=511\n","Skipping y_hat=669\n","Skipping y_hat=742\n","Skipping y_hat=174\n","Skipping y_hat=394\n","Skipping y_hat=9\n","Skipping y_hat=222\n","Skipping y_hat=100\n","Skipping y_hat=506\n","Skipping y_hat=1\n","Skipping y_hat=386\n","Skipping y_hat=331\n","Skipping y_hat=148\n","Skipping y_hat=10\n","Skipping y_hat=782\n","Skipping y_hat=794\n","Skipping y_hat=7\n","Skipping y_hat=195\n","Skipping y_hat=345\n","Skipping y_hat=722\n"," 96% 24/25 [00:00<00:00, 31.77it/s]Skipping y_hat=54\n","Skipping y_hat=71\n","Skipping y_hat=58\n","Skipping y_hat=35\n","100% 25/25 [00:00<00:00, 32.09it/s]\n","accuracy of 3000 examples: 2927/3000 (97.56666666666666%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.55it/s]\n","accuracy of 3000 examples: 2940/3000 (98.0%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 32.22it/s]Skipping y_hat=725\n","Skipping y_hat=548\n","Skipping y_hat=446\n","Skipping y_hat=125\n","Skipping y_hat=322\n","Skipping y_hat=107\n","Skipping y_hat=520\n","Skipping y_hat=146\n","Skipping y_hat=899\n","Skipping y_hat=501\n","Skipping y_hat=809\n","Skipping y_hat=476\n","Skipping y_hat=889\n","Skipping y_hat=682\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=246\n","Skipping y_hat=922\n","Skipping y_hat=398\n","Skipping y_hat=813\n","Skipping y_hat=405\n","Skipping y_hat=746\n","Skipping y_hat=103\n","Skipping y_hat=747\n","Skipping y_hat=155\n","Skipping y_hat=888\n","Skipping y_hat=966\n","Skipping y_hat=107\n","Skipping y_hat=210\n","Skipping y_hat=896\n","Skipping y_hat=340\n","Skipping y_hat=259\n","Skipping y_hat=822\n","Skipping y_hat=346\n","Skipping y_hat=92\n","Skipping y_hat=106\n","Skipping y_hat=992\n","Skipping y_hat=740\n","Skipping y_hat=289\n","Skipping y_hat=109\n","Skipping y_hat=108\n","Skipping y_hat=378\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=123\n","Skipping y_hat=280\n","Skipping y_hat=322\n","Skipping y_hat=103\n","Skipping y_hat=711\n","Skipping y_hat=174\n","Skipping y_hat=374\n","Skipping y_hat=29\n","Skipping y_hat=782\n","Skipping y_hat=170\n","Skipping y_hat=586\n","Skipping y_hat=41\n","Skipping y_hat=606\n","Skipping y_hat=162\n","Skipping y_hat=138\n","Skipping y_hat=20\n","Skipping y_hat=722\n","Skipping y_hat=81\n","Skipping y_hat=784\n","Skipping y_hat=87\n","Skipping y_hat=95\n","Skipping y_hat=285\n","Skipping y_hat=442\n"," 96% 24/25 [00:00<00:00, 31.62it/s]Skipping y_hat=15\n","Skipping y_hat=54\n","Skipping y_hat=44\n","Skipping y_hat=71\n","100% 25/25 [00:00<00:00, 31.61it/s]\n","accuracy of 3000 examples: 2928/3000 (97.6%)\n","\n","Test Results:\n","test: 97.60%\n","\n","iter 3400: train loss 1.0245, val loss 1.0479\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.67it/s]Skipping y_hat=295\n","Skipping y_hat=808\n","Skipping y_hat=446\n","Skipping y_hat=475\n","Skipping y_hat=322\n","Skipping y_hat=107\n","Skipping y_hat=590\n","Skipping y_hat=146\n","Skipping y_hat=939\n","Skipping y_hat=409\n","Skipping y_hat=876\n","Skipping y_hat=909\n","Skipping y_hat=968\n","Skipping y_hat=582\n","Skipping y_hat=109\n","Skipping y_hat=120\n","Skipping y_hat=176\n","Skipping y_hat=722\n","Skipping y_hat=688\n","Skipping y_hat=881\n","Skipping y_hat=575\n","Skipping y_hat=746\n","Skipping y_hat=103\n","Skipping y_hat=747\n","Skipping y_hat=775\n","Skipping y_hat=818\n","Skipping y_hat=346\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=730\n","Skipping y_hat=586\n","Skipping y_hat=530\n","Skipping y_hat=909\n","Skipping y_hat=882\n","Skipping y_hat=346\n","Skipping y_hat=112\n","Skipping y_hat=126\n","Skipping y_hat=682\n","Skipping y_hat=770\n","Skipping y_hat=289\n","Skipping y_hat=49\n","Skipping y_hat=98\n","Skipping y_hat=748\n","Skipping y_hat=86\n","Skipping y_hat=340\n","Skipping y_hat=400\n","Skipping y_hat=430\n","Skipping y_hat=123\n","Skipping y_hat=280\n","Skipping y_hat=122\n","Skipping y_hat=3\n","Skipping y_hat=241\n","Skipping y_hat=669\n","Skipping y_hat=712\n","Skipping y_hat=54\n","Skipping y_hat=394\n","Skipping y_hat=722\n","Skipping y_hat=150\n","Skipping y_hat=366\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=846\n","Skipping y_hat=128\n","Skipping y_hat=218\n","Skipping y_hat=10\n","Skipping y_hat=772\n","Skipping y_hat=994\n","Skipping y_hat=27\n","Skipping y_hat=185\n","Skipping y_hat=925\n","Skipping y_hat=882\n"," 96% 24/25 [00:00<00:00, 31.00it/s]Skipping y_hat=77\n","Skipping y_hat=11\n","Skipping y_hat=55\n","100% 25/25 [00:00<00:00, 31.51it/s]\n","accuracy of 3000 examples: 2925/3000 (97.5%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 35.79it/s]\n","accuracy of 3000 examples: 2947/3000 (98.23333333333333%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 30.73it/s]Skipping y_hat=295\n","Skipping y_hat=478\n","Skipping y_hat=446\n","Skipping y_hat=785\n","Skipping y_hat=342\n","Skipping y_hat=77\n","Skipping y_hat=590\n","Skipping y_hat=799\n","Skipping y_hat=269\n","Skipping y_hat=206\n","Skipping y_hat=609\n","Skipping y_hat=988\n","Skipping y_hat=882\n","Skipping y_hat=169\n","Skipping y_hat=100\n","Skipping y_hat=476\n","Skipping y_hat=782\n","Skipping y_hat=348\n","Skipping y_hat=813\n","Skipping y_hat=575\n","Skipping y_hat=103\n","Skipping y_hat=747\n","Skipping y_hat=375\n","Skipping y_hat=928\n","Skipping y_hat=976\n","Skipping y_hat=67\n","Skipping y_hat=296\n","Skipping y_hat=796\n","Skipping y_hat=310\n","Skipping y_hat=109\n","Skipping y_hat=822\n","Skipping y_hat=346\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=282\n","Skipping y_hat=750\n","Skipping y_hat=89\n","Skipping y_hat=108\n","Skipping y_hat=788\n","Skipping y_hat=86\n","Skipping y_hat=230\n","Skipping y_hat=230\n","Skipping y_hat=220\n","Skipping y_hat=123\n","Skipping y_hat=722\n","Skipping y_hat=3\n","Skipping y_hat=331\n","Skipping y_hat=669\n","Skipping y_hat=84\n","Skipping y_hat=444\n","Skipping y_hat=99\n","Skipping y_hat=522\n","Skipping y_hat=100\n","Skipping y_hat=676\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=556\n","Skipping y_hat=148\n","Skipping y_hat=278\n","Skipping y_hat=20\n","Skipping y_hat=742\n","Skipping y_hat=584\n","Skipping y_hat=745\n","Skipping y_hat=582\n"," 96% 24/25 [00:00<00:00, 30.15it/s]Skipping y_hat=97\n","Skipping y_hat=83\n","Skipping y_hat=46\n","Skipping y_hat=85\n","Skipping y_hat=26\n","100% 25/25 [00:00<00:00, 30.79it/s]\n","accuracy of 3000 examples: 2931/3000 (97.7%)\n","\n","Test Results:\n","test: 97.70%\n","\n","iter 3500: train loss 1.0169, val loss 1.0480\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 30.69it/s]Skipping y_hat=795\n","Skipping y_hat=468\n","Skipping y_hat=446\n","Skipping y_hat=775\n","Skipping y_hat=342\n","Skipping y_hat=187\n","Skipping y_hat=590\n","Skipping y_hat=789\n","Skipping y_hat=591\n","Skipping y_hat=799\n","Skipping y_hat=276\n","Skipping y_hat=899\n","Skipping y_hat=988\n","Skipping y_hat=782\n","Skipping y_hat=99\n","Skipping y_hat=140\n","Skipping y_hat=276\n","Skipping y_hat=872\n","Skipping y_hat=228\n","Skipping y_hat=891\n","Skipping y_hat=813\n","Skipping y_hat=785\n","Skipping y_hat=776\n","Skipping y_hat=73\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=545\n","Skipping y_hat=888\n","Skipping y_hat=446\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=660\n","Skipping y_hat=786\n","Skipping y_hat=430\n","Skipping y_hat=409\n","Skipping y_hat=92\n","Skipping y_hat=146\n","Skipping y_hat=542\n","Skipping y_hat=129\n","Skipping y_hat=188\n","Skipping y_hat=548\n","Skipping y_hat=86\n","Skipping y_hat=430\n","Skipping y_hat=440\n","Skipping y_hat=330\n","Skipping y_hat=113\n","Skipping y_hat=290\n","Skipping y_hat=482\n","Skipping y_hat=73\n","Skipping y_hat=311\n","Skipping y_hat=689\n","Skipping y_hat=742\n","Skipping y_hat=154\n","Skipping y_hat=874\n","Skipping y_hat=89\n","Skipping y_hat=772\n","Skipping y_hat=180\n","Skipping y_hat=476\n","Skipping y_hat=81\n","Skipping y_hat=476\n","Skipping y_hat=182\n","Skipping y_hat=391\n","Skipping y_hat=88\n","Skipping y_hat=218\n","Skipping y_hat=20\n","Skipping y_hat=772\n","Skipping y_hat=21\n","Skipping y_hat=694\n","Skipping y_hat=67\n","Skipping y_hat=185\n","Skipping y_hat=545\n","Skipping y_hat=272\n"," 96% 24/25 [00:00<00:00, 30.72it/s]Skipping y_hat=72\n","Skipping y_hat=59\n","Skipping y_hat=10\n","Skipping y_hat=73\n","Skipping y_hat=52\n","100% 25/25 [00:00<00:00, 30.97it/s]\n","accuracy of 3000 examples: 2923/3000 (97.43333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 35.21it/s]\n","accuracy of 3000 examples: 2937/3000 (97.89999999999999%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.23it/s]Skipping y_hat=755\n","Skipping y_hat=618\n","Skipping y_hat=446\n","Skipping y_hat=495\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=789\n","Skipping y_hat=969\n","Skipping y_hat=746\n","Skipping y_hat=889\n","Skipping y_hat=988\n","Skipping y_hat=782\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=776\n","Skipping y_hat=552\n","Skipping y_hat=428\n","Skipping y_hat=813\n","Skipping y_hat=885\n","Skipping y_hat=756\n","Skipping y_hat=73\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=175\n","Skipping y_hat=148\n","Skipping y_hat=446\n","Skipping y_hat=197\n","Skipping y_hat=296\n","Skipping y_hat=360\n","Skipping y_hat=886\n","Skipping y_hat=480\n","Skipping y_hat=449\n","Skipping y_hat=346\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=482\n","Skipping y_hat=770\n","Skipping y_hat=69\n","Skipping y_hat=198\n","Skipping y_hat=588\n","Skipping y_hat=96\n","Skipping y_hat=330\n","Skipping y_hat=650\n","Skipping y_hat=440\n","Skipping y_hat=123\n","Skipping y_hat=280\n","Skipping y_hat=282\n","Skipping y_hat=3\n","Skipping y_hat=411\n","Skipping y_hat=689\n","Skipping y_hat=894\n","Skipping y_hat=9\n","Skipping y_hat=692\n","Skipping y_hat=180\n","Skipping y_hat=516\n","Skipping y_hat=21\n","Skipping y_hat=976\n","Skipping y_hat=162\n","Skipping y_hat=301\n","Skipping y_hat=128\n","Skipping y_hat=20\n","Skipping y_hat=752\n","Skipping y_hat=474\n","Skipping y_hat=67\n","Skipping y_hat=185\n","Skipping y_hat=755\n","Skipping y_hat=282\n"," 96% 24/25 [00:00<00:00, 30.77it/s]Skipping y_hat=86\n","Skipping y_hat=12\n","Skipping y_hat=50\n","Skipping y_hat=36\n","100% 25/25 [00:00<00:00, 31.03it/s]\n","accuracy of 3000 examples: 2927/3000 (97.56666666666666%)\n","\n","Test Results:\n","test: 97.57%\n","\n","iter 3600: train loss 1.0149, val loss 1.0499\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 30.29it/s]Skipping y_hat=855\n","Skipping y_hat=218\n","Skipping y_hat=446\n","Skipping y_hat=565\n","Skipping y_hat=342\n","Skipping y_hat=77\n","Skipping y_hat=590\n","Skipping y_hat=789\n","Skipping y_hat=209\n","Skipping y_hat=346\n","Skipping y_hat=709\n","Skipping y_hat=792\n","Skipping y_hat=109\n","Skipping y_hat=120\n","Skipping y_hat=226\n","Skipping y_hat=582\n","Skipping y_hat=408\n","Skipping y_hat=385\n","Skipping y_hat=476\n","Skipping y_hat=73\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=675\n","Skipping y_hat=908\n","Skipping y_hat=246\n","Skipping y_hat=197\n","Skipping y_hat=296\n","Skipping y_hat=270\n","Skipping y_hat=996\n","Skipping y_hat=510\n","Skipping y_hat=929\n","Skipping y_hat=872\n","Skipping y_hat=346\n","Skipping y_hat=146\n","Skipping y_hat=882\n","Skipping y_hat=770\n","Skipping y_hat=289\n","Skipping y_hat=89\n","Skipping y_hat=108\n","Skipping y_hat=188\n","Skipping y_hat=106\n","Skipping y_hat=810\n","Skipping y_hat=420\n","Skipping y_hat=310\n","Skipping y_hat=113\n","Skipping y_hat=250\n","Skipping y_hat=772\n","Skipping y_hat=3\n","Skipping y_hat=831\n","Skipping y_hat=669\n","Skipping y_hat=712\n","Skipping y_hat=144\n","Skipping y_hat=934\n","Skipping y_hat=79\n","Skipping y_hat=622\n","Skipping y_hat=100\n","Skipping y_hat=566\n","Skipping y_hat=21\n","Skipping y_hat=145\n","Skipping y_hat=416\n","Skipping y_hat=128\n","Skipping y_hat=248\n","Skipping y_hat=20\n","Skipping y_hat=21\n","Skipping y_hat=784\n","Skipping y_hat=27\n","Skipping y_hat=115\n","Skipping y_hat=415\n","Skipping y_hat=672\n"," 96% 24/25 [00:00<00:00, 29.20it/s]Skipping y_hat=58\n","Skipping y_hat=22\n","Skipping y_hat=56\n","Skipping y_hat=62\n","100% 25/25 [00:00<00:00, 30.31it/s]\n","accuracy of 3000 examples: 2926/3000 (97.53333333333333%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 35.94it/s]\n","accuracy of 3000 examples: 2942/3000 (98.06666666666666%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.24it/s]Skipping y_hat=775\n","Skipping y_hat=518\n","Skipping y_hat=446\n","Skipping y_hat=875\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=176\n","Skipping y_hat=879\n","Skipping y_hat=609\n","Skipping y_hat=476\n","Skipping y_hat=789\n","Skipping y_hat=988\n","Skipping y_hat=892\n","Skipping y_hat=109\n","Skipping y_hat=120\n","Skipping y_hat=776\n","Skipping y_hat=792\n","Skipping y_hat=428\n","Skipping y_hat=813\n","Skipping y_hat=795\n","Skipping y_hat=726\n","Skipping y_hat=103\n","Skipping y_hat=747\n","Skipping y_hat=175\n","Skipping y_hat=988\n","Skipping y_hat=216\n","Skipping y_hat=197\n","Skipping y_hat=510\n","Skipping y_hat=386\n","Skipping y_hat=330\n","Skipping y_hat=989\n","Skipping y_hat=872\n","Skipping y_hat=346\n","Skipping y_hat=72\n","Skipping y_hat=106\n","Skipping y_hat=582\n","Skipping y_hat=169\n","Skipping y_hat=198\n","Skipping y_hat=708\n","Skipping y_hat=86\n","Skipping y_hat=530\n","Skipping y_hat=860\n","Skipping y_hat=320\n","Skipping y_hat=113\n","Skipping y_hat=272\n","Skipping y_hat=3\n","Skipping y_hat=311\n","Skipping y_hat=699\n","Skipping y_hat=164\n","Skipping y_hat=454\n","Skipping y_hat=19\n","Skipping y_hat=892\n","Skipping y_hat=90\n","Skipping y_hat=376\n","Skipping y_hat=41\n","Skipping y_hat=306\n","Skipping y_hat=128\n","Skipping y_hat=258\n","Skipping y_hat=20\n","Skipping y_hat=21\n","Skipping y_hat=984\n","Skipping y_hat=67\n","Skipping y_hat=475\n","Skipping y_hat=772\n"," 96% 24/25 [00:00<00:00, 31.34it/s]Skipping y_hat=52\n","Skipping y_hat=49\n","Skipping y_hat=75\n","Skipping y_hat=77\n","100% 25/25 [00:00<00:00, 31.56it/s]\n","accuracy of 3000 examples: 2930/3000 (97.66666666666667%)\n","\n","Test Results:\n","test: 97.67%\n","\n","iter 3700: train loss 1.0145, val loss 1.0547\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 30.46it/s]Skipping y_hat=495\n","Skipping y_hat=118\n","Skipping y_hat=446\n","Skipping y_hat=875\n","Skipping y_hat=342\n","Skipping y_hat=97\n","Skipping y_hat=590\n","Skipping y_hat=889\n","Skipping y_hat=561\n","Skipping y_hat=699\n","Skipping y_hat=276\n","Skipping y_hat=429\n","Skipping y_hat=968\n","Skipping y_hat=782\n","Skipping y_hat=109\n","Skipping y_hat=140\n","Skipping y_hat=746\n","Skipping y_hat=672\n","Skipping y_hat=678\n","Skipping y_hat=813\n","Skipping y_hat=775\n","Skipping y_hat=956\n","Skipping y_hat=173\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=575\n","Skipping y_hat=448\n","Skipping y_hat=446\n","Skipping y_hat=97\n","Skipping y_hat=226\n","Skipping y_hat=630\n","Skipping y_hat=896\n","Skipping y_hat=310\n","Skipping y_hat=979\n","Skipping y_hat=822\n","Skipping y_hat=106\n","Skipping y_hat=462\n","Skipping y_hat=289\n","Skipping y_hat=89\n","Skipping y_hat=78\n","Skipping y_hat=878\n","Skipping y_hat=116\n","Skipping y_hat=430\n","Skipping y_hat=460\n","Skipping y_hat=113\n","Skipping y_hat=280\n","Skipping y_hat=482\n","Skipping y_hat=103\n","Skipping y_hat=411\n","Skipping y_hat=712\n","Skipping y_hat=174\n","Skipping y_hat=954\n","Skipping y_hat=19\n","Skipping y_hat=622\n","Skipping y_hat=180\n","Skipping y_hat=376\n","Skipping y_hat=21\n","Skipping y_hat=145\n","Skipping y_hat=406\n","Skipping y_hat=148\n","Skipping y_hat=218\n","Skipping y_hat=20\n","Skipping y_hat=742\n","Skipping y_hat=11\n","Skipping y_hat=784\n","Skipping y_hat=87\n","Skipping y_hat=85\n","Skipping y_hat=925\n","Skipping y_hat=182\n"," 96% 24/25 [00:00<00:00, 30.38it/s]Skipping y_hat=85\n","Skipping y_hat=88\n","Skipping y_hat=50\n","Skipping y_hat=73\n","Skipping y_hat=50\n","100% 25/25 [00:00<00:00, 30.84it/s]\n","accuracy of 3000 examples: 2926/3000 (97.53333333333333%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 35.54it/s]\n","accuracy of 3000 examples: 2948/3000 (98.26666666666667%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.08it/s]Skipping y_hat=708\n","Skipping y_hat=446\n","Skipping y_hat=795\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=899\n","Skipping y_hat=869\n","Skipping y_hat=276\n","Skipping y_hat=889\n","Skipping y_hat=988\n","Skipping y_hat=432\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=746\n","Skipping y_hat=722\n","Skipping y_hat=268\n","Skipping y_hat=813\n","Skipping y_hat=775\n","Skipping y_hat=776\n","Skipping y_hat=103\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=485\n","Skipping y_hat=448\n","Skipping y_hat=646\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=830\n","Skipping y_hat=596\n","Skipping y_hat=430\n","Skipping y_hat=409\n","Skipping y_hat=92\n","Skipping y_hat=106\n","Skipping y_hat=782\n","Skipping y_hat=790\n","Skipping y_hat=289\n","Skipping y_hat=59\n","Skipping y_hat=108\n","Skipping y_hat=878\n","Skipping y_hat=86\n","Skipping y_hat=740\n","Skipping y_hat=960\n","Skipping y_hat=830\n","Skipping y_hat=113\n","Skipping y_hat=260\n","Skipping y_hat=782\n","Skipping y_hat=3\n","Skipping y_hat=411\n","Skipping y_hat=669\n","Skipping y_hat=712\n","Skipping y_hat=354\n","Skipping y_hat=29\n","Skipping y_hat=722\n","Skipping y_hat=180\n","Skipping y_hat=616\n","Skipping y_hat=91\n","Skipping y_hat=185\n","Skipping y_hat=526\n","Skipping y_hat=162\n","Skipping y_hat=168\n","Skipping y_hat=80\n","Skipping y_hat=21\n","Skipping y_hat=824\n","Skipping y_hat=87\n","Skipping y_hat=845\n","Skipping y_hat=752\n"," 96% 24/25 [00:00<00:00, 30.91it/s]Skipping y_hat=79\n","Skipping y_hat=78\n","Skipping y_hat=85\n","Skipping y_hat=0\n","100% 25/25 [00:00<00:00, 31.11it/s]\n","accuracy of 3000 examples: 2929/3000 (97.63333333333334%)\n","\n","Test Results:\n","test: 97.63%\n","\n","iter 3800: train loss 1.0185, val loss 1.0571\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 88% 22/25 [00:00<00:00, 30.23it/s]Skipping y_hat=755\n","Skipping y_hat=446\n","Skipping y_hat=485\n","Skipping y_hat=342\n","Skipping y_hat=107\n","Skipping y_hat=590\n","Skipping y_hat=166\n","Skipping y_hat=509\n","Skipping y_hat=809\n","Skipping y_hat=616\n","Skipping y_hat=209\n","Skipping y_hat=988\n","Skipping y_hat=162\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=776\n","Skipping y_hat=772\n","Skipping y_hat=228\n","Skipping y_hat=813\n","Skipping y_hat=495\n","Skipping y_hat=176\n","Skipping y_hat=103\n","Skipping y_hat=602\n","Skipping y_hat=747\n","Skipping y_hat=775\n","Skipping y_hat=988\n","Skipping y_hat=276\n","Skipping y_hat=77\n","Skipping y_hat=296\n","Skipping y_hat=530\n","Skipping y_hat=786\n","Skipping y_hat=910\n","Skipping y_hat=949\n","Skipping y_hat=92\n","Skipping y_hat=136\n","Skipping y_hat=682\n","Skipping y_hat=750\n","Skipping y_hat=169\n","Skipping y_hat=198\n","Skipping y_hat=788\n","Skipping y_hat=86\n","Skipping y_hat=730\n","Skipping y_hat=430\n","Skipping y_hat=630\n","Skipping y_hat=113\n","Skipping y_hat=260\n","Skipping y_hat=122\n","Skipping y_hat=103\n","Skipping y_hat=211\n","Skipping y_hat=712\n","Skipping y_hat=104\n","Skipping y_hat=824\n","Skipping y_hat=29\n","Skipping y_hat=122\n","Skipping y_hat=170\n","Skipping y_hat=806\n","Skipping y_hat=21\n","Skipping y_hat=876\n","Skipping y_hat=128\n","Skipping y_hat=218\n","Skipping y_hat=80\n","Skipping y_hat=772\n","Skipping y_hat=21\n","Skipping y_hat=894\n","Skipping y_hat=87\n","Skipping y_hat=255\n","Skipping y_hat=762\n","Skipping y_hat=77\n","Skipping y_hat=17\n","Skipping y_hat=52\n","Skipping y_hat=82\n","100% 25/25 [00:00<00:00, 30.11it/s]\n","accuracy of 3000 examples: 2928/3000 (97.6%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.30it/s]\n","accuracy of 3000 examples: 2944/3000 (98.13333333333333%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.88it/s]Skipping y_hat=585\n","Skipping y_hat=108\n","Skipping y_hat=446\n","Skipping y_hat=795\n","Skipping y_hat=342\n","Skipping y_hat=197\n","Skipping y_hat=590\n","Skipping y_hat=409\n","Skipping y_hat=809\n","Skipping y_hat=676\n","Skipping y_hat=409\n","Skipping y_hat=988\n","Skipping y_hat=782\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=276\n","Skipping y_hat=722\n","Skipping y_hat=968\n","Skipping y_hat=813\n","Skipping y_hat=485\n","Skipping y_hat=476\n","Skipping y_hat=73\n","Skipping y_hat=747\n","Skipping y_hat=385\n","Skipping y_hat=948\n","Skipping y_hat=216\n","Skipping y_hat=97\n","Skipping y_hat=226\n","Skipping y_hat=330\n","Skipping y_hat=886\n","Skipping y_hat=410\n","Skipping y_hat=809\n","Skipping y_hat=126\n","Skipping y_hat=472\n","Skipping y_hat=309\n","Skipping y_hat=109\n","Skipping y_hat=108\n","Skipping y_hat=708\n","Skipping y_hat=86\n","Skipping y_hat=430\n","Skipping y_hat=660\n","Skipping y_hat=420\n","Skipping y_hat=522\n","Skipping y_hat=103\n","Skipping y_hat=411\n","Skipping y_hat=669\n","Skipping y_hat=712\n","Skipping y_hat=184\n","Skipping y_hat=474\n","Skipping y_hat=79\n","Skipping y_hat=722\n","Skipping y_hat=170\n","Skipping y_hat=706\n","Skipping y_hat=21\n","Skipping y_hat=986\n","Skipping y_hat=192\n","Skipping y_hat=148\n","Skipping y_hat=248\n","Skipping y_hat=10\n","Skipping y_hat=732\n","Skipping y_hat=21\n","Skipping y_hat=484\n","Skipping y_hat=87\n","Skipping y_hat=755\n","Skipping y_hat=472\n"," 96% 24/25 [00:00<00:00, 31.47it/s]Skipping y_hat=82\n","Skipping y_hat=79\n","Skipping y_hat=70\n","Skipping y_hat=52\n","100% 25/25 [00:00<00:00, 31.72it/s]\n","accuracy of 3000 examples: 2930/3000 (97.66666666666667%)\n","\n","Test Results:\n","test: 97.67%\n","\n","iter 3900: train loss 1.0075, val loss 1.0475\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.64it/s]Skipping y_hat=755\n","Skipping y_hat=618\n","Skipping y_hat=446\n","Skipping y_hat=585\n","Skipping y_hat=342\n","Skipping y_hat=67\n","Skipping y_hat=590\n","Skipping y_hat=209\n","Skipping y_hat=509\n","Skipping y_hat=266\n","Skipping y_hat=609\n","Skipping y_hat=872\n","Skipping y_hat=109\n","Skipping y_hat=110\n","Skipping y_hat=766\n","Skipping y_hat=282\n","Skipping y_hat=848\n","Skipping y_hat=813\n","Skipping y_hat=475\n","Skipping y_hat=476\n","Skipping y_hat=173\n","Skipping y_hat=672\n","Skipping y_hat=727\n","Skipping y_hat=575\n","Skipping y_hat=658\n","Skipping y_hat=286\n","Skipping y_hat=77\n","Skipping y_hat=596\n","Skipping y_hat=430\n","Skipping y_hat=589\n","Skipping y_hat=192\n","Skipping y_hat=126\n","Skipping y_hat=782\n","Skipping y_hat=750\n","Skipping y_hat=309\n","Skipping y_hat=109\n","Skipping y_hat=178\n","Skipping y_hat=778\n","Skipping y_hat=86\n","Skipping y_hat=840\n","Skipping y_hat=810\n","Skipping y_hat=430\n","Skipping y_hat=113\n","Skipping y_hat=240\n","Skipping y_hat=522\n","Skipping y_hat=103\n","Skipping y_hat=211\n","Skipping y_hat=689\n","Skipping y_hat=712\n","Skipping y_hat=354\n","Skipping y_hat=29\n","Skipping y_hat=782\n","Skipping y_hat=160\n","Skipping y_hat=946\n","Skipping y_hat=1\n","Skipping y_hat=145\n","Skipping y_hat=286\n","Skipping y_hat=148\n","Skipping y_hat=80\n","Skipping y_hat=21\n","Skipping y_hat=494\n","Skipping y_hat=77\n","Skipping y_hat=275\n","Skipping y_hat=782\n"," 96% 24/25 [00:00<00:00, 31.33it/s]Skipping y_hat=74\n","Skipping y_hat=32\n","Skipping y_hat=80\n","Skipping y_hat=65\n","Skipping y_hat=72\n","100% 25/25 [00:00<00:00, 31.63it/s]\n","accuracy of 3000 examples: 2931/3000 (97.7%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.12it/s]\n","accuracy of 3000 examples: 2947/3000 (98.23333333333333%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 92% 23/25 [00:00<00:00, 31.03it/s]Skipping y_hat=795\n","Skipping y_hat=268\n","Skipping y_hat=446\n","Skipping y_hat=855\n","Skipping y_hat=342\n","Skipping y_hat=177\n","Skipping y_hat=590\n","Skipping y_hat=146\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=276\n","Skipping y_hat=889\n","Skipping y_hat=988\n","Skipping y_hat=482\n","Skipping y_hat=109\n","Skipping y_hat=120\n","Skipping y_hat=776\n","Skipping y_hat=232\n","Skipping y_hat=748\n","Skipping y_hat=975\n","Skipping y_hat=116\n","Skipping y_hat=173\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=575\n","Skipping y_hat=948\n","Skipping y_hat=376\n","Skipping y_hat=97\n","Skipping y_hat=296\n","Skipping y_hat=530\n","Skipping y_hat=896\n","Skipping y_hat=740\n","Skipping y_hat=429\n","Skipping y_hat=146\n","Skipping y_hat=732\n","Skipping y_hat=750\n","Skipping y_hat=289\n","Skipping y_hat=169\n","Skipping y_hat=198\n","Skipping y_hat=768\n","Skipping y_hat=86\n","Skipping y_hat=360\n","Skipping y_hat=680\n","Skipping y_hat=240\n","Skipping y_hat=113\n","Skipping y_hat=762\n","Skipping y_hat=103\n","Skipping y_hat=711\n","Skipping y_hat=689\n","Skipping y_hat=712\n","Skipping y_hat=704\n","Skipping y_hat=89\n","Skipping y_hat=562\n","Skipping y_hat=90\n","Skipping y_hat=716\n","Skipping y_hat=21\n","Skipping y_hat=145\n","Skipping y_hat=276\n","Skipping y_hat=182\n","Skipping y_hat=108\n","Skipping y_hat=268\n","Skipping y_hat=10\n","Skipping y_hat=772\n","Skipping y_hat=584\n","Skipping y_hat=87\n","Skipping y_hat=455\n","Skipping y_hat=782\n","Skipping y_hat=75\n","Skipping y_hat=55\n","Skipping y_hat=79\n","Skipping y_hat=55\n","Skipping y_hat=29\n","100% 25/25 [00:00<00:00, 30.87it/s]\n","accuracy of 3000 examples: 2928/3000 (97.6%)\n","\n","Test Results:\n","test: 97.60%\n","\n","iter 4000: train loss 1.0160, val loss 1.0549\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 88% 22/25 [00:00<00:00, 28.74it/s]Skipping y_hat=425\n","Skipping y_hat=618\n","Skipping y_hat=446\n","Skipping y_hat=775\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=909\n","Skipping y_hat=799\n","Skipping y_hat=746\n","Skipping y_hat=909\n","Skipping y_hat=682\n","Skipping y_hat=109\n","Skipping y_hat=120\n","Skipping y_hat=176\n","Skipping y_hat=722\n","Skipping y_hat=128\n","Skipping y_hat=813\n","Skipping y_hat=285\n","Skipping y_hat=376\n","Skipping y_hat=73\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=475\n","Skipping y_hat=488\n","Skipping y_hat=846\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=530\n","Skipping y_hat=496\n","Skipping y_hat=530\n","Skipping y_hat=309\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=482\n","Skipping y_hat=770\n","Skipping y_hat=309\n","Skipping y_hat=169\n","Skipping y_hat=78\n","Skipping y_hat=788\n","Skipping y_hat=86\n","Skipping y_hat=630\n","Skipping y_hat=160\n","Skipping y_hat=440\n","Skipping y_hat=113\n","Skipping y_hat=622\n","Skipping y_hat=3\n","Skipping y_hat=711\n","Skipping y_hat=689\n","Skipping y_hat=712\n","Skipping y_hat=154\n","Skipping y_hat=844\n","Skipping y_hat=89\n","Skipping y_hat=652\n","Skipping y_hat=180\n","Skipping y_hat=366\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=266\n","Skipping y_hat=162\n","Skipping y_hat=128\n","Skipping y_hat=20\n","Skipping y_hat=782\n","Skipping y_hat=11\n","Skipping y_hat=784\n","Skipping y_hat=67\n","Skipping y_hat=245\n","Skipping y_hat=822\n","Skipping y_hat=72\n","Skipping y_hat=74\n","Skipping y_hat=14\n","Skipping y_hat=89\n","100% 25/25 [00:00<00:00, 29.53it/s]\n","accuracy of 3000 examples: 2928/3000 (97.6%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 33.77it/s]\n","accuracy of 3000 examples: 2955/3000 (98.5%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 88% 22/25 [00:00<00:00, 30.48it/s]Skipping y_hat=595\n","Skipping y_hat=278\n","Skipping y_hat=446\n","Skipping y_hat=775\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=639\n","Skipping y_hat=809\n","Skipping y_hat=576\n","Skipping y_hat=609\n","Skipping y_hat=988\n","Skipping y_hat=762\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=776\n","Skipping y_hat=622\n","Skipping y_hat=238\n","Skipping y_hat=813\n","Skipping y_hat=775\n","Skipping y_hat=876\n","Skipping y_hat=173\n","Skipping y_hat=602\n","Skipping y_hat=747\n","Skipping y_hat=465\n","Skipping y_hat=478\n","Skipping y_hat=446\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=330\n","Skipping y_hat=356\n","Skipping y_hat=740\n","Skipping y_hat=899\n","Skipping y_hat=126\n","Skipping y_hat=482\n","Skipping y_hat=169\n","Skipping y_hat=108\n","Skipping y_hat=588\n","Skipping y_hat=86\n","Skipping y_hat=430\n","Skipping y_hat=440\n","Skipping y_hat=830\n","Skipping y_hat=113\n","Skipping y_hat=280\n","Skipping y_hat=732\n","Skipping y_hat=103\n","Skipping y_hat=311\n","Skipping y_hat=689\n","Skipping y_hat=712\n","Skipping y_hat=174\n","Skipping y_hat=234\n","Skipping y_hat=79\n","Skipping y_hat=622\n","Skipping y_hat=160\n","Skipping y_hat=576\n","Skipping y_hat=41\n","Skipping y_hat=145\n","Skipping y_hat=776\n","Skipping y_hat=128\n","Skipping y_hat=20\n","Skipping y_hat=732\n","Skipping y_hat=184\n","Skipping y_hat=77\n","Skipping y_hat=345\n","Skipping y_hat=862\n","Skipping y_hat=77\n","Skipping y_hat=17\n","Skipping y_hat=80\n","Skipping y_hat=54\n","Skipping y_hat=72\n","100% 25/25 [00:00<00:00, 29.82it/s]\n","accuracy of 3000 examples: 2930/3000 (97.66666666666667%)\n","\n","Test Results:\n","test: 97.67%\n","\n","iter 4100: train loss 1.0100, val loss 1.0462\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 88% 22/25 [00:00<00:00, 30.43it/s]Skipping y_hat=185\n","Skipping y_hat=446\n","Skipping y_hat=975\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=146\n","Skipping y_hat=949\n","Skipping y_hat=779\n","Skipping y_hat=376\n","Skipping y_hat=409\n","Skipping y_hat=968\n","Skipping y_hat=762\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=276\n","Skipping y_hat=422\n","Skipping y_hat=648\n","Skipping y_hat=775\n","Skipping y_hat=176\n","Skipping y_hat=73\n","Skipping y_hat=415\n","Skipping y_hat=928\n","Skipping y_hat=876\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=796\n","Skipping y_hat=630\n","Skipping y_hat=419\n","Skipping y_hat=126\n","Skipping y_hat=272\n","Skipping y_hat=770\n","Skipping y_hat=59\n","Skipping y_hat=198\n","Skipping y_hat=588\n","Skipping y_hat=86\n","Skipping y_hat=830\n","Skipping y_hat=660\n","Skipping y_hat=430\n","Skipping y_hat=113\n","Skipping y_hat=260\n","Skipping y_hat=532\n","Skipping y_hat=23\n","Skipping y_hat=531\n","Skipping y_hat=669\n","Skipping y_hat=104\n","Skipping y_hat=944\n","Skipping y_hat=89\n","Skipping y_hat=150\n","Skipping y_hat=616\n","Skipping y_hat=21\n","Skipping y_hat=276\n","Skipping y_hat=108\n","Skipping y_hat=248\n","Skipping y_hat=20\n","Skipping y_hat=772\n","Skipping y_hat=21\n","Skipping y_hat=784\n","Skipping y_hat=87\n","Skipping y_hat=245\n","Skipping y_hat=682\n","Skipping y_hat=74\n","Skipping y_hat=47\n","Skipping y_hat=43\n","100% 25/25 [00:00<00:00, 29.87it/s]\n","accuracy of 3000 examples: 2935/3000 (97.83333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 35.29it/s]\n","accuracy of 3000 examples: 2954/3000 (98.46666666666667%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 28% 7/25 [00:00<00:00, 30.44it/s]Skipping y_hat=1044\n"," 92% 23/25 [00:00<00:00, 30.96it/s]Skipping y_hat=595\n","Skipping y_hat=518\n","Skipping y_hat=446\n","Skipping y_hat=375\n","Skipping y_hat=342\n","Skipping y_hat=177\n","Skipping y_hat=590\n","Skipping y_hat=789\n","Skipping y_hat=669\n","Skipping y_hat=276\n","Skipping y_hat=779\n","Skipping y_hat=762\n","Skipping y_hat=149\n","Skipping y_hat=110\n","Skipping y_hat=246\n","Skipping y_hat=722\n","Skipping y_hat=648\n","Skipping y_hat=813\n","Skipping y_hat=375\n","Skipping y_hat=546\n","Skipping y_hat=73\n","Skipping y_hat=747\n","Skipping y_hat=715\n","Skipping y_hat=248\n","Skipping y_hat=466\n","Skipping y_hat=77\n","Skipping y_hat=296\n","Skipping y_hat=310\n","Skipping y_hat=186\n","Skipping y_hat=630\n","Skipping y_hat=809\n","Skipping y_hat=146\n","Skipping y_hat=582\n","Skipping y_hat=770\n","Skipping y_hat=69\n","Skipping y_hat=108\n","Skipping y_hat=708\n","Skipping y_hat=86\n","Skipping y_hat=430\n","Skipping y_hat=460\n","Skipping y_hat=430\n","Skipping y_hat=113\n","Skipping y_hat=250\n","Skipping y_hat=622\n","Skipping y_hat=3\n","Skipping y_hat=211\n","Skipping y_hat=689\n","Skipping y_hat=154\n","Skipping y_hat=894\n","Skipping y_hat=9\n","Skipping y_hat=522\n","Skipping y_hat=150\n","Skipping y_hat=266\n","Skipping y_hat=71\n","Skipping y_hat=145\n","Skipping y_hat=616\n","Skipping y_hat=162\n","Skipping y_hat=128\n","Skipping y_hat=20\n","Skipping y_hat=772\n","Skipping y_hat=784\n","Skipping y_hat=87\n","Skipping y_hat=325\n","Skipping y_hat=762\n","Skipping y_hat=74\n","Skipping y_hat=18\n","Skipping y_hat=24\n","Skipping y_hat=84\n","Skipping y_hat=97\n","100% 25/25 [00:00<00:00, 30.74it/s]\n","accuracy of 3000 examples: 2930/3000 (97.66666666666667%)\n","\n","Test Results:\n","test: 97.67%\n","\n","iter 4200: train loss 1.0109, val loss 1.0553\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 32.08it/s]Skipping y_hat=795\n","Skipping y_hat=718\n","Skipping y_hat=446\n","Skipping y_hat=575\n","Skipping y_hat=342\n","Skipping y_hat=97\n","Skipping y_hat=590\n","Skipping y_hat=419\n","Skipping y_hat=689\n","Skipping y_hat=276\n","Skipping y_hat=949\n","Skipping y_hat=988\n","Skipping y_hat=872\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=476\n","Skipping y_hat=522\n","Skipping y_hat=268\n","Skipping y_hat=813\n","Skipping y_hat=775\n","Skipping y_hat=446\n","Skipping y_hat=173\n","Skipping y_hat=672\n","Skipping y_hat=875\n","Skipping y_hat=448\n","Skipping y_hat=276\n","Skipping y_hat=77\n","Skipping y_hat=286\n","Skipping y_hat=710\n","Skipping y_hat=496\n","Skipping y_hat=130\n","Skipping y_hat=989\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=472\n","Skipping y_hat=69\n","Skipping y_hat=108\n","Skipping y_hat=178\n","Skipping y_hat=430\n","Skipping y_hat=120\n","Skipping y_hat=360\n","Skipping y_hat=113\n","Skipping y_hat=240\n","Skipping y_hat=622\n","Skipping y_hat=103\n","Skipping y_hat=321\n","Skipping y_hat=689\n","Skipping y_hat=712\n","Skipping y_hat=144\n","Skipping y_hat=254\n","Skipping y_hat=79\n","Skipping y_hat=192\n","Skipping y_hat=150\n","Skipping y_hat=616\n","Skipping y_hat=1\n","Skipping y_hat=145\n","Skipping y_hat=276\n","Skipping y_hat=148\n","Skipping y_hat=218\n","Skipping y_hat=80\n","Skipping y_hat=772\n","Skipping y_hat=21\n","Skipping y_hat=784\n","Skipping y_hat=87\n","Skipping y_hat=445\n","Skipping y_hat=162\n"," 96% 24/25 [00:00<00:00, 31.08it/s]Skipping y_hat=62\n","Skipping y_hat=11\n","Skipping y_hat=80\n","Skipping y_hat=82\n","Skipping y_hat=65\n","100% 25/25 [00:00<00:00, 31.67it/s]\n","accuracy of 3000 examples: 2929/3000 (97.63333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 35.74it/s]\n","accuracy of 3000 examples: 2953/3000 (98.43333333333332%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 30.58it/s]Skipping y_hat=545\n","Skipping y_hat=268\n","Skipping y_hat=446\n","Skipping y_hat=575\n","Skipping y_hat=342\n","Skipping y_hat=77\n","Skipping y_hat=590\n","Skipping y_hat=146\n","Skipping y_hat=899\n","Skipping y_hat=169\n","Skipping y_hat=846\n","Skipping y_hat=409\n","Skipping y_hat=988\n","Skipping y_hat=682\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=276\n","Skipping y_hat=762\n","Skipping y_hat=748\n","Skipping y_hat=813\n","Skipping y_hat=575\n","Skipping y_hat=746\n","Skipping y_hat=173\n","Skipping y_hat=747\n","Skipping y_hat=475\n","Skipping y_hat=468\n","Skipping y_hat=346\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=430\n","Skipping y_hat=216\n","Skipping y_hat=740\n","Skipping y_hat=479\n","Skipping y_hat=346\n","Skipping y_hat=156\n","Skipping y_hat=782\n","Skipping y_hat=289\n","Skipping y_hat=69\n","Skipping y_hat=88\n","Skipping y_hat=148\n","Skipping y_hat=86\n","Skipping y_hat=440\n","Skipping y_hat=430\n","Skipping y_hat=440\n","Skipping y_hat=123\n","Skipping y_hat=122\n","Skipping y_hat=103\n","Skipping y_hat=511\n","Skipping y_hat=669\n","Skipping y_hat=712\n","Skipping y_hat=144\n","Skipping y_hat=294\n","Skipping y_hat=79\n","Skipping y_hat=322\n","Skipping y_hat=130\n","Skipping y_hat=346\n","Skipping y_hat=145\n","Skipping y_hat=716\n","Skipping y_hat=248\n","Skipping y_hat=20\n","Skipping y_hat=742\n","Skipping y_hat=11\n","Skipping y_hat=884\n","Skipping y_hat=87\n","Skipping y_hat=445\n","Skipping y_hat=562\n"," 96% 24/25 [00:00<00:00, 30.31it/s]Skipping y_hat=68\n","Skipping y_hat=29\n","Skipping y_hat=68\n","Skipping y_hat=72\n","100% 25/25 [00:00<00:00, 30.71it/s]\n","accuracy of 3000 examples: 2929/3000 (97.63333333333334%)\n","\n","Test Results:\n","test: 97.63%\n","\n","iter 4300: train loss 1.0132, val loss 1.0571\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.58it/s]Skipping y_hat=925\n","Skipping y_hat=978\n","Skipping y_hat=446\n","Skipping y_hat=175\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=809\n","Skipping y_hat=109\n","Skipping y_hat=276\n","Skipping y_hat=709\n","Skipping y_hat=968\n","Skipping y_hat=472\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=576\n","Skipping y_hat=622\n","Skipping y_hat=398\n","Skipping y_hat=813\n","Skipping y_hat=885\n","Skipping y_hat=176\n","Skipping y_hat=73\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=475\n","Skipping y_hat=848\n","Skipping y_hat=276\n","Skipping y_hat=77\n","Skipping y_hat=296\n","Skipping y_hat=330\n","Skipping y_hat=896\n","Skipping y_hat=430\n","Skipping y_hat=119\n","Skipping y_hat=92\n","Skipping y_hat=106\n","Skipping y_hat=472\n","Skipping y_hat=169\n","Skipping y_hat=198\n","Skipping y_hat=808\n","Skipping y_hat=86\n","Skipping y_hat=560\n","Skipping y_hat=850\n","Skipping y_hat=440\n","Skipping y_hat=113\n","Skipping y_hat=922\n","Skipping y_hat=103\n","Skipping y_hat=311\n","Skipping y_hat=104\n","Skipping y_hat=894\n","Skipping y_hat=69\n","Skipping y_hat=872\n","Skipping y_hat=150\n","Skipping y_hat=356\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=956\n","Skipping y_hat=128\n","Skipping y_hat=248\n","Skipping y_hat=20\n","Skipping y_hat=772\n","Skipping y_hat=784\n","Skipping y_hat=77\n","Skipping y_hat=445\n","Skipping y_hat=772\n"," 96% 24/25 [00:00<00:00, 31.55it/s]Skipping y_hat=52\n","Skipping y_hat=85\n","Skipping y_hat=44\n","Skipping y_hat=89\n","100% 25/25 [00:00<00:00, 31.38it/s]\n","accuracy of 3000 examples: 2931/3000 (97.7%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 34.65it/s]\n","accuracy of 3000 examples: 2960/3000 (98.66666666666667%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 88% 22/25 [00:00<00:00, 31.17it/s]Skipping y_hat=445\n","Skipping y_hat=268\n","Skipping y_hat=446\n","Skipping y_hat=785\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=789\n","Skipping y_hat=809\n","Skipping y_hat=176\n","Skipping y_hat=609\n","Skipping y_hat=968\n","Skipping y_hat=982\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=256\n","Skipping y_hat=222\n","Skipping y_hat=438\n","Skipping y_hat=813\n","Skipping y_hat=885\n","Skipping y_hat=176\n","Skipping y_hat=173\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=475\n","Skipping y_hat=448\n","Skipping y_hat=276\n","Skipping y_hat=77\n","Skipping y_hat=830\n","Skipping y_hat=596\n","Skipping y_hat=630\n","Skipping y_hat=909\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=482\n","Skipping y_hat=750\n","Skipping y_hat=89\n","Skipping y_hat=108\n","Skipping y_hat=548\n","Skipping y_hat=86\n","Skipping y_hat=330\n","Skipping y_hat=600\n","Skipping y_hat=330\n","Skipping y_hat=113\n","Skipping y_hat=622\n","Skipping y_hat=23\n","Skipping y_hat=411\n","Skipping y_hat=669\n","Skipping y_hat=104\n","Skipping y_hat=354\n","Skipping y_hat=89\n","Skipping y_hat=722\n","Skipping y_hat=180\n","Skipping y_hat=166\n","Skipping y_hat=91\n","Skipping y_hat=145\n","Skipping y_hat=266\n","Skipping y_hat=301\n","Skipping y_hat=128\n","Skipping y_hat=20\n","Skipping y_hat=752\n","Skipping y_hat=784\n","Skipping y_hat=87\n","Skipping y_hat=345\n","Skipping y_hat=772\n","Skipping y_hat=59\n","Skipping y_hat=70\n","Skipping y_hat=42\n","Skipping y_hat=70\n","100% 25/25 [00:00<00:00, 30.41it/s]\n","accuracy of 3000 examples: 2930/3000 (97.66666666666667%)\n","\n","Test Results:\n","test: 97.67%\n","\n","iter 4400: train loss 1.0171, val loss 1.0488\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 30.53it/s]Skipping y_hat=495\n","Skipping y_hat=418\n","Skipping y_hat=446\n","Skipping y_hat=275\n","Skipping y_hat=342\n","Skipping y_hat=97\n","Skipping y_hat=590\n","Skipping y_hat=146\n","Skipping y_hat=949\n","Skipping y_hat=609\n","Skipping y_hat=276\n","Skipping y_hat=889\n","Skipping y_hat=988\n","Skipping y_hat=782\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=576\n","Skipping y_hat=722\n","Skipping y_hat=468\n","Skipping y_hat=813\n","Skipping y_hat=875\n","Skipping y_hat=546\n","Skipping y_hat=103\n","Skipping y_hat=747\n","Skipping y_hat=775\n","Skipping y_hat=848\n","Skipping y_hat=276\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=830\n","Skipping y_hat=396\n","Skipping y_hat=410\n","Skipping y_hat=419\n","Skipping y_hat=316\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=472\n","Skipping y_hat=710\n","Skipping y_hat=169\n","Skipping y_hat=198\n","Skipping y_hat=888\n","Skipping y_hat=86\n","Skipping y_hat=430\n","Skipping y_hat=260\n","Skipping y_hat=430\n","Skipping y_hat=113\n","Skipping y_hat=280\n","Skipping y_hat=822\n","Skipping y_hat=103\n","Skipping y_hat=811\n","Skipping y_hat=689\n","Skipping y_hat=712\n","Skipping y_hat=154\n","Skipping y_hat=254\n","Skipping y_hat=79\n","Skipping y_hat=622\n","Skipping y_hat=150\n","Skipping y_hat=276\n","Skipping y_hat=71\n","Skipping y_hat=145\n","Skipping y_hat=266\n","Skipping y_hat=108\n","Skipping y_hat=20\n","Skipping y_hat=752\n","Skipping y_hat=21\n","Skipping y_hat=484\n","Skipping y_hat=87\n","Skipping y_hat=245\n","Skipping y_hat=892\n"," 96% 24/25 [00:00<00:00, 30.33it/s]Skipping y_hat=54\n","Skipping y_hat=59\n","Skipping y_hat=42\n","Skipping y_hat=53\n","Skipping y_hat=52\n","100% 25/25 [00:00<00:00, 30.58it/s]\n","accuracy of 3000 examples: 2926/3000 (97.53333333333333%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 34.99it/s]\n","accuracy of 3000 examples: 2958/3000 (98.6%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 92% 23/25 [00:00<00:00, 31.26it/s]Skipping y_hat=595\n","Skipping y_hat=608\n","Skipping y_hat=446\n","Skipping y_hat=575\n","Skipping y_hat=342\n","Skipping y_hat=77\n","Skipping y_hat=590\n","Skipping y_hat=146\n","Skipping y_hat=919\n","Skipping y_hat=769\n","Skipping y_hat=266\n","Skipping y_hat=449\n","Skipping y_hat=862\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=476\n","Skipping y_hat=722\n","Skipping y_hat=448\n","Skipping y_hat=813\n","Skipping y_hat=585\n","Skipping y_hat=846\n","Skipping y_hat=73\n","Skipping y_hat=747\n","Skipping y_hat=175\n","Skipping y_hat=548\n","Skipping y_hat=646\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=630\n","Skipping y_hat=296\n","Skipping y_hat=660\n","Skipping y_hat=429\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=482\n","Skipping y_hat=770\n","Skipping y_hat=309\n","Skipping y_hat=169\n","Skipping y_hat=78\n","Skipping y_hat=758\n","Skipping y_hat=86\n","Skipping y_hat=430\n","Skipping y_hat=260\n","Skipping y_hat=460\n","Skipping y_hat=113\n","Skipping y_hat=280\n","Skipping y_hat=652\n","Skipping y_hat=103\n","Skipping y_hat=811\n","Skipping y_hat=154\n","Skipping y_hat=354\n","Skipping y_hat=89\n","Skipping y_hat=140\n","Skipping y_hat=266\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=266\n","Skipping y_hat=182\n","Skipping y_hat=148\n","Skipping y_hat=20\n","Skipping y_hat=732\n","Skipping y_hat=21\n","Skipping y_hat=484\n","Skipping y_hat=87\n","Skipping y_hat=225\n","Skipping y_hat=872\n","Skipping y_hat=75\n","Skipping y_hat=86\n","Skipping y_hat=73\n","100% 25/25 [00:00<00:00, 31.03it/s]\n","accuracy of 3000 examples: 2929/3000 (97.63333333333334%)\n","\n","Test Results:\n","test: 97.63%\n","\n","iter 4500: train loss 1.0106, val loss 1.0578\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 92% 23/25 [00:00<00:00, 30.90it/s]Skipping y_hat=295\n","Skipping y_hat=268\n","Skipping y_hat=446\n","Skipping y_hat=775\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=459\n","Skipping y_hat=799\n","Skipping y_hat=246\n","Skipping y_hat=409\n","Skipping y_hat=968\n","Skipping y_hat=882\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=246\n","Skipping y_hat=622\n","Skipping y_hat=648\n","Skipping y_hat=813\n","Skipping y_hat=875\n","Skipping y_hat=576\n","Skipping y_hat=173\n","Skipping y_hat=747\n","Skipping y_hat=575\n","Skipping y_hat=848\n","Skipping y_hat=276\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=310\n","Skipping y_hat=996\n","Skipping y_hat=830\n","Skipping y_hat=709\n","Skipping y_hat=92\n","Skipping y_hat=106\n","Skipping y_hat=482\n","Skipping y_hat=309\n","Skipping y_hat=109\n","Skipping y_hat=108\n","Skipping y_hat=978\n","Skipping y_hat=86\n","Skipping y_hat=430\n","Skipping y_hat=960\n","Skipping y_hat=740\n","Skipping y_hat=113\n","Skipping y_hat=250\n","Skipping y_hat=622\n","Skipping y_hat=103\n","Skipping y_hat=831\n","Skipping y_hat=689\n","Skipping y_hat=712\n","Skipping y_hat=104\n","Skipping y_hat=154\n","Skipping y_hat=79\n","Skipping y_hat=322\n","Skipping y_hat=150\n","Skipping y_hat=366\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=176\n","Skipping y_hat=162\n","Skipping y_hat=20\n","Skipping y_hat=742\n","Skipping y_hat=111\n","Skipping y_hat=194\n","Skipping y_hat=245\n","Skipping y_hat=762\n","Skipping y_hat=11\n","Skipping y_hat=80\n","Skipping y_hat=55\n","Skipping y_hat=19\n","100% 25/25 [00:00<00:00, 30.79it/s]\n","accuracy of 3000 examples: 2929/3000 (97.63333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 34.93it/s]\n","accuracy of 3000 examples: 2961/3000 (98.7%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 30.79it/s]Skipping y_hat=725\n","Skipping y_hat=618\n","Skipping y_hat=446\n","Skipping y_hat=875\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=409\n","Skipping y_hat=709\n","Skipping y_hat=276\n","Skipping y_hat=639\n","Skipping y_hat=968\n","Skipping y_hat=862\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=216\n","Skipping y_hat=122\n","Skipping y_hat=648\n","Skipping y_hat=813\n","Skipping y_hat=875\n","Skipping y_hat=176\n","Skipping y_hat=73\n","Skipping y_hat=747\n","Skipping y_hat=575\n","Skipping y_hat=448\n","Skipping y_hat=276\n","Skipping y_hat=97\n","Skipping y_hat=216\n","Skipping y_hat=530\n","Skipping y_hat=186\n","Skipping y_hat=330\n","Skipping y_hat=789\n","Skipping y_hat=92\n","Skipping y_hat=146\n","Skipping y_hat=782\n","Skipping y_hat=750\n","Skipping y_hat=309\n","Skipping y_hat=169\n","Skipping y_hat=108\n","Skipping y_hat=288\n","Skipping y_hat=86\n","Skipping y_hat=420\n","Skipping y_hat=440\n","Skipping y_hat=730\n","Skipping y_hat=113\n","Skipping y_hat=280\n","Skipping y_hat=822\n","Skipping y_hat=103\n","Skipping y_hat=611\n","Skipping y_hat=669\n","Skipping y_hat=84\n","Skipping y_hat=794\n","Skipping y_hat=79\n","Skipping y_hat=722\n","Skipping y_hat=150\n","Skipping y_hat=376\n","Skipping y_hat=21\n","Skipping y_hat=145\n","Skipping y_hat=276\n","Skipping y_hat=108\n","Skipping y_hat=20\n","Skipping y_hat=752\n","Skipping y_hat=11\n","Skipping y_hat=484\n","Skipping y_hat=87\n","Skipping y_hat=605\n","Skipping y_hat=772\n"," 96% 24/25 [00:00<00:00, 30.98it/s]Skipping y_hat=67\n","Skipping y_hat=35\n","Skipping y_hat=39\n","Skipping y_hat=54\n","Skipping y_hat=28\n","100% 25/25 [00:00<00:00, 30.93it/s]\n","accuracy of 3000 examples: 2928/3000 (97.6%)\n","\n","Test Results:\n","test: 97.60%\n","\n","iter 4600: train loss 1.0078, val loss 1.0629\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.42it/s]Skipping y_hat=505\n","Skipping y_hat=618\n","Skipping y_hat=446\n","Skipping y_hat=775\n","Skipping y_hat=77\n","Skipping y_hat=590\n","Skipping y_hat=146\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=276\n","Skipping y_hat=409\n","Skipping y_hat=968\n","Skipping y_hat=962\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=176\n","Skipping y_hat=222\n","Skipping y_hat=348\n","Skipping y_hat=813\n","Skipping y_hat=775\n","Skipping y_hat=446\n","Skipping y_hat=173\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=475\n","Skipping y_hat=948\n","Skipping y_hat=626\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=330\n","Skipping y_hat=306\n","Skipping y_hat=410\n","Skipping y_hat=849\n","Skipping y_hat=92\n","Skipping y_hat=136\n","Skipping y_hat=482\n","Skipping y_hat=750\n","Skipping y_hat=309\n","Skipping y_hat=108\n","Skipping y_hat=888\n","Skipping y_hat=86\n","Skipping y_hat=330\n","Skipping y_hat=430\n","Skipping y_hat=430\n","Skipping y_hat=113\n","Skipping y_hat=260\n","Skipping y_hat=422\n","Skipping y_hat=103\n","Skipping y_hat=311\n","Skipping y_hat=669\n","Skipping y_hat=184\n","Skipping y_hat=894\n","Skipping y_hat=79\n","Skipping y_hat=222\n","Skipping y_hat=170\n","Skipping y_hat=266\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=576\n","Skipping y_hat=128\n","Skipping y_hat=10\n","Skipping y_hat=772\n","Skipping y_hat=61\n","Skipping y_hat=784\n","Skipping y_hat=87\n","Skipping y_hat=745\n","Skipping y_hat=182\n"," 96% 24/25 [00:00<00:00, 31.27it/s]Skipping y_hat=64\n","Skipping y_hat=27\n","Skipping y_hat=54\n","Skipping y_hat=84\n","100% 25/25 [00:00<00:00, 31.37it/s]\n","accuracy of 3000 examples: 2928/3000 (97.6%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 36.46it/s]\n","accuracy of 3000 examples: 2962/3000 (98.73333333333333%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 30.44it/s]Skipping y_hat=445\n","Skipping y_hat=208\n","Skipping y_hat=436\n","Skipping y_hat=475\n","Skipping y_hat=342\n","Skipping y_hat=77\n","Skipping y_hat=409\n","Skipping y_hat=709\n","Skipping y_hat=776\n","Skipping y_hat=909\n","Skipping y_hat=968\n","Skipping y_hat=782\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=546\n","Skipping y_hat=922\n","Skipping y_hat=248\n","Skipping y_hat=813\n","Skipping y_hat=585\n","Skipping y_hat=176\n","Skipping y_hat=173\n","Skipping y_hat=757\n","Skipping y_hat=475\n","Skipping y_hat=848\n","Skipping y_hat=776\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=630\n","Skipping y_hat=896\n","Skipping y_hat=410\n","Skipping y_hat=409\n","Skipping y_hat=126\n","Skipping y_hat=782\n","Skipping y_hat=309\n","Skipping y_hat=169\n","Skipping y_hat=88\n","Skipping y_hat=878\n","Skipping y_hat=86\n","Skipping y_hat=450\n","Skipping y_hat=950\n","Skipping y_hat=630\n","Skipping y_hat=113\n","Skipping y_hat=522\n","Skipping y_hat=103\n","Skipping y_hat=311\n","Skipping y_hat=712\n","Skipping y_hat=104\n","Skipping y_hat=294\n","Skipping y_hat=89\n","Skipping y_hat=422\n","Skipping y_hat=150\n","Skipping y_hat=676\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=276\n","Skipping y_hat=148\n","Skipping y_hat=80\n","Skipping y_hat=742\n","Skipping y_hat=784\n","Skipping y_hat=67\n","Skipping y_hat=745\n","Skipping y_hat=272\n"," 96% 24/25 [00:00<00:00, 29.90it/s]Skipping y_hat=75\n","Skipping y_hat=40\n","Skipping y_hat=42\n","Skipping y_hat=50\n","100% 25/25 [00:00<00:00, 29.83it/s]\n","accuracy of 3000 examples: 2933/3000 (97.76666666666667%)\n","\n","Test Results:\n","test: 97.77%\n","\n","iter 4700: train loss 1.0030, val loss 1.0588\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 92% 23/25 [00:00<00:00, 30.64it/s]Skipping y_hat=795\n","Skipping y_hat=278\n","Skipping y_hat=446\n","Skipping y_hat=475\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=146\n","Skipping y_hat=429\n","Skipping y_hat=609\n","Skipping y_hat=376\n","Skipping y_hat=409\n","Skipping y_hat=372\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=716\n","Skipping y_hat=622\n","Skipping y_hat=648\n","Skipping y_hat=813\n","Skipping y_hat=575\n","Skipping y_hat=176\n","Skipping y_hat=173\n","Skipping y_hat=672\n","Skipping y_hat=727\n","Skipping y_hat=475\n","Skipping y_hat=948\n","Skipping y_hat=246\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=830\n","Skipping y_hat=496\n","Skipping y_hat=560\n","Skipping y_hat=899\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=382\n","Skipping y_hat=169\n","Skipping y_hat=98\n","Skipping y_hat=548\n","Skipping y_hat=86\n","Skipping y_hat=440\n","Skipping y_hat=620\n","Skipping y_hat=440\n","Skipping y_hat=113\n","Skipping y_hat=672\n","Skipping y_hat=103\n","Skipping y_hat=711\n","Skipping y_hat=712\n","Skipping y_hat=104\n","Skipping y_hat=254\n","Skipping y_hat=39\n","Skipping y_hat=732\n","Skipping y_hat=150\n","Skipping y_hat=566\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=976\n","Skipping y_hat=108\n","Skipping y_hat=20\n","Skipping y_hat=31\n","Skipping y_hat=774\n","Skipping y_hat=67\n","Skipping y_hat=245\n","Skipping y_hat=872\n","Skipping y_hat=72\n","Skipping y_hat=56\n","Skipping y_hat=84\n","Skipping y_hat=82\n","100% 25/25 [00:00<00:00, 30.03it/s]\n","accuracy of 3000 examples: 2931/3000 (97.7%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 35.22it/s]\n","accuracy of 3000 examples: 2965/3000 (98.83333333333333%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 30.98it/s]Skipping y_hat=795\n","Skipping y_hat=648\n","Skipping y_hat=446\n","Skipping y_hat=475\n","Skipping y_hat=97\n","Skipping y_hat=590\n","Skipping y_hat=439\n","Skipping y_hat=469\n","Skipping y_hat=246\n","Skipping y_hat=409\n","Skipping y_hat=662\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=216\n","Skipping y_hat=722\n","Skipping y_hat=668\n","Skipping y_hat=775\n","Skipping y_hat=776\n","Skipping y_hat=73\n","Skipping y_hat=747\n","Skipping y_hat=175\n","Skipping y_hat=448\n","Skipping y_hat=376\n","Skipping y_hat=77\n","Skipping y_hat=430\n","Skipping y_hat=496\n","Skipping y_hat=710\n","Skipping y_hat=419\n","Skipping y_hat=346\n","Skipping y_hat=92\n","Skipping y_hat=146\n","Skipping y_hat=282\n","Skipping y_hat=169\n","Skipping y_hat=178\n","Skipping y_hat=648\n","Skipping y_hat=86\n","Skipping y_hat=330\n","Skipping y_hat=460\n","Skipping y_hat=530\n","Skipping y_hat=113\n","Skipping y_hat=250\n","Skipping y_hat=622\n","Skipping y_hat=103\n","Skipping y_hat=811\n","Skipping y_hat=712\n","Skipping y_hat=84\n","Skipping y_hat=884\n","Skipping y_hat=89\n","Skipping y_hat=222\n","Skipping y_hat=150\n","Skipping y_hat=276\n","Skipping y_hat=41\n","Skipping y_hat=145\n","Skipping y_hat=376\n","Skipping y_hat=128\n","Skipping y_hat=20\n","Skipping y_hat=742\n","Skipping y_hat=111\n","Skipping y_hat=484\n","Skipping y_hat=87\n","Skipping y_hat=785\n","Skipping y_hat=782\n"," 96% 24/25 [00:00<00:00, 30.78it/s]Skipping y_hat=27\n","Skipping y_hat=82\n","Skipping y_hat=53\n","Skipping y_hat=72\n","100% 25/25 [00:00<00:00, 30.93it/s]\n","accuracy of 3000 examples: 2933/3000 (97.76666666666667%)\n","\n","Test Results:\n","test: 97.77%\n","\n","iter 4800: train loss 1.0054, val loss 1.0509\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.24it/s]Skipping y_hat=295\n","Skipping y_hat=618\n","Skipping y_hat=446\n","Skipping y_hat=985\n","Skipping y_hat=342\n","Skipping y_hat=107\n","Skipping y_hat=590\n","Skipping y_hat=799\n","Skipping y_hat=669\n","Skipping y_hat=276\n","Skipping y_hat=769\n","Skipping y_hat=968\n","Skipping y_hat=782\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=276\n","Skipping y_hat=622\n","Skipping y_hat=368\n","Skipping y_hat=813\n","Skipping y_hat=575\n","Skipping y_hat=776\n","Skipping y_hat=173\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=875\n","Skipping y_hat=948\n","Skipping y_hat=276\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=830\n","Skipping y_hat=396\n","Skipping y_hat=670\n","Skipping y_hat=409\n","Skipping y_hat=346\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=492\n","Skipping y_hat=309\n","Skipping y_hat=169\n","Skipping y_hat=108\n","Skipping y_hat=288\n","Skipping y_hat=86\n","Skipping y_hat=840\n","Skipping y_hat=440\n","Skipping y_hat=420\n","Skipping y_hat=113\n","Skipping y_hat=722\n","Skipping y_hat=103\n","Skipping y_hat=711\n","Skipping y_hat=669\n","Skipping y_hat=84\n","Skipping y_hat=354\n","Skipping y_hat=99\n","Skipping y_hat=922\n","Skipping y_hat=160\n","Skipping y_hat=276\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=276\n","Skipping y_hat=20\n","Skipping y_hat=772\n","Skipping y_hat=11\n","Skipping y_hat=194\n","Skipping y_hat=87\n","Skipping y_hat=745\n","Skipping y_hat=862\n"," 96% 24/25 [00:00<00:00, 30.46it/s]Skipping y_hat=68\n","Skipping y_hat=59\n","Skipping y_hat=83\n","Skipping y_hat=43\n","Skipping y_hat=82\n","100% 25/25 [00:00<00:00, 30.89it/s]\n","accuracy of 3000 examples: 2929/3000 (97.63333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 35.86it/s]\n","accuracy of 3000 examples: 2968/3000 (98.93333333333332%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 30.89it/s]Skipping y_hat=278\n","Skipping y_hat=446\n","Skipping y_hat=875\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=609\n","Skipping y_hat=469\n","Skipping y_hat=276\n","Skipping y_hat=409\n","Skipping y_hat=968\n","Skipping y_hat=762\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=622\n","Skipping y_hat=648\n","Skipping y_hat=813\n","Skipping y_hat=585\n","Skipping y_hat=746\n","Skipping y_hat=103\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=475\n","Skipping y_hat=948\n","Skipping y_hat=246\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=830\n","Skipping y_hat=896\n","Skipping y_hat=430\n","Skipping y_hat=409\n","Skipping y_hat=346\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=782\n","Skipping y_hat=770\n","Skipping y_hat=169\n","Skipping y_hat=108\n","Skipping y_hat=788\n","Skipping y_hat=86\n","Skipping y_hat=530\n","Skipping y_hat=250\n","Skipping y_hat=440\n","Skipping y_hat=113\n","Skipping y_hat=722\n","Skipping y_hat=103\n","Skipping y_hat=411\n","Skipping y_hat=689\n","Skipping y_hat=702\n","Skipping y_hat=184\n","Skipping y_hat=294\n","Skipping y_hat=89\n","Skipping y_hat=722\n","Skipping y_hat=150\n","Skipping y_hat=266\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=256\n","Skipping y_hat=148\n","Skipping y_hat=248\n","Skipping y_hat=20\n","Skipping y_hat=752\n","Skipping y_hat=31\n","Skipping y_hat=484\n","Skipping y_hat=87\n","Skipping y_hat=325\n","Skipping y_hat=762\n"," 96% 24/25 [00:00<00:00, 30.60it/s]Skipping y_hat=65\n","Skipping y_hat=40\n","Skipping y_hat=54\n","Skipping y_hat=85\n","100% 25/25 [00:00<00:00, 31.01it/s]\n","accuracy of 3000 examples: 2928/3000 (97.6%)\n","\n","Test Results:\n","test: 97.60%\n","\n","iter 4900: train loss 1.0002, val loss 1.0576\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 92% 23/25 [00:00<00:00, 31.49it/s]Skipping y_hat=795\n","Skipping y_hat=248\n","Skipping y_hat=446\n","Skipping y_hat=185\n","Skipping y_hat=342\n","Skipping y_hat=97\n","Skipping y_hat=709\n","Skipping y_hat=409\n","Skipping y_hat=276\n","Skipping y_hat=429\n","Skipping y_hat=968\n","Skipping y_hat=782\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=216\n","Skipping y_hat=712\n","Skipping y_hat=668\n","Skipping y_hat=813\n","Skipping y_hat=975\n","Skipping y_hat=776\n","Skipping y_hat=113\n","Skipping y_hat=672\n","Skipping y_hat=747\n","Skipping y_hat=575\n","Skipping y_hat=948\n","Skipping y_hat=446\n","Skipping y_hat=77\n","Skipping y_hat=810\n","Skipping y_hat=586\n","Skipping y_hat=710\n","Skipping y_hat=709\n","Skipping y_hat=346\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=982\n","Skipping y_hat=169\n","Skipping y_hat=118\n","Skipping y_hat=608\n","Skipping y_hat=86\n","Skipping y_hat=240\n","Skipping y_hat=860\n","Skipping y_hat=440\n","Skipping y_hat=113\n","Skipping y_hat=722\n","Skipping y_hat=103\n","Skipping y_hat=211\n","Skipping y_hat=689\n","Skipping y_hat=712\n","Skipping y_hat=104\n","Skipping y_hat=334\n","Skipping y_hat=9\n","Skipping y_hat=722\n","Skipping y_hat=150\n","Skipping y_hat=996\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=276\n","Skipping y_hat=162\n","Skipping y_hat=108\n","Skipping y_hat=20\n","Skipping y_hat=772\n","Skipping y_hat=11\n","Skipping y_hat=484\n","Skipping y_hat=67\n","Skipping y_hat=925\n","Skipping y_hat=782\n","Skipping y_hat=77\n","Skipping y_hat=24\n","Skipping y_hat=40\n","Skipping y_hat=54\n","Skipping y_hat=99\n","100% 25/25 [00:00<00:00, 30.85it/s]\n","accuracy of 3000 examples: 2929/3000 (97.63333333333334%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 35.19it/s]\n","accuracy of 3000 examples: 2967/3000 (98.9%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 92% 23/25 [00:00<00:00, 31.09it/s]Skipping y_hat=255\n","Skipping y_hat=668\n","Skipping y_hat=446\n","Skipping y_hat=985\n","Skipping y_hat=77\n","Skipping y_hat=590\n","Skipping y_hat=409\n","Skipping y_hat=969\n","Skipping y_hat=776\n","Skipping y_hat=609\n","Skipping y_hat=968\n","Skipping y_hat=782\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=206\n","Skipping y_hat=722\n","Skipping y_hat=748\n","Skipping y_hat=813\n","Skipping y_hat=875\n","Skipping y_hat=546\n","Skipping y_hat=173\n","Skipping y_hat=747\n","Skipping y_hat=875\n","Skipping y_hat=648\n","Skipping y_hat=276\n","Skipping y_hat=177\n","Skipping y_hat=226\n","Skipping y_hat=830\n","Skipping y_hat=696\n","Skipping y_hat=730\n","Skipping y_hat=409\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=282\n","Skipping y_hat=309\n","Skipping y_hat=169\n","Skipping y_hat=198\n","Skipping y_hat=588\n","Skipping y_hat=86\n","Skipping y_hat=330\n","Skipping y_hat=760\n","Skipping y_hat=730\n","Skipping y_hat=113\n","Skipping y_hat=522\n","Skipping y_hat=103\n","Skipping y_hat=431\n","Skipping y_hat=689\n","Skipping y_hat=712\n","Skipping y_hat=124\n","Skipping y_hat=354\n","Skipping y_hat=89\n","Skipping y_hat=722\n","Skipping y_hat=180\n","Skipping y_hat=176\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=666\n","Skipping y_hat=128\n","Skipping y_hat=20\n","Skipping y_hat=772\n","Skipping y_hat=31\n","Skipping y_hat=484\n","Skipping y_hat=87\n","Skipping y_hat=255\n","Skipping y_hat=852\n","Skipping y_hat=72\n","Skipping y_hat=55\n","Skipping y_hat=77\n","Skipping y_hat=83\n","Skipping y_hat=10\n","100% 25/25 [00:00<00:00, 30.85it/s]\n","accuracy of 3000 examples: 2930/3000 (97.66666666666667%)\n","\n","Test Results:\n","test: 97.67%\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 30.78it/s]Skipping y_hat=705\n","Skipping y_hat=208\n","Skipping y_hat=446\n","Skipping y_hat=985\n","Skipping y_hat=342\n","Skipping y_hat=97\n","Skipping y_hat=590\n","Skipping y_hat=146\n","Skipping y_hat=889\n","Skipping y_hat=409\n","Skipping y_hat=276\n","Skipping y_hat=609\n","Skipping y_hat=382\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=722\n","Skipping y_hat=698\n","Skipping y_hat=813\n","Skipping y_hat=875\n","Skipping y_hat=476\n","Skipping y_hat=103\n","Skipping y_hat=747\n","Skipping y_hat=775\n","Skipping y_hat=948\n","Skipping y_hat=376\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=430\n","Skipping y_hat=586\n","Skipping y_hat=830\n","Skipping y_hat=959\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=372\n","Skipping y_hat=770\n","Skipping y_hat=309\n","Skipping y_hat=169\n","Skipping y_hat=198\n","Skipping y_hat=448\n","Skipping y_hat=86\n","Skipping y_hat=430\n","Skipping y_hat=720\n","Skipping y_hat=440\n","Skipping y_hat=113\n","Skipping y_hat=280\n","Skipping y_hat=222\n","Skipping y_hat=103\n","Skipping y_hat=311\n","Skipping y_hat=649\n","Skipping y_hat=712\n","Skipping y_hat=124\n","Skipping y_hat=224\n","Skipping y_hat=79\n","Skipping y_hat=712\n","Skipping y_hat=150\n","Skipping y_hat=276\n","Skipping y_hat=71\n","Skipping y_hat=876\n","Skipping y_hat=148\n","Skipping y_hat=20\n","Skipping y_hat=21\n","Skipping y_hat=794\n","Skipping y_hat=87\n","Skipping y_hat=855\n","Skipping y_hat=762\n"," 96% 24/25 [00:00<00:00, 30.70it/s]Skipping y_hat=54\n","Skipping y_hat=88\n","100% 25/25 [00:00<00:00, 30.79it/s]\n","accuracy of 3000 examples: 2930/3000 (97.66666666666667%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/train_eval.txt\n","Created 27 batches\n","Max number of tokens 5.\n","100% 27/27 [00:00<00:00, 35.06it/s]\n","accuracy of 3000 examples: 2961/3000 (98.7%)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform_gold_by_computing/test/test.txt\n","Created 25 batches\n","Max number of tokens 5.\n"," 80% 20/25 [00:00<00:00, 31.02it/s]Skipping y_hat=925\n","Skipping y_hat=278\n","Skipping y_hat=446\n","Skipping y_hat=875\n","Skipping y_hat=342\n","Skipping y_hat=87\n","Skipping y_hat=590\n","Skipping y_hat=789\n","Skipping y_hat=849\n","Skipping y_hat=276\n","Skipping y_hat=829\n","Skipping y_hat=968\n","Skipping y_hat=762\n","Skipping y_hat=109\n","Skipping y_hat=180\n","Skipping y_hat=216\n","Skipping y_hat=622\n","Skipping y_hat=368\n","Skipping y_hat=813\n","Skipping y_hat=895\n","Skipping y_hat=746\n","Skipping y_hat=103\n","Skipping y_hat=747\n","Skipping y_hat=815\n","Skipping y_hat=448\n","Skipping y_hat=746\n","Skipping y_hat=77\n","Skipping y_hat=226\n","Skipping y_hat=580\n","Skipping y_hat=296\n","Skipping y_hat=510\n","Skipping y_hat=409\n","Skipping y_hat=92\n","Skipping y_hat=126\n","Skipping y_hat=892\n","Skipping y_hat=108\n","Skipping y_hat=188\n","Skipping y_hat=86\n","Skipping y_hat=430\n","Skipping y_hat=460\n","Skipping y_hat=430\n","Skipping y_hat=113\n","Skipping y_hat=240\n","Skipping y_hat=622\n","Skipping y_hat=103\n","Skipping y_hat=731\n","Skipping y_hat=689\n","Skipping y_hat=712\n","Skipping y_hat=184\n","Skipping y_hat=724\n","Skipping y_hat=89\n","Skipping y_hat=612\n","Skipping y_hat=150\n","Skipping y_hat=816\n","Skipping y_hat=81\n","Skipping y_hat=145\n","Skipping y_hat=976\n","Skipping y_hat=20\n","Skipping y_hat=772\n","Skipping y_hat=21\n","Skipping y_hat=484\n","Skipping y_hat=87\n","Skipping y_hat=745\n","Skipping y_hat=962\n"," 96% 24/25 [00:00<00:00, 30.48it/s]Skipping y_hat=28\n","Skipping y_hat=40\n","Skipping y_hat=82\n","Skipping y_hat=82\n","100% 25/25 [00:00<00:00, 30.88it/s]\n","accuracy of 3000 examples: 2931/3000 (97.7%)\n","\n","Final Test Results:\n","test: 97.70%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m2_operands_0_to_999_uniform_gold_by_computing_reverse\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/o3s160ik\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m2_operands_0_to_999_uniform/gold_by_computing_reverse_out/wandb/run-20250808_205756-o3s160ik/logs\u001b[0m\n"]}],"source":["!python train.py 2_operands_addition_reversed.txt"]},{"cell_type":"markdown","metadata":{"id":"-tPvoOxsLMt6"},"source":["## 2 Operands Output w/o Leading Digit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":110,"status":"ok","timestamp":1752241785293,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"_N5wbrGsLXd0","outputId":"b76ff57a-1aad-47eb-fcfe-56c23df9e485"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/addition\n","\u001b[0m\u001b[01;34m0_to_99999_times_1_digit\u001b[0m/\n","\u001b[01;34m0_to_six_digit_times_1_digit\u001b[0m/\n","\u001b[01;34m2_operands_0_to_999_balanced_digit_plain\u001b[0m/\n","\u001b[01;34m2_operands_0_to_999_balanced_digit_reversed\u001b[0m/\n","\u001b[01;34m2_operands_0_to_999_output_wo_leading_digit\u001b[0m/\n","\u001b[01;34m2_operands_0_to_999_their_data\u001b[0m/\n","\u001b[01;34m2_operands_0_to_999_their_training_data_only_more_eval\u001b[0m/\n","\u001b[01;34m2_operands_0_to_999_their_training_our_testing\u001b[0m/\n","\u001b[01;34m2_operands_0_to_999_uniform\u001b[0m/\n","\u001b[01;34m2_operands_0_to_999_uniform_using_new_code\u001b[0m/\n","\u001b[01;34m2_operands_3_digit_output_1000+\u001b[0m/\n","\u001b[01;34m2_operands_3_digit_output_padding\u001b[0m/\n","2_operands_addition_plain.txt\n","2_operands_addition_reversed.txt\n","2_operands_mul_plain.txt\n","2_operands_mul_reversed.txt\n","3_operands_addition.txt\n","\u001b[01;34m4_operands_0_to_999_balanced_digit\u001b[0m/\n","\u001b[01;34m4_operands_0_to_999_uniform_wo_padding\u001b[0m/\n","\u001b[01;34m4_operands_0_to_999_uniform_w_padding\u001b[0m/\n","\u001b[01;34m4_operands_3_digit_uniform_output_padding\u001b[0m/\n","4_operands_addition_3_digit.txt\n","4_operands_addition_plain.txt\n","4_operands_addition_reversed.txt\n","5_operands_addition.txt\n","\u001b[01;34m6_operands_0_to_999_balanced_digit\u001b[0m/\n","6_operands_addition_reversed.txt\n","7_operands_addition.txt\n","ckpt_iter_145100_acc.pt\n","configurator.py\n","\u001b[01;34mdata\u001b[0m/\n","eight_operand_examples.txt\n","evaluation_by_reading_groundtruth.py\n","evaluation.py\n","generate_addition_data.py\n","main_utilities.py\n","model.py\n","myStart_New.ipynb\n","original.txt\n","playTest.ipynb\n","\u001b[01;34m__pycache__\u001b[0m/\n","README.md\n","reversed_new.txt\n","reversed.txt\n","simple_test.ipynb\n","test_checkpoint.ipynb\n","train_end_padding_auto_val.py\n","train_end_padding_more_early_eval.py\n","train_end_padding_NoPE.py\n","train_end_padding.py\n","train_NoPE.py\n","train.py\n","train_with_eval_by_reading.py\n","triple_operand_examples.txt\n"]}],"source":["%cd /content/drive/MyDrive/addition\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1017,"status":"ok","timestamp":1752358570189,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"sBgrf4xGPRD6","outputId":"a9daf183-beb1-4018-a7bf-3c21fe4a735e"},"outputs":[{"name":"stdout","output_type":"stream","text":["# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 100\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '2_operands_0_to_999_output_wo_leading_digit_plain'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'algo_reasoning'\n","data_format='plain'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_plain.pt'\n","\n","# to edit: whether the result is reversed\n","reverse_c = False\n","eval_addition = True\n","\n","analysis = False\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 4\n","\n","# whether need zero‐padding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory\n","out_dir = '/content/drive/MyDrive/addition/2_operands_0_to_999_output_wo_leading_digit/plain_out'\n","\n","# to edit: training data\n","train_data_path = '/content/drive/MyDrive/addition/data/2_operands_0_to_999_output_wo_leading_digit/train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","start_train = \"FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_output_wo_leading_digit/train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_path = '/content/drive/MyDrive/addition/data/2_operands_0_to_999_output_wo_leading_digit/val.txt'\n","\n","# to edit: test data (start is just the test file)\n","start = 'FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_output_wo_leading_digit/test_plain/test.txt'\n","\n","# (optional param) to_edit: whether to enable detailed metric recording at each eval_interval\n","# test_dir: the directory storing test files\n","test_dir = '/content/drive/MyDrive/addition/data/2_operands_0_to_999_output_wo_leading_digit/test_plain'\n","eval_additional_test = True  # whether to evaluate on additional test files \n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval1 = False\n","early_eval_interval1 = 25\n","early_eval_iters1 = 3000\n","\n","more_early_eval2 = False\n","early_eval_interval2 = 5\n","early_eval_iters2 = 750"]}],"source":["%cat 2_operands_addition_plain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":114,"status":"ok","timestamp":1752357295093,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"wv_74iKwW-VE","outputId":"ded6ef3a-a983-4bd0-b70c-c0c37c1de5fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["from main_utilities import *\n","from tqdm.auto import tqdm\n","import torch\n","import numpy as np\n","import random\n","import math\n","import os\n","import pandas as pd\n","import csv\n","\n","\n","def get_abc_new(abc: str, zero_pad=False, reverse_ab=False, binary=False):\n","    \"\"\"Parse addition expression and return operands and result.\n","    Args:\n","        abc: String in format \"a+b+c...=result\"\n","        zero_pad: Whether to remove zero padding\n","        reverse_ab: Whether to reverse operands\n","        binary: Whether operands are in binary\n","    Returns:\n","        tuple: (operands_str, result_int, result_str, operation)\n","    \"\"\"\n","    if '+' in abc:\n","        operation = '+'\n","    elif '-' in abc:\n","        operation = '-' \n","    elif '*' in abc:\n","        operation = '*'\n","    else:\n","        print(f'operation not found, abc: {abc}')\n","        return None, None, None\n","\n","    # Split the input string into parts\n","    parts = abc.split('=')\n","    if len(parts) != 2:\n","        print(f'Invalid format, expected \"a+b+c...=result\", got: {abc}')\n","        return None, None, None\n","\n","    # Get the operands part (before =)\n","    operands_str = parts[0]\n","    if operands_str[0] == '$':\n","        operands_str = operands_str[1:]\n","    if operands_str.startswith('Input:\\n'):\n","        operands_str = operands_str.split('Input:\\n')[-1]\n","    if 'Target' in operands_str:\n","        operands_str = operands_str.split('\\nTarget')[0]\n","\n","    # Split into individual operands\n","    operands = [op.strip() for op in operands_str.split(operation)]\n","    \n","    # Clean up operands\n","    operands = [op.replace(' ', '') for op in operands]\n","    \n","    if binary:\n","        # Convert all operands to binary and sum\n","        result = sum(int(op, 2) for op in operands)\n","        return operands_str, result, operation\n","\n","    if zero_pad:\n","        operands = [remove_zero_pad(op) for op in operands]\n","\n","    if reverse_ab:\n","        operands = [reverse_string(op) for op in operands]\n","\n","    # --- parse the gold answer from after the '=' ---\n","    # parts[1] is whatever you put in the file after '='\n","    result_str = parts[1].strip()\n","    # if you have a trailing '$' or newline, strip it:\n","    result_str = result_str.rstrip('$').strip()\n","    # now convert to int (or float if you allow decimals)\n","    try:\n","        result_int = int(result_str)\n","    except ValueError:\n","        result_int = float(result_str)  # or handle errors however you like\n","\n","    return operands_str, result_int, result_str, operation\n","\n","_precomputed_batches = {}\n","def prepare_addition_batches(config, encode, num_digit=3, zero_pad=False, reverse_ab=False, binary=False,  data_type='binary', operator='+', data_format='plain', add_space=False, simple=False):\n","    device = config['device']\n","    test_batch_size = config['test_batch_size'] if 'test_batch_size' in config.keys() else 128\n","    start = config['start'] if 'start' in config.keys() else \"FILE:prompt/prompt_addition_pad_test_0.01.txt\"\n","    print(f\"Preparing batches from: {start}\")\n","    \n","    if start.startswith('FILE:'): # start is just the test file path\n","        with open(start[5:], 'r', encoding='utf-8') as f:\n","            lines = [line.rstrip() for line in f]\n","    else:\n","        lines = start.splitlines()\n","\n","    total = len(lines)\n","    print(f'Preparing batches for {total} examples from: {start}')\n","    \n","    # Process all lines and group by prompt length\n","    prompt_dict = {}\n","    for line in lines:\n","        # split off gold answer\n","        # e.g. line = \"123+456=579\"\n","        prompt_str = line.split('=')[0] + '='      # \"123+456=\"\n","        prompt_ids = encode(prompt_str)\n","        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","        prompt_length = x.size(1)\n","\n","        # parse out gold for evaluation later\n","        operands, result_int, result_str, op = get_abc_new(\n","            line,\n","            zero_pad=zero_pad,\n","            reverse_ab=reverse_ab,\n","            binary=binary\n","        )\n","\n","        entry = (x, prompt_length, prompt_str[0], operands,\n","                 result_int, result_str)\n","        prompt_dict.setdefault(prompt_length, []).append(entry)\n","\n","    # Construct batches of prompts\n","    batch_list = []\n","    for prompt_length in prompt_dict.keys():\n","        input_tuple_list = prompt_dict[prompt_length]\n","        for batch_idx in range(math.ceil(len(input_tuple_list)/test_batch_size)):\n","            batch_list.append(input_tuple_list[batch_idx*test_batch_size:(batch_idx+1)*test_batch_size])\n","\n","    print(f'Created {len(batch_list)} batches')\n","    \n","    # Cache the batches using a hash of the configuration\n","    config_hash = hash(frozenset({k: str(v) for k, v in config.items() if k != 'device'}.items()))\n","    batch_key = f\"{config_hash}_{data_type}_{operator}_{num_digit}_{zero_pad}_{reverse_ab}_{data_format}_{add_space}\"\n","    _precomputed_batches[batch_key] = (batch_list, total)\n","    \n","    return batch_list, total\n","\n","# Modified evaluation function that uses pre-created batches\n","def evaluate_addition_precomputed(config, model, ctx, decode, batch_list, total,\n","                                  verbose=False, num_digit=3, zero_pad=False, reverse_c=False,\n","                                  add_space=False, operator='+', verbose_correct=False, analyze=False):\n","    model.eval()\n","    device = config['device']\n","    max_new_tokens = config['max_new_tokens'] if 'max_new_tokens' in config.keys() else num_digit+2\n","    temperature = config['temperature'] if 'temperature' in config.keys() else 0.8\n","    top_k = config['top_k'] if 'top_k' in config.keys() else 200\n","\n","    if add_space:\n","        max_new_tokens = 2 * num_digit + 3\n","\n","    correct = 0\n","\n","    if analyze:\n","        # analyze various metrics\n","        error_dict = {'y': [], 'y_hat': [], 'accuracy_eps0': [], 'accuracy_eps5e-4': [],\n","                      'accuracy_eps5e-3': [], 'mse': [], 'normalized_mse': [],\n","                      'digit_wise_difference': [], 'incorrect_digit_count': []}\n","        list_not_num = []\n","        list_outlier_num = []\n","    op = operator\n","    correct_examples = []\n","    incorrect_examples = []\n","    print(f\"Max number of tokens {max_new_tokens}.\")\n","    for batch_idx in tqdm(range(len(batch_list))):\n","        batch = batch_list[batch_idx]\n","        x_list = [input_tuple[0] for input_tuple in batch]\n","        x = torch.cat(x_list, dim=0)\n","\n","        # Run generation\n","        with torch.no_grad():\n","            with ctx:\n","                eos_id = config['eos_id']\n","                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","                outcome_list = [decode(y_i.tolist()) for y_i in y]\n","\n","                for i, outcome in enumerate(outcome_list):\n","                    _, len_x, line_start, operands, result_int, result = batch[i]\n","                    \n","                    c_hat2 = outcome.split('=')[1].split('$')[0].strip()\n","\n","                    # Check correctness\n","                    if op in ['+', '-', '*']:\n","                        if result == c_hat2:\n","                            correct += 1\n","                            correct_examples.append((operands, result, outcome, c_hat2))\n","                            if verbose_correct:\n","                                print('outputs(o): ', outcome)\n","                                print(f'correct: {operands}={result}')\n","                        else:\n","                            incorrect_examples.append((operands, result, outcome, c_hat2))\n","                            if verbose:\n","                                print('outputs(x): ', outcome)\n","                                print(f'wrong  : {operands}={c_hat2}')\n","                                print(f'correct: {operands}={result}')\n","                    # Calculate metrics if analyzing\n","                    if analyze:\n","                        error_dict['y'].append(result)\n","                        error_dict['y_hat'].append(c_hat2)\n","\n","                        metric_types = ['mse', 'normalized_mse', 'digit_wise_difference', 'incorrect_digit_count']\n","                        for metric_type in metric_types:\n","                            error, list_not_num, list_outlier_num = get_error_metric(result, c_hat2, metric_type, eps=config.get('eps', 0),\n","                                                                                    list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                            error_dict[f'{metric_type}'].append(error)\n","\n","                        error, _, _ = get_error_metric(result, c_hat2, 'accuracy', eps=0, list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                        error_dict[f'accuracy_eps0'].append(error * 100)\n","                        error, _, _ = get_error_metric(result, c_hat2, 'accuracy', eps=5e-4, list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                        error_dict[f'accuracy_eps5e-4'].append(error * 100)\n","                        error, _, _ = get_error_metric(result, c_hat2, 'accuracy', eps=5e-3, list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                        error_dict[f'accuracy_eps5e-3'].append(error * 100)\n","\n","    accuracy = correct / total * 100\n","    print(f\"accuracy of {total} examples: {correct}/{total} ({accuracy}%)\")\n","\n","    accuracy_dictionary = {}\n","    if analyze:\n","        error_df = pd.DataFrame(error_dict)\n","        result_dir = config.get('result_dir')\n","        if result_dir is None:\n","            result_dir = get_results_dir(config)\n","        error_df.to_csv(os.path.join(result_dir, 'error_df.csv'), index=False)\n","\n","        error_mean_dict = {\n","            metric_type: np.nanmean(error_dict[f'{metric_type}'])\n","            for metric_type in ['accuracy_eps0', 'accuracy_eps5e-4', 'accuracy_eps5e-3',\n","                               'mse', 'normalized_mse', 'digit_wise_difference', 'incorrect_digit_count']\n","        }\n","        error_mean_dict['num_not_num'] = len(list_not_num) / len(metric_types)\n","        error_mean_dict['num_outlier_num'] = len(list_outlier_num) / len(metric_types)\n","        error_mean_dict['median_mse'] = error_df.mse.median()\n","        error_mean_dict['median_normalized_mse'] = error_df.normalized_mse.median()\n","        accuracy_dictionary.update(error_mean_dict)\n","\n","    model.train()\n","    return accuracy, accuracy_dictionary, correct_examples, incorrect_examples\n","\n","# Keep the original function for backward compatibility, but make it use the new functions\n","def evaluate_addition_batch(config, model, ctx, encode, decode, verbose=False, num_digit=3, zero_pad=False, \n","                          reverse_ab=False, reverse_c=False, data_type='binary', operator='+', \n","                          data_format='plain', add_space=False, verbose_correct=False, analyze=False):\n","    config_hash = hash(frozenset({k: str(v) for k, v in config.items() if k != 'device'}.items()))\n","    batch_key = f\"{config_hash}_{data_type}_{operator}_{num_digit}_{zero_pad}_{reverse_ab}_{data_format}_{add_space}\"\n","    \n","    if batch_key in _precomputed_batches:\n","        print(\"Using precomputed batches\")\n","        batch_list, total = _precomputed_batches[batch_key]\n","    else:\n","        print(\"Creating new batches\")\n","        batch_list, total = prepare_addition_batches(\n","            config, encode, num_digit=num_digit, zero_pad=zero_pad, reverse_ab=reverse_ab,\n","            data_type=data_type, operator=operator, data_format=data_format, add_space=add_space\n","        )\n","\n","    # Evaluate using the batches\n","    return evaluate_addition_precomputed(\n","        config, model, ctx, decode, batch_list, total, verbose=verbose,\n","        num_digit=num_digit, zero_pad=zero_pad, reverse_c=reverse_c,\n","        add_space=add_space, operator=operator, verbose_correct=verbose_correct, analyze=analyze\n","    )\n","\n","def evaluate_multiple_files(config, model, ctx, encode, decode, test_files, iter_num, result_dir,\n","                          verbose=False, num_digit=3, zero_pad=False, reverse_ab=False, reverse_c=False,\n","                          data_type='binary', operator='+', data_format='plain', add_space=False, analyze=False):\n","    \"\"\"\n","    Evaluate model on multiple test files and store results.\n","    Args:\n","        test_files: List of test file paths\n","        iter_num: Current iteration number\n","        result_dir: Directory to store results\n","    Returns:\n","        dict: Dictionary containing accuracies for each test file\n","    \"\"\"\n","    results = {}\n","    \n","    for test_file in test_files:\n","        # Get test file name without path and extension\n","        test_name = os.path.splitext(os.path.basename(test_file))[0]\n","        \n","        # Set the current test file as start\n","        config['start'] = f\"FILE:{test_file}\"\n","        \n","        # Run evaluation\n","        accuracy, metrics, correct, incorrect = evaluate_addition_batch(\n","            config, model, ctx, encode=encode, decode=decode,\n","            verbose=verbose, num_digit=num_digit, zero_pad=zero_pad,\n","            reverse_ab=reverse_ab, reverse_c=reverse_c,\n","            data_type=data_type, operator=operator,\n","            data_format=data_format, analyze=analyze\n","        )\n","        \n","        results[test_name] = accuracy\n","        \n","        # Path for this test file's results\n","        results_file = os.path.join(result_dir, f'{test_name}_results.csv')\n","        \n","        # Combine correct and incorrect examples and sort by operands to maintain consistent order\n","        all_examples = correct + incorrect\n","        all_examples.sort(key=lambda x: x[0])  # Sort by operands\n","\n","        if reverse_c:\n","            reversed_actuals = [ex[1] for ex in all_examples]\n","            reversed_preds = [ex[3] for ex in all_examples]\n","\n","            normal_actuals = [s[::-1] for s in reversed_actuals]\n","            normal_preds = [s[::-1] for s in reversed_preds]\n","        \n","        # Create new DataFrame with operands and actual results\n","        new_df = pd.DataFrame({\n","            'operands': [ex[0] for ex in all_examples],\n","            'actual': normal_actuals if reverse_c else [ex[1] for ex in all_examples],\n","            f'pred_iter_{iter_num}': normal_preds if reverse_c else [ex[3] for ex in all_examples],\n","        })\n","        \n","        # Read existing results if file exists and merge\n","        if os.path.exists(results_file):\n","            old_df = pd.read_csv(results_file)\n","            # # Merge based on operands, keeping all predictions\n","            # if 'operands' in old_df.columns:\n","            #     merged_df = pd.merge(old_df, new_df, on=['operands', 'actual'], how='outer')\n","            # else:\n","            #     merged_df = new_df\n","            # ── Normalize keys so they truly match ──\n","            for df in (old_df, new_df):\n","                # strip whitespace from the operands strings\n","                df['operands'] = df['operands'].str.strip()\n","                # ensure actual is an integer\n","                df['actual']   = df['actual'].astype(int)\n","\n","            merged_df = pd.merge(\n","                old_df, new_df,\n","                on=['operands', 'actual'],\n","                how='outer'\n","            )\n","        else:\n","            merged_df = new_df\n","        \n","        # Save results\n","        merged_df.to_csv(results_file, index=False)\n","        \n","        # Save accuracy separately in a summary file\n","        accuracy_file = os.path.join(result_dir, f'{test_name}_accuracy.csv')\n","        if os.path.exists(accuracy_file):\n","            acc_df = pd.read_csv(accuracy_file)\n","        else:\n","            acc_df = pd.DataFrame(columns=['iteration', 'accuracy'])\n","        \n","        # Add new accuracy\n","        new_row = pd.DataFrame({'iteration': [iter_num], 'accuracy': [accuracy]})\n","        acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","        acc_df.to_csv(accuracy_file, index=False)\n","    \n","    return results"]}],"source":["%cat ./evaluation_by_reading_groundtruth.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":663822,"status":"ok","timestamp":1752359238931,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"JEVVotiiLQj6","outputId":"8321675a-c784-4009-8f60-77b524a4d62a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=476\n","Skipping y_hat=476\n","Skipping y_hat=476\n","Skipping y_hat=476\n","Skipping y_hat=476\n","Skipping y_hat=476\n","Skipping y_hat=476\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=819\n","Skipping y_hat=819\n","Skipping y_hat=819\n","Skipping y_hat=819\n","Skipping y_hat=819\n","Skipping y_hat=819\n","Skipping y_hat=819\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=489\n","Skipping y_hat=489\n","Skipping y_hat=489\n","Skipping y_hat=489\n","Skipping y_hat=489\n","Skipping y_hat=489\n","Skipping y_hat=489\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n"," 75% 18/24 [00:02<00:00,  7.74it/s]Skipping y_hat=339\n","Skipping y_hat=339\n","Skipping y_hat=339\n","Skipping y_hat=339\n","Skipping y_hat=339\n","Skipping y_hat=339\n","Skipping y_hat=339\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=625\n","Skipping y_hat=625\n","Skipping y_hat=625\n","Skipping y_hat=625\n","Skipping y_hat=625\n","Skipping y_hat=625\n","Skipping y_hat=625\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=234\n","Skipping y_hat=234\n","Skipping y_hat=234\n","Skipping y_hat=234\n","Skipping y_hat=234\n","Skipping y_hat=234\n","Skipping y_hat=234\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=841\n","Skipping y_hat=841\n","Skipping y_hat=841\n","Skipping y_hat=841\n","Skipping y_hat=841\n","Skipping y_hat=841\n","Skipping y_hat=841\n","Skipping y_hat=524\n","Skipping y_hat=524\n","Skipping y_hat=524\n","Skipping y_hat=524\n","Skipping y_hat=524\n","Skipping y_hat=524\n","Skipping y_hat=524\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=439\n","Skipping y_hat=439\n","Skipping y_hat=439\n","Skipping y_hat=439\n","Skipping y_hat=439\n","Skipping y_hat=439\n","Skipping y_hat=439\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=951\n","Skipping y_hat=951\n","Skipping y_hat=951\n","Skipping y_hat=951\n","Skipping y_hat=951\n","Skipping y_hat=951\n","Skipping y_hat=951\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=348\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=176\n","Skipping y_hat=176\n","Skipping y_hat=176\n","Skipping y_hat=176\n","Skipping y_hat=176\n","Skipping y_hat=176\n","Skipping y_hat=176\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=405\n"," 79% 19/24 [00:02<00:00,  8.13it/s]Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=382\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=922\n","Skipping y_hat=922\n","Skipping y_hat=922\n","Skipping y_hat=922\n","Skipping y_hat=922\n","Skipping y_hat=922\n","Skipping y_hat=922\n","Skipping y_hat=208\n","Skipping y_hat=208\n","Skipping y_hat=208\n","Skipping y_hat=208\n","Skipping y_hat=208\n","Skipping y_hat=208\n","Skipping y_hat=208\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=562\n","Skipping y_hat=562\n","Skipping y_hat=562\n","Skipping y_hat=562\n","Skipping y_hat=562\n","Skipping y_hat=562\n","Skipping y_hat=562\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=298\n","Skipping y_hat=298\n","Skipping y_hat=298\n","Skipping y_hat=298\n","Skipping y_hat=298\n","Skipping y_hat=298\n","Skipping y_hat=298\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=816\n","Skipping y_hat=816\n","Skipping y_hat=816\n","Skipping y_hat=816\n","Skipping y_hat=816\n","Skipping y_hat=816\n","Skipping y_hat=816\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=794\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=198\n","Skipping y_hat=198\n","Skipping y_hat=198\n","Skipping y_hat=198\n","Skipping y_hat=198\n","Skipping y_hat=198\n","Skipping y_hat=198\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=389\n","Skipping y_hat=389\n","Skipping y_hat=389\n","Skipping y_hat=389\n","Skipping y_hat=389\n","Skipping y_hat=389\n","Skipping y_hat=389\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=396\n","Skipping y_hat=396\n","Skipping y_hat=396\n","Skipping y_hat=396\n","Skipping y_hat=396\n","Skipping y_hat=396\n","Skipping y_hat=396\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=329\n","Skipping y_hat=329\n","Skipping y_hat=329\n","Skipping y_hat=329\n","Skipping y_hat=329\n","Skipping y_hat=329\n","Skipping y_hat=329\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=051\n","Skipping y_hat=051\n","Skipping y_hat=051\n","Skipping y_hat=051\n","Skipping y_hat=051\n","Skipping y_hat=051\n","Skipping y_hat=051\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=765\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=163\n","Skipping y_hat=163\n","Skipping y_hat=163\n","Skipping y_hat=163\n","Skipping y_hat=163\n","Skipping y_hat=163\n","Skipping y_hat=163\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=910\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=044\n","Skipping y_hat=044\n","Skipping y_hat=044\n","Skipping y_hat=044\n","Skipping y_hat=044\n","Skipping y_hat=044\n","Skipping y_hat=044\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=705\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n"," 83% 20/24 [00:02<00:00,  8.55it/s]Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=510\n","Skipping y_hat=510\n","Skipping y_hat=510\n","Skipping y_hat=510\n","Skipping y_hat=510\n","Skipping y_hat=510\n","Skipping y_hat=510\n","Skipping y_hat=617\n","Skipping y_hat=617\n","Skipping y_hat=617\n","Skipping y_hat=617\n","Skipping y_hat=617\n","Skipping y_hat=617\n","Skipping y_hat=617\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=299\n","Skipping y_hat=299\n","Skipping y_hat=299\n","Skipping y_hat=299\n","Skipping y_hat=299\n","Skipping y_hat=299\n","Skipping y_hat=299\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=437\n","Skipping y_hat=437\n","Skipping y_hat=437\n","Skipping y_hat=437\n","Skipping y_hat=437\n","Skipping y_hat=437\n","Skipping y_hat=437\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=174\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=224\n","Skipping y_hat=224\n","Skipping y_hat=224\n","Skipping y_hat=224\n","Skipping y_hat=224\n","Skipping y_hat=224\n","Skipping y_hat=224\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=587\n","Skipping y_hat=587\n","Skipping y_hat=587\n","Skipping y_hat=587\n","Skipping y_hat=587\n","Skipping y_hat=587\n","Skipping y_hat=587\n","Skipping y_hat=108\n","Skipping y_hat=108\n","Skipping y_hat=108\n","Skipping y_hat=108\n","Skipping y_hat=108\n","Skipping y_hat=108\n","Skipping y_hat=108\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=588\n","Skipping y_hat=588\n","Skipping y_hat=588\n","Skipping y_hat=588\n","Skipping y_hat=588\n","Skipping y_hat=588\n","Skipping y_hat=588\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=276\n","Skipping y_hat=276\n","Skipping y_hat=276\n","Skipping y_hat=276\n","Skipping y_hat=276\n","Skipping y_hat=276\n","Skipping y_hat=276\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=648\n","Skipping y_hat=648\n","Skipping y_hat=648\n","Skipping y_hat=648\n","Skipping y_hat=648\n","Skipping y_hat=648\n","Skipping y_hat=648\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=415\n","Skipping y_hat=415\n","Skipping y_hat=415\n","Skipping y_hat=415\n","Skipping y_hat=415\n","Skipping y_hat=415\n","Skipping y_hat=415\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=855\n","Skipping y_hat=855\n","Skipping y_hat=855\n","Skipping y_hat=855\n","Skipping y_hat=855\n","Skipping y_hat=855\n","Skipping y_hat=855\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=311\n","Skipping y_hat=311\n","Skipping y_hat=311\n","Skipping y_hat=311\n","Skipping y_hat=311\n","Skipping y_hat=311\n","Skipping y_hat=311\n","Skipping y_hat=782\n","Skipping y_hat=782\n","Skipping y_hat=782\n","Skipping y_hat=782\n","Skipping y_hat=782\n","Skipping y_hat=782\n","Skipping y_hat=782\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=260\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=235\n","Skipping y_hat=235\n","Skipping y_hat=235\n","Skipping y_hat=235\n","Skipping y_hat=235\n","Skipping y_hat=235\n","Skipping y_hat=235\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=168\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=746\n","Skipping y_hat=746\n","Skipping y_hat=746\n","Skipping y_hat=746\n","Skipping y_hat=746\n","Skipping y_hat=746\n","Skipping y_hat=746\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n"," 88% 21/24 [00:02<00:00,  8.66it/s]Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=244\n","Skipping y_hat=244\n","Skipping y_hat=244\n","Skipping y_hat=244\n","Skipping y_hat=244\n","Skipping y_hat=244\n","Skipping y_hat=244\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=839\n","Skipping y_hat=839\n","Skipping y_hat=839\n","Skipping y_hat=839\n","Skipping y_hat=839\n","Skipping y_hat=839\n","Skipping y_hat=839\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=024\n","Skipping y_hat=024\n","Skipping y_hat=024\n","Skipping y_hat=024\n","Skipping y_hat=024\n","Skipping y_hat=024\n","Skipping y_hat=024\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=758\n","Skipping y_hat=758\n","Skipping y_hat=758\n","Skipping y_hat=758\n","Skipping y_hat=758\n","Skipping y_hat=758\n","Skipping y_hat=758\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=022\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=005\n","Skipping y_hat=005\n","Skipping y_hat=005\n","Skipping y_hat=005\n","Skipping y_hat=005\n","Skipping y_hat=005\n","Skipping y_hat=005\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=541\n","Skipping y_hat=541\n","Skipping y_hat=541\n","Skipping y_hat=541\n","Skipping y_hat=541\n","Skipping y_hat=541\n","Skipping y_hat=541\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=842\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=774\n","Skipping y_hat=774\n","Skipping y_hat=774\n","Skipping y_hat=774\n","Skipping y_hat=774\n","Skipping y_hat=774\n","Skipping y_hat=774\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=328\n","Skipping y_hat=328\n","Skipping y_hat=328\n","Skipping y_hat=328\n","Skipping y_hat=328\n","Skipping y_hat=328\n","Skipping y_hat=328\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=002\n","Skipping y_hat=002\n","Skipping y_hat=002\n","Skipping y_hat=002\n","Skipping y_hat=002\n","Skipping y_hat=002\n","Skipping y_hat=002\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=277\n","Skipping y_hat=277\n","Skipping y_hat=277\n","Skipping y_hat=277\n","Skipping y_hat=277\n","Skipping y_hat=277\n","Skipping y_hat=277\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=004\n","Skipping y_hat=450\n","Skipping y_hat=450\n","Skipping y_hat=450\n","Skipping y_hat=450\n","Skipping y_hat=450\n","Skipping y_hat=450\n","Skipping y_hat=450\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=753\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=722\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=351\n","Skipping y_hat=351\n","Skipping y_hat=351\n","Skipping y_hat=351\n","Skipping y_hat=351\n","Skipping y_hat=351\n","Skipping y_hat=351\n","Skipping y_hat=869\n","Skipping y_hat=869\n","Skipping y_hat=869\n","Skipping y_hat=869\n","Skipping y_hat=869\n","Skipping y_hat=869\n","Skipping y_hat=869\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n"," 92% 22/24 [00:02<00:00,  8.78it/s]Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=628\n","Skipping y_hat=628\n","Skipping y_hat=628\n","Skipping y_hat=628\n","Skipping y_hat=628\n","Skipping y_hat=628\n","Skipping y_hat=628\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=605\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=438\n","Skipping y_hat=438\n","Skipping y_hat=438\n","Skipping y_hat=438\n","Skipping y_hat=438\n","Skipping y_hat=438\n","Skipping y_hat=438\n","Skipping y_hat=072\n","Skipping y_hat=072\n","Skipping y_hat=072\n","Skipping y_hat=072\n","Skipping y_hat=072\n","Skipping y_hat=072\n","Skipping y_hat=072\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=412\n","Skipping y_hat=412\n","Skipping y_hat=412\n","Skipping y_hat=412\n","Skipping y_hat=412\n","Skipping y_hat=412\n","Skipping y_hat=412\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=254\n","Skipping y_hat=254\n","Skipping y_hat=254\n","Skipping y_hat=254\n","Skipping y_hat=254\n","Skipping y_hat=254\n","Skipping y_hat=254\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=350\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=563\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=873\n","Skipping y_hat=873\n","Skipping y_hat=873\n","Skipping y_hat=873\n","Skipping y_hat=873\n","Skipping y_hat=873\n","Skipping y_hat=873\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=144\n","Skipping y_hat=418\n","Skipping y_hat=418\n","Skipping y_hat=418\n","Skipping y_hat=418\n","Skipping y_hat=418\n","Skipping y_hat=418\n","Skipping y_hat=418\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=527\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=482\n","Skipping y_hat=482\n","Skipping y_hat=482\n","Skipping y_hat=482\n","Skipping y_hat=482\n","Skipping y_hat=482\n","Skipping y_hat=482\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=762\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=650\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=134\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=453\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=627\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=678\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=845\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=920\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=783\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=386\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=405\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=931\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=824\n","Skipping y_hat=824\n","Skipping y_hat=824\n","Skipping y_hat=824\n","Skipping y_hat=824\n","Skipping y_hat=824\n","Skipping y_hat=824\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=841\n","Skipping y_hat=841\n","Skipping y_hat=841\n","Skipping y_hat=841\n","Skipping y_hat=841\n","Skipping y_hat=841\n","Skipping y_hat=841\n"," 96% 23/24 [00:02<00:00,  8.96it/s]Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=499\n","Skipping y_hat=499\n","Skipping y_hat=499\n","Skipping y_hat=499\n","Skipping y_hat=499\n","Skipping y_hat=499\n","Skipping y_hat=499\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=508\n","Skipping y_hat=508\n","Skipping y_hat=508\n","Skipping y_hat=508\n","Skipping y_hat=508\n","Skipping y_hat=508\n","Skipping y_hat=508\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=538\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=875\n","Skipping y_hat=875\n","Skipping y_hat=875\n","Skipping y_hat=875\n","Skipping y_hat=875\n","Skipping y_hat=875\n","Skipping y_hat=875\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=047\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=088\n","Skipping y_hat=088\n","Skipping y_hat=088\n","Skipping y_hat=088\n","Skipping y_hat=088\n","Skipping y_hat=088\n","Skipping y_hat=088\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=911\n","Skipping y_hat=911\n","Skipping y_hat=911\n","Skipping y_hat=911\n","Skipping y_hat=911\n","Skipping y_hat=911\n","Skipping y_hat=911\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=926\n","Skipping y_hat=926\n","Skipping y_hat=926\n","Skipping y_hat=926\n","Skipping y_hat=926\n","Skipping y_hat=926\n","Skipping y_hat=926\n","100% 24/24 [00:02<00:00,  8.02it/s]\n","accuracy of 3000 examples: 2863/3000 (95.43333333333334%)\n","\n","Final Test Results:\n","test: 95.43%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m2_operands_0_to_999_output_wo_leading_digit_plain\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/kz8lhsbs\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m2_operands_0_to_999_output_wo_leading_digit/plain_out/wandb/run-20250712_221619-kz8lhsbs/logs\u001b[0m\n"]}],"source":["!python train_with_eval_by_reading.py 2_operands_addition_plain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":772830,"status":"ok","timestamp":1752354766864,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"-r_qlw0OXXCh","outputId":"5278b7c5-ec3d-47cd-d7ca-f1b7642a8aef"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=674\n","Skipping y_hat=674\n","Skipping y_hat=674\n","Skipping y_hat=674\n","Skipping y_hat=674\n","Skipping y_hat=674\n","Skipping y_hat=674\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=701\n","Skipping y_hat=701\n","Skipping y_hat=701\n","Skipping y_hat=701\n","Skipping y_hat=701\n","Skipping y_hat=701\n","Skipping y_hat=701\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n"," 75% 18/24 [00:02<00:00,  8.33it/s]Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=049\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=886\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=776\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=443\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=426\n","Skipping y_hat=426\n","Skipping y_hat=426\n","Skipping y_hat=426\n","Skipping y_hat=426\n","Skipping y_hat=426\n","Skipping y_hat=426\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=205\n","Skipping y_hat=205\n","Skipping y_hat=205\n","Skipping y_hat=205\n","Skipping y_hat=205\n","Skipping y_hat=205\n","Skipping y_hat=205\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=927\n","Skipping y_hat=927\n","Skipping y_hat=927\n","Skipping y_hat=927\n","Skipping y_hat=927\n","Skipping y_hat=927\n","Skipping y_hat=927\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=934\n","Skipping y_hat=934\n","Skipping y_hat=934\n","Skipping y_hat=934\n","Skipping y_hat=934\n","Skipping y_hat=934\n","Skipping y_hat=934\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=012\n","Skipping y_hat=164\n","Skipping y_hat=164\n","Skipping y_hat=164\n","Skipping y_hat=164\n","Skipping y_hat=164\n","Skipping y_hat=164\n","Skipping y_hat=164\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=008\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=269\n","Skipping y_hat=269\n","Skipping y_hat=269\n","Skipping y_hat=269\n","Skipping y_hat=269\n","Skipping y_hat=269\n","Skipping y_hat=269\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=603\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=038\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=661\n","Skipping y_hat=457\n","Skipping y_hat=457\n","Skipping y_hat=457\n","Skipping y_hat=457\n","Skipping y_hat=457\n","Skipping y_hat=457\n","Skipping y_hat=457\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=360\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=760\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=067\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=778\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=484\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n"," 79% 19/24 [00:02<00:00,  8.25it/s]Skipping y_hat=036\n","Skipping y_hat=036\n","Skipping y_hat=036\n","Skipping y_hat=036\n","Skipping y_hat=036\n","Skipping y_hat=036\n","Skipping y_hat=036\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=283\n","Skipping y_hat=195\n","Skipping y_hat=195\n","Skipping y_hat=195\n","Skipping y_hat=195\n","Skipping y_hat=195\n","Skipping y_hat=195\n","Skipping y_hat=195\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=496\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=877\n","Skipping y_hat=892\n","Skipping y_hat=892\n","Skipping y_hat=892\n","Skipping y_hat=892\n","Skipping y_hat=892\n","Skipping y_hat=892\n","Skipping y_hat=892\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=313\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=455\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=852\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=891\n","Skipping y_hat=891\n","Skipping y_hat=891\n","Skipping y_hat=891\n","Skipping y_hat=891\n","Skipping y_hat=891\n","Skipping y_hat=891\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=533\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=789\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=346\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=723\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=358\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=693\n","Skipping y_hat=693\n","Skipping y_hat=693\n","Skipping y_hat=693\n","Skipping y_hat=693\n","Skipping y_hat=693\n","Skipping y_hat=693\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=923\n","Skipping y_hat=923\n","Skipping y_hat=923\n","Skipping y_hat=923\n","Skipping y_hat=923\n","Skipping y_hat=923\n","Skipping y_hat=923\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=160\n","Skipping y_hat=160\n","Skipping y_hat=160\n","Skipping y_hat=160\n","Skipping y_hat=160\n","Skipping y_hat=160\n","Skipping y_hat=160\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=166\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=150\n","Skipping y_hat=150\n","Skipping y_hat=150\n","Skipping y_hat=150\n","Skipping y_hat=150\n","Skipping y_hat=150\n","Skipping y_hat=150\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=735\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=157\n","Skipping y_hat=157\n","Skipping y_hat=157\n","Skipping y_hat=157\n","Skipping y_hat=157\n","Skipping y_hat=157\n","Skipping y_hat=157\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=657\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=019\n","Skipping y_hat=230\n","Skipping y_hat=230\n","Skipping y_hat=230\n","Skipping y_hat=230\n","Skipping y_hat=230\n","Skipping y_hat=230\n","Skipping y_hat=230\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=606\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=849\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=607\n","Skipping y_hat=870\n","Skipping y_hat=870\n","Skipping y_hat=870\n","Skipping y_hat=870\n","Skipping y_hat=870\n","Skipping y_hat=870\n","Skipping y_hat=870\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=976\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=096\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=932\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=507\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=699\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=743\n","Skipping y_hat=743\n","Skipping y_hat=743\n","Skipping y_hat=743\n","Skipping y_hat=743\n","Skipping y_hat=743\n","Skipping y_hat=743\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=687\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=499\n","Skipping y_hat=499\n","Skipping y_hat=499\n","Skipping y_hat=499\n","Skipping y_hat=499\n","Skipping y_hat=499\n","Skipping y_hat=499\n"," 83% 20/24 [00:02<00:00,  8.32it/s]Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=268\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=349\n","Skipping y_hat=349\n","Skipping y_hat=349\n","Skipping y_hat=349\n","Skipping y_hat=349\n","Skipping y_hat=349\n","Skipping y_hat=349\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=595\n","Skipping y_hat=595\n","Skipping y_hat=595\n","Skipping y_hat=595\n","Skipping y_hat=595\n","Skipping y_hat=595\n","Skipping y_hat=595\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=471\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=738\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=098\n","Skipping y_hat=098\n","Skipping y_hat=098\n","Skipping y_hat=098\n","Skipping y_hat=098\n","Skipping y_hat=098\n","Skipping y_hat=098\n","Skipping y_hat=380\n","Skipping y_hat=380\n","Skipping y_hat=380\n","Skipping y_hat=380\n","Skipping y_hat=380\n","Skipping y_hat=380\n","Skipping y_hat=380\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=014\n","Skipping y_hat=014\n","Skipping y_hat=014\n","Skipping y_hat=014\n","Skipping y_hat=014\n","Skipping y_hat=014\n","Skipping y_hat=014\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=336\n","Skipping y_hat=336\n","Skipping y_hat=336\n","Skipping y_hat=336\n","Skipping y_hat=336\n","Skipping y_hat=336\n","Skipping y_hat=336\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=094\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=985\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=522\n","Skipping y_hat=522\n","Skipping y_hat=522\n","Skipping y_hat=522\n","Skipping y_hat=522\n","Skipping y_hat=522\n","Skipping y_hat=522\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=155\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=023\n","Skipping y_hat=514\n","Skipping y_hat=514\n","Skipping y_hat=514\n","Skipping y_hat=514\n","Skipping y_hat=514\n","Skipping y_hat=514\n","Skipping y_hat=514\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=312\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=654\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=681\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=558\n","Skipping y_hat=558\n","Skipping y_hat=558\n","Skipping y_hat=558\n","Skipping y_hat=558\n","Skipping y_hat=558\n","Skipping y_hat=558\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=406\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=113\n","Skipping y_hat=113\n","Skipping y_hat=113\n","Skipping y_hat=113\n","Skipping y_hat=113\n","Skipping y_hat=113\n","Skipping y_hat=113\n","Skipping y_hat=287\n","Skipping y_hat=287\n","Skipping y_hat=287\n","Skipping y_hat=287\n","Skipping y_hat=287\n","Skipping y_hat=287\n","Skipping y_hat=287\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=695\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=290\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=062\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=829\n","Skipping y_hat=532\n","Skipping y_hat=532\n","Skipping y_hat=532\n","Skipping y_hat=532\n","Skipping y_hat=532\n","Skipping y_hat=532\n","Skipping y_hat=532\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=787\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=543\n","Skipping y_hat=689\n","Skipping y_hat=689\n","Skipping y_hat=689\n","Skipping y_hat=689\n","Skipping y_hat=689\n","Skipping y_hat=689\n","Skipping y_hat=689\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=540\n","Skipping y_hat=540\n","Skipping y_hat=540\n","Skipping y_hat=540\n","Skipping y_hat=540\n","Skipping y_hat=540\n","Skipping y_hat=540\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=228\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=710\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=888\n","Skipping y_hat=547\n","Skipping y_hat=547\n","Skipping y_hat=547\n","Skipping y_hat=547\n","Skipping y_hat=547\n","Skipping y_hat=547\n","Skipping y_hat=547\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=039\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=057\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n"," 88% 21/24 [00:02<00:00,  8.20it/s]Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=432\n","Skipping y_hat=432\n","Skipping y_hat=432\n","Skipping y_hat=432\n","Skipping y_hat=432\n","Skipping y_hat=432\n","Skipping y_hat=432\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=938\n","Skipping y_hat=938\n","Skipping y_hat=938\n","Skipping y_hat=938\n","Skipping y_hat=938\n","Skipping y_hat=938\n","Skipping y_hat=938\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=262\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=388\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=246\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=344\n","Skipping y_hat=780\n","Skipping y_hat=780\n","Skipping y_hat=780\n","Skipping y_hat=780\n","Skipping y_hat=780\n","Skipping y_hat=780\n","Skipping y_hat=780\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=323\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=209\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=857\n","Skipping y_hat=857\n","Skipping y_hat=857\n","Skipping y_hat=857\n","Skipping y_hat=857\n","Skipping y_hat=857\n","Skipping y_hat=857\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=362\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=935\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=256\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=811\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=872\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=182\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=832\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=596\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=248\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=679\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=001\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=397\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=940\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=295\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=090\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=715\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=592\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=947\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=429\n","Skipping y_hat=429\n","Skipping y_hat=429\n","Skipping y_hat=429\n","Skipping y_hat=429\n","Skipping y_hat=429\n","Skipping y_hat=429\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=227\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=634\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=153\n","Skipping y_hat=153\n","Skipping y_hat=153\n","Skipping y_hat=153\n","Skipping y_hat=153\n","Skipping y_hat=153\n","Skipping y_hat=153\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=955\n","Skipping y_hat=955\n","Skipping y_hat=955\n","Skipping y_hat=955\n","Skipping y_hat=955\n","Skipping y_hat=955\n","Skipping y_hat=955\n"," 92% 22/24 [00:02<00:00,  8.12it/s]Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=498\n","Skipping y_hat=498\n","Skipping y_hat=498\n","Skipping y_hat=498\n","Skipping y_hat=498\n","Skipping y_hat=498\n","Skipping y_hat=498\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=948\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=173\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=506\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=122\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=767\n","Skipping y_hat=834\n","Skipping y_hat=834\n","Skipping y_hat=834\n","Skipping y_hat=834\n","Skipping y_hat=834\n","Skipping y_hat=834\n","Skipping y_hat=834\n","Skipping y_hat=270\n","Skipping y_hat=270\n","Skipping y_hat=270\n","Skipping y_hat=270\n","Skipping y_hat=270\n","Skipping y_hat=270\n","Skipping y_hat=270\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=186\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=214\n","Skipping y_hat=214\n","Skipping y_hat=214\n","Skipping y_hat=214\n","Skipping y_hat=214\n","Skipping y_hat=214\n","Skipping y_hat=214\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=189\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=641\n","Skipping y_hat=641\n","Skipping y_hat=641\n","Skipping y_hat=641\n","Skipping y_hat=641\n","Skipping y_hat=641\n","Skipping y_hat=641\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=053\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=492\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=242\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=737\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=542\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=136\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=520\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=238\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=171\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=614\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=028\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=060\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=441\n","Skipping y_hat=441\n","Skipping y_hat=441\n","Skipping y_hat=441\n","Skipping y_hat=441\n","Skipping y_hat=441\n","Skipping y_hat=441\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=725\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=675\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=284\n","Skipping y_hat=284\n","Skipping y_hat=284\n","Skipping y_hat=284\n","Skipping y_hat=284\n","Skipping y_hat=284\n","Skipping y_hat=284\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=560\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=707\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=074\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=056\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=431\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=325\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=354\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=478\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=643\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=893\n","Skipping y_hat=893\n","Skipping y_hat=893\n","Skipping y_hat=893\n","Skipping y_hat=893\n","Skipping y_hat=893\n","Skipping y_hat=893\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=703\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=353\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=364\n","Skipping y_hat=364\n","Skipping y_hat=364\n","Skipping y_hat=364\n","Skipping y_hat=364\n","Skipping y_hat=364\n","Skipping y_hat=364\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=548\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=029\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=385\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=375\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=387\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=806\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=343\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=965\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=138\n","Skipping y_hat=322\n","Skipping y_hat=322\n","Skipping y_hat=322\n","Skipping y_hat=322\n","Skipping y_hat=322\n","Skipping y_hat=322\n","Skipping y_hat=322\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=464\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=139\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=188\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=101\n","Skipping y_hat=428\n","Skipping y_hat=428\n","Skipping y_hat=428\n","Skipping y_hat=428\n","Skipping y_hat=428\n","Skipping y_hat=428\n","Skipping y_hat=428\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=791\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=992\n","Skipping y_hat=016\n","Skipping y_hat=016\n","Skipping y_hat=016\n","Skipping y_hat=016\n","Skipping y_hat=016\n","Skipping y_hat=016\n","Skipping y_hat=016\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n"," 96% 23/24 [00:02<00:00,  8.10it/s]Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=467\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=479\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=652\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=799\n","Skipping y_hat=799\n","Skipping y_hat=799\n","Skipping y_hat=799\n","Skipping y_hat=799\n","Skipping y_hat=799\n","Skipping y_hat=799\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=334\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=521\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=381\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=080\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=576\n","Skipping y_hat=863\n","Skipping y_hat=863\n","Skipping y_hat=863\n","Skipping y_hat=863\n","Skipping y_hat=863\n","Skipping y_hat=863\n","Skipping y_hat=863\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=073\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=756\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=433\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=721\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=865\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=698\n","Skipping y_hat=752\n","Skipping y_hat=752\n","Skipping y_hat=752\n","Skipping y_hat=752\n","Skipping y_hat=752\n","Skipping y_hat=752\n","Skipping y_hat=752\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=304\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=226\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=363\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=629\n","Skipping y_hat=629\n","Skipping y_hat=629\n","Skipping y_hat=629\n","Skipping y_hat=629\n","Skipping y_hat=629\n","Skipping y_hat=629\n","100% 24/24 [00:02<00:00,  8.52it/s]\n","accuracy of 3000 examples: 2999/3000 (99.96666666666667%)\n","\n","Final Test Results:\n","test_reverse: 99.97%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m2_operands_0_to_999_output_wo_leading_digit_reverse\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/tltft0a1\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m2_operands_0_to_999_output_wo_leading_digit/reverse_out/wandb/run-20250712_205957-tltft0a1/logs\u001b[0m\n"]}],"source":["!python train_with_eval_by_reading.py 2_operands_addition_reversed.txt"]},{"cell_type":"markdown","metadata":{"id":"zsvKH8oIkfzz"},"source":["## 4 Operands 0-to-999 Uniform with Padding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":116,"status":"ok","timestamp":1752165914348,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"7Pnfk-7YEKW5","outputId":"9609cae5-dc22-4e97-d268-30b319c4cce2"},"outputs":[{"name":"stdout","output_type":"stream","text":["shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n","cat: 4_operands_addition_reversed.txt: Transport endpoint is not connected\n"]}],"source":["%cat 4_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1632842,"status":"ok","timestamp":1751923973334,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"pTpa-h3Pkr7X","outputId":"5fb14373-2747-4cd4-df5a-57f2eb90506c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=1810\n","Skipping y_hat=1810\n","Skipping y_hat=1810\n","Skipping y_hat=1810\n","Skipping y_hat=1810\n","Skipping y_hat=1810\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=2135\n","Skipping y_hat=2135\n","Skipping y_hat=2135\n","Skipping y_hat=2135\n","Skipping y_hat=2135\n","Skipping y_hat=2135\n","Skipping y_hat=2135\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=2491\n","Skipping y_hat=2491\n","Skipping y_hat=2491\n","Skipping y_hat=2491\n","Skipping y_hat=2491\n","Skipping y_hat=2491\n","Skipping y_hat=2491\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=2312\n","Skipping y_hat=2312\n","Skipping y_hat=2312\n","Skipping y_hat=2312\n","Skipping y_hat=2312\n","Skipping y_hat=2312\n","Skipping y_hat=2312\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=2497\n","Skipping y_hat=2497\n","Skipping y_hat=2497\n","Skipping y_hat=2497\n","Skipping y_hat=2497\n","Skipping y_hat=2497\n","Skipping y_hat=2497\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=2785\n","Skipping y_hat=2785\n","Skipping y_hat=2785\n","Skipping y_hat=2785\n","Skipping y_hat=2785\n","Skipping y_hat=2785\n","Skipping y_hat=2785\n","Skipping y_hat=3072\n","Skipping y_hat=3072\n","Skipping y_hat=3072\n","Skipping y_hat=3072\n","Skipping y_hat=3072\n","Skipping y_hat=3072\n","Skipping y_hat=3072\n","Skipping y_hat=2626\n","Skipping y_hat=2626\n","Skipping y_hat=2626\n","Skipping y_hat=2626\n","Skipping y_hat=2626\n","Skipping y_hat=2626\n","Skipping y_hat=2626\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2313\n","Skipping y_hat=2313\n","Skipping y_hat=2313\n","Skipping y_hat=2313\n","Skipping y_hat=2313\n","Skipping y_hat=2313\n","Skipping y_hat=2313\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=2740\n","Skipping y_hat=2740\n","Skipping y_hat=2740\n","Skipping y_hat=2740\n","Skipping y_hat=2740\n","Skipping y_hat=2740\n","Skipping y_hat=2740\n","Skipping y_hat=2225\n","Skipping y_hat=2225\n","Skipping y_hat=2225\n","Skipping y_hat=2225\n","Skipping y_hat=2225\n","Skipping y_hat=2225\n","Skipping y_hat=2225\n","Skipping y_hat=2110\n","Skipping y_hat=2110\n","Skipping y_hat=2110\n","Skipping y_hat=2110\n","Skipping y_hat=2110\n","Skipping y_hat=2110\n","Skipping y_hat=2110\n","Skipping y_hat=2576\n","Skipping y_hat=2576\n","Skipping y_hat=2576\n","Skipping y_hat=2576\n","Skipping y_hat=2576\n","Skipping y_hat=2576\n","Skipping y_hat=2576\n","Skipping y_hat=1339\n","Skipping y_hat=1339\n","Skipping y_hat=1339\n","Skipping y_hat=1339\n","Skipping y_hat=1339\n","Skipping y_hat=1339\n","Skipping y_hat=1339\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=3173\n","Skipping y_hat=3173\n","Skipping y_hat=3173\n","Skipping y_hat=3173\n","Skipping y_hat=3173\n","Skipping y_hat=3173\n","Skipping y_hat=3173\n","Skipping y_hat=3073\n","Skipping y_hat=3073\n","Skipping y_hat=3073\n","Skipping y_hat=3073\n","Skipping y_hat=3073\n","Skipping y_hat=3073\n","Skipping y_hat=3073\n","Skipping y_hat=3226\n","Skipping y_hat=3226\n","Skipping y_hat=3226\n","Skipping y_hat=3226\n","Skipping y_hat=3226\n","Skipping y_hat=3226\n","Skipping y_hat=3226\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2915\n","Skipping y_hat=2915\n","Skipping y_hat=2915\n","Skipping y_hat=2915\n","Skipping y_hat=2915\n","Skipping y_hat=2915\n","Skipping y_hat=2915\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=2218\n","Skipping y_hat=2218\n","Skipping y_hat=2218\n","Skipping y_hat=2218\n","Skipping y_hat=2218\n","Skipping y_hat=2218\n","Skipping y_hat=2218\n","Skipping y_hat=1707\n","Skipping y_hat=1707\n","Skipping y_hat=1707\n","Skipping y_hat=1707\n","Skipping y_hat=1707\n","Skipping y_hat=1707\n","Skipping y_hat=1707\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1552\n","Skipping y_hat=1552\n","Skipping y_hat=1552\n","Skipping y_hat=1552\n","Skipping y_hat=1552\n","Skipping y_hat=1552\n","Skipping y_hat=1552\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=1742\n","Skipping y_hat=1742\n","Skipping y_hat=1742\n","Skipping y_hat=1742\n","Skipping y_hat=1742\n","Skipping y_hat=1742\n","Skipping y_hat=1742\n","Skipping y_hat=2114\n","Skipping y_hat=2114\n","Skipping y_hat=2114\n","Skipping y_hat=2114\n","Skipping y_hat=2114\n","Skipping y_hat=2114\n","Skipping y_hat=2114\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=2869\n","Skipping y_hat=2869\n","Skipping y_hat=2869\n","Skipping y_hat=2869\n","Skipping y_hat=2869\n","Skipping y_hat=2869\n","Skipping y_hat=2869\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2149\n","Skipping y_hat=2149\n","Skipping y_hat=2149\n","Skipping y_hat=2149\n","Skipping y_hat=2149\n","Skipping y_hat=2149\n","Skipping y_hat=2149\n"," 92% 73/79 [00:09<00:00,  7.66it/s]Skipping y_hat=1747\n","Skipping y_hat=1747\n","Skipping y_hat=1747\n","Skipping y_hat=1747\n","Skipping y_hat=1747\n","Skipping y_hat=1747\n","Skipping y_hat=1747\n","Skipping y_hat=2163\n","Skipping y_hat=2163\n","Skipping y_hat=2163\n","Skipping y_hat=2163\n","Skipping y_hat=2163\n","Skipping y_hat=2163\n","Skipping y_hat=2163\n","Skipping y_hat=2585\n","Skipping y_hat=2585\n","Skipping y_hat=2585\n","Skipping y_hat=2585\n","Skipping y_hat=2585\n","Skipping y_hat=2585\n","Skipping y_hat=2585\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2666\n","Skipping y_hat=2476\n","Skipping y_hat=2476\n","Skipping y_hat=2476\n","Skipping y_hat=2476\n","Skipping y_hat=2476\n","Skipping y_hat=2476\n","Skipping y_hat=2476\n","Skipping y_hat=2365\n","Skipping y_hat=2365\n","Skipping y_hat=2365\n","Skipping y_hat=2365\n","Skipping y_hat=2365\n","Skipping y_hat=2365\n","Skipping y_hat=2365\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=2597\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=2772\n","Skipping y_hat=2772\n","Skipping y_hat=2772\n","Skipping y_hat=2772\n","Skipping y_hat=2772\n","Skipping y_hat=2772\n","Skipping y_hat=2772\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=2611\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=3545\n","Skipping y_hat=3545\n","Skipping y_hat=3545\n","Skipping y_hat=3545\n","Skipping y_hat=3545\n","Skipping y_hat=3545\n","Skipping y_hat=3545\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=1942\n","Skipping y_hat=1942\n","Skipping y_hat=1942\n","Skipping y_hat=1942\n","Skipping y_hat=1942\n","Skipping y_hat=1942\n","Skipping y_hat=1942\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1505\n","Skipping y_hat=1505\n","Skipping y_hat=1505\n","Skipping y_hat=1505\n","Skipping y_hat=1505\n","Skipping y_hat=1505\n","Skipping y_hat=1505\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=3026\n","Skipping y_hat=3026\n","Skipping y_hat=3026\n","Skipping y_hat=3026\n","Skipping y_hat=3026\n","Skipping y_hat=3026\n","Skipping y_hat=3026\n","Skipping y_hat=3469\n","Skipping y_hat=3469\n","Skipping y_hat=3469\n","Skipping y_hat=3469\n","Skipping y_hat=3469\n","Skipping y_hat=3469\n","Skipping y_hat=3469\n","Skipping y_hat=2501\n","Skipping y_hat=2501\n","Skipping y_hat=2501\n","Skipping y_hat=2501\n","Skipping y_hat=2501\n","Skipping y_hat=2501\n","Skipping y_hat=2501\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=3458\n","Skipping y_hat=3458\n","Skipping y_hat=3458\n","Skipping y_hat=3458\n","Skipping y_hat=3458\n","Skipping y_hat=3458\n","Skipping y_hat=3458\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=2462\n","Skipping y_hat=2462\n","Skipping y_hat=2462\n","Skipping y_hat=2462\n","Skipping y_hat=2462\n","Skipping y_hat=2462\n","Skipping y_hat=2462\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2776\n","Skipping y_hat=2776\n","Skipping y_hat=2776\n","Skipping y_hat=2776\n","Skipping y_hat=2776\n","Skipping y_hat=2776\n","Skipping y_hat=2776\n","Skipping y_hat=1300\n","Skipping y_hat=1300\n","Skipping y_hat=1300\n","Skipping y_hat=1300\n","Skipping y_hat=1300\n","Skipping y_hat=1300\n","Skipping y_hat=1300\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=1966\n","Skipping y_hat=1966\n","Skipping y_hat=1966\n","Skipping y_hat=1966\n","Skipping y_hat=1966\n","Skipping y_hat=1966\n","Skipping y_hat=1966\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=3264\n","Skipping y_hat=3264\n","Skipping y_hat=3264\n","Skipping y_hat=3264\n","Skipping y_hat=3264\n","Skipping y_hat=3264\n","Skipping y_hat=3264\n","Skipping y_hat=2203\n","Skipping y_hat=2203\n","Skipping y_hat=2203\n","Skipping y_hat=2203\n","Skipping y_hat=2203\n","Skipping y_hat=2203\n","Skipping y_hat=2203\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=1584\n","Skipping y_hat=1584\n","Skipping y_hat=1584\n","Skipping y_hat=1584\n","Skipping y_hat=1584\n","Skipping y_hat=1584\n","Skipping y_hat=1584\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=2295\n","Skipping y_hat=2295\n","Skipping y_hat=2295\n","Skipping y_hat=2295\n","Skipping y_hat=2295\n","Skipping y_hat=2295\n","Skipping y_hat=2295\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=2387\n","Skipping y_hat=2387\n","Skipping y_hat=2387\n","Skipping y_hat=2387\n","Skipping y_hat=2387\n","Skipping y_hat=2387\n","Skipping y_hat=2387\n","Skipping y_hat=2795\n","Skipping y_hat=2795\n","Skipping y_hat=2795\n","Skipping y_hat=2795\n","Skipping y_hat=2795\n","Skipping y_hat=2795\n","Skipping y_hat=2795\n","Skipping y_hat=2540\n","Skipping y_hat=2540\n","Skipping y_hat=2540\n","Skipping y_hat=2540\n","Skipping y_hat=2540\n","Skipping y_hat=2540\n","Skipping y_hat=2540\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1867\n","Skipping y_hat=1867\n","Skipping y_hat=1867\n","Skipping y_hat=1867\n","Skipping y_hat=1867\n","Skipping y_hat=1867\n","Skipping y_hat=1867\n","Skipping y_hat=3236\n","Skipping y_hat=3236\n","Skipping y_hat=3236\n","Skipping y_hat=3236\n","Skipping y_hat=3236\n","Skipping y_hat=3236\n","Skipping y_hat=3236\n","Skipping y_hat=1105\n","Skipping y_hat=1105\n","Skipping y_hat=1105\n","Skipping y_hat=1105\n","Skipping y_hat=1105\n","Skipping y_hat=1105\n","Skipping y_hat=1105\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=2802\n","Skipping y_hat=2802\n","Skipping y_hat=2802\n","Skipping y_hat=2802\n","Skipping y_hat=2802\n","Skipping y_hat=2802\n","Skipping y_hat=2802\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1577\n","Skipping y_hat=1577\n","Skipping y_hat=1577\n","Skipping y_hat=1577\n","Skipping y_hat=1577\n","Skipping y_hat=1577\n","Skipping y_hat=1577\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1726\n","Skipping y_hat=1726\n","Skipping y_hat=1726\n","Skipping y_hat=1726\n","Skipping y_hat=1726\n","Skipping y_hat=1726\n","Skipping y_hat=1726\n","Skipping y_hat=1757\n","Skipping y_hat=1757\n","Skipping y_hat=1757\n","Skipping y_hat=1757\n","Skipping y_hat=1757\n","Skipping y_hat=1757\n","Skipping y_hat=1757\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=3003\n","Skipping y_hat=3003\n","Skipping y_hat=3003\n","Skipping y_hat=3003\n","Skipping y_hat=3003\n","Skipping y_hat=3003\n","Skipping y_hat=3003\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2237\n","Skipping y_hat=2300\n","Skipping y_hat=2300\n","Skipping y_hat=2300\n","Skipping y_hat=2300\n","Skipping y_hat=2300\n","Skipping y_hat=2300\n","Skipping y_hat=2300\n","Skipping y_hat=1238\n","Skipping y_hat=1238\n","Skipping y_hat=1238\n","Skipping y_hat=1238\n","Skipping y_hat=1238\n","Skipping y_hat=1238\n","Skipping y_hat=1238\n","Skipping y_hat=1850\n","Skipping y_hat=1850\n","Skipping y_hat=1850\n","Skipping y_hat=1850\n","Skipping y_hat=1850\n","Skipping y_hat=1850\n","Skipping y_hat=1850\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1775\n","Skipping y_hat=1775\n","Skipping y_hat=1775\n","Skipping y_hat=1775\n","Skipping y_hat=1775\n","Skipping y_hat=1775\n","Skipping y_hat=1775\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=1054\n","Skipping y_hat=1054\n","Skipping y_hat=1054\n","Skipping y_hat=1054\n","Skipping y_hat=1054\n","Skipping y_hat=1054\n","Skipping y_hat=1054\n","Skipping y_hat=2932\n","Skipping y_hat=2932\n","Skipping y_hat=2932\n","Skipping y_hat=2932\n","Skipping y_hat=2932\n","Skipping y_hat=2932\n","Skipping y_hat=2932\n","Skipping y_hat=2898\n","Skipping y_hat=2898\n","Skipping y_hat=2898\n","Skipping y_hat=2898\n","Skipping y_hat=2898\n","Skipping y_hat=2898\n","Skipping y_hat=2898\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2707\n","Skipping y_hat=2707\n","Skipping y_hat=2707\n","Skipping y_hat=2707\n","Skipping y_hat=2707\n","Skipping y_hat=2707\n","Skipping y_hat=2707\n","Skipping y_hat=2408\n","Skipping y_hat=2408\n","Skipping y_hat=2408\n","Skipping y_hat=2408\n","Skipping y_hat=2408\n","Skipping y_hat=2408\n","Skipping y_hat=2408\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=2426\n","Skipping y_hat=2426\n","Skipping y_hat=2426\n","Skipping y_hat=2426\n","Skipping y_hat=2426\n","Skipping y_hat=2426\n","Skipping y_hat=2426\n","Skipping y_hat=3190\n","Skipping y_hat=3190\n","Skipping y_hat=3190\n","Skipping y_hat=3190\n","Skipping y_hat=3190\n","Skipping y_hat=3190\n","Skipping y_hat=3190\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=3166\n","Skipping y_hat=3166\n","Skipping y_hat=3166\n","Skipping y_hat=3166\n","Skipping y_hat=3166\n","Skipping y_hat=3166\n","Skipping y_hat=3166\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=3193\n","Skipping y_hat=3193\n","Skipping y_hat=3193\n","Skipping y_hat=3193\n","Skipping y_hat=3193\n","Skipping y_hat=3193\n","Skipping y_hat=3193\n","Skipping y_hat=2484\n","Skipping y_hat=2484\n","Skipping y_hat=2484\n","Skipping y_hat=2484\n","Skipping y_hat=2484\n","Skipping y_hat=2484\n","Skipping y_hat=2484\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=2232\n","Skipping y_hat=2232\n","Skipping y_hat=2232\n","Skipping y_hat=2232\n","Skipping y_hat=2232\n","Skipping y_hat=2232\n","Skipping y_hat=2232\n","Skipping y_hat=2978\n","Skipping y_hat=2978\n","Skipping y_hat=2978\n","Skipping y_hat=2978\n","Skipping y_hat=2978\n","Skipping y_hat=2978\n","Skipping y_hat=2978\n","Skipping y_hat=1624\n","Skipping y_hat=1624\n","Skipping y_hat=1624\n","Skipping y_hat=1624\n","Skipping y_hat=1624\n","Skipping y_hat=1624\n","Skipping y_hat=1624\n","Skipping y_hat=1804\n","Skipping y_hat=1804\n","Skipping y_hat=1804\n","Skipping y_hat=1804\n","Skipping y_hat=1804\n","Skipping y_hat=1804\n","Skipping y_hat=1804\n","Skipping y_hat=3069\n","Skipping y_hat=3069\n","Skipping y_hat=3069\n","Skipping y_hat=3069\n","Skipping y_hat=3069\n","Skipping y_hat=3069\n","Skipping y_hat=3069\n","Skipping y_hat=2583\n","Skipping y_hat=2583\n","Skipping y_hat=2583\n","Skipping y_hat=2583\n","Skipping y_hat=2583\n","Skipping y_hat=2583\n","Skipping y_hat=2583\n","Skipping y_hat=3475\n","Skipping y_hat=3475\n","Skipping y_hat=3475\n","Skipping y_hat=3475\n","Skipping y_hat=3475\n","Skipping y_hat=3475\n","Skipping y_hat=3475\n","Skipping y_hat=2610\n","Skipping y_hat=2610\n","Skipping y_hat=2610\n","Skipping y_hat=2610\n","Skipping y_hat=2610\n","Skipping y_hat=2610\n","Skipping y_hat=2610\n","Skipping y_hat=2766\n","Skipping y_hat=2766\n","Skipping y_hat=2766\n","Skipping y_hat=2766\n","Skipping y_hat=2766\n","Skipping y_hat=2766\n","Skipping y_hat=2766\n","Skipping y_hat=2078\n","Skipping y_hat=2078\n","Skipping y_hat=2078\n","Skipping y_hat=2078\n","Skipping y_hat=2078\n","Skipping y_hat=2078\n","Skipping y_hat=2078\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=2551\n","Skipping y_hat=2551\n","Skipping y_hat=2551\n","Skipping y_hat=2551\n","Skipping y_hat=2551\n","Skipping y_hat=2551\n","Skipping y_hat=2551\n","Skipping y_hat=2334\n","Skipping y_hat=2334\n","Skipping y_hat=2334\n","Skipping y_hat=2334\n","Skipping y_hat=2334\n","Skipping y_hat=2334\n","Skipping y_hat=2334\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2061\n","Skipping y_hat=2061\n","Skipping y_hat=2061\n","Skipping y_hat=2061\n","Skipping y_hat=2061\n","Skipping y_hat=2061\n","Skipping y_hat=2061\n","Skipping y_hat=2838\n","Skipping y_hat=2838\n","Skipping y_hat=2838\n","Skipping y_hat=2838\n","Skipping y_hat=2838\n","Skipping y_hat=2838\n","Skipping y_hat=2838\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=1346\n","Skipping y_hat=1346\n","Skipping y_hat=1346\n","Skipping y_hat=1346\n","Skipping y_hat=1346\n","Skipping y_hat=1346\n","Skipping y_hat=1346\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=2563\n","Skipping y_hat=2563\n","Skipping y_hat=2563\n","Skipping y_hat=2563\n","Skipping y_hat=2563\n","Skipping y_hat=2563\n","Skipping y_hat=2563\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=2700\n","Skipping y_hat=2700\n","Skipping y_hat=2700\n","Skipping y_hat=2700\n","Skipping y_hat=2700\n","Skipping y_hat=2700\n","Skipping y_hat=2700\n","Skipping y_hat=2957\n","Skipping y_hat=2957\n","Skipping y_hat=2957\n","Skipping y_hat=2957\n","Skipping y_hat=2957\n","Skipping y_hat=2957\n","Skipping y_hat=2957\n","Skipping y_hat=2134\n","Skipping y_hat=2134\n","Skipping y_hat=2134\n","Skipping y_hat=2134\n","Skipping y_hat=2134\n","Skipping y_hat=2134\n","Skipping y_hat=2134\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=2358\n","Skipping y_hat=2358\n","Skipping y_hat=2358\n","Skipping y_hat=2358\n","Skipping y_hat=2358\n","Skipping y_hat=2358\n","Skipping y_hat=2358\n","Skipping y_hat=2533\n","Skipping y_hat=2533\n","Skipping y_hat=2533\n","Skipping y_hat=2533\n","Skipping y_hat=2533\n","Skipping y_hat=2533\n","Skipping y_hat=2533\n","Skipping y_hat=2366\n","Skipping y_hat=2366\n","Skipping y_hat=2366\n","Skipping y_hat=2366\n","Skipping y_hat=2366\n","Skipping y_hat=2366\n","Skipping y_hat=2366\n","Skipping y_hat=2671\n","Skipping y_hat=2671\n","Skipping y_hat=2671\n","Skipping y_hat=2671\n","Skipping y_hat=2671\n","Skipping y_hat=2671\n","Skipping y_hat=2671\n","Skipping y_hat=2006\n","Skipping y_hat=2006\n","Skipping y_hat=2006\n","Skipping y_hat=2006\n","Skipping y_hat=2006\n","Skipping y_hat=2006\n","Skipping y_hat=2006\n"," 94% 74/79 [00:09<00:00,  8.12it/s]Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1840\n","Skipping y_hat=1840\n","Skipping y_hat=1840\n","Skipping y_hat=1840\n","Skipping y_hat=1840\n","Skipping y_hat=1840\n","Skipping y_hat=1840\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2471\n","Skipping y_hat=2471\n","Skipping y_hat=2471\n","Skipping y_hat=2471\n","Skipping y_hat=2471\n","Skipping y_hat=2471\n","Skipping y_hat=2471\n","Skipping y_hat=1735\n","Skipping y_hat=1735\n","Skipping y_hat=1735\n","Skipping y_hat=1735\n","Skipping y_hat=1735\n","Skipping y_hat=1735\n","Skipping y_hat=1735\n","Skipping y_hat=1941\n","Skipping y_hat=1941\n","Skipping y_hat=1941\n","Skipping y_hat=1941\n","Skipping y_hat=1941\n","Skipping y_hat=1941\n","Skipping y_hat=1941\n","Skipping y_hat=1455\n","Skipping y_hat=1455\n","Skipping y_hat=1455\n","Skipping y_hat=1455\n","Skipping y_hat=1455\n","Skipping y_hat=1455\n","Skipping y_hat=1455\n","Skipping y_hat=1650\n","Skipping y_hat=1650\n","Skipping y_hat=1650\n","Skipping y_hat=1650\n","Skipping y_hat=1650\n","Skipping y_hat=1650\n","Skipping y_hat=1650\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2102\n","Skipping y_hat=2102\n","Skipping y_hat=2102\n","Skipping y_hat=2102\n","Skipping y_hat=2102\n","Skipping y_hat=2102\n","Skipping y_hat=2102\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1956\n","Skipping y_hat=1956\n","Skipping y_hat=1956\n","Skipping y_hat=1956\n","Skipping y_hat=1956\n","Skipping y_hat=1956\n","Skipping y_hat=1956\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=2216\n","Skipping y_hat=2216\n","Skipping y_hat=2216\n","Skipping y_hat=2216\n","Skipping y_hat=2216\n","Skipping y_hat=2216\n","Skipping y_hat=2216\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=3287\n","Skipping y_hat=3287\n","Skipping y_hat=3287\n","Skipping y_hat=3287\n","Skipping y_hat=3287\n","Skipping y_hat=3287\n","Skipping y_hat=3287\n","Skipping y_hat=2174\n","Skipping y_hat=2174\n","Skipping y_hat=2174\n","Skipping y_hat=2174\n","Skipping y_hat=2174\n","Skipping y_hat=2174\n","Skipping y_hat=2174\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=2249\n","Skipping y_hat=2249\n","Skipping y_hat=2249\n","Skipping y_hat=2249\n","Skipping y_hat=2249\n","Skipping y_hat=2249\n","Skipping y_hat=2249\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=2456\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=3398\n","Skipping y_hat=3398\n","Skipping y_hat=3398\n","Skipping y_hat=3398\n","Skipping y_hat=3398\n","Skipping y_hat=3398\n","Skipping y_hat=3398\n","Skipping y_hat=1803\n","Skipping y_hat=1803\n","Skipping y_hat=1803\n","Skipping y_hat=1803\n","Skipping y_hat=1803\n","Skipping y_hat=1803\n","Skipping y_hat=1803\n","Skipping y_hat=2868\n","Skipping y_hat=2868\n","Skipping y_hat=2868\n","Skipping y_hat=2868\n","Skipping y_hat=2868\n","Skipping y_hat=2868\n","Skipping y_hat=2868\n","Skipping y_hat=2172\n","Skipping y_hat=2172\n","Skipping y_hat=2172\n","Skipping y_hat=2172\n","Skipping y_hat=2172\n","Skipping y_hat=2172\n","Skipping y_hat=2172\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2087\n","Skipping y_hat=2087\n","Skipping y_hat=2087\n","Skipping y_hat=2087\n","Skipping y_hat=2087\n","Skipping y_hat=2087\n","Skipping y_hat=2087\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=1615\n","Skipping y_hat=1615\n","Skipping y_hat=1615\n","Skipping y_hat=1615\n","Skipping y_hat=1615\n","Skipping y_hat=1615\n","Skipping y_hat=1615\n","Skipping y_hat=3513\n","Skipping y_hat=3513\n","Skipping y_hat=3513\n","Skipping y_hat=3513\n","Skipping y_hat=3513\n","Skipping y_hat=3513\n","Skipping y_hat=3513\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=2296\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1832\n","Skipping y_hat=1832\n","Skipping y_hat=1832\n","Skipping y_hat=1832\n","Skipping y_hat=1832\n","Skipping y_hat=1832\n","Skipping y_hat=1832\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2659\n","Skipping y_hat=2659\n","Skipping y_hat=2659\n","Skipping y_hat=2659\n","Skipping y_hat=2659\n","Skipping y_hat=2659\n","Skipping y_hat=2659\n","Skipping y_hat=2045\n","Skipping y_hat=2045\n","Skipping y_hat=2045\n","Skipping y_hat=2045\n","Skipping y_hat=2045\n","Skipping y_hat=2045\n","Skipping y_hat=2045\n","Skipping y_hat=2506\n","Skipping y_hat=2506\n","Skipping y_hat=2506\n","Skipping y_hat=2506\n","Skipping y_hat=2506\n","Skipping y_hat=2506\n","Skipping y_hat=2506\n","Skipping y_hat=3102\n","Skipping y_hat=3102\n","Skipping y_hat=3102\n","Skipping y_hat=3102\n","Skipping y_hat=3102\n","Skipping y_hat=3102\n","Skipping y_hat=3102\n","Skipping y_hat=1593\n","Skipping y_hat=1593\n","Skipping y_hat=1593\n","Skipping y_hat=1593\n","Skipping y_hat=1593\n","Skipping y_hat=1593\n","Skipping y_hat=1593\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=1852\n","Skipping y_hat=1852\n","Skipping y_hat=1852\n","Skipping y_hat=1852\n","Skipping y_hat=1852\n","Skipping y_hat=1852\n","Skipping y_hat=1852\n","Skipping y_hat=2734\n","Skipping y_hat=2734\n","Skipping y_hat=2734\n","Skipping y_hat=2734\n","Skipping y_hat=2734\n","Skipping y_hat=2734\n","Skipping y_hat=2734\n","Skipping y_hat=2065\n","Skipping y_hat=2065\n","Skipping y_hat=2065\n","Skipping y_hat=2065\n","Skipping y_hat=2065\n","Skipping y_hat=2065\n","Skipping y_hat=2065\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=1699\n","Skipping y_hat=1699\n","Skipping y_hat=1699\n","Skipping y_hat=1699\n","Skipping y_hat=1699\n","Skipping y_hat=1699\n","Skipping y_hat=1699\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2607\n","Skipping y_hat=2607\n","Skipping y_hat=2607\n","Skipping y_hat=2607\n","Skipping y_hat=2607\n","Skipping y_hat=2607\n","Skipping y_hat=2607\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=1357\n","Skipping y_hat=1357\n","Skipping y_hat=1357\n","Skipping y_hat=1357\n","Skipping y_hat=1357\n","Skipping y_hat=1357\n","Skipping y_hat=1357\n","Skipping y_hat=1582\n","Skipping y_hat=1582\n","Skipping y_hat=1582\n","Skipping y_hat=1582\n","Skipping y_hat=1582\n","Skipping y_hat=1582\n","Skipping y_hat=1582\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2088\n","Skipping y_hat=2088\n","Skipping y_hat=2088\n","Skipping y_hat=2088\n","Skipping y_hat=2088\n","Skipping y_hat=2088\n","Skipping y_hat=2088\n","Skipping y_hat=2355\n","Skipping y_hat=2355\n","Skipping y_hat=2355\n","Skipping y_hat=2355\n","Skipping y_hat=2355\n","Skipping y_hat=2355\n","Skipping y_hat=2355\n","Skipping y_hat=2338\n","Skipping y_hat=2338\n","Skipping y_hat=2338\n","Skipping y_hat=2338\n","Skipping y_hat=2338\n","Skipping y_hat=2338\n","Skipping y_hat=2338\n","Skipping y_hat=1293\n","Skipping y_hat=1293\n","Skipping y_hat=1293\n","Skipping y_hat=1293\n","Skipping y_hat=1293\n","Skipping y_hat=1293\n","Skipping y_hat=1293\n","Skipping y_hat=2592\n","Skipping y_hat=2592\n","Skipping y_hat=2592\n","Skipping y_hat=2592\n","Skipping y_hat=2592\n","Skipping y_hat=2592\n","Skipping y_hat=2592\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=2266\n","Skipping y_hat=2266\n","Skipping y_hat=2266\n","Skipping y_hat=2266\n","Skipping y_hat=2266\n","Skipping y_hat=2266\n","Skipping y_hat=2266\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2600\n","Skipping y_hat=2044\n","Skipping y_hat=2044\n","Skipping y_hat=2044\n","Skipping y_hat=2044\n","Skipping y_hat=2044\n","Skipping y_hat=2044\n","Skipping y_hat=2044\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=1427\n","Skipping y_hat=1427\n","Skipping y_hat=1427\n","Skipping y_hat=1427\n","Skipping y_hat=1427\n","Skipping y_hat=1427\n","Skipping y_hat=1427\n","Skipping y_hat=2688\n","Skipping y_hat=2688\n","Skipping y_hat=2688\n","Skipping y_hat=2688\n","Skipping y_hat=2688\n","Skipping y_hat=2688\n","Skipping y_hat=2688\n","Skipping y_hat=2394\n","Skipping y_hat=2394\n","Skipping y_hat=2394\n","Skipping y_hat=2394\n","Skipping y_hat=2394\n","Skipping y_hat=2394\n","Skipping y_hat=2394\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2987\n","Skipping y_hat=2987\n","Skipping y_hat=2987\n","Skipping y_hat=2987\n","Skipping y_hat=2987\n","Skipping y_hat=2987\n","Skipping y_hat=2987\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=2013\n","Skipping y_hat=2351\n","Skipping y_hat=2351\n","Skipping y_hat=2351\n","Skipping y_hat=2351\n","Skipping y_hat=2351\n","Skipping y_hat=2351\n","Skipping y_hat=2351\n","Skipping y_hat=1298\n","Skipping y_hat=1298\n","Skipping y_hat=1298\n","Skipping y_hat=1298\n","Skipping y_hat=1298\n","Skipping y_hat=1298\n","Skipping y_hat=1298\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2633\n","Skipping y_hat=2099\n","Skipping y_hat=2099\n","Skipping y_hat=2099\n","Skipping y_hat=2099\n","Skipping y_hat=2099\n","Skipping y_hat=2099\n","Skipping y_hat=2099\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=2565\n","Skipping y_hat=1666\n","Skipping y_hat=1666\n","Skipping y_hat=1666\n","Skipping y_hat=1666\n","Skipping y_hat=1666\n","Skipping y_hat=1666\n","Skipping y_hat=1666\n","Skipping y_hat=3116\n","Skipping y_hat=3116\n","Skipping y_hat=3116\n","Skipping y_hat=3116\n","Skipping y_hat=3116\n","Skipping y_hat=3116\n","Skipping y_hat=3116\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=3211\n","Skipping y_hat=3211\n","Skipping y_hat=3211\n","Skipping y_hat=3211\n","Skipping y_hat=3211\n","Skipping y_hat=3211\n","Skipping y_hat=3211\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=2812\n","Skipping y_hat=2812\n","Skipping y_hat=2812\n","Skipping y_hat=2812\n","Skipping y_hat=2812\n","Skipping y_hat=2812\n","Skipping y_hat=2812\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=2983\n","Skipping y_hat=2983\n","Skipping y_hat=2983\n","Skipping y_hat=2983\n","Skipping y_hat=2983\n","Skipping y_hat=2983\n","Skipping y_hat=2983\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=2489\n","Skipping y_hat=2489\n","Skipping y_hat=2489\n","Skipping y_hat=2489\n","Skipping y_hat=2489\n","Skipping y_hat=2489\n","Skipping y_hat=2489\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=1761\n","Skipping y_hat=1761\n","Skipping y_hat=1761\n","Skipping y_hat=1761\n","Skipping y_hat=1761\n","Skipping y_hat=1761\n","Skipping y_hat=1761\n","Skipping y_hat=2657\n","Skipping y_hat=2657\n","Skipping y_hat=2657\n","Skipping y_hat=2657\n","Skipping y_hat=2657\n","Skipping y_hat=2657\n","Skipping y_hat=2657\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=1919\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2125\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=2746\n","Skipping y_hat=2746\n","Skipping y_hat=2746\n","Skipping y_hat=2746\n","Skipping y_hat=2746\n","Skipping y_hat=2746\n","Skipping y_hat=2746\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2175\n","Skipping y_hat=2175\n","Skipping y_hat=2175\n","Skipping y_hat=2175\n","Skipping y_hat=2175\n","Skipping y_hat=2175\n","Skipping y_hat=2175\n","Skipping y_hat=2946\n","Skipping y_hat=2946\n","Skipping y_hat=2946\n","Skipping y_hat=2946\n","Skipping y_hat=2946\n","Skipping y_hat=2946\n","Skipping y_hat=2946\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2207\n","Skipping y_hat=2207\n","Skipping y_hat=2207\n","Skipping y_hat=2207\n","Skipping y_hat=2207\n","Skipping y_hat=2207\n","Skipping y_hat=2207\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=2882\n","Skipping y_hat=2882\n","Skipping y_hat=2882\n","Skipping y_hat=2882\n","Skipping y_hat=2882\n","Skipping y_hat=2882\n","Skipping y_hat=2882\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=2786\n","Skipping y_hat=2786\n","Skipping y_hat=2786\n","Skipping y_hat=2786\n","Skipping y_hat=2786\n","Skipping y_hat=2786\n","Skipping y_hat=2786\n","Skipping y_hat=0924\n","Skipping y_hat=0924\n","Skipping y_hat=0924\n","Skipping y_hat=0924\n","Skipping y_hat=0924\n","Skipping y_hat=0924\n","Skipping y_hat=0924\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2252\n","Skipping y_hat=2906\n","Skipping y_hat=2906\n","Skipping y_hat=2906\n","Skipping y_hat=2906\n","Skipping y_hat=2906\n","Skipping y_hat=2906\n","Skipping y_hat=2906\n","Skipping y_hat=2594\n","Skipping y_hat=2594\n","Skipping y_hat=2594\n","Skipping y_hat=2594\n","Skipping y_hat=2594\n","Skipping y_hat=2594\n","Skipping y_hat=2594\n","Skipping y_hat=2395\n","Skipping y_hat=2395\n","Skipping y_hat=2395\n","Skipping y_hat=2395\n","Skipping y_hat=2395\n","Skipping y_hat=2395\n","Skipping y_hat=2395\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=3343\n","Skipping y_hat=3343\n","Skipping y_hat=3343\n","Skipping y_hat=3343\n","Skipping y_hat=3343\n","Skipping y_hat=3343\n","Skipping y_hat=3343\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=1912\n","Skipping y_hat=2076\n","Skipping y_hat=2076\n","Skipping y_hat=2076\n","Skipping y_hat=2076\n","Skipping y_hat=2076\n","Skipping y_hat=2076\n","Skipping y_hat=2076\n","Skipping y_hat=2002\n","Skipping y_hat=2002\n","Skipping y_hat=2002\n","Skipping y_hat=2002\n","Skipping y_hat=2002\n","Skipping y_hat=2002\n","Skipping y_hat=2002\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=2030\n","Skipping y_hat=2030\n","Skipping y_hat=2030\n","Skipping y_hat=2030\n","Skipping y_hat=2030\n","Skipping y_hat=2030\n","Skipping y_hat=2030\n"," 95% 75/79 [00:09<00:00,  7.85it/s]Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=1108\n","Skipping y_hat=1108\n","Skipping y_hat=1108\n","Skipping y_hat=1108\n","Skipping y_hat=1108\n","Skipping y_hat=1108\n","Skipping y_hat=1108\n","Skipping y_hat=1910\n","Skipping y_hat=1910\n","Skipping y_hat=1910\n","Skipping y_hat=1910\n","Skipping y_hat=1910\n","Skipping y_hat=1910\n","Skipping y_hat=1910\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=2804\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=3199\n","Skipping y_hat=3199\n","Skipping y_hat=3199\n","Skipping y_hat=3199\n","Skipping y_hat=3199\n","Skipping y_hat=3199\n","Skipping y_hat=3199\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=2639\n","Skipping y_hat=2639\n","Skipping y_hat=2639\n","Skipping y_hat=2639\n","Skipping y_hat=2639\n","Skipping y_hat=2639\n","Skipping y_hat=2639\n","Skipping y_hat=2320\n","Skipping y_hat=2320\n","Skipping y_hat=2320\n","Skipping y_hat=2320\n","Skipping y_hat=2320\n","Skipping y_hat=2320\n","Skipping y_hat=2320\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=2137\n","Skipping y_hat=3595\n","Skipping y_hat=3595\n","Skipping y_hat=3595\n","Skipping y_hat=3595\n","Skipping y_hat=3595\n","Skipping y_hat=3595\n","Skipping y_hat=3595\n","Skipping y_hat=3119\n","Skipping y_hat=3119\n","Skipping y_hat=3119\n","Skipping y_hat=3119\n","Skipping y_hat=3119\n","Skipping y_hat=3119\n","Skipping y_hat=3119\n","Skipping y_hat=2941\n","Skipping y_hat=2941\n","Skipping y_hat=2941\n","Skipping y_hat=2941\n","Skipping y_hat=2941\n","Skipping y_hat=2941\n","Skipping y_hat=2941\n","Skipping y_hat=2217\n","Skipping y_hat=2217\n","Skipping y_hat=2217\n","Skipping y_hat=2217\n","Skipping y_hat=2217\n","Skipping y_hat=2217\n","Skipping y_hat=2217\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=2556\n","Skipping y_hat=2556\n","Skipping y_hat=2556\n","Skipping y_hat=2556\n","Skipping y_hat=2556\n","Skipping y_hat=2556\n","Skipping y_hat=2556\n","Skipping y_hat=2056\n","Skipping y_hat=2056\n","Skipping y_hat=2056\n","Skipping y_hat=2056\n","Skipping y_hat=2056\n","Skipping y_hat=2056\n","Skipping y_hat=2056\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1791\n","Skipping y_hat=1791\n","Skipping y_hat=1791\n","Skipping y_hat=1791\n","Skipping y_hat=1791\n","Skipping y_hat=1791\n","Skipping y_hat=1791\n","Skipping y_hat=2763\n","Skipping y_hat=2763\n","Skipping y_hat=2763\n","Skipping y_hat=2763\n","Skipping y_hat=2763\n","Skipping y_hat=2763\n","Skipping y_hat=2763\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=1643\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=2477\n","Skipping y_hat=2477\n","Skipping y_hat=2477\n","Skipping y_hat=2477\n","Skipping y_hat=2477\n","Skipping y_hat=2477\n","Skipping y_hat=2477\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2450\n","Skipping y_hat=2450\n","Skipping y_hat=2450\n","Skipping y_hat=2450\n","Skipping y_hat=2450\n","Skipping y_hat=2450\n","Skipping y_hat=2450\n","Skipping y_hat=2942\n","Skipping y_hat=2942\n","Skipping y_hat=2942\n","Skipping y_hat=2942\n","Skipping y_hat=2942\n","Skipping y_hat=2942\n","Skipping y_hat=2942\n","Skipping y_hat=2653\n","Skipping y_hat=2653\n","Skipping y_hat=2653\n","Skipping y_hat=2653\n","Skipping y_hat=2653\n","Skipping y_hat=2653\n","Skipping y_hat=2653\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=2113\n","Skipping y_hat=2113\n","Skipping y_hat=2113\n","Skipping y_hat=2113\n","Skipping y_hat=2113\n","Skipping y_hat=2113\n","Skipping y_hat=2113\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=1723\n","Skipping y_hat=1723\n","Skipping y_hat=1723\n","Skipping y_hat=1723\n","Skipping y_hat=1723\n","Skipping y_hat=1723\n","Skipping y_hat=1723\n","Skipping y_hat=2581\n","Skipping y_hat=2581\n","Skipping y_hat=2581\n","Skipping y_hat=2581\n","Skipping y_hat=2581\n","Skipping y_hat=2581\n","Skipping y_hat=2581\n","Skipping y_hat=1729\n","Skipping y_hat=1729\n","Skipping y_hat=1729\n","Skipping y_hat=1729\n","Skipping y_hat=1729\n","Skipping y_hat=1729\n","Skipping y_hat=1729\n","Skipping y_hat=2918\n","Skipping y_hat=2918\n","Skipping y_hat=2918\n","Skipping y_hat=2918\n","Skipping y_hat=2918\n","Skipping y_hat=2918\n","Skipping y_hat=2918\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2923\n","Skipping y_hat=2791\n","Skipping y_hat=2791\n","Skipping y_hat=2791\n","Skipping y_hat=2791\n","Skipping y_hat=2791\n","Skipping y_hat=2791\n","Skipping y_hat=2791\n","Skipping y_hat=2349\n","Skipping y_hat=2349\n","Skipping y_hat=2349\n","Skipping y_hat=2349\n","Skipping y_hat=2349\n","Skipping y_hat=2349\n","Skipping y_hat=2349\n","Skipping y_hat=2528\n","Skipping y_hat=2528\n","Skipping y_hat=2528\n","Skipping y_hat=2528\n","Skipping y_hat=2528\n","Skipping y_hat=2528\n","Skipping y_hat=2528\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=2552\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1935\n","Skipping y_hat=1935\n","Skipping y_hat=1935\n","Skipping y_hat=1935\n","Skipping y_hat=1935\n","Skipping y_hat=1935\n","Skipping y_hat=1935\n","Skipping y_hat=2052\n","Skipping y_hat=2052\n","Skipping y_hat=2052\n","Skipping y_hat=2052\n","Skipping y_hat=2052\n","Skipping y_hat=2052\n","Skipping y_hat=2052\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=2128\n","Skipping y_hat=2128\n","Skipping y_hat=2128\n","Skipping y_hat=2128\n","Skipping y_hat=2128\n","Skipping y_hat=2128\n","Skipping y_hat=2128\n","Skipping y_hat=1308\n","Skipping y_hat=1308\n","Skipping y_hat=1308\n","Skipping y_hat=1308\n","Skipping y_hat=1308\n","Skipping y_hat=1308\n","Skipping y_hat=1308\n","Skipping y_hat=2420\n","Skipping y_hat=2420\n","Skipping y_hat=2420\n","Skipping y_hat=2420\n","Skipping y_hat=2420\n","Skipping y_hat=2420\n","Skipping y_hat=2420\n","Skipping y_hat=2323\n","Skipping y_hat=2323\n","Skipping y_hat=2323\n","Skipping y_hat=2323\n","Skipping y_hat=2323\n","Skipping y_hat=2323\n","Skipping y_hat=2323\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2728\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2704\n","Skipping y_hat=2704\n","Skipping y_hat=2704\n","Skipping y_hat=2704\n","Skipping y_hat=2704\n","Skipping y_hat=2704\n","Skipping y_hat=2704\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=2743\n","Skipping y_hat=2743\n","Skipping y_hat=2743\n","Skipping y_hat=2743\n","Skipping y_hat=2743\n","Skipping y_hat=2743\n","Skipping y_hat=2743\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=2705\n","Skipping y_hat=2705\n","Skipping y_hat=2705\n","Skipping y_hat=2705\n","Skipping y_hat=2705\n","Skipping y_hat=2705\n","Skipping y_hat=2705\n","Skipping y_hat=3497\n","Skipping y_hat=3497\n","Skipping y_hat=3497\n","Skipping y_hat=3497\n","Skipping y_hat=3497\n","Skipping y_hat=3497\n","Skipping y_hat=3497\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=2241\n","Skipping y_hat=2241\n","Skipping y_hat=2241\n","Skipping y_hat=2241\n","Skipping y_hat=2241\n","Skipping y_hat=2241\n","Skipping y_hat=2241\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=2434\n","Skipping y_hat=2434\n","Skipping y_hat=2434\n","Skipping y_hat=2434\n","Skipping y_hat=2434\n","Skipping y_hat=2434\n","Skipping y_hat=2434\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2672\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2495\n","Skipping y_hat=2495\n","Skipping y_hat=2495\n","Skipping y_hat=2495\n","Skipping y_hat=2495\n","Skipping y_hat=2495\n","Skipping y_hat=2495\n","Skipping y_hat=1654\n","Skipping y_hat=1654\n","Skipping y_hat=1654\n","Skipping y_hat=1654\n","Skipping y_hat=1654\n","Skipping y_hat=1654\n","Skipping y_hat=1654\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2084\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=2388\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2299\n","Skipping y_hat=2299\n","Skipping y_hat=2299\n","Skipping y_hat=2299\n","Skipping y_hat=2299\n","Skipping y_hat=2299\n","Skipping y_hat=2299\n","Skipping y_hat=2933\n","Skipping y_hat=2933\n","Skipping y_hat=2933\n","Skipping y_hat=2933\n","Skipping y_hat=2933\n","Skipping y_hat=2933\n","Skipping y_hat=2933\n","Skipping y_hat=2845\n","Skipping y_hat=2845\n","Skipping y_hat=2845\n","Skipping y_hat=2845\n","Skipping y_hat=2845\n","Skipping y_hat=2845\n","Skipping y_hat=2845\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=2347\n","Skipping y_hat=2347\n","Skipping y_hat=2347\n","Skipping y_hat=2347\n","Skipping y_hat=2347\n","Skipping y_hat=2347\n","Skipping y_hat=2347\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=1736\n","Skipping y_hat=2644\n","Skipping y_hat=2644\n","Skipping y_hat=2644\n","Skipping y_hat=2644\n","Skipping y_hat=2644\n","Skipping y_hat=2644\n","Skipping y_hat=2644\n","Skipping y_hat=3168\n","Skipping y_hat=3168\n","Skipping y_hat=3168\n","Skipping y_hat=3168\n","Skipping y_hat=3168\n","Skipping y_hat=3168\n","Skipping y_hat=3168\n","Skipping y_hat=1641\n","Skipping y_hat=1641\n","Skipping y_hat=1641\n","Skipping y_hat=1641\n","Skipping y_hat=1641\n","Skipping y_hat=1641\n","Skipping y_hat=1641\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=2055\n","Skipping y_hat=2055\n","Skipping y_hat=2055\n","Skipping y_hat=2055\n","Skipping y_hat=2055\n","Skipping y_hat=2055\n","Skipping y_hat=2055\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=2089\n","Skipping y_hat=2089\n","Skipping y_hat=2089\n","Skipping y_hat=2089\n","Skipping y_hat=2089\n","Skipping y_hat=2089\n","Skipping y_hat=2089\n","Skipping y_hat=3176\n","Skipping y_hat=3176\n","Skipping y_hat=3176\n","Skipping y_hat=3176\n","Skipping y_hat=3176\n","Skipping y_hat=3176\n","Skipping y_hat=3176\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=3292\n","Skipping y_hat=3292\n","Skipping y_hat=3292\n","Skipping y_hat=3292\n","Skipping y_hat=3292\n","Skipping y_hat=3292\n","Skipping y_hat=3292\n","Skipping y_hat=3097\n","Skipping y_hat=3097\n","Skipping y_hat=3097\n","Skipping y_hat=3097\n","Skipping y_hat=3097\n","Skipping y_hat=3097\n","Skipping y_hat=3097\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=0949\n","Skipping y_hat=0949\n","Skipping y_hat=0949\n","Skipping y_hat=0949\n","Skipping y_hat=0949\n","Skipping y_hat=0949\n","Skipping y_hat=0949\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=3422\n","Skipping y_hat=3422\n","Skipping y_hat=3422\n","Skipping y_hat=3422\n","Skipping y_hat=3422\n","Skipping y_hat=3422\n","Skipping y_hat=3422\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=2466\n","Skipping y_hat=2466\n","Skipping y_hat=2466\n","Skipping y_hat=2466\n","Skipping y_hat=2466\n","Skipping y_hat=2466\n","Skipping y_hat=2466\n","Skipping y_hat=1787\n","Skipping y_hat=1787\n","Skipping y_hat=1787\n","Skipping y_hat=1787\n","Skipping y_hat=1787\n","Skipping y_hat=1787\n","Skipping y_hat=1787\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=2926\n","Skipping y_hat=2926\n","Skipping y_hat=2926\n","Skipping y_hat=2926\n","Skipping y_hat=2926\n","Skipping y_hat=2926\n","Skipping y_hat=2926\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=2188\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=1960\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=2888\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=3058\n","Skipping y_hat=3058\n","Skipping y_hat=3058\n","Skipping y_hat=3058\n","Skipping y_hat=3058\n","Skipping y_hat=3058\n","Skipping y_hat=3058\n","Skipping y_hat=2615\n","Skipping y_hat=2615\n","Skipping y_hat=2615\n","Skipping y_hat=2615\n","Skipping y_hat=2615\n","Skipping y_hat=2615\n","Skipping y_hat=2615\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=2441\n","Skipping y_hat=2441\n","Skipping y_hat=2441\n","Skipping y_hat=2441\n","Skipping y_hat=2441\n","Skipping y_hat=2441\n","Skipping y_hat=2441\n","Skipping y_hat=2504\n","Skipping y_hat=2504\n","Skipping y_hat=2504\n","Skipping y_hat=2504\n","Skipping y_hat=2504\n","Skipping y_hat=2504\n","Skipping y_hat=2504\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=2204\n","Skipping y_hat=2269\n","Skipping y_hat=2269\n","Skipping y_hat=2269\n","Skipping y_hat=2269\n","Skipping y_hat=2269\n","Skipping y_hat=2269\n","Skipping y_hat=2269\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=1887\n","Skipping y_hat=1887\n","Skipping y_hat=1887\n","Skipping y_hat=1887\n","Skipping y_hat=1887\n","Skipping y_hat=1887\n","Skipping y_hat=1887\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=3335\n","Skipping y_hat=3335\n","Skipping y_hat=3335\n","Skipping y_hat=3335\n","Skipping y_hat=3335\n","Skipping y_hat=3335\n","Skipping y_hat=3335\n","Skipping y_hat=2520\n","Skipping y_hat=2520\n","Skipping y_hat=2520\n","Skipping y_hat=2520\n","Skipping y_hat=2520\n","Skipping y_hat=2520\n","Skipping y_hat=2520\n"," 96% 76/79 [00:09<00:00,  7.93it/s]Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n","Skipping y_hat=1906\n","Skipping y_hat=1906\n","Skipping y_hat=1906\n","Skipping y_hat=1906\n","Skipping y_hat=1906\n","Skipping y_hat=1906\n","Skipping y_hat=1906\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2012\n","Skipping y_hat=2012\n","Skipping y_hat=2012\n","Skipping y_hat=2012\n","Skipping y_hat=2012\n","Skipping y_hat=2012\n","Skipping y_hat=2012\n","Skipping y_hat=2699\n","Skipping y_hat=2699\n","Skipping y_hat=2699\n","Skipping y_hat=2699\n","Skipping y_hat=2699\n","Skipping y_hat=2699\n","Skipping y_hat=2699\n","Skipping y_hat=3274\n","Skipping y_hat=3274\n","Skipping y_hat=3274\n","Skipping y_hat=3274\n","Skipping y_hat=3274\n","Skipping y_hat=3274\n","Skipping y_hat=3274\n","Skipping y_hat=0844\n","Skipping y_hat=0844\n","Skipping y_hat=0844\n","Skipping y_hat=0844\n","Skipping y_hat=0844\n","Skipping y_hat=0844\n","Skipping y_hat=0844\n","Skipping y_hat=1881\n","Skipping y_hat=1881\n","Skipping y_hat=1881\n","Skipping y_hat=1881\n","Skipping y_hat=1881\n","Skipping y_hat=1881\n","Skipping y_hat=1881\n","Skipping y_hat=3369\n","Skipping y_hat=3369\n","Skipping y_hat=3369\n","Skipping y_hat=3369\n","Skipping y_hat=3369\n","Skipping y_hat=3369\n","Skipping y_hat=3369\n","Skipping y_hat=2634\n","Skipping y_hat=2634\n","Skipping y_hat=2634\n","Skipping y_hat=2634\n","Skipping y_hat=2634\n","Skipping y_hat=2634\n","Skipping y_hat=2634\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1879\n","Skipping y_hat=1879\n","Skipping y_hat=1879\n","Skipping y_hat=1879\n","Skipping y_hat=1879\n","Skipping y_hat=1879\n","Skipping y_hat=1879\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1469\n","Skipping y_hat=1469\n","Skipping y_hat=1469\n","Skipping y_hat=1469\n","Skipping y_hat=1469\n","Skipping y_hat=1469\n","Skipping y_hat=1469\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=3014\n","Skipping y_hat=3014\n","Skipping y_hat=3014\n","Skipping y_hat=3014\n","Skipping y_hat=3014\n","Skipping y_hat=3014\n","Skipping y_hat=3014\n","Skipping y_hat=2498\n","Skipping y_hat=2498\n","Skipping y_hat=2498\n","Skipping y_hat=2498\n","Skipping y_hat=2498\n","Skipping y_hat=2498\n","Skipping y_hat=2498\n","Skipping y_hat=2243\n","Skipping y_hat=2243\n","Skipping y_hat=2243\n","Skipping y_hat=2243\n","Skipping y_hat=2243\n","Skipping y_hat=2243\n","Skipping y_hat=2243\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=0999\n","Skipping y_hat=1097\n","Skipping y_hat=1097\n","Skipping y_hat=1097\n","Skipping y_hat=1097\n","Skipping y_hat=1097\n","Skipping y_hat=1097\n","Skipping y_hat=1097\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=2029\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=1668\n","Skipping y_hat=1668\n","Skipping y_hat=1668\n","Skipping y_hat=1668\n","Skipping y_hat=1668\n","Skipping y_hat=1668\n","Skipping y_hat=1668\n","Skipping y_hat=2865\n","Skipping y_hat=2865\n","Skipping y_hat=2865\n","Skipping y_hat=2865\n","Skipping y_hat=2865\n","Skipping y_hat=2865\n","Skipping y_hat=2865\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=1475\n","Skipping y_hat=1475\n","Skipping y_hat=1475\n","Skipping y_hat=1475\n","Skipping y_hat=1475\n","Skipping y_hat=1475\n","Skipping y_hat=1475\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2673\n","Skipping y_hat=2673\n","Skipping y_hat=2673\n","Skipping y_hat=2673\n","Skipping y_hat=2673\n","Skipping y_hat=2673\n","Skipping y_hat=2673\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2825\n","Skipping y_hat=2250\n","Skipping y_hat=2250\n","Skipping y_hat=2250\n","Skipping y_hat=2250\n","Skipping y_hat=2250\n","Skipping y_hat=2250\n","Skipping y_hat=2250\n","Skipping y_hat=3047\n","Skipping y_hat=3047\n","Skipping y_hat=3047\n","Skipping y_hat=3047\n","Skipping y_hat=3047\n","Skipping y_hat=3047\n","Skipping y_hat=3047\n","Skipping y_hat=2756\n","Skipping y_hat=2756\n","Skipping y_hat=2756\n","Skipping y_hat=2756\n","Skipping y_hat=2756\n","Skipping y_hat=2756\n","Skipping y_hat=2756\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2874\n","Skipping y_hat=2874\n","Skipping y_hat=2874\n","Skipping y_hat=2874\n","Skipping y_hat=2874\n","Skipping y_hat=2874\n","Skipping y_hat=2874\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=2742\n","Skipping y_hat=2742\n","Skipping y_hat=2742\n","Skipping y_hat=2742\n","Skipping y_hat=2742\n","Skipping y_hat=2742\n","Skipping y_hat=2742\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=2444\n","Skipping y_hat=1351\n","Skipping y_hat=1351\n","Skipping y_hat=1351\n","Skipping y_hat=1351\n","Skipping y_hat=1351\n","Skipping y_hat=1351\n","Skipping y_hat=1351\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=1864\n","Skipping y_hat=2173\n","Skipping y_hat=2173\n","Skipping y_hat=2173\n","Skipping y_hat=2173\n","Skipping y_hat=2173\n","Skipping y_hat=2173\n","Skipping y_hat=2173\n","Skipping y_hat=1807\n","Skipping y_hat=1807\n","Skipping y_hat=1807\n","Skipping y_hat=1807\n","Skipping y_hat=1807\n","Skipping y_hat=1807\n","Skipping y_hat=1807\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=2525\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=1015\n","Skipping y_hat=1015\n","Skipping y_hat=1015\n","Skipping y_hat=1015\n","Skipping y_hat=1015\n","Skipping y_hat=1015\n","Skipping y_hat=1015\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=2239\n","Skipping y_hat=2158\n","Skipping y_hat=2158\n","Skipping y_hat=2158\n","Skipping y_hat=2158\n","Skipping y_hat=2158\n","Skipping y_hat=2158\n","Skipping y_hat=2158\n","Skipping y_hat=2778\n","Skipping y_hat=2778\n","Skipping y_hat=2778\n","Skipping y_hat=2778\n","Skipping y_hat=2778\n","Skipping y_hat=2778\n","Skipping y_hat=2778\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2098\n","Skipping y_hat=2098\n","Skipping y_hat=2098\n","Skipping y_hat=2098\n","Skipping y_hat=2098\n","Skipping y_hat=2098\n","Skipping y_hat=2098\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1931\n","Skipping y_hat=1857\n","Skipping y_hat=1857\n","Skipping y_hat=1857\n","Skipping y_hat=1857\n","Skipping y_hat=1857\n","Skipping y_hat=1857\n","Skipping y_hat=1857\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=3028\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=1893\n","Skipping y_hat=1893\n","Skipping y_hat=1893\n","Skipping y_hat=1893\n","Skipping y_hat=1893\n","Skipping y_hat=1893\n","Skipping y_hat=1893\n","Skipping y_hat=1683\n","Skipping y_hat=1683\n","Skipping y_hat=1683\n","Skipping y_hat=1683\n","Skipping y_hat=1683\n","Skipping y_hat=1683\n","Skipping y_hat=1683\n","Skipping y_hat=1193\n","Skipping y_hat=1193\n","Skipping y_hat=1193\n","Skipping y_hat=1193\n","Skipping y_hat=1193\n","Skipping y_hat=1193\n","Skipping y_hat=1193\n","Skipping y_hat=2535\n","Skipping y_hat=2535\n","Skipping y_hat=2535\n","Skipping y_hat=2535\n","Skipping y_hat=2535\n","Skipping y_hat=2535\n","Skipping y_hat=2535\n","Skipping y_hat=2363\n","Skipping y_hat=2363\n","Skipping y_hat=2363\n","Skipping y_hat=2363\n","Skipping y_hat=2363\n","Skipping y_hat=2363\n","Skipping y_hat=2363\n","Skipping y_hat=2997\n","Skipping y_hat=2997\n","Skipping y_hat=2997\n","Skipping y_hat=2997\n","Skipping y_hat=2997\n","Skipping y_hat=2997\n","Skipping y_hat=2997\n","Skipping y_hat=3196\n","Skipping y_hat=3196\n","Skipping y_hat=3196\n","Skipping y_hat=3196\n","Skipping y_hat=3196\n","Skipping y_hat=3196\n","Skipping y_hat=3196\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=1967\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=2784\n","Skipping y_hat=2784\n","Skipping y_hat=2784\n","Skipping y_hat=2784\n","Skipping y_hat=2784\n","Skipping y_hat=2784\n","Skipping y_hat=2784\n","Skipping y_hat=1616\n","Skipping y_hat=1616\n","Skipping y_hat=1616\n","Skipping y_hat=1616\n","Skipping y_hat=1616\n","Skipping y_hat=1616\n","Skipping y_hat=1616\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=2333\n","Skipping y_hat=2333\n","Skipping y_hat=2333\n","Skipping y_hat=2333\n","Skipping y_hat=2333\n","Skipping y_hat=2333\n","Skipping y_hat=2333\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2474\n","Skipping y_hat=2454\n","Skipping y_hat=2454\n","Skipping y_hat=2454\n","Skipping y_hat=2454\n","Skipping y_hat=2454\n","Skipping y_hat=2454\n","Skipping y_hat=2454\n","Skipping y_hat=1252\n","Skipping y_hat=1252\n","Skipping y_hat=1252\n","Skipping y_hat=1252\n","Skipping y_hat=1252\n","Skipping y_hat=1252\n","Skipping y_hat=1252\n","Skipping y_hat=1738\n","Skipping y_hat=1738\n","Skipping y_hat=1738\n","Skipping y_hat=1738\n","Skipping y_hat=1738\n","Skipping y_hat=1738\n","Skipping y_hat=1738\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1937\n","Skipping y_hat=1657\n","Skipping y_hat=1657\n","Skipping y_hat=1657\n","Skipping y_hat=1657\n","Skipping y_hat=1657\n","Skipping y_hat=1657\n","Skipping y_hat=1657\n","Skipping y_hat=2527\n","Skipping y_hat=2527\n","Skipping y_hat=2527\n","Skipping y_hat=2527\n","Skipping y_hat=2527\n","Skipping y_hat=2527\n","Skipping y_hat=2527\n","Skipping y_hat=1504\n","Skipping y_hat=1504\n","Skipping y_hat=1504\n","Skipping y_hat=1504\n","Skipping y_hat=1504\n","Skipping y_hat=1504\n","Skipping y_hat=1504\n","Skipping y_hat=1929\n","Skipping y_hat=1929\n","Skipping y_hat=1929\n","Skipping y_hat=1929\n","Skipping y_hat=1929\n","Skipping y_hat=1929\n","Skipping y_hat=1929\n","Skipping y_hat=2779\n","Skipping y_hat=2779\n","Skipping y_hat=2779\n","Skipping y_hat=2779\n","Skipping y_hat=2779\n","Skipping y_hat=2779\n","Skipping y_hat=2779\n","Skipping y_hat=2849\n","Skipping y_hat=2849\n","Skipping y_hat=2849\n","Skipping y_hat=2849\n","Skipping y_hat=2849\n","Skipping y_hat=2849\n","Skipping y_hat=2849\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2605\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2340\n","Skipping y_hat=2340\n","Skipping y_hat=2340\n","Skipping y_hat=2340\n","Skipping y_hat=2340\n","Skipping y_hat=2340\n","Skipping y_hat=2340\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=1969\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=2755\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=2764\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1403\n","Skipping y_hat=1403\n","Skipping y_hat=1403\n","Skipping y_hat=1403\n","Skipping y_hat=1403\n","Skipping y_hat=1403\n","Skipping y_hat=1403\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n","Skipping y_hat=1648\n","Skipping y_hat=1648\n","Skipping y_hat=1648\n","Skipping y_hat=1648\n","Skipping y_hat=1648\n","Skipping y_hat=1648\n","Skipping y_hat=1648\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2899\n","Skipping y_hat=2724\n","Skipping y_hat=2724\n","Skipping y_hat=2724\n","Skipping y_hat=2724\n","Skipping y_hat=2724\n","Skipping y_hat=2724\n","Skipping y_hat=2724\n","Skipping y_hat=3030\n","Skipping y_hat=3030\n","Skipping y_hat=3030\n","Skipping y_hat=3030\n","Skipping y_hat=3030\n","Skipping y_hat=3030\n","Skipping y_hat=3030\n","Skipping y_hat=1416\n","Skipping y_hat=1416\n","Skipping y_hat=1416\n","Skipping y_hat=1416\n","Skipping y_hat=1416\n","Skipping y_hat=1416\n","Skipping y_hat=1416\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2573\n","Skipping y_hat=2573\n","Skipping y_hat=2573\n","Skipping y_hat=2573\n","Skipping y_hat=2573\n","Skipping y_hat=2573\n","Skipping y_hat=2573\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=2251\n","Skipping y_hat=1564\n","Skipping y_hat=1564\n","Skipping y_hat=1564\n","Skipping y_hat=1564\n","Skipping y_hat=1564\n","Skipping y_hat=1564\n","Skipping y_hat=1564\n","Skipping y_hat=1876\n","Skipping y_hat=1876\n","Skipping y_hat=1876\n","Skipping y_hat=1876\n","Skipping y_hat=1876\n","Skipping y_hat=1876\n","Skipping y_hat=1876\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=2397\n","Skipping y_hat=1835\n","Skipping y_hat=1835\n","Skipping y_hat=1835\n","Skipping y_hat=1835\n","Skipping y_hat=1835\n","Skipping y_hat=1835\n","Skipping y_hat=1835\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=1713\n","Skipping y_hat=1713\n","Skipping y_hat=1713\n","Skipping y_hat=1713\n","Skipping y_hat=1713\n","Skipping y_hat=1713\n","Skipping y_hat=1713\n"," 97% 77/79 [00:09<00:00,  8.06it/s]Skipping y_hat=2949\n","Skipping y_hat=2949\n","Skipping y_hat=2949\n","Skipping y_hat=2949\n","Skipping y_hat=2949\n","Skipping y_hat=2949\n","Skipping y_hat=2949\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1806\n","Skipping y_hat=1806\n","Skipping y_hat=1806\n","Skipping y_hat=1806\n","Skipping y_hat=1806\n","Skipping y_hat=1806\n","Skipping y_hat=1806\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=1908\n","Skipping y_hat=1908\n","Skipping y_hat=1908\n","Skipping y_hat=1908\n","Skipping y_hat=1908\n","Skipping y_hat=1908\n","Skipping y_hat=1908\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2231\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=2046\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=3134\n","Skipping y_hat=3134\n","Skipping y_hat=3134\n","Skipping y_hat=3134\n","Skipping y_hat=3134\n","Skipping y_hat=3134\n","Skipping y_hat=3134\n","Skipping y_hat=2530\n","Skipping y_hat=2530\n","Skipping y_hat=2530\n","Skipping y_hat=2530\n","Skipping y_hat=2530\n","Skipping y_hat=2530\n","Skipping y_hat=2530\n","Skipping y_hat=1886\n","Skipping y_hat=1886\n","Skipping y_hat=1886\n","Skipping y_hat=1886\n","Skipping y_hat=1886\n","Skipping y_hat=1886\n","Skipping y_hat=1886\n","Skipping y_hat=2327\n","Skipping y_hat=2327\n","Skipping y_hat=2327\n","Skipping y_hat=2327\n","Skipping y_hat=2327\n","Skipping y_hat=2327\n","Skipping y_hat=2327\n","Skipping y_hat=2561\n","Skipping y_hat=2561\n","Skipping y_hat=2561\n","Skipping y_hat=2561\n","Skipping y_hat=2561\n","Skipping y_hat=2561\n","Skipping y_hat=2561\n","Skipping y_hat=2846\n","Skipping y_hat=2846\n","Skipping y_hat=2846\n","Skipping y_hat=2846\n","Skipping y_hat=2846\n","Skipping y_hat=2846\n","Skipping y_hat=2846\n","Skipping y_hat=2736\n","Skipping y_hat=2736\n","Skipping y_hat=2736\n","Skipping y_hat=2736\n","Skipping y_hat=2736\n","Skipping y_hat=2736\n","Skipping y_hat=2736\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=2400\n","Skipping y_hat=2400\n","Skipping y_hat=2400\n","Skipping y_hat=2400\n","Skipping y_hat=2400\n","Skipping y_hat=2400\n","Skipping y_hat=2400\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=3361\n","Skipping y_hat=3361\n","Skipping y_hat=3361\n","Skipping y_hat=3361\n","Skipping y_hat=3361\n","Skipping y_hat=3361\n","Skipping y_hat=3361\n","Skipping y_hat=2536\n","Skipping y_hat=2536\n","Skipping y_hat=2536\n","Skipping y_hat=2536\n","Skipping y_hat=2536\n","Skipping y_hat=2536\n","Skipping y_hat=2536\n","Skipping y_hat=2190\n","Skipping y_hat=2190\n","Skipping y_hat=2190\n","Skipping y_hat=2190\n","Skipping y_hat=2190\n","Skipping y_hat=2190\n","Skipping y_hat=2190\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=2273\n","Skipping y_hat=2273\n","Skipping y_hat=2273\n","Skipping y_hat=2273\n","Skipping y_hat=2273\n","Skipping y_hat=2273\n","Skipping y_hat=2273\n","Skipping y_hat=3644\n","Skipping y_hat=3644\n","Skipping y_hat=3644\n","Skipping y_hat=3644\n","Skipping y_hat=3644\n","Skipping y_hat=3644\n","Skipping y_hat=3644\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=3164\n","Skipping y_hat=3164\n","Skipping y_hat=3164\n","Skipping y_hat=3164\n","Skipping y_hat=3164\n","Skipping y_hat=3164\n","Skipping y_hat=3164\n","Skipping y_hat=3181\n","Skipping y_hat=3181\n","Skipping y_hat=3181\n","Skipping y_hat=3181\n","Skipping y_hat=3181\n","Skipping y_hat=3181\n","Skipping y_hat=3181\n","Skipping y_hat=2464\n","Skipping y_hat=2464\n","Skipping y_hat=2464\n","Skipping y_hat=2464\n","Skipping y_hat=2464\n","Skipping y_hat=2464\n","Skipping y_hat=2464\n","Skipping y_hat=2457\n","Skipping y_hat=2457\n","Skipping y_hat=2457\n","Skipping y_hat=2457\n","Skipping y_hat=2457\n","Skipping y_hat=2457\n","Skipping y_hat=2457\n","Skipping y_hat=2617\n","Skipping y_hat=2617\n","Skipping y_hat=2617\n","Skipping y_hat=2617\n","Skipping y_hat=2617\n","Skipping y_hat=2617\n","Skipping y_hat=2617\n","Skipping y_hat=2384\n","Skipping y_hat=2384\n","Skipping y_hat=2384\n","Skipping y_hat=2384\n","Skipping y_hat=2384\n","Skipping y_hat=2384\n","Skipping y_hat=2384\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2428\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2693\n","Skipping y_hat=2343\n","Skipping y_hat=2343\n","Skipping y_hat=2343\n","Skipping y_hat=2343\n","Skipping y_hat=2343\n","Skipping y_hat=2343\n","Skipping y_hat=2343\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2003\n","Skipping y_hat=2003\n","Skipping y_hat=2003\n","Skipping y_hat=2003\n","Skipping y_hat=2003\n","Skipping y_hat=2003\n","Skipping y_hat=2003\n","Skipping y_hat=2930\n","Skipping y_hat=2930\n","Skipping y_hat=2930\n","Skipping y_hat=2930\n","Skipping y_hat=2930\n","Skipping y_hat=2930\n","Skipping y_hat=2930\n","Skipping y_hat=2715\n","Skipping y_hat=2715\n","Skipping y_hat=2715\n","Skipping y_hat=2715\n","Skipping y_hat=2715\n","Skipping y_hat=2715\n","Skipping y_hat=2715\n","Skipping y_hat=2176\n","Skipping y_hat=2176\n","Skipping y_hat=2176\n","Skipping y_hat=2176\n","Skipping y_hat=2176\n","Skipping y_hat=2176\n","Skipping y_hat=2176\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=2449\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=2640\n","Skipping y_hat=3233\n","Skipping y_hat=3233\n","Skipping y_hat=3233\n","Skipping y_hat=3233\n","Skipping y_hat=3233\n","Skipping y_hat=3233\n","Skipping y_hat=3233\n","Skipping y_hat=2051\n","Skipping y_hat=2051\n","Skipping y_hat=2051\n","Skipping y_hat=2051\n","Skipping y_hat=2051\n","Skipping y_hat=2051\n","Skipping y_hat=2051\n","Skipping y_hat=2069\n","Skipping y_hat=2069\n","Skipping y_hat=2069\n","Skipping y_hat=2069\n","Skipping y_hat=2069\n","Skipping y_hat=2069\n","Skipping y_hat=2069\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2758\n","Skipping y_hat=2382\n","Skipping y_hat=2382\n","Skipping y_hat=2382\n","Skipping y_hat=2382\n","Skipping y_hat=2382\n","Skipping y_hat=2382\n","Skipping y_hat=2382\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1982\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=2026\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=1926\n","Skipping y_hat=2622\n","Skipping y_hat=2622\n","Skipping y_hat=2622\n","Skipping y_hat=2622\n","Skipping y_hat=2622\n","Skipping y_hat=2622\n","Skipping y_hat=2622\n","Skipping y_hat=2716\n","Skipping y_hat=2716\n","Skipping y_hat=2716\n","Skipping y_hat=2716\n","Skipping y_hat=2716\n","Skipping y_hat=2716\n","Skipping y_hat=2716\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=2297\n","Skipping y_hat=2297\n","Skipping y_hat=2297\n","Skipping y_hat=2297\n","Skipping y_hat=2297\n","Skipping y_hat=2297\n","Skipping y_hat=2297\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=1975\n","Skipping y_hat=2588\n","Skipping y_hat=2588\n","Skipping y_hat=2588\n","Skipping y_hat=2588\n","Skipping y_hat=2588\n","Skipping y_hat=2588\n","Skipping y_hat=2588\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2723\n","Skipping y_hat=2022\n","Skipping y_hat=2022\n","Skipping y_hat=2022\n","Skipping y_hat=2022\n","Skipping y_hat=2022\n","Skipping y_hat=2022\n","Skipping y_hat=2022\n","Skipping y_hat=2107\n","Skipping y_hat=2107\n","Skipping y_hat=2107\n","Skipping y_hat=2107\n","Skipping y_hat=2107\n","Skipping y_hat=2107\n","Skipping y_hat=2107\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=1543\n","Skipping y_hat=1745\n","Skipping y_hat=1745\n","Skipping y_hat=1745\n","Skipping y_hat=1745\n","Skipping y_hat=1745\n","Skipping y_hat=1745\n","Skipping y_hat=1745\n","Skipping y_hat=2458\n","Skipping y_hat=2458\n","Skipping y_hat=2458\n","Skipping y_hat=2458\n","Skipping y_hat=2458\n","Skipping y_hat=2458\n","Skipping y_hat=2458\n","Skipping y_hat=2442\n","Skipping y_hat=2442\n","Skipping y_hat=2442\n","Skipping y_hat=2442\n","Skipping y_hat=2442\n","Skipping y_hat=2442\n","Skipping y_hat=2442\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=1786\n","Skipping y_hat=3285\n","Skipping y_hat=3285\n","Skipping y_hat=3285\n","Skipping y_hat=3285\n","Skipping y_hat=3285\n","Skipping y_hat=3285\n","Skipping y_hat=3285\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1658\n","Skipping y_hat=1658\n","Skipping y_hat=1658\n","Skipping y_hat=1658\n","Skipping y_hat=1658\n","Skipping y_hat=1658\n","Skipping y_hat=1658\n","Skipping y_hat=2687\n","Skipping y_hat=2687\n","Skipping y_hat=2687\n","Skipping y_hat=2687\n","Skipping y_hat=2687\n","Skipping y_hat=2687\n","Skipping y_hat=2687\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=2423\n","Skipping y_hat=2423\n","Skipping y_hat=2423\n","Skipping y_hat=2423\n","Skipping y_hat=2423\n","Skipping y_hat=2423\n","Skipping y_hat=2423\n","Skipping y_hat=1526\n","Skipping y_hat=1526\n","Skipping y_hat=1526\n","Skipping y_hat=1526\n","Skipping y_hat=1526\n","Skipping y_hat=1526\n","Skipping y_hat=1526\n","Skipping y_hat=2425\n","Skipping y_hat=2425\n","Skipping y_hat=2425\n","Skipping y_hat=2425\n","Skipping y_hat=2425\n","Skipping y_hat=2425\n","Skipping y_hat=2425\n","Skipping y_hat=1891\n","Skipping y_hat=1891\n","Skipping y_hat=1891\n","Skipping y_hat=1891\n","Skipping y_hat=1891\n","Skipping y_hat=1891\n","Skipping y_hat=1891\n","Skipping y_hat=2948\n","Skipping y_hat=2948\n","Skipping y_hat=2948\n","Skipping y_hat=2948\n","Skipping y_hat=2948\n","Skipping y_hat=2948\n","Skipping y_hat=2948\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=1633\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2356\n","Skipping y_hat=2192\n","Skipping y_hat=2192\n","Skipping y_hat=2192\n","Skipping y_hat=2192\n","Skipping y_hat=2192\n","Skipping y_hat=2192\n","Skipping y_hat=2192\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=2214\n","Skipping y_hat=2214\n","Skipping y_hat=2214\n","Skipping y_hat=2214\n","Skipping y_hat=2214\n","Skipping y_hat=2214\n","Skipping y_hat=2214\n","Skipping y_hat=2429\n","Skipping y_hat=2429\n","Skipping y_hat=2429\n","Skipping y_hat=2429\n","Skipping y_hat=2429\n","Skipping y_hat=2429\n","Skipping y_hat=2429\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=1793\n","Skipping y_hat=2543\n","Skipping y_hat=2543\n","Skipping y_hat=2543\n","Skipping y_hat=2543\n","Skipping y_hat=2543\n","Skipping y_hat=2543\n","Skipping y_hat=2543\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=2385\n","Skipping y_hat=3128\n","Skipping y_hat=3128\n","Skipping y_hat=3128\n","Skipping y_hat=3128\n","Skipping y_hat=3128\n","Skipping y_hat=3128\n","Skipping y_hat=3128\n","Skipping y_hat=2223\n","Skipping y_hat=2223\n","Skipping y_hat=2223\n","Skipping y_hat=2223\n","Skipping y_hat=2223\n","Skipping y_hat=2223\n","Skipping y_hat=2223\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=1548\n","Skipping y_hat=2668\n","Skipping y_hat=2668\n","Skipping y_hat=2668\n","Skipping y_hat=2668\n","Skipping y_hat=2668\n","Skipping y_hat=2668\n","Skipping y_hat=2668\n","Skipping y_hat=1764\n","Skipping y_hat=1764\n","Skipping y_hat=1764\n","Skipping y_hat=1764\n","Skipping y_hat=1764\n","Skipping y_hat=1764\n","Skipping y_hat=1764\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=3100\n","Skipping y_hat=3100\n","Skipping y_hat=3100\n","Skipping y_hat=3100\n","Skipping y_hat=3100\n","Skipping y_hat=3100\n","Skipping y_hat=3100\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=2283\n","Skipping y_hat=2283\n","Skipping y_hat=2283\n","Skipping y_hat=2283\n","Skipping y_hat=2283\n","Skipping y_hat=2283\n","Skipping y_hat=2283\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=2487\n","Skipping y_hat=2683\n","Skipping y_hat=2683\n","Skipping y_hat=2683\n","Skipping y_hat=2683\n","Skipping y_hat=2683\n","Skipping y_hat=2683\n","Skipping y_hat=2683\n","Skipping y_hat=2438\n","Skipping y_hat=2438\n","Skipping y_hat=2438\n","Skipping y_hat=2438\n","Skipping y_hat=2438\n","Skipping y_hat=2438\n","Skipping y_hat=2438\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=3044\n","Skipping y_hat=3044\n","Skipping y_hat=3044\n","Skipping y_hat=3044\n","Skipping y_hat=3044\n","Skipping y_hat=3044\n","Skipping y_hat=3044\n","Skipping y_hat=2686\n","Skipping y_hat=2686\n","Skipping y_hat=2686\n","Skipping y_hat=2686\n","Skipping y_hat=2686\n","Skipping y_hat=2686\n","Skipping y_hat=2686\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=2310\n","Skipping y_hat=2310\n","Skipping y_hat=2310\n","Skipping y_hat=2310\n","Skipping y_hat=2310\n","Skipping y_hat=2310\n","Skipping y_hat=2310\n","Skipping y_hat=2547\n","Skipping y_hat=2547\n","Skipping y_hat=2547\n","Skipping y_hat=2547\n","Skipping y_hat=2547\n","Skipping y_hat=2547\n","Skipping y_hat=2547\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2608\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=2182\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=2253\n","Skipping y_hat=1974\n","Skipping y_hat=1974\n","Skipping y_hat=1974\n","Skipping y_hat=1974\n","Skipping y_hat=1974\n","Skipping y_hat=1974\n","Skipping y_hat=1974\n","Skipping y_hat=2010\n","Skipping y_hat=2010\n","Skipping y_hat=2010\n","Skipping y_hat=2010\n","Skipping y_hat=2010\n","Skipping y_hat=2010\n","Skipping y_hat=2010\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=1393\n","Skipping y_hat=2063\n","Skipping y_hat=2063\n","Skipping y_hat=2063\n","Skipping y_hat=2063\n","Skipping y_hat=2063\n","Skipping y_hat=2063\n","Skipping y_hat=2063\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=1734\n","Skipping y_hat=2375\n","Skipping y_hat=2375\n","Skipping y_hat=2375\n","Skipping y_hat=2375\n","Skipping y_hat=2375\n","Skipping y_hat=2375\n","Skipping y_hat=2375\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2308\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n","Skipping y_hat=2086\n"," 99% 78/79 [00:09<00:00,  7.98it/s]Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2105\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=2744\n","Skipping y_hat=2744\n","Skipping y_hat=2744\n","Skipping y_hat=2744\n","Skipping y_hat=2744\n","Skipping y_hat=2744\n","Skipping y_hat=2744\n","Skipping y_hat=1725\n","Skipping y_hat=1725\n","Skipping y_hat=1725\n","Skipping y_hat=1725\n","Skipping y_hat=1725\n","Skipping y_hat=1725\n","Skipping y_hat=1725\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2499\n","Skipping y_hat=2524\n","Skipping y_hat=2524\n","Skipping y_hat=2524\n","Skipping y_hat=2524\n","Skipping y_hat=2524\n","Skipping y_hat=2524\n","Skipping y_hat=2524\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2649\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2833\n","Skipping y_hat=2104\n","Skipping y_hat=2104\n","Skipping y_hat=2104\n","Skipping y_hat=2104\n","Skipping y_hat=2104\n","Skipping y_hat=2104\n","Skipping y_hat=2104\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=2986\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=2667\n","Skipping y_hat=2667\n","Skipping y_hat=2667\n","Skipping y_hat=2667\n","Skipping y_hat=2667\n","Skipping y_hat=2667\n","Skipping y_hat=2667\n","Skipping y_hat=2238\n","Skipping y_hat=2238\n","Skipping y_hat=2238\n","Skipping y_hat=2238\n","Skipping y_hat=2238\n","Skipping y_hat=2238\n","Skipping y_hat=2238\n","Skipping y_hat=2651\n","Skipping y_hat=2651\n","Skipping y_hat=2651\n","Skipping y_hat=2651\n","Skipping y_hat=2651\n","Skipping y_hat=2651\n","Skipping y_hat=2651\n","100% 79/79 [00:09<00:00,  8.07it/s]\n","accuracy of 10000 examples: 9980/10000 (99.8%)\n","\n","Test Results:\n","test: 99.80%\n","\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/addition/train_with_eval_by_reading.py\", line 442, in <module>\n","    logits, loss = model(X, Y)\n","                   ^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n","    return fn(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/addition/model.py\", line 150, in forward\n","    def forward(self, idx, targets=None):\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 745, in _fn\n","    return fn(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 1184, in forward\n","    return compiled_fn(full_args)\n","           ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 310, in runtime_wrapper\n","    all_outs = call_func_at_runtime_with_args(\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n","    out = normalize_as_list(f(args))\n","                            ^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 100, in g\n","    return f(*args)\n","           ^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\", line 575, in apply\n","    return super().apply(*args, **kwargs)  # type: ignore[misc]\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 1585, in forward\n","    fw_outs = call_func_at_runtime_with_args(\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n","    out = normalize_as_list(f(args))\n","                            ^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 490, in wrapper\n","    return compiled_fn(runtime_args)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 672, in inner_fn\n","    outs = compiled_fn(args)\n","           ^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/output_code.py\", line 466, in __call__\n","    return self.current_callable(inputs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/utils.py\", line 2128, in run\n","    return model(new_inputs)\n","           ^^^^^^^^^^^^^^^^^\n","  File \"/tmp/torchinductor_root/t3/ct35bp2ifstxpiux64kf7vii3omnqhjmd7mt7dlbuoxcnethxdhr.py\", line 1447, in call\n","    return (reinterpret_tensor(buf192, (s0, 32, 14), (448, 14, 1), 0), buf211, primals_2, primals_5, primals_8, primals_11, primals_14, primals_17, primals_20, primals_23, primals_26, primals_29, primals_32, primals_35, primals_38, primals_41, primals_43, buf0, buf1, buf2, buf5, buf6, buf9, reinterpret_tensor(buf11, (32*s0, 384), (384, 1), 0), reinterpret_tensor(buf12, (s0, 6, 32, 64), (36864, 64, 1152, 1), 384), reinterpret_tensor(buf12, (s0, 6, 32, 64), (36864, 64, 1152, 1), 0), reinterpret_tensor(buf12, (s0, 6, 32, 64), (36864, 64, 1152, 1), 768), buf14, buf15, buf16, buf17, buf22, buf27, reinterpret_tensor(buf29, (32*s0, 384), (384, 1), 0), buf30, reinterpret_tensor(buf32, (32*s0, 1536), (1536, 1), 0), buf35, buf39, reinterpret_tensor(buf41, (32*s0, 384), (384, 1), 0), reinterpret_tensor(buf42, (s0, 6, 32, 64), (36864, 64, 1152, 1), 384), reinterpret_tensor(buf42, (s0, 6, 32, 64), (36864, 64, 1152, 1), 0), reinterpret_tensor(buf42, (s0, 6, 32, 64), (36864, 64, 1152, 1), 768), buf44, buf45, buf46, buf47, buf52, buf57, reinterpret_tensor(buf59, (32*s0, 384), (384, 1), 0), buf60, reinterpret_tensor(buf62, (32*s0, 1536), (1536, 1), 0), buf65, buf69, reinterpret_tensor(buf71, (32*s0, 384), (384, 1), 0), reinterpret_tensor(buf72, (s0, 6, 32, 64), (36864, 64, 1152, 1), 384), reinterpret_tensor(buf72, (s0, 6, 32, 64), (36864, 64, 1152, 1), 0), reinterpret_tensor(buf72, (s0, 6, 32, 64), (36864, 64, 1152, 1), 768), buf74, buf75, buf76, buf77, buf82, buf87, reinterpret_tensor(buf89, (32*s0, 384), (384, 1), 0), buf90, reinterpret_tensor(buf92, (32*s0, 1536), (1536, 1), 0), buf95, buf99, reinterpret_tensor(buf101, (32*s0, 384), (384, 1), 0), reinterpret_tensor(buf102, (s0, 6, 32, 64), (36864, 64, 1152, 1), 384), reinterpret_tensor(buf102, (s0, 6, 32, 64), (36864, 64, 1152, 1), 0), reinterpret_tensor(buf102, (s0, 6, 32, 64), (36864, 64, 1152, 1), 768), buf104, buf105, buf106, buf107, buf112, buf117, reinterpret_tensor(buf119, (32*s0, 384), (384, 1), 0), buf120, reinterpret_tensor(buf122, (32*s0, 1536), (1536, 1), 0), buf125, buf129, reinterpret_tensor(buf131, (32*s0, 384), (384, 1), 0), reinterpret_tensor(buf132, (s0, 6, 32, 64), (36864, 64, 1152, 1), 384), reinterpret_tensor(buf132, (s0, 6, 32, 64), (36864, 64, 1152, 1), 0), reinterpret_tensor(buf132, (s0, 6, 32, 64), (36864, 64, 1152, 1), 768), buf134, buf135, buf136, buf137, buf142, buf147, reinterpret_tensor(buf149, (32*s0, 384), (384, 1), 0), buf150, reinterpret_tensor(buf152, (32*s0, 1536), (1536, 1), 0), buf155, buf159, reinterpret_tensor(buf161, (32*s0, 384), (384, 1), 0), reinterpret_tensor(buf162, (s0, 6, 32, 64), (36864, 64, 1152, 1), 384), reinterpret_tensor(buf162, (s0, 6, 32, 64), (36864, 64, 1152, 1), 0), reinterpret_tensor(buf162, (s0, 6, 32, 64), (36864, 64, 1152, 1), 768), buf164, buf165, buf166, buf167, buf172, buf177, reinterpret_tensor(buf179, (32*s0, 384), (384, 1), 0), buf180, reinterpret_tensor(buf182, (32*s0, 1536), (1536, 1), 0), buf185, buf189, reinterpret_tensor(buf191, (32*s0, 384), (384, 1), 0), buf195, buf197, reinterpret_tensor(buf190, (14, 384), (384, 1), 0), buf199, reinterpret_tensor(buf181, (384, 1536), (1536, 1), 0), reinterpret_tensor(buf178, (1536, 384), (384, 1), 0), buf200, reinterpret_tensor(buf169, (384, 384), (384, 1), 0), reinterpret_tensor(buf160, (1152, 384), (384, 1), 0), buf201, reinterpret_tensor(buf151, (384, 1536), (1536, 1), 0), reinterpret_tensor(buf148, (1536, 384), (384, 1), 0), buf202, reinterpret_tensor(buf139, (384, 384), (384, 1), 0), reinterpret_tensor(buf130, (1152, 384), (384, 1), 0), buf203, reinterpret_tensor(buf121, (384, 1536), (1536, 1), 0), reinterpret_tensor(buf118, (1536, 384), (384, 1), 0), buf204, reinterpret_tensor(buf109, (384, 384), (384, 1), 0), reinterpret_tensor(buf100, (1152, 384), (384, 1), 0), buf205, reinterpret_tensor(buf91, (384, 1536), (1536, 1), 0), reinterpret_tensor(buf88, (1536, 384), (384, 1), 0), buf206, reinterpret_tensor(buf79, (384, 384), (384, 1), 0), reinterpret_tensor(buf70, (1152, 384), (384, 1), 0), buf207, reinterpret_tensor(buf61, (384, 1536), (1536, 1), 0), reinterpret_tensor(buf58, (1536, 384), (384, 1), 0), buf208, reinterpret_tensor(buf49, (384, 384), (384, 1), 0), reinterpret_tensor(buf40, (1152, 384), (384, 1), 0), buf209, reinterpret_tensor(buf31, (384, 1536), (1536, 1), 0), reinterpret_tensor(buf28, (1536, 384), (384, 1), 0), buf210, reinterpret_tensor(buf19, (384, 384), (384, 1), 0), reinterpret_tensor(buf10, (1152, 384), (384, 1), 0), s0, s0, 32*s0, )\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/addition/train_with_eval_by_reading.py\", line 441, in <module>\n","    with ctx:\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\", line 384, in __exit__\n","    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any):  # type: ignore[override]\n","    \n","KeyboardInterrupt\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m4_operands_3_digit_uniform_output_padding\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/dnllayjb\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m4_operands_3_digit_uniform_output_padding/plain_out/wandb/run-20250707_193151-dnllayjb/logs\u001b[0m\n"]}],"source":["!python train_with_eval_by_reading.py 4_operands_addition_plain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":154411,"status":"ok","timestamp":1752182143963,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"Vq3_TEheVMmW","outputId":"442fc0ab-b394-47d5-8bfb-43a5feb32f6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=5252\n","Skipping y_hat=5252\n","Skipping y_hat=5252\n","Skipping y_hat=5252\n","Skipping y_hat=5252\n","Skipping y_hat=3742\n","Skipping y_hat=3742\n","Skipping y_hat=3742\n","Skipping y_hat=3742\n","Skipping y_hat=3742\n","Skipping y_hat=3742\n","Skipping y_hat=3742\n","Skipping y_hat=6881\n","Skipping y_hat=6881\n","Skipping y_hat=6881\n","Skipping y_hat=6881\n","Skipping y_hat=6881\n","Skipping y_hat=6881\n","Skipping y_hat=6881\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=6041\n","Skipping y_hat=6041\n","Skipping y_hat=6041\n","Skipping y_hat=6041\n","Skipping y_hat=6041\n","Skipping y_hat=6041\n","Skipping y_hat=6041\n","Skipping y_hat=8613\n","Skipping y_hat=8613\n","Skipping y_hat=8613\n","Skipping y_hat=8613\n","Skipping y_hat=8613\n","Skipping y_hat=8613\n","Skipping y_hat=8613\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=2781\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=9103\n","Skipping y_hat=9103\n","Skipping y_hat=9103\n","Skipping y_hat=9103\n","Skipping y_hat=9103\n","Skipping y_hat=9103\n","Skipping y_hat=9103\n","Skipping y_hat=4731\n","Skipping y_hat=4731\n","Skipping y_hat=4731\n","Skipping y_hat=4731\n","Skipping y_hat=4731\n","Skipping y_hat=4731\n","Skipping y_hat=4731\n","Skipping y_hat=5202\n","Skipping y_hat=5202\n","Skipping y_hat=5202\n","Skipping y_hat=5202\n","Skipping y_hat=5202\n","Skipping y_hat=5202\n","Skipping y_hat=5202\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=2681\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=9833\n","Skipping y_hat=9833\n","Skipping y_hat=9833\n","Skipping y_hat=9833\n","Skipping y_hat=9833\n","Skipping y_hat=9833\n","Skipping y_hat=9833\n","Skipping y_hat=0782\n","Skipping y_hat=0782\n","Skipping y_hat=0782\n","Skipping y_hat=0782\n","Skipping y_hat=0782\n","Skipping y_hat=0782\n","Skipping y_hat=0782\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=9461\n","Skipping y_hat=9461\n","Skipping y_hat=9461\n","Skipping y_hat=9461\n","Skipping y_hat=9461\n","Skipping y_hat=9461\n","Skipping y_hat=9461\n","Skipping y_hat=5591\n","Skipping y_hat=5591\n","Skipping y_hat=5591\n","Skipping y_hat=5591\n","Skipping y_hat=5591\n","Skipping y_hat=5591\n","Skipping y_hat=5591\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=9061\n","Skipping y_hat=9061\n","Skipping y_hat=9061\n","Skipping y_hat=9061\n","Skipping y_hat=9061\n","Skipping y_hat=9061\n","Skipping y_hat=9061\n","Skipping y_hat=3562\n","Skipping y_hat=3562\n","Skipping y_hat=3562\n","Skipping y_hat=3562\n","Skipping y_hat=3562\n","Skipping y_hat=3562\n","Skipping y_hat=3562\n","Skipping y_hat=2303\n","Skipping y_hat=2303\n","Skipping y_hat=2303\n","Skipping y_hat=2303\n","Skipping y_hat=2303\n","Skipping y_hat=2303\n","Skipping y_hat=2303\n","Skipping y_hat=0392\n","Skipping y_hat=0392\n","Skipping y_hat=0392\n","Skipping y_hat=0392\n","Skipping y_hat=0392\n","Skipping y_hat=0392\n","Skipping y_hat=0392\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=9161\n","Skipping y_hat=9161\n","Skipping y_hat=9161\n","Skipping y_hat=9161\n","Skipping y_hat=9161\n","Skipping y_hat=9161\n","Skipping y_hat=9161\n","Skipping y_hat=9370\n","Skipping y_hat=9370\n","Skipping y_hat=9370\n","Skipping y_hat=9370\n","Skipping y_hat=9370\n","Skipping y_hat=9370\n","Skipping y_hat=9370\n","Skipping y_hat=3061\n","Skipping y_hat=3061\n","Skipping y_hat=3061\n","Skipping y_hat=3061\n","Skipping y_hat=3061\n","Skipping y_hat=3061\n","Skipping y_hat=3061\n","Skipping y_hat=8932\n","Skipping y_hat=8932\n","Skipping y_hat=8932\n","Skipping y_hat=8932\n","Skipping y_hat=8932\n","Skipping y_hat=8932\n","Skipping y_hat=8932\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=0771\n","Skipping y_hat=0771\n","Skipping y_hat=0771\n","Skipping y_hat=0771\n","Skipping y_hat=0771\n","Skipping y_hat=0771\n","Skipping y_hat=0771\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=6581\n","Skipping y_hat=6581\n","Skipping y_hat=6581\n","Skipping y_hat=6581\n","Skipping y_hat=6581\n","Skipping y_hat=6581\n","Skipping y_hat=6581\n","Skipping y_hat=1102\n","Skipping y_hat=1102\n","Skipping y_hat=1102\n","Skipping y_hat=1102\n","Skipping y_hat=1102\n","Skipping y_hat=1102\n","Skipping y_hat=1102\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=0602\n","Skipping y_hat=0602\n","Skipping y_hat=0602\n","Skipping y_hat=0602\n","Skipping y_hat=0602\n","Skipping y_hat=0602\n","Skipping y_hat=0602\n","Skipping y_hat=3670\n","Skipping y_hat=3670\n","Skipping y_hat=3670\n","Skipping y_hat=3670\n","Skipping y_hat=3670\n","Skipping y_hat=3670\n","Skipping y_hat=3670\n","Skipping y_hat=7632\n","Skipping y_hat=7632\n","Skipping y_hat=7632\n","Skipping y_hat=7632\n","Skipping y_hat=7632\n","Skipping y_hat=7632\n","Skipping y_hat=7632\n","Skipping y_hat=3492\n","Skipping y_hat=3492\n","Skipping y_hat=3492\n","Skipping y_hat=3492\n","Skipping y_hat=3492\n","Skipping y_hat=3492\n","Skipping y_hat=3492\n","Skipping y_hat=6101\n","Skipping y_hat=6101\n","Skipping y_hat=6101\n","Skipping y_hat=6101\n","Skipping y_hat=6101\n","Skipping y_hat=6101\n","Skipping y_hat=6101\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=7282\n","Skipping y_hat=7282\n","Skipping y_hat=7282\n","Skipping y_hat=7282\n","Skipping y_hat=7282\n","Skipping y_hat=7282\n","Skipping y_hat=7282\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=4241\n","Skipping y_hat=4241\n","Skipping y_hat=4241\n","Skipping y_hat=4241\n","Skipping y_hat=4241\n","Skipping y_hat=4241\n","Skipping y_hat=4241\n","Skipping y_hat=5091\n","Skipping y_hat=5091\n","Skipping y_hat=5091\n","Skipping y_hat=5091\n","Skipping y_hat=5091\n","Skipping y_hat=5091\n","Skipping y_hat=5091\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n"," 92% 73/79 [00:08<00:00,  8.44it/s]Skipping y_hat=2112\n","Skipping y_hat=2112\n","Skipping y_hat=2112\n","Skipping y_hat=2112\n","Skipping y_hat=2112\n","Skipping y_hat=2112\n","Skipping y_hat=2112\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4422\n","Skipping y_hat=4422\n","Skipping y_hat=4422\n","Skipping y_hat=4422\n","Skipping y_hat=4422\n","Skipping y_hat=4422\n","Skipping y_hat=4422\n","Skipping y_hat=0131\n","Skipping y_hat=0131\n","Skipping y_hat=0131\n","Skipping y_hat=0131\n","Skipping y_hat=0131\n","Skipping y_hat=0131\n","Skipping y_hat=0131\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=6342\n","Skipping y_hat=6342\n","Skipping y_hat=6342\n","Skipping y_hat=6342\n","Skipping y_hat=6342\n","Skipping y_hat=6342\n","Skipping y_hat=6342\n","Skipping y_hat=9660\n","Skipping y_hat=9660\n","Skipping y_hat=9660\n","Skipping y_hat=9660\n","Skipping y_hat=9660\n","Skipping y_hat=9660\n","Skipping y_hat=9660\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=8951\n","Skipping y_hat=8951\n","Skipping y_hat=8951\n","Skipping y_hat=8951\n","Skipping y_hat=8951\n","Skipping y_hat=8951\n","Skipping y_hat=8951\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9672\n","Skipping y_hat=9672\n","Skipping y_hat=9672\n","Skipping y_hat=9672\n","Skipping y_hat=9672\n","Skipping y_hat=9672\n","Skipping y_hat=9672\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=1441\n","Skipping y_hat=1441\n","Skipping y_hat=1441\n","Skipping y_hat=1441\n","Skipping y_hat=1441\n","Skipping y_hat=1441\n","Skipping y_hat=1441\n","Skipping y_hat=2031\n","Skipping y_hat=2031\n","Skipping y_hat=2031\n","Skipping y_hat=2031\n","Skipping y_hat=2031\n","Skipping y_hat=2031\n","Skipping y_hat=2031\n","Skipping y_hat=5032\n","Skipping y_hat=5032\n","Skipping y_hat=5032\n","Skipping y_hat=5032\n","Skipping y_hat=5032\n","Skipping y_hat=5032\n","Skipping y_hat=5032\n","Skipping y_hat=5971\n","Skipping y_hat=5971\n","Skipping y_hat=5971\n","Skipping y_hat=5971\n","Skipping y_hat=5971\n","Skipping y_hat=5971\n","Skipping y_hat=5971\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=6402\n","Skipping y_hat=6402\n","Skipping y_hat=6402\n","Skipping y_hat=6402\n","Skipping y_hat=6402\n","Skipping y_hat=6402\n","Skipping y_hat=6402\n","Skipping y_hat=2881\n","Skipping y_hat=2881\n","Skipping y_hat=2881\n","Skipping y_hat=2881\n","Skipping y_hat=2881\n","Skipping y_hat=2881\n","Skipping y_hat=2881\n","Skipping y_hat=7812\n","Skipping y_hat=7812\n","Skipping y_hat=7812\n","Skipping y_hat=7812\n","Skipping y_hat=7812\n","Skipping y_hat=7812\n","Skipping y_hat=7812\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=5671\n","Skipping y_hat=5671\n","Skipping y_hat=5671\n","Skipping y_hat=5671\n","Skipping y_hat=5671\n","Skipping y_hat=5671\n","Skipping y_hat=5671\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=5932\n","Skipping y_hat=5932\n","Skipping y_hat=5932\n","Skipping y_hat=5932\n","Skipping y_hat=5932\n","Skipping y_hat=5932\n","Skipping y_hat=5932\n","Skipping y_hat=4581\n","Skipping y_hat=4581\n","Skipping y_hat=4581\n","Skipping y_hat=4581\n","Skipping y_hat=4581\n","Skipping y_hat=4581\n","Skipping y_hat=4581\n","Skipping y_hat=8522\n","Skipping y_hat=8522\n","Skipping y_hat=8522\n","Skipping y_hat=8522\n","Skipping y_hat=8522\n","Skipping y_hat=8522\n","Skipping y_hat=8522\n","Skipping y_hat=2702\n","Skipping y_hat=2702\n","Skipping y_hat=2702\n","Skipping y_hat=2702\n","Skipping y_hat=2702\n","Skipping y_hat=2702\n","Skipping y_hat=2702\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=7752\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=1402\n","Skipping y_hat=1402\n","Skipping y_hat=1402\n","Skipping y_hat=1402\n","Skipping y_hat=1402\n","Skipping y_hat=1402\n","Skipping y_hat=1402\n","Skipping y_hat=3132\n","Skipping y_hat=3132\n","Skipping y_hat=3132\n","Skipping y_hat=3132\n","Skipping y_hat=3132\n","Skipping y_hat=3132\n","Skipping y_hat=3132\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=5203\n","Skipping y_hat=5203\n","Skipping y_hat=5203\n","Skipping y_hat=5203\n","Skipping y_hat=5203\n","Skipping y_hat=5203\n","Skipping y_hat=5203\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=6522\n","Skipping y_hat=6522\n","Skipping y_hat=6522\n","Skipping y_hat=6522\n","Skipping y_hat=6522\n","Skipping y_hat=6522\n","Skipping y_hat=6522\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=6301\n","Skipping y_hat=6301\n","Skipping y_hat=6301\n","Skipping y_hat=6301\n","Skipping y_hat=6301\n","Skipping y_hat=6301\n","Skipping y_hat=6301\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=0181\n","Skipping y_hat=0181\n","Skipping y_hat=0181\n","Skipping y_hat=0181\n","Skipping y_hat=0181\n","Skipping y_hat=0181\n","Skipping y_hat=0181\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=3991\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=6032\n","Skipping y_hat=6032\n","Skipping y_hat=6032\n","Skipping y_hat=6032\n","Skipping y_hat=6032\n","Skipping y_hat=6032\n","Skipping y_hat=6032\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=4761\n","Skipping y_hat=4761\n","Skipping y_hat=4761\n","Skipping y_hat=4761\n","Skipping y_hat=4761\n","Skipping y_hat=4761\n","Skipping y_hat=4761\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=0581\n","Skipping y_hat=0051\n","Skipping y_hat=0051\n","Skipping y_hat=0051\n","Skipping y_hat=0051\n","Skipping y_hat=0051\n","Skipping y_hat=0051\n","Skipping y_hat=0051\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=3442\n","Skipping y_hat=3442\n","Skipping y_hat=3442\n","Skipping y_hat=3442\n","Skipping y_hat=3442\n","Skipping y_hat=3442\n","Skipping y_hat=3442\n","Skipping y_hat=0561\n","Skipping y_hat=0561\n","Skipping y_hat=0561\n","Skipping y_hat=0561\n","Skipping y_hat=0561\n","Skipping y_hat=0561\n","Skipping y_hat=0561\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=5651\n","Skipping y_hat=5651\n","Skipping y_hat=5651\n","Skipping y_hat=5651\n","Skipping y_hat=5651\n","Skipping y_hat=5651\n","Skipping y_hat=5651\n","Skipping y_hat=7971\n","Skipping y_hat=7971\n","Skipping y_hat=7971\n","Skipping y_hat=7971\n","Skipping y_hat=7971\n","Skipping y_hat=7971\n","Skipping y_hat=7971\n","Skipping y_hat=7622\n","Skipping y_hat=7622\n","Skipping y_hat=7622\n","Skipping y_hat=7622\n","Skipping y_hat=7622\n","Skipping y_hat=7622\n","Skipping y_hat=7622\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=3023\n","Skipping y_hat=3023\n","Skipping y_hat=3023\n","Skipping y_hat=3023\n","Skipping y_hat=3023\n","Skipping y_hat=3023\n","Skipping y_hat=3023\n","Skipping y_hat=3951\n","Skipping y_hat=3951\n","Skipping y_hat=3951\n","Skipping y_hat=3951\n","Skipping y_hat=3951\n","Skipping y_hat=3951\n","Skipping y_hat=3951\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n","Skipping y_hat=0532\n","Skipping y_hat=0532\n","Skipping y_hat=0532\n","Skipping y_hat=0532\n","Skipping y_hat=0532\n","Skipping y_hat=0532\n","Skipping y_hat=0532\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=1922\n","Skipping y_hat=1922\n","Skipping y_hat=1922\n","Skipping y_hat=1922\n","Skipping y_hat=1922\n","Skipping y_hat=1922\n","Skipping y_hat=1922\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=7512\n","Skipping y_hat=7512\n","Skipping y_hat=7512\n","Skipping y_hat=7512\n","Skipping y_hat=7512\n","Skipping y_hat=7512\n","Skipping y_hat=7512\n","Skipping y_hat=3062\n","Skipping y_hat=3062\n","Skipping y_hat=3062\n","Skipping y_hat=3062\n","Skipping y_hat=3062\n","Skipping y_hat=3062\n","Skipping y_hat=3062\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=3122\n","Skipping y_hat=3122\n","Skipping y_hat=3122\n","Skipping y_hat=3122\n","Skipping y_hat=3122\n","Skipping y_hat=3122\n","Skipping y_hat=3122\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=8081\n","Skipping y_hat=8081\n","Skipping y_hat=8081\n","Skipping y_hat=8081\n","Skipping y_hat=8081\n","Skipping y_hat=8081\n","Skipping y_hat=8081\n","Skipping y_hat=5232\n","Skipping y_hat=5232\n","Skipping y_hat=5232\n","Skipping y_hat=5232\n","Skipping y_hat=5232\n","Skipping y_hat=5232\n","Skipping y_hat=5232\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=4362\n","Skipping y_hat=4362\n","Skipping y_hat=4362\n","Skipping y_hat=4362\n","Skipping y_hat=4362\n","Skipping y_hat=4362\n","Skipping y_hat=4362\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=9822\n","Skipping y_hat=9822\n","Skipping y_hat=9822\n","Skipping y_hat=9822\n","Skipping y_hat=9822\n","Skipping y_hat=9822\n","Skipping y_hat=9822\n","Skipping y_hat=2541\n","Skipping y_hat=2541\n","Skipping y_hat=2541\n","Skipping y_hat=2541\n","Skipping y_hat=2541\n","Skipping y_hat=2541\n","Skipping y_hat=2541\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=3942\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=3162\n","Skipping y_hat=3782\n","Skipping y_hat=3782\n","Skipping y_hat=3782\n","Skipping y_hat=3782\n","Skipping y_hat=3782\n","Skipping y_hat=3782\n","Skipping y_hat=3782\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=8652\n","Skipping y_hat=8652\n","Skipping y_hat=8652\n","Skipping y_hat=8652\n","Skipping y_hat=8652\n","Skipping y_hat=8652\n","Skipping y_hat=8652\n","Skipping y_hat=5490\n","Skipping y_hat=5490\n","Skipping y_hat=5490\n","Skipping y_hat=5490\n","Skipping y_hat=5490\n","Skipping y_hat=5490\n","Skipping y_hat=5490\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=8781\n","Skipping y_hat=8781\n","Skipping y_hat=8781\n","Skipping y_hat=8781\n","Skipping y_hat=8781\n","Skipping y_hat=8781\n","Skipping y_hat=8781\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=3291\n","Skipping y_hat=3291\n","Skipping y_hat=3291\n","Skipping y_hat=3291\n","Skipping y_hat=3291\n","Skipping y_hat=3291\n","Skipping y_hat=3291\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=2771\n","Skipping y_hat=2771\n","Skipping y_hat=2771\n","Skipping y_hat=2771\n","Skipping y_hat=2771\n","Skipping y_hat=2771\n","Skipping y_hat=2771\n","Skipping y_hat=0171\n","Skipping y_hat=0171\n","Skipping y_hat=0171\n","Skipping y_hat=0171\n","Skipping y_hat=0171\n","Skipping y_hat=0171\n","Skipping y_hat=0171\n","Skipping y_hat=9681\n","Skipping y_hat=9681\n","Skipping y_hat=9681\n","Skipping y_hat=9681\n","Skipping y_hat=9681\n","Skipping y_hat=9681\n","Skipping y_hat=9681\n","Skipping y_hat=0351\n","Skipping y_hat=0351\n","Skipping y_hat=0351\n","Skipping y_hat=0351\n","Skipping y_hat=0351\n","Skipping y_hat=0351\n","Skipping y_hat=0351\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=2271\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=4391\n","Skipping y_hat=4391\n","Skipping y_hat=4391\n","Skipping y_hat=4391\n","Skipping y_hat=4391\n","Skipping y_hat=4391\n","Skipping y_hat=4391\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n"," 94% 74/79 [00:09<00:00,  8.51it/s]Skipping y_hat=6191\n","Skipping y_hat=6191\n","Skipping y_hat=6191\n","Skipping y_hat=6191\n","Skipping y_hat=6191\n","Skipping y_hat=6191\n","Skipping y_hat=6191\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=6651\n","Skipping y_hat=6651\n","Skipping y_hat=6651\n","Skipping y_hat=6651\n","Skipping y_hat=6651\n","Skipping y_hat=6651\n","Skipping y_hat=6651\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=4791\n","Skipping y_hat=4791\n","Skipping y_hat=4791\n","Skipping y_hat=4791\n","Skipping y_hat=4791\n","Skipping y_hat=4791\n","Skipping y_hat=4791\n","Skipping y_hat=4902\n","Skipping y_hat=4902\n","Skipping y_hat=4902\n","Skipping y_hat=4902\n","Skipping y_hat=4902\n","Skipping y_hat=4902\n","Skipping y_hat=4902\n","Skipping y_hat=8212\n","Skipping y_hat=8212\n","Skipping y_hat=8212\n","Skipping y_hat=8212\n","Skipping y_hat=8212\n","Skipping y_hat=8212\n","Skipping y_hat=8212\n","Skipping y_hat=3690\n","Skipping y_hat=3690\n","Skipping y_hat=3690\n","Skipping y_hat=3690\n","Skipping y_hat=3690\n","Skipping y_hat=3690\n","Skipping y_hat=3690\n","Skipping y_hat=3301\n","Skipping y_hat=3301\n","Skipping y_hat=3301\n","Skipping y_hat=3301\n","Skipping y_hat=3301\n","Skipping y_hat=3301\n","Skipping y_hat=3301\n","Skipping y_hat=4851\n","Skipping y_hat=4851\n","Skipping y_hat=4851\n","Skipping y_hat=4851\n","Skipping y_hat=4851\n","Skipping y_hat=4851\n","Skipping y_hat=4851\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=7192\n","Skipping y_hat=7192\n","Skipping y_hat=7192\n","Skipping y_hat=7192\n","Skipping y_hat=7192\n","Skipping y_hat=7192\n","Skipping y_hat=7192\n","Skipping y_hat=4591\n","Skipping y_hat=4591\n","Skipping y_hat=4591\n","Skipping y_hat=4591\n","Skipping y_hat=4591\n","Skipping y_hat=4591\n","Skipping y_hat=4591\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=8232\n","Skipping y_hat=3643\n","Skipping y_hat=3643\n","Skipping y_hat=3643\n","Skipping y_hat=3643\n","Skipping y_hat=3643\n","Skipping y_hat=3643\n","Skipping y_hat=3643\n","Skipping y_hat=5081\n","Skipping y_hat=5081\n","Skipping y_hat=5081\n","Skipping y_hat=5081\n","Skipping y_hat=5081\n","Skipping y_hat=5081\n","Skipping y_hat=5081\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=2342\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=4481\n","Skipping y_hat=4481\n","Skipping y_hat=4481\n","Skipping y_hat=4481\n","Skipping y_hat=4481\n","Skipping y_hat=4481\n","Skipping y_hat=4481\n","Skipping y_hat=1651\n","Skipping y_hat=1651\n","Skipping y_hat=1651\n","Skipping y_hat=1651\n","Skipping y_hat=1651\n","Skipping y_hat=1651\n","Skipping y_hat=1651\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=4632\n","Skipping y_hat=4632\n","Skipping y_hat=4632\n","Skipping y_hat=4632\n","Skipping y_hat=4632\n","Skipping y_hat=4632\n","Skipping y_hat=4632\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=3090\n","Skipping y_hat=3090\n","Skipping y_hat=3090\n","Skipping y_hat=3090\n","Skipping y_hat=3090\n","Skipping y_hat=3090\n","Skipping y_hat=3090\n","Skipping y_hat=2191\n","Skipping y_hat=2191\n","Skipping y_hat=2191\n","Skipping y_hat=2191\n","Skipping y_hat=2191\n","Skipping y_hat=2191\n","Skipping y_hat=2191\n","Skipping y_hat=1302\n","Skipping y_hat=1302\n","Skipping y_hat=1302\n","Skipping y_hat=1302\n","Skipping y_hat=1302\n","Skipping y_hat=1302\n","Skipping y_hat=1302\n","Skipping y_hat=4482\n","Skipping y_hat=4482\n","Skipping y_hat=4482\n","Skipping y_hat=4482\n","Skipping y_hat=4482\n","Skipping y_hat=4482\n","Skipping y_hat=4482\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=6642\n","Skipping y_hat=6642\n","Skipping y_hat=6642\n","Skipping y_hat=6642\n","Skipping y_hat=6642\n","Skipping y_hat=6642\n","Skipping y_hat=6642\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7042\n","Skipping y_hat=7042\n","Skipping y_hat=7042\n","Skipping y_hat=7042\n","Skipping y_hat=7042\n","Skipping y_hat=7042\n","Skipping y_hat=7042\n","Skipping y_hat=5211\n","Skipping y_hat=5211\n","Skipping y_hat=5211\n","Skipping y_hat=5211\n","Skipping y_hat=5211\n","Skipping y_hat=5211\n","Skipping y_hat=5211\n","Skipping y_hat=0522\n","Skipping y_hat=0522\n","Skipping y_hat=0522\n","Skipping y_hat=0522\n","Skipping y_hat=0522\n","Skipping y_hat=0522\n","Skipping y_hat=0522\n","Skipping y_hat=1591\n","Skipping y_hat=1591\n","Skipping y_hat=1591\n","Skipping y_hat=1591\n","Skipping y_hat=1591\n","Skipping y_hat=1591\n","Skipping y_hat=1591\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=5422\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=0991\n","Skipping y_hat=0991\n","Skipping y_hat=0991\n","Skipping y_hat=0991\n","Skipping y_hat=0991\n","Skipping y_hat=0991\n","Skipping y_hat=0991\n","Skipping y_hat=4621\n","Skipping y_hat=4621\n","Skipping y_hat=4621\n","Skipping y_hat=4621\n","Skipping y_hat=4621\n","Skipping y_hat=4621\n","Skipping y_hat=4621\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=1412\n","Skipping y_hat=1412\n","Skipping y_hat=1412\n","Skipping y_hat=1412\n","Skipping y_hat=1412\n","Skipping y_hat=1412\n","Skipping y_hat=1412\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=7781\n","Skipping y_hat=7781\n","Skipping y_hat=7781\n","Skipping y_hat=7781\n","Skipping y_hat=7781\n","Skipping y_hat=7781\n","Skipping y_hat=7781\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=8282\n","Skipping y_hat=8282\n","Skipping y_hat=8282\n","Skipping y_hat=8282\n","Skipping y_hat=8282\n","Skipping y_hat=8282\n","Skipping y_hat=8282\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=5332\n","Skipping y_hat=9852\n","Skipping y_hat=9852\n","Skipping y_hat=9852\n","Skipping y_hat=9852\n","Skipping y_hat=9852\n","Skipping y_hat=9852\n","Skipping y_hat=9852\n","Skipping y_hat=5462\n","Skipping y_hat=5462\n","Skipping y_hat=5462\n","Skipping y_hat=5462\n","Skipping y_hat=5462\n","Skipping y_hat=5462\n","Skipping y_hat=5462\n","Skipping y_hat=8481\n","Skipping y_hat=8481\n","Skipping y_hat=8481\n","Skipping y_hat=8481\n","Skipping y_hat=8481\n","Skipping y_hat=8481\n","Skipping y_hat=8481\n","Skipping y_hat=2042\n","Skipping y_hat=2042\n","Skipping y_hat=2042\n","Skipping y_hat=2042\n","Skipping y_hat=2042\n","Skipping y_hat=2042\n","Skipping y_hat=2042\n","Skipping y_hat=9512\n","Skipping y_hat=9512\n","Skipping y_hat=9512\n","Skipping y_hat=9512\n","Skipping y_hat=9512\n","Skipping y_hat=9512\n","Skipping y_hat=9512\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=0382\n","Skipping y_hat=0382\n","Skipping y_hat=0382\n","Skipping y_hat=0382\n","Skipping y_hat=0382\n","Skipping y_hat=0382\n","Skipping y_hat=0382\n","Skipping y_hat=3342\n","Skipping y_hat=3342\n","Skipping y_hat=3342\n","Skipping y_hat=3342\n","Skipping y_hat=3342\n","Skipping y_hat=3342\n","Skipping y_hat=3342\n","Skipping y_hat=7571\n","Skipping y_hat=7571\n","Skipping y_hat=7571\n","Skipping y_hat=7571\n","Skipping y_hat=7571\n","Skipping y_hat=7571\n","Skipping y_hat=7571\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=0781\n","Skipping y_hat=0781\n","Skipping y_hat=0781\n","Skipping y_hat=0781\n","Skipping y_hat=0781\n","Skipping y_hat=0781\n","Skipping y_hat=0781\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=5981\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=9823\n","Skipping y_hat=9823\n","Skipping y_hat=9823\n","Skipping y_hat=9823\n","Skipping y_hat=9823\n","Skipping y_hat=9823\n","Skipping y_hat=9823\n","Skipping y_hat=6252\n","Skipping y_hat=6252\n","Skipping y_hat=6252\n","Skipping y_hat=6252\n","Skipping y_hat=6252\n","Skipping y_hat=6252\n","Skipping y_hat=6252\n","Skipping y_hat=6441\n","Skipping y_hat=6441\n","Skipping y_hat=6441\n","Skipping y_hat=6441\n","Skipping y_hat=6441\n","Skipping y_hat=6441\n","Skipping y_hat=6441\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=4781\n","Skipping y_hat=4781\n","Skipping y_hat=4781\n","Skipping y_hat=4781\n","Skipping y_hat=4781\n","Skipping y_hat=4781\n","Skipping y_hat=4781\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=8003\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=4603\n","Skipping y_hat=4603\n","Skipping y_hat=4603\n","Skipping y_hat=4603\n","Skipping y_hat=4603\n","Skipping y_hat=4603\n","Skipping y_hat=4603\n","Skipping y_hat=6362\n","Skipping y_hat=6362\n","Skipping y_hat=6362\n","Skipping y_hat=6362\n","Skipping y_hat=6362\n","Skipping y_hat=6362\n","Skipping y_hat=6362\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=9542\n","Skipping y_hat=0292\n","Skipping y_hat=0292\n","Skipping y_hat=0292\n","Skipping y_hat=0292\n","Skipping y_hat=0292\n","Skipping y_hat=0292\n","Skipping y_hat=0292\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0343\n","Skipping y_hat=0343\n","Skipping y_hat=0343\n","Skipping y_hat=0343\n","Skipping y_hat=0343\n","Skipping y_hat=0343\n","Skipping y_hat=0343\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=8871\n","Skipping y_hat=8871\n","Skipping y_hat=8871\n","Skipping y_hat=8871\n","Skipping y_hat=8871\n","Skipping y_hat=8871\n","Skipping y_hat=8871\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9362\n","Skipping y_hat=9362\n","Skipping y_hat=9362\n","Skipping y_hat=9362\n","Skipping y_hat=9362\n","Skipping y_hat=9362\n","Skipping y_hat=9362\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=2741\n","Skipping y_hat=2741\n","Skipping y_hat=2741\n","Skipping y_hat=2741\n","Skipping y_hat=2741\n","Skipping y_hat=2741\n","Skipping y_hat=2741\n","Skipping y_hat=6952\n","Skipping y_hat=6952\n","Skipping y_hat=6952\n","Skipping y_hat=6952\n","Skipping y_hat=6952\n","Skipping y_hat=6952\n","Skipping y_hat=6952\n","Skipping y_hat=7652\n","Skipping y_hat=7652\n","Skipping y_hat=7652\n","Skipping y_hat=7652\n","Skipping y_hat=7652\n","Skipping y_hat=7652\n","Skipping y_hat=7652\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=3902\n","Skipping y_hat=8272\n","Skipping y_hat=8272\n","Skipping y_hat=8272\n","Skipping y_hat=8272\n","Skipping y_hat=8272\n","Skipping y_hat=8272\n","Skipping y_hat=8272\n","Skipping y_hat=8872\n","Skipping y_hat=8872\n","Skipping y_hat=8872\n","Skipping y_hat=8872\n","Skipping y_hat=8872\n","Skipping y_hat=8872\n","Skipping y_hat=8872\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=5752\n","Skipping y_hat=6580\n","Skipping y_hat=6580\n","Skipping y_hat=6580\n","Skipping y_hat=6580\n","Skipping y_hat=6580\n","Skipping y_hat=6580\n","Skipping y_hat=6580\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=3731\n","Skipping y_hat=4172\n","Skipping y_hat=4172\n","Skipping y_hat=4172\n","Skipping y_hat=4172\n","Skipping y_hat=4172\n","Skipping y_hat=4172\n","Skipping y_hat=4172\n","Skipping y_hat=9811\n","Skipping y_hat=9811\n","Skipping y_hat=9811\n","Skipping y_hat=9811\n","Skipping y_hat=9811\n","Skipping y_hat=9811\n","Skipping y_hat=9811\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=3501\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=3182\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=7252\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=7912\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=5791\n","Skipping y_hat=5791\n","Skipping y_hat=5791\n","Skipping y_hat=5791\n","Skipping y_hat=5791\n","Skipping y_hat=5791\n","Skipping y_hat=5791\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=3272\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=4451\n","Skipping y_hat=4451\n","Skipping y_hat=4451\n","Skipping y_hat=4451\n","Skipping y_hat=4451\n","Skipping y_hat=4451\n","Skipping y_hat=4451\n","Skipping y_hat=6592\n","Skipping y_hat=6592\n","Skipping y_hat=6592\n","Skipping y_hat=6592\n","Skipping y_hat=6592\n","Skipping y_hat=6592\n","Skipping y_hat=6592\n"," 95% 75/79 [00:09<00:00,  8.50it/s]Skipping y_hat=9371\n","Skipping y_hat=9371\n","Skipping y_hat=9371\n","Skipping y_hat=9371\n","Skipping y_hat=9371\n","Skipping y_hat=9371\n","Skipping y_hat=9371\n","Skipping y_hat=0402\n","Skipping y_hat=0402\n","Skipping y_hat=0402\n","Skipping y_hat=0402\n","Skipping y_hat=0402\n","Skipping y_hat=0402\n","Skipping y_hat=0402\n","Skipping y_hat=4171\n","Skipping y_hat=4171\n","Skipping y_hat=4171\n","Skipping y_hat=4171\n","Skipping y_hat=4171\n","Skipping y_hat=4171\n","Skipping y_hat=4171\n","Skipping y_hat=7991\n","Skipping y_hat=7991\n","Skipping y_hat=7991\n","Skipping y_hat=7991\n","Skipping y_hat=7991\n","Skipping y_hat=7991\n","Skipping y_hat=7991\n","Skipping y_hat=9992\n","Skipping y_hat=9992\n","Skipping y_hat=9992\n","Skipping y_hat=9992\n","Skipping y_hat=9992\n","Skipping y_hat=9992\n","Skipping y_hat=9992\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=2361\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=6903\n","Skipping y_hat=6903\n","Skipping y_hat=6903\n","Skipping y_hat=6903\n","Skipping y_hat=6903\n","Skipping y_hat=6903\n","Skipping y_hat=6903\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=4392\n","Skipping y_hat=4392\n","Skipping y_hat=4392\n","Skipping y_hat=4392\n","Skipping y_hat=4392\n","Skipping y_hat=4392\n","Skipping y_hat=4392\n","Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=2221\n","Skipping y_hat=7033\n","Skipping y_hat=7033\n","Skipping y_hat=7033\n","Skipping y_hat=7033\n","Skipping y_hat=7033\n","Skipping y_hat=7033\n","Skipping y_hat=7033\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=6161\n","Skipping y_hat=6161\n","Skipping y_hat=6161\n","Skipping y_hat=6161\n","Skipping y_hat=6161\n","Skipping y_hat=6161\n","Skipping y_hat=6161\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=1632\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=1502\n","Skipping y_hat=0742\n","Skipping y_hat=0742\n","Skipping y_hat=0742\n","Skipping y_hat=0742\n","Skipping y_hat=0742\n","Skipping y_hat=0742\n","Skipping y_hat=0742\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=9712\n","Skipping y_hat=9712\n","Skipping y_hat=9712\n","Skipping y_hat=9712\n","Skipping y_hat=9712\n","Skipping y_hat=9712\n","Skipping y_hat=9712\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=5762\n","Skipping y_hat=5762\n","Skipping y_hat=5762\n","Skipping y_hat=5762\n","Skipping y_hat=5762\n","Skipping y_hat=5762\n","Skipping y_hat=5762\n","Skipping y_hat=3661\n","Skipping y_hat=3661\n","Skipping y_hat=3661\n","Skipping y_hat=3661\n","Skipping y_hat=3661\n","Skipping y_hat=3661\n","Skipping y_hat=3661\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=2481\n","Skipping y_hat=7331\n","Skipping y_hat=7331\n","Skipping y_hat=7331\n","Skipping y_hat=7331\n","Skipping y_hat=7331\n","Skipping y_hat=7331\n","Skipping y_hat=7331\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=2522\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=5902\n","Skipping y_hat=5902\n","Skipping y_hat=5902\n","Skipping y_hat=5902\n","Skipping y_hat=5902\n","Skipping y_hat=5902\n","Skipping y_hat=5902\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=2281\n","Skipping y_hat=5212\n","Skipping y_hat=5212\n","Skipping y_hat=5212\n","Skipping y_hat=5212\n","Skipping y_hat=5212\n","Skipping y_hat=5212\n","Skipping y_hat=5212\n","Skipping y_hat=4472\n","Skipping y_hat=4472\n","Skipping y_hat=4472\n","Skipping y_hat=4472\n","Skipping y_hat=4472\n","Skipping y_hat=4472\n","Skipping y_hat=4472\n","Skipping y_hat=9931\n","Skipping y_hat=9931\n","Skipping y_hat=9931\n","Skipping y_hat=9931\n","Skipping y_hat=9931\n","Skipping y_hat=9931\n","Skipping y_hat=9931\n","Skipping y_hat=5702\n","Skipping y_hat=5702\n","Skipping y_hat=5702\n","Skipping y_hat=5702\n","Skipping y_hat=5702\n","Skipping y_hat=5702\n","Skipping y_hat=5702\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=5742\n","Skipping y_hat=7822\n","Skipping y_hat=7822\n","Skipping y_hat=7822\n","Skipping y_hat=7822\n","Skipping y_hat=7822\n","Skipping y_hat=7822\n","Skipping y_hat=7822\n","Skipping y_hat=5252\n","Skipping y_hat=5252\n","Skipping y_hat=5252\n","Skipping y_hat=5252\n","Skipping y_hat=5252\n","Skipping y_hat=5252\n","Skipping y_hat=5252\n","Skipping y_hat=3871\n","Skipping y_hat=3871\n","Skipping y_hat=3871\n","Skipping y_hat=3871\n","Skipping y_hat=3871\n","Skipping y_hat=3871\n","Skipping y_hat=3871\n","Skipping y_hat=5442\n","Skipping y_hat=5442\n","Skipping y_hat=5442\n","Skipping y_hat=5442\n","Skipping y_hat=5442\n","Skipping y_hat=5442\n","Skipping y_hat=5442\n","Skipping y_hat=6891\n","Skipping y_hat=6891\n","Skipping y_hat=6891\n","Skipping y_hat=6891\n","Skipping y_hat=6891\n","Skipping y_hat=6891\n","Skipping y_hat=6891\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=5561\n","Skipping y_hat=5561\n","Skipping y_hat=5561\n","Skipping y_hat=5561\n","Skipping y_hat=5561\n","Skipping y_hat=5561\n","Skipping y_hat=5561\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=7333\n","Skipping y_hat=9713\n","Skipping y_hat=9713\n","Skipping y_hat=9713\n","Skipping y_hat=9713\n","Skipping y_hat=9713\n","Skipping y_hat=9713\n","Skipping y_hat=9713\n","Skipping y_hat=3402\n","Skipping y_hat=3402\n","Skipping y_hat=3402\n","Skipping y_hat=3402\n","Skipping y_hat=3402\n","Skipping y_hat=3402\n","Skipping y_hat=3402\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=9382\n","Skipping y_hat=9382\n","Skipping y_hat=9382\n","Skipping y_hat=9382\n","Skipping y_hat=9382\n","Skipping y_hat=9382\n","Skipping y_hat=9382\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=2023\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=7132\n","Skipping y_hat=9791\n","Skipping y_hat=9791\n","Skipping y_hat=9791\n","Skipping y_hat=9791\n","Skipping y_hat=9791\n","Skipping y_hat=9791\n","Skipping y_hat=9791\n","Skipping y_hat=0341\n","Skipping y_hat=0341\n","Skipping y_hat=0341\n","Skipping y_hat=0341\n","Skipping y_hat=0341\n","Skipping y_hat=0341\n","Skipping y_hat=0341\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=4562\n","Skipping y_hat=4562\n","Skipping y_hat=4562\n","Skipping y_hat=4562\n","Skipping y_hat=4562\n","Skipping y_hat=4562\n","Skipping y_hat=4562\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=3762\n","Skipping y_hat=1571\n","Skipping y_hat=1571\n","Skipping y_hat=1571\n","Skipping y_hat=1571\n","Skipping y_hat=1571\n","Skipping y_hat=1571\n","Skipping y_hat=1571\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=7881\n","Skipping y_hat=7881\n","Skipping y_hat=7881\n","Skipping y_hat=7881\n","Skipping y_hat=7881\n","Skipping y_hat=7881\n","Skipping y_hat=7881\n","Skipping y_hat=8092\n","Skipping y_hat=8092\n","Skipping y_hat=8092\n","Skipping y_hat=8092\n","Skipping y_hat=8092\n","Skipping y_hat=8092\n","Skipping y_hat=8092\n","Skipping y_hat=8911\n","Skipping y_hat=8911\n","Skipping y_hat=8911\n","Skipping y_hat=8911\n","Skipping y_hat=8911\n","Skipping y_hat=8911\n","Skipping y_hat=8911\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=1672\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=6532\n","Skipping y_hat=7321\n","Skipping y_hat=7321\n","Skipping y_hat=7321\n","Skipping y_hat=7321\n","Skipping y_hat=7321\n","Skipping y_hat=7321\n","Skipping y_hat=7321\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8161\n","Skipping y_hat=8422\n","Skipping y_hat=8422\n","Skipping y_hat=8422\n","Skipping y_hat=8422\n","Skipping y_hat=8422\n","Skipping y_hat=8422\n","Skipping y_hat=8422\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=4612\n","Skipping y_hat=0831\n","Skipping y_hat=0831\n","Skipping y_hat=0831\n","Skipping y_hat=0831\n","Skipping y_hat=0831\n","Skipping y_hat=0831\n","Skipping y_hat=0831\n","Skipping y_hat=0981\n","Skipping y_hat=0981\n","Skipping y_hat=0981\n","Skipping y_hat=0981\n","Skipping y_hat=0981\n","Skipping y_hat=0981\n","Skipping y_hat=0981\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=4412\n","Skipping y_hat=7562\n","Skipping y_hat=7562\n","Skipping y_hat=7562\n","Skipping y_hat=7562\n","Skipping y_hat=7562\n","Skipping y_hat=7562\n","Skipping y_hat=7562\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=3803\n","Skipping y_hat=3803\n","Skipping y_hat=3803\n","Skipping y_hat=3803\n","Skipping y_hat=3803\n","Skipping y_hat=3803\n","Skipping y_hat=3803\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=3681\n","Skipping y_hat=3681\n","Skipping y_hat=3681\n","Skipping y_hat=3681\n","Skipping y_hat=3681\n","Skipping y_hat=3681\n","Skipping y_hat=3681\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=3411\n","Skipping y_hat=3411\n","Skipping y_hat=3411\n","Skipping y_hat=3411\n","Skipping y_hat=3411\n","Skipping y_hat=3411\n","Skipping y_hat=3411\n","Skipping y_hat=1702\n","Skipping y_hat=1702\n","Skipping y_hat=1702\n","Skipping y_hat=1702\n","Skipping y_hat=1702\n","Skipping y_hat=1702\n","Skipping y_hat=1702\n","Skipping y_hat=5622\n","Skipping y_hat=5622\n","Skipping y_hat=5622\n","Skipping y_hat=5622\n","Skipping y_hat=5622\n","Skipping y_hat=5622\n","Skipping y_hat=5622\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=2371\n","Skipping y_hat=8832\n","Skipping y_hat=8832\n","Skipping y_hat=8832\n","Skipping y_hat=8832\n","Skipping y_hat=8832\n","Skipping y_hat=8832\n","Skipping y_hat=8832\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=7051\n","Skipping y_hat=7051\n","Skipping y_hat=7051\n","Skipping y_hat=7051\n","Skipping y_hat=7051\n","Skipping y_hat=7051\n","Skipping y_hat=7051\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=1802\n","Skipping y_hat=1802\n","Skipping y_hat=1802\n","Skipping y_hat=1802\n","Skipping y_hat=1802\n","Skipping y_hat=1802\n","Skipping y_hat=1802\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=4002\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=9861\n","Skipping y_hat=9861\n","Skipping y_hat=9861\n","Skipping y_hat=9861\n","Skipping y_hat=9861\n","Skipping y_hat=9861\n","Skipping y_hat=9861\n","Skipping y_hat=4741\n","Skipping y_hat=4741\n","Skipping y_hat=4741\n","Skipping y_hat=4741\n","Skipping y_hat=4741\n","Skipping y_hat=4741\n","Skipping y_hat=4741\n","Skipping y_hat=0472\n","Skipping y_hat=0472\n","Skipping y_hat=0472\n","Skipping y_hat=0472\n","Skipping y_hat=0472\n","Skipping y_hat=0472\n","Skipping y_hat=0472\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=6241\n","Skipping y_hat=4872\n","Skipping y_hat=4872\n","Skipping y_hat=4872\n","Skipping y_hat=4872\n","Skipping y_hat=4872\n","Skipping y_hat=4872\n","Skipping y_hat=4872\n","Skipping y_hat=6982\n","Skipping y_hat=6982\n","Skipping y_hat=6982\n","Skipping y_hat=6982\n","Skipping y_hat=6982\n","Skipping y_hat=6982\n","Skipping y_hat=6982\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=3852\n","Skipping y_hat=3852\n","Skipping y_hat=3852\n","Skipping y_hat=3852\n","Skipping y_hat=3852\n","Skipping y_hat=3852\n","Skipping y_hat=3852\n","Skipping y_hat=0282\n","Skipping y_hat=0282\n","Skipping y_hat=0282\n","Skipping y_hat=0282\n","Skipping y_hat=0282\n","Skipping y_hat=0282\n","Skipping y_hat=0282\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=2761\n","Skipping y_hat=9562\n","Skipping y_hat=9562\n","Skipping y_hat=9562\n","Skipping y_hat=9562\n","Skipping y_hat=9562\n","Skipping y_hat=9562\n","Skipping y_hat=9562\n","Skipping y_hat=5013\n","Skipping y_hat=5013\n","Skipping y_hat=5013\n","Skipping y_hat=5013\n","Skipping y_hat=5013\n","Skipping y_hat=5013\n","Skipping y_hat=5013\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=1462\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=6771\n","Skipping y_hat=6771\n","Skipping y_hat=6771\n","Skipping y_hat=6771\n","Skipping y_hat=6771\n","Skipping y_hat=6771\n","Skipping y_hat=6771\n","Skipping y_hat=8912\n","Skipping y_hat=8912\n","Skipping y_hat=8912\n","Skipping y_hat=8912\n","Skipping y_hat=8912\n","Skipping y_hat=8912\n","Skipping y_hat=8912\n","Skipping y_hat=1952\n","Skipping y_hat=1952\n","Skipping y_hat=1952\n","Skipping y_hat=1952\n","Skipping y_hat=1952\n","Skipping y_hat=1952\n","Skipping y_hat=1952\n","Skipping y_hat=9261\n","Skipping y_hat=9261\n","Skipping y_hat=9261\n","Skipping y_hat=9261\n","Skipping y_hat=9261\n","Skipping y_hat=9261\n","Skipping y_hat=9261\n"," 96% 76/79 [00:09<00:00,  8.77it/s]Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=2091\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6261\n","Skipping y_hat=6632\n","Skipping y_hat=6632\n","Skipping y_hat=6632\n","Skipping y_hat=6632\n","Skipping y_hat=6632\n","Skipping y_hat=6632\n","Skipping y_hat=6632\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=1981\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=0241\n","Skipping y_hat=0241\n","Skipping y_hat=0241\n","Skipping y_hat=0241\n","Skipping y_hat=0241\n","Skipping y_hat=0241\n","Skipping y_hat=0241\n","Skipping y_hat=0123\n","Skipping y_hat=0123\n","Skipping y_hat=0123\n","Skipping y_hat=0123\n","Skipping y_hat=0123\n","Skipping y_hat=0123\n","Skipping y_hat=0123\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5522\n","Skipping y_hat=5522\n","Skipping y_hat=5522\n","Skipping y_hat=5522\n","Skipping y_hat=5522\n","Skipping y_hat=5522\n","Skipping y_hat=5522\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=7712\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7791\n","Skipping y_hat=7922\n","Skipping y_hat=7922\n","Skipping y_hat=7922\n","Skipping y_hat=7922\n","Skipping y_hat=7922\n","Skipping y_hat=7922\n","Skipping y_hat=7922\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=6772\n","Skipping y_hat=7141\n","Skipping y_hat=7141\n","Skipping y_hat=7141\n","Skipping y_hat=7141\n","Skipping y_hat=7141\n","Skipping y_hat=7141\n","Skipping y_hat=7141\n","Skipping y_hat=9202\n","Skipping y_hat=9202\n","Skipping y_hat=9202\n","Skipping y_hat=9202\n","Skipping y_hat=9202\n","Skipping y_hat=9202\n","Skipping y_hat=9202\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=4991\n","Skipping y_hat=4991\n","Skipping y_hat=4991\n","Skipping y_hat=4991\n","Skipping y_hat=4991\n","Skipping y_hat=4991\n","Skipping y_hat=4991\n","Skipping y_hat=0442\n","Skipping y_hat=0442\n","Skipping y_hat=0442\n","Skipping y_hat=0442\n","Skipping y_hat=0442\n","Skipping y_hat=0442\n","Skipping y_hat=0442\n","Skipping y_hat=0122\n","Skipping y_hat=0122\n","Skipping y_hat=0122\n","Skipping y_hat=0122\n","Skipping y_hat=0122\n","Skipping y_hat=0122\n","Skipping y_hat=0122\n","Skipping y_hat=0471\n","Skipping y_hat=0471\n","Skipping y_hat=0471\n","Skipping y_hat=0471\n","Skipping y_hat=0471\n","Skipping y_hat=0471\n","Skipping y_hat=0471\n","Skipping y_hat=8002\n","Skipping y_hat=8002\n","Skipping y_hat=8002\n","Skipping y_hat=8002\n","Skipping y_hat=8002\n","Skipping y_hat=8002\n","Skipping y_hat=8002\n","Skipping y_hat=0912\n","Skipping y_hat=0912\n","Skipping y_hat=0912\n","Skipping y_hat=0912\n","Skipping y_hat=0912\n","Skipping y_hat=0912\n","Skipping y_hat=0912\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=6641\n","Skipping y_hat=6641\n","Skipping y_hat=6641\n","Skipping y_hat=6641\n","Skipping y_hat=6641\n","Skipping y_hat=6641\n","Skipping y_hat=6641\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=6813\n","Skipping y_hat=6813\n","Skipping y_hat=6813\n","Skipping y_hat=6813\n","Skipping y_hat=6813\n","Skipping y_hat=6813\n","Skipping y_hat=6813\n","Skipping y_hat=5122\n","Skipping y_hat=5122\n","Skipping y_hat=5122\n","Skipping y_hat=5122\n","Skipping y_hat=5122\n","Skipping y_hat=5122\n","Skipping y_hat=5122\n","Skipping y_hat=5881\n","Skipping y_hat=5881\n","Skipping y_hat=5881\n","Skipping y_hat=5881\n","Skipping y_hat=5881\n","Skipping y_hat=5881\n","Skipping y_hat=5881\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=5052\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=2712\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=2991\n","Skipping y_hat=2991\n","Skipping y_hat=2991\n","Skipping y_hat=2991\n","Skipping y_hat=2991\n","Skipping y_hat=2991\n","Skipping y_hat=2991\n","Skipping y_hat=7702\n","Skipping y_hat=7702\n","Skipping y_hat=7702\n","Skipping y_hat=7702\n","Skipping y_hat=7702\n","Skipping y_hat=7702\n","Skipping y_hat=7702\n","Skipping y_hat=1051\n","Skipping y_hat=1051\n","Skipping y_hat=1051\n","Skipping y_hat=1051\n","Skipping y_hat=1051\n","Skipping y_hat=1051\n","Skipping y_hat=1051\n","Skipping y_hat=9571\n","Skipping y_hat=9571\n","Skipping y_hat=9571\n","Skipping y_hat=9571\n","Skipping y_hat=9571\n","Skipping y_hat=9571\n","Skipping y_hat=9571\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=7171\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=2381\n","Skipping y_hat=9051\n","Skipping y_hat=9051\n","Skipping y_hat=9051\n","Skipping y_hat=9051\n","Skipping y_hat=9051\n","Skipping y_hat=9051\n","Skipping y_hat=9051\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=2332\n","Skipping y_hat=7660\n","Skipping y_hat=7660\n","Skipping y_hat=7660\n","Skipping y_hat=7660\n","Skipping y_hat=7660\n","Skipping y_hat=7660\n","Skipping y_hat=7660\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=6372\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=2222\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=1381\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=7611\n","Skipping y_hat=7611\n","Skipping y_hat=7611\n","Skipping y_hat=7611\n","Skipping y_hat=7611\n","Skipping y_hat=7611\n","Skipping y_hat=7611\n","Skipping y_hat=1732\n","Skipping y_hat=1732\n","Skipping y_hat=1732\n","Skipping y_hat=1732\n","Skipping y_hat=1732\n","Skipping y_hat=1732\n","Skipping y_hat=1732\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=0780\n","Skipping y_hat=0780\n","Skipping y_hat=0780\n","Skipping y_hat=0780\n","Skipping y_hat=0780\n","Skipping y_hat=0780\n","Skipping y_hat=0780\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=6222\n","Skipping y_hat=6222\n","Skipping y_hat=6222\n","Skipping y_hat=6222\n","Skipping y_hat=6222\n","Skipping y_hat=6222\n","Skipping y_hat=6222\n","Skipping y_hat=4302\n","Skipping y_hat=4302\n","Skipping y_hat=4302\n","Skipping y_hat=4302\n","Skipping y_hat=4302\n","Skipping y_hat=4302\n","Skipping y_hat=4302\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1342\n","Skipping y_hat=1342\n","Skipping y_hat=1342\n","Skipping y_hat=1342\n","Skipping y_hat=1342\n","Skipping y_hat=1342\n","Skipping y_hat=1342\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=9452\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=4261\n","Skipping y_hat=4261\n","Skipping y_hat=4261\n","Skipping y_hat=4261\n","Skipping y_hat=4261\n","Skipping y_hat=4261\n","Skipping y_hat=4261\n","Skipping y_hat=7262\n","Skipping y_hat=7262\n","Skipping y_hat=7262\n","Skipping y_hat=7262\n","Skipping y_hat=7262\n","Skipping y_hat=7262\n","Skipping y_hat=7262\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=4931\n","Skipping y_hat=4931\n","Skipping y_hat=4931\n","Skipping y_hat=4931\n","Skipping y_hat=4931\n","Skipping y_hat=4931\n","Skipping y_hat=4931\n","Skipping y_hat=0642\n","Skipping y_hat=0642\n","Skipping y_hat=0642\n","Skipping y_hat=0642\n","Skipping y_hat=0642\n","Skipping y_hat=0642\n","Skipping y_hat=0642\n","Skipping y_hat=0791\n","Skipping y_hat=0791\n","Skipping y_hat=0791\n","Skipping y_hat=0791\n","Skipping y_hat=0791\n","Skipping y_hat=0791\n","Skipping y_hat=0791\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=2842\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=0132\n","Skipping y_hat=7531\n","Skipping y_hat=7531\n","Skipping y_hat=7531\n","Skipping y_hat=7531\n","Skipping y_hat=7531\n","Skipping y_hat=7531\n","Skipping y_hat=7531\n","Skipping y_hat=0631\n","Skipping y_hat=0631\n","Skipping y_hat=0631\n","Skipping y_hat=0631\n","Skipping y_hat=0631\n","Skipping y_hat=0631\n","Skipping y_hat=0631\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=7642\n","Skipping y_hat=7642\n","Skipping y_hat=7642\n","Skipping y_hat=7642\n","Skipping y_hat=7642\n","Skipping y_hat=7642\n","Skipping y_hat=7642\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=9191\n","Skipping y_hat=2762\n","Skipping y_hat=2762\n","Skipping y_hat=2762\n","Skipping y_hat=2762\n","Skipping y_hat=2762\n","Skipping y_hat=2762\n","Skipping y_hat=2762\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=7061\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=0112\n","Skipping y_hat=4051\n","Skipping y_hat=4051\n","Skipping y_hat=4051\n","Skipping y_hat=4051\n","Skipping y_hat=4051\n","Skipping y_hat=4051\n","Skipping y_hat=4051\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=4352\n","Skipping y_hat=4342\n","Skipping y_hat=4342\n","Skipping y_hat=4342\n","Skipping y_hat=4342\n","Skipping y_hat=4342\n","Skipping y_hat=4342\n","Skipping y_hat=4342\n","Skipping y_hat=7841\n","Skipping y_hat=7841\n","Skipping y_hat=7841\n","Skipping y_hat=7841\n","Skipping y_hat=7841\n","Skipping y_hat=7841\n","Skipping y_hat=7841\n","Skipping y_hat=7232\n","Skipping y_hat=7232\n","Skipping y_hat=7232\n","Skipping y_hat=7232\n","Skipping y_hat=7232\n","Skipping y_hat=7232\n","Skipping y_hat=7232\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=5151\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=6742\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=7972\n","Skipping y_hat=7972\n","Skipping y_hat=7972\n","Skipping y_hat=7972\n","Skipping y_hat=7972\n","Skipping y_hat=7972\n","Skipping y_hat=7972\n","Skipping y_hat=3791\n","Skipping y_hat=3791\n","Skipping y_hat=3791\n","Skipping y_hat=3791\n","Skipping y_hat=3791\n","Skipping y_hat=3791\n","Skipping y_hat=3791\n","Skipping y_hat=1071\n","Skipping y_hat=1071\n","Skipping y_hat=1071\n","Skipping y_hat=1071\n","Skipping y_hat=1071\n","Skipping y_hat=1071\n","Skipping y_hat=1071\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=9742\n","Skipping y_hat=2502\n","Skipping y_hat=2502\n","Skipping y_hat=2502\n","Skipping y_hat=2502\n","Skipping y_hat=2502\n","Skipping y_hat=2502\n","Skipping y_hat=2502\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=8271\n","Skipping y_hat=7482\n","Skipping y_hat=7482\n","Skipping y_hat=7482\n","Skipping y_hat=7482\n","Skipping y_hat=7482\n","Skipping y_hat=7482\n","Skipping y_hat=7482\n","Skipping y_hat=6781\n","Skipping y_hat=6781\n","Skipping y_hat=6781\n","Skipping y_hat=6781\n","Skipping y_hat=6781\n","Skipping y_hat=6781\n","Skipping y_hat=6781\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=3450\n","Skipping y_hat=3450\n","Skipping y_hat=3450\n","Skipping y_hat=3450\n","Skipping y_hat=3450\n","Skipping y_hat=3450\n","Skipping y_hat=3450\n","Skipping y_hat=4602\n","Skipping y_hat=4602\n","Skipping y_hat=4602\n","Skipping y_hat=4602\n","Skipping y_hat=4602\n","Skipping y_hat=4602\n","Skipping y_hat=4602\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=6461\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=7272\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=7442\n","Skipping y_hat=7442\n","Skipping y_hat=7442\n","Skipping y_hat=7442\n","Skipping y_hat=7442\n","Skipping y_hat=7442\n","Skipping y_hat=7442\n","Skipping y_hat=0021\n","Skipping y_hat=0021\n","Skipping y_hat=0021\n","Skipping y_hat=0021\n","Skipping y_hat=0021\n","Skipping y_hat=0021\n","Skipping y_hat=0021\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=2532\n","Skipping y_hat=6103\n","Skipping y_hat=6103\n","Skipping y_hat=6103\n","Skipping y_hat=6103\n","Skipping y_hat=6103\n","Skipping y_hat=6103\n","Skipping y_hat=6103\n","Skipping y_hat=7122\n","Skipping y_hat=7122\n","Skipping y_hat=7122\n","Skipping y_hat=7122\n","Skipping y_hat=7122\n","Skipping y_hat=7122\n","Skipping y_hat=7122\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=6271\n","Skipping y_hat=6271\n","Skipping y_hat=6271\n","Skipping y_hat=6271\n","Skipping y_hat=6271\n","Skipping y_hat=6271\n","Skipping y_hat=6271\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=6751\n","Skipping y_hat=6751\n","Skipping y_hat=6751\n","Skipping y_hat=6751\n","Skipping y_hat=6751\n","Skipping y_hat=6751\n","Skipping y_hat=6751\n","Skipping y_hat=0262\n","Skipping y_hat=0262\n","Skipping y_hat=0262\n","Skipping y_hat=0262\n","Skipping y_hat=0262\n","Skipping y_hat=0262\n","Skipping y_hat=0262\n","Skipping y_hat=5992\n","Skipping y_hat=5992\n","Skipping y_hat=5992\n","Skipping y_hat=5992\n","Skipping y_hat=5992\n","Skipping y_hat=5992\n","Skipping y_hat=5992\n","Skipping y_hat=3452\n","Skipping y_hat=3452\n","Skipping y_hat=3452\n","Skipping y_hat=3452\n","Skipping y_hat=3452\n","Skipping y_hat=3452\n","Skipping y_hat=3452\n","Skipping y_hat=8722\n","Skipping y_hat=8722\n","Skipping y_hat=8722\n","Skipping y_hat=8722\n","Skipping y_hat=8722\n","Skipping y_hat=8722\n","Skipping y_hat=8722\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=0322\n","Skipping y_hat=2891\n","Skipping y_hat=2891\n","Skipping y_hat=2891\n","Skipping y_hat=2891\n","Skipping y_hat=2891\n","Skipping y_hat=2891\n","Skipping y_hat=2891\n","Skipping y_hat=9652\n","Skipping y_hat=9652\n","Skipping y_hat=9652\n","Skipping y_hat=9652\n","Skipping y_hat=9652\n","Skipping y_hat=9652\n","Skipping y_hat=9652\n","Skipping y_hat=6162\n","Skipping y_hat=6162\n","Skipping y_hat=6162\n","Skipping y_hat=6162\n","Skipping y_hat=6162\n","Skipping y_hat=6162\n","Skipping y_hat=6162\n","Skipping y_hat=7242\n","Skipping y_hat=7242\n","Skipping y_hat=7242\n","Skipping y_hat=7242\n","Skipping y_hat=7242\n","Skipping y_hat=7242\n","Skipping y_hat=7242\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n","Skipping y_hat=2451\n"," 97% 77/79 [00:09<00:00,  8.72it/s]Skipping y_hat=2142\n","Skipping y_hat=2142\n","Skipping y_hat=2142\n","Skipping y_hat=2142\n","Skipping y_hat=2142\n","Skipping y_hat=2142\n","Skipping y_hat=2142\n","Skipping y_hat=5152\n","Skipping y_hat=5152\n","Skipping y_hat=5152\n","Skipping y_hat=5152\n","Skipping y_hat=5152\n","Skipping y_hat=5152\n","Skipping y_hat=5152\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=7312\n","Skipping y_hat=7312\n","Skipping y_hat=7312\n","Skipping y_hat=7312\n","Skipping y_hat=7312\n","Skipping y_hat=7312\n","Skipping y_hat=7312\n","Skipping y_hat=4821\n","Skipping y_hat=4821\n","Skipping y_hat=4821\n","Skipping y_hat=4821\n","Skipping y_hat=4821\n","Skipping y_hat=4821\n","Skipping y_hat=4821\n","Skipping y_hat=6571\n","Skipping y_hat=6571\n","Skipping y_hat=6571\n","Skipping y_hat=6571\n","Skipping y_hat=6571\n","Skipping y_hat=6571\n","Skipping y_hat=6571\n","Skipping y_hat=6481\n","Skipping y_hat=6481\n","Skipping y_hat=6481\n","Skipping y_hat=6481\n","Skipping y_hat=6481\n","Skipping y_hat=6481\n","Skipping y_hat=6481\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=1021\n","Skipping y_hat=9052\n","Skipping y_hat=9052\n","Skipping y_hat=9052\n","Skipping y_hat=9052\n","Skipping y_hat=9052\n","Skipping y_hat=9052\n","Skipping y_hat=9052\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=8432\n","Skipping y_hat=8432\n","Skipping y_hat=8432\n","Skipping y_hat=8432\n","Skipping y_hat=8432\n","Skipping y_hat=8432\n","Skipping y_hat=8432\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=8122\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=4631\n","Skipping y_hat=4631\n","Skipping y_hat=4631\n","Skipping y_hat=4631\n","Skipping y_hat=4631\n","Skipping y_hat=4631\n","Skipping y_hat=4631\n","Skipping y_hat=1512\n","Skipping y_hat=1512\n","Skipping y_hat=1512\n","Skipping y_hat=1512\n","Skipping y_hat=1512\n","Skipping y_hat=1512\n","Skipping y_hat=1512\n","Skipping y_hat=5631\n","Skipping y_hat=5631\n","Skipping y_hat=5631\n","Skipping y_hat=5631\n","Skipping y_hat=5631\n","Skipping y_hat=5631\n","Skipping y_hat=5631\n","Skipping y_hat=1311\n","Skipping y_hat=1311\n","Skipping y_hat=1311\n","Skipping y_hat=1311\n","Skipping y_hat=1311\n","Skipping y_hat=1311\n","Skipping y_hat=1311\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=9281\n","Skipping y_hat=3142\n","Skipping y_hat=3142\n","Skipping y_hat=3142\n","Skipping y_hat=3142\n","Skipping y_hat=3142\n","Skipping y_hat=3142\n","Skipping y_hat=3142\n","Skipping y_hat=8972\n","Skipping y_hat=8972\n","Skipping y_hat=8972\n","Skipping y_hat=8972\n","Skipping y_hat=8972\n","Skipping y_hat=8972\n","Skipping y_hat=8972\n","Skipping y_hat=9481\n","Skipping y_hat=9481\n","Skipping y_hat=9481\n","Skipping y_hat=9481\n","Skipping y_hat=9481\n","Skipping y_hat=9481\n","Skipping y_hat=9481\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=6082\n","Skipping y_hat=6082\n","Skipping y_hat=6082\n","Skipping y_hat=6082\n","Skipping y_hat=6082\n","Skipping y_hat=6082\n","Skipping y_hat=6082\n","Skipping y_hat=6122\n","Skipping y_hat=6122\n","Skipping y_hat=6122\n","Skipping y_hat=6122\n","Skipping y_hat=6122\n","Skipping y_hat=6122\n","Skipping y_hat=6122\n","Skipping y_hat=9950\n","Skipping y_hat=9950\n","Skipping y_hat=9950\n","Skipping y_hat=9950\n","Skipping y_hat=9950\n","Skipping y_hat=9950\n","Skipping y_hat=9950\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=9462\n","Skipping y_hat=9462\n","Skipping y_hat=9462\n","Skipping y_hat=9462\n","Skipping y_hat=9462\n","Skipping y_hat=9462\n","Skipping y_hat=9462\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=6442\n","Skipping y_hat=5991\n","Skipping y_hat=5991\n","Skipping y_hat=5991\n","Skipping y_hat=5991\n","Skipping y_hat=5991\n","Skipping y_hat=5991\n","Skipping y_hat=5991\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=6102\n","Skipping y_hat=2851\n","Skipping y_hat=2851\n","Skipping y_hat=2851\n","Skipping y_hat=2851\n","Skipping y_hat=2851\n","Skipping y_hat=2851\n","Skipping y_hat=2851\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=6602\n","Skipping y_hat=6602\n","Skipping y_hat=6602\n","Skipping y_hat=6602\n","Skipping y_hat=6602\n","Skipping y_hat=6602\n","Skipping y_hat=6602\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=1932\n","Skipping y_hat=7502\n","Skipping y_hat=7502\n","Skipping y_hat=7502\n","Skipping y_hat=7502\n","Skipping y_hat=7502\n","Skipping y_hat=7502\n","Skipping y_hat=7502\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=4031\n","Skipping y_hat=4871\n","Skipping y_hat=4871\n","Skipping y_hat=4871\n","Skipping y_hat=4871\n","Skipping y_hat=4871\n","Skipping y_hat=4871\n","Skipping y_hat=4871\n","Skipping y_hat=0971\n","Skipping y_hat=0971\n","Skipping y_hat=0971\n","Skipping y_hat=0971\n","Skipping y_hat=0971\n","Skipping y_hat=0971\n","Skipping y_hat=0971\n","Skipping y_hat=4532\n","Skipping y_hat=4532\n","Skipping y_hat=4532\n","Skipping y_hat=4532\n","Skipping y_hat=4532\n","Skipping y_hat=4532\n","Skipping y_hat=4532\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=4802\n","Skipping y_hat=8502\n","Skipping y_hat=8502\n","Skipping y_hat=8502\n","Skipping y_hat=8502\n","Skipping y_hat=8502\n","Skipping y_hat=8502\n","Skipping y_hat=8502\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=6932\n","Skipping y_hat=6932\n","Skipping y_hat=6932\n","Skipping y_hat=6932\n","Skipping y_hat=6932\n","Skipping y_hat=6932\n","Skipping y_hat=6932\n","Skipping y_hat=4642\n","Skipping y_hat=4642\n","Skipping y_hat=4642\n","Skipping y_hat=4642\n","Skipping y_hat=4642\n","Skipping y_hat=4642\n","Skipping y_hat=4642\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=0691\n","Skipping y_hat=0691\n","Skipping y_hat=0691\n","Skipping y_hat=0691\n","Skipping y_hat=0691\n","Skipping y_hat=0691\n","Skipping y_hat=0691\n","Skipping y_hat=7821\n","Skipping y_hat=7821\n","Skipping y_hat=7821\n","Skipping y_hat=7821\n","Skipping y_hat=7821\n","Skipping y_hat=7821\n","Skipping y_hat=7821\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=7552\n","Skipping y_hat=7552\n","Skipping y_hat=7552\n","Skipping y_hat=7552\n","Skipping y_hat=7552\n","Skipping y_hat=7552\n","Skipping y_hat=7552\n","Skipping y_hat=8561\n","Skipping y_hat=8561\n","Skipping y_hat=8561\n","Skipping y_hat=8561\n","Skipping y_hat=8561\n","Skipping y_hat=8561\n","Skipping y_hat=8561\n","Skipping y_hat=8512\n","Skipping y_hat=8512\n","Skipping y_hat=8512\n","Skipping y_hat=8512\n","Skipping y_hat=8512\n","Skipping y_hat=8512\n","Skipping y_hat=8512\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=6272\n","Skipping y_hat=6272\n","Skipping y_hat=6272\n","Skipping y_hat=6272\n","Skipping y_hat=6272\n","Skipping y_hat=6272\n","Skipping y_hat=6272\n","Skipping y_hat=0072\n","Skipping y_hat=0072\n","Skipping y_hat=0072\n","Skipping y_hat=0072\n","Skipping y_hat=0072\n","Skipping y_hat=0072\n","Skipping y_hat=0072\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=0191\n","Skipping y_hat=0191\n","Skipping y_hat=0191\n","Skipping y_hat=0191\n","Skipping y_hat=0191\n","Skipping y_hat=0191\n","Skipping y_hat=0191\n","Skipping y_hat=4541\n","Skipping y_hat=4541\n","Skipping y_hat=4541\n","Skipping y_hat=4541\n","Skipping y_hat=4541\n","Skipping y_hat=4541\n","Skipping y_hat=4541\n","Skipping y_hat=3241\n","Skipping y_hat=3241\n","Skipping y_hat=3241\n","Skipping y_hat=3241\n","Skipping y_hat=3241\n","Skipping y_hat=3241\n","Skipping y_hat=3241\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=6151\n","Skipping y_hat=6151\n","Skipping y_hat=6151\n","Skipping y_hat=6151\n","Skipping y_hat=6151\n","Skipping y_hat=6151\n","Skipping y_hat=6151\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=2962\n","Skipping y_hat=9522\n","Skipping y_hat=9522\n","Skipping y_hat=9522\n","Skipping y_hat=9522\n","Skipping y_hat=9522\n","Skipping y_hat=9522\n","Skipping y_hat=9522\n","Skipping y_hat=0192\n","Skipping y_hat=0192\n","Skipping y_hat=0192\n","Skipping y_hat=0192\n","Skipping y_hat=0192\n","Skipping y_hat=0192\n","Skipping y_hat=0192\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=8303\n","Skipping y_hat=8303\n","Skipping y_hat=8303\n","Skipping y_hat=8303\n","Skipping y_hat=8303\n","Skipping y_hat=8303\n","Skipping y_hat=8303\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=9602\n","Skipping y_hat=9602\n","Skipping y_hat=9602\n","Skipping y_hat=9602\n","Skipping y_hat=9602\n","Skipping y_hat=9602\n","Skipping y_hat=9602\n","Skipping y_hat=3722\n","Skipping y_hat=3722\n","Skipping y_hat=3722\n","Skipping y_hat=3722\n","Skipping y_hat=3722\n","Skipping y_hat=3722\n","Skipping y_hat=3722\n","Skipping y_hat=2823\n","Skipping y_hat=2823\n","Skipping y_hat=2823\n","Skipping y_hat=2823\n","Skipping y_hat=2823\n","Skipping y_hat=2823\n","Skipping y_hat=2823\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=7222\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=9951\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=5602\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=0521\n","Skipping y_hat=0521\n","Skipping y_hat=0521\n","Skipping y_hat=0521\n","Skipping y_hat=0521\n","Skipping y_hat=0521\n","Skipping y_hat=0521\n","Skipping y_hat=7852\n","Skipping y_hat=7852\n","Skipping y_hat=7852\n","Skipping y_hat=7852\n","Skipping y_hat=7852\n","Skipping y_hat=7852\n","Skipping y_hat=7852\n","Skipping y_hat=5061\n","Skipping y_hat=5061\n","Skipping y_hat=5061\n","Skipping y_hat=5061\n","Skipping y_hat=5061\n","Skipping y_hat=5061\n","Skipping y_hat=5061\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=9991\n","Skipping y_hat=9991\n","Skipping y_hat=9991\n","Skipping y_hat=9991\n","Skipping y_hat=9991\n","Skipping y_hat=9991\n","Skipping y_hat=9991\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=2151\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=8281\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=6002\n","Skipping y_hat=0113\n","Skipping y_hat=0113\n","Skipping y_hat=0113\n","Skipping y_hat=0113\n","Skipping y_hat=0113\n","Skipping y_hat=0113\n","Skipping y_hat=0113\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=9232\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=5142\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=4502\n","Skipping y_hat=4502\n","Skipping y_hat=4502\n","Skipping y_hat=4502\n","Skipping y_hat=4502\n","Skipping y_hat=4502\n","Skipping y_hat=4502\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=9662\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=9412\n","Skipping y_hat=9412\n","Skipping y_hat=9412\n","Skipping y_hat=9412\n","Skipping y_hat=9412\n","Skipping y_hat=9412\n","Skipping y_hat=9412\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=5642\n","Skipping y_hat=8922\n","Skipping y_hat=8922\n","Skipping y_hat=8922\n","Skipping y_hat=8922\n","Skipping y_hat=8922\n","Skipping y_hat=8922\n","Skipping y_hat=8922\n","Skipping y_hat=4592\n","Skipping y_hat=4592\n","Skipping y_hat=4592\n","Skipping y_hat=4592\n","Skipping y_hat=4592\n","Skipping y_hat=4592\n","Skipping y_hat=4592\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=4932\n","Skipping y_hat=4932\n","Skipping y_hat=4932\n","Skipping y_hat=4932\n","Skipping y_hat=4932\n","Skipping y_hat=4932\n","Skipping y_hat=4932\n","Skipping y_hat=9122\n","Skipping y_hat=9122\n","Skipping y_hat=9122\n","Skipping y_hat=9122\n","Skipping y_hat=9122\n","Skipping y_hat=9122\n","Skipping y_hat=9122\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=2880\n","Skipping y_hat=2571\n","Skipping y_hat=2571\n","Skipping y_hat=2571\n","Skipping y_hat=2571\n","Skipping y_hat=2571\n","Skipping y_hat=2571\n","Skipping y_hat=2571\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=4332\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=3532\n","Skipping y_hat=9891\n","Skipping y_hat=9891\n","Skipping y_hat=9891\n","Skipping y_hat=9891\n","Skipping y_hat=9891\n","Skipping y_hat=9891\n","Skipping y_hat=9891\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n","Skipping y_hat=6061\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=8991\n","Skipping y_hat=8991\n","Skipping y_hat=8991\n","Skipping y_hat=8991\n","Skipping y_hat=8991\n","Skipping y_hat=8991\n","Skipping y_hat=8991\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=6392\n","Skipping y_hat=6392\n","Skipping y_hat=6392\n","Skipping y_hat=6392\n","Skipping y_hat=6392\n","Skipping y_hat=6392\n","Skipping y_hat=6392\n","Skipping y_hat=0301\n","Skipping y_hat=0301\n","Skipping y_hat=0301\n","Skipping y_hat=0301\n","Skipping y_hat=0301\n","Skipping y_hat=0301\n","Skipping y_hat=0301\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=0942\n","Skipping y_hat=0942\n","Skipping y_hat=0942\n","Skipping y_hat=0942\n","Skipping y_hat=0942\n","Skipping y_hat=0942\n","Skipping y_hat=0942\n","Skipping y_hat=0811\n","Skipping y_hat=0811\n","Skipping y_hat=0811\n","Skipping y_hat=0811\n","Skipping y_hat=0811\n","Skipping y_hat=0811\n","Skipping y_hat=0811\n"," 99% 78/79 [00:09<00:00,  8.82it/s]Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=1781\n","Skipping y_hat=7412\n","Skipping y_hat=7412\n","Skipping y_hat=7412\n","Skipping y_hat=7412\n","Skipping y_hat=7412\n","Skipping y_hat=7412\n","Skipping y_hat=7412\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=3012\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=8891\n","Skipping y_hat=8112\n","Skipping y_hat=8112\n","Skipping y_hat=8112\n","Skipping y_hat=8112\n","Skipping y_hat=8112\n","Skipping y_hat=8112\n","Skipping y_hat=8112\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=1262\n","Skipping y_hat=0421\n","Skipping y_hat=0421\n","Skipping y_hat=0421\n","Skipping y_hat=0421\n","Skipping y_hat=0421\n","Skipping y_hat=0421\n","Skipping y_hat=0421\n","Skipping y_hat=7580\n","Skipping y_hat=7580\n","Skipping y_hat=7580\n","Skipping y_hat=7580\n","Skipping y_hat=7580\n","Skipping y_hat=7580\n","Skipping y_hat=7580\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=2922\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9012\n","Skipping y_hat=9012\n","Skipping y_hat=9012\n","Skipping y_hat=9012\n","Skipping y_hat=9012\n","Skipping y_hat=9012\n","Skipping y_hat=9012\n","Skipping y_hat=2803\n","Skipping y_hat=2803\n","Skipping y_hat=2803\n","Skipping y_hat=2803\n","Skipping y_hat=2803\n","Skipping y_hat=2803\n","Skipping y_hat=2803\n","Skipping y_hat=0272\n","Skipping y_hat=0272\n","Skipping y_hat=0272\n","Skipping y_hat=0272\n","Skipping y_hat=0272\n","Skipping y_hat=0272\n","Skipping y_hat=0272\n","Skipping y_hat=3322\n","Skipping y_hat=3322\n","Skipping y_hat=3322\n","Skipping y_hat=3322\n","Skipping y_hat=3322\n","Skipping y_hat=3322\n","Skipping y_hat=3322\n","Skipping y_hat=3032\n","Skipping y_hat=3032\n","Skipping y_hat=3032\n","Skipping y_hat=3032\n","Skipping y_hat=3032\n","Skipping y_hat=3032\n","Skipping y_hat=3032\n","100% 79/79 [00:09<00:00,  8.29it/s]\n","accuracy of 10000 examples: 979/10000 (9.790000000000001%)\n","/content/drive/MyDrive/addition/evaluation_by_reading_groundtruth.py:310: DtypeWarning: Columns (102) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Final Test Results:\n","test_reverse: 9.79%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m4_operands_0_to_999_uniform_w_padding_reverse\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/3njzauq0\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m4_operands_0_to_999_uniform_w_padding/reverse_out/wandb/run-20250710_164858-3njzauq0/logs\u001b[0m\n"]}],"source":["!python train_with_eval_by_reading.py 4_operands_addition_reversed.txt"]},{"cell_type":"markdown","metadata":{"id":"t5Ro9JOTc3QU"},"source":["## With More Statistical Measurement"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1753634180890,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"3GvXFLuuc7ej","outputId":"906868c1-ce36-42e2-8515-f51f1ca24565"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/addition\n"]}],"source":["%cd /content/drive/MyDrive/addition"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1327,"status":"ok","timestamp":1753634836033,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"TrbxsZ8Jf3CV","outputId":"92ff01e8-f8d0-4a3e-ff54-4643e13ebeb5"},"outputs":[{"name":"stdout","output_type":"stream","text":["import os\n","import pickle\n","import requests\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","import copy\n","import time\n","\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import yaml\n","from torch.utils.data import Dataset, DataLoader\n","import wandb\n","import torch.nn.functional as F\n","import math\n","\n","from model import GPTConfig, GPT\n","from main_utilities import *\n","from evaluation_by_reading_groundtruth import *\n","from statistical_measurements import *\n","\n","import re\n","\n","def create_meta_for_addition(data):\n","    \"\"\"Create metadata for addition data.\"\"\"\n","    # Define the vocabulary for addition problems\n","    # This includes digits, operators, equals sign, and newline\n","    chars = sorted(list(set(data)))\n","    vocab_size = len(chars)\n","    # Create encoder and decoder dictionaries\n","    stoi = {ch: i for i, ch in enumerate(chars)}\n","    itos = {i: ch for i, ch in enumerate(chars)}\n","    \n","    meta = {\n","        'vocab_size': vocab_size,\n","        'vocab': chars,\n","        'stoi': stoi,\n","        'itos': itos\n","    }\n","    return meta\n","\n","def encode_addition(text, meta):\n","    \"\"\"Encode text to tensor using the metadata.\"\"\"\n","    return torch.tensor([meta['stoi'][c] for c in text], dtype=torch.long)\n","\n","def decode_addition(tensor, meta):\n","    \"\"\"Decode tensor to text using the metadata.\"\"\"\n","    if isinstance(tensor, torch.Tensor):\n","        return ''.join([meta['itos'][i.item()] for i in tensor])\n","    else:\n","        return ''.join([meta['itos'][i] for i in tensor])\n","    \n","def pad_sequence(x: torch.Tensor, length: int, pad_value: int):\n","    if x.size(0) < length:\n","        padding = torch.full((length - x.size(0),), pad_value, dtype=torch.long)\n","        return torch.cat([x, padding], dim=0)\n","    else:\n","        return x\n","\n","class AdditionDataset(Dataset):\n","    def __init__(self, file_path, meta):\n","        self.meta = meta\n","        # Read the text file\n","        with open(file_path, 'r') as f:\n","            self.lines = f.readlines()\n","        # Remove any empty lines and strip whitespace\n","        self.lines = [line.strip() for line in self.lines if line.strip()]\n","        self.block_size = block_size  # from your config\n","        \n","    def __len__(self):\n","        return len(self.lines)\n","    \n","    def __getitem__(self, idx):\n","        line = self.lines[idx]\n","        # Convert the line to tensor using our encoder\n","        raw = encode_addition(line, self.meta)\n","        x = pad_sequence(raw[:-1], self.block_size, pad_value=meta['stoi']['$'])  # all but last char\n","        y = pad_sequence(raw[1:], self.block_size, pad_value=-1)   # all but first char\n","        return x, y\n","\n","# I/O\n","\n","out_dir = '/drive/MyDrive/addition/plain_no_pad/out'\n","resume_dir = None\n","resume_iter = False # if True, resume from saved iter_num, otherwise resume from iter_num 0\n","eval_interval = 2000\n","log_interval = 1\n","eval_iters = 200\n","eval_only = False # if True, script exits right after the first eval\n","always_save_checkpoint = True # if True, always save a checkpoint after each eval\n","init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n","\n","# wandb logging\n","wandb_entity = 'ssdd'\n","wandb_log = False # disabled by default\n","wandb_project = 'owt'\n","wandb_run_name = 'gpt2' # 'run' + str(time.time())\n","exp_name = 'default_exp_name'\n","\n","# data\n","dataset = 'bal'\n","gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n","test_batch_size = 128\n","batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n","block_size = 1024\n","train_data_path = 'train.bin'\n","val_data_path = 'val.bin'\n","multi_digit = False\n","num_digit = 3\n","max_new_tokens = 5\n","binary = False\n","\n","# using two data - data1 = text / data2 = addition\n","train_both = False # use seperate text/add data for train/val (get_batch uses this to sample from two differernt datasets)\n","data_ratio = 0.2 # ratio of data_path2 compared with data_path1\n","train_data_path2 = 'train_addition.bin' # only used when train_both = True\n","val_data_path2 = 'val_addition.bin'\n","\n","# evaluation\n","eval_text = False # if True get perplexity using eval_text_data_path\n","eval_text_data_path = None # directory to text data (.bin file) - ex. 'data/shakespeare_add_ar_mixed/val_text.bin'\n","eval_addition = False # if True compute test accuracy of \"a+b=\"\n","start = None\n","eval_addition_ar = False\n","start_ar = None\n","eval_other = False # use this to evaluate other operations (ex. train on operator '-' but evaluate on other_operator '+')\n","start_other = None\n","other_operator = '+'\n","eval_addition_train = False\n","start_train = None\n","reverse_ab = False\n","reverse_c = False\n","zero_pad = False\n","algo_reason = False\n","add_space = False\n","analysis = False\n","\n","# model\n","n_layer = 6\n","n_head = 6\n","n_embd = 768\n","dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n","bias = False # do we use bias inside LayerNorm and Linear layers?\n","ckpt_path_name = 'ckpt.pt'\n","save_final = True\n","\n","# adamw optimizer\n","learning_rate = 6e-4 # max learning rate\n","max_iters = 600000 # total number of training iterations\n","weight_decay = 1e-1\n","beta1 = 0.9\n","beta2 = 0.95\n","grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n","# learning rate decay settings\n","decay_lr = True # whether to decay the learning rate\n","warmup_iters = 2000 # how many steps to warm up for\n","lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n","min_lr = None # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n","\n","# DDP settings\n","backend = 'nccl' # 'nccl', 'gloo', etc.\n","# system\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n","dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n","compile = True # use PyTorch 2.0 to compile the model to be faster\n","use_flash = True\n","data_type = 'binary' # 'binary' by default, can be 'text'\n","operator = '+' # can be '+', '-', '*', 'sin', 'sqrt'\n","data_shuffle = True\n","data_format = 'plain' # 'plain' or 'reverse' or 'algo_reasoning'\n","vocabulary = 'all_ascii_chars' # can be 'all_ascii_chars' or 'numbers_only' or 'custom_input_data'\n","meta_path_specified = True # use saved meta_file (False if data_type='text')\n","eps = 0\n","tokenizer = 'char' # by default, use char level tokenizer. but for pretrained models, use openai tokenizer eg: 'gpt2'\n","\n","simple=False\n","random_A=False\n","random_C=False\n","\n","use_lora = False # use lora (from minLoRA)\n","print_interval = 2  # if we're using gpt-2 model, I want to see it prompted on text\n","\n","# Add this near the top with other config variables\n","test_dir = None  # Directory containing test files\n","eval_additional_test = False  # whether to evaluate on additional test files\n","\n","more_early_eval1 = False # if True, do early eval on train and val data\n","early_eval_interval1 = 25\n","early_eval_iters1 = 1000\n","\n","more_early_eval2 = False # if True, do early eval on train and val data\n","early_eval_interval2 = 5\n","early_eval_iters2 = 500\n","\n","stats_measurement_data_file_path = \"\"\n","\n","drop_leading_digit = False\n","\n","config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str, type(None)))]\n","exec(open('configurator.py').read()) # overrides from command line or config file\n","config = {k: globals()[k] for k in config_keys} # will be useful for logging\n","\n","# additional statistical measurements\n","measure_iters = set(\n","    list(range(0,  200000, 5000)) +    # every 20 steps before 200\n","    # list(range(100000, 100000, 20)) +   # every 50 steps from 200 up to 1500\n","    list(range(200000, max_iters+1, 5000))  # every 100 steps thereafter\n",")\n","\n","# function to set seed for all random number generators\n","def set_seed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    # to make sure GPU runs are deterministic even if they are slower set this to True\n","    torch.backends.cudnn.deterministic = False\n","    # warning: this causes the code to vary across runs\n","    torch.backends.cudnn.benchmark = True\n","    print(\"Seeded everything: {}\".format(seed))\n","\n","if min_lr == None:\n","    min_lr = learning_rate/10\n","master_process = True\n","seed_offset = 0\n","if master_process:\n","  os.makedirs(out_dir, exist_ok=True)\n","torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","torch.backends.cudnn.benchmark = True # cudnn auto-tuner\n","torch.backends.cudnn.deterministic = False # cudnn auto-tuner\n","# this is probably overkill but seed everything again\n","set_seed(1337 + seed_offset)\n","\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","# note: float16 data type will automatically use a GradScaler\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# Read the data files\n","with open(train_data_path, 'r') as f:\n","    train_data = f.read()\n","with open(val_data_path, 'r') as f:\n","    val_data = f.read()\n","\n","# Create metadata from the combined data\n","all_data = train_data + val_data\n","meta = create_meta_for_addition(train_data)\n","meta_vocab_size = meta['vocab_size']\n","print(f\"Using vocabulary size: {meta_vocab_size}\")\n","\n","config['eos_id'] = meta['stoi']['$']\n","\n","with open(stats_measurement_data_file_path, 'r', encoding='utf-8') as f:\n","    lines = [line.rstrip() for line in f]\n","\n","if drop_leading_digit:\n","        S = num_digit\n","else:\n","    S = num_digit + 1\n","# a simple way to parse test strings\n","padded_lines = [] # add 0 padding, remove $; an example padded_lines[6] is '932+084+230+349=5951'\n","for i in range(len(lines)):\n","    numbers = re.split(r'[+=]', lines[i])\n","    numbers[-1] = numbers[-1][:-1]\n","    for k, number in enumerate(numbers[:-1]):\n","        numbers[k] = '0' * (3-len(number)) + number\n","    numbers[-1] = numbers[-1] + '0' * (4-len(numbers[-1]))\n","    padded_lines.append(\"+\".join(numbers[:-1]) + \"=\" + numbers[-1])\n","\n","stats_measurement_data = torch.cat([encode_addition(padded_lines[i], meta).unsqueeze(0) for i in range(len(padded_lines))], dim=0)\n","\n","# # get 16 different datasets (including the base dataset) by randomizing input/output integers of the base dataset\n","# stats_measurement_dataset_list = gen_randomized_datasets(\n","#     stats_measurement_data,\n","#     meta,\n","#     digits_per_num=num_digit,\n","#     base_seed=2005,\n","#     reverse_input=reverse_ab,\n","#     reverse_output=reverse_c\n","# )\n","\n","# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n","iter_num = 0\n","best_val_loss = 1e9\n","best_perplexity = 1e9 # on text data\n","best_accuracy = -1 # on addition data\n","\n","\n","model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n","                  bias=bias, vocab_size=None, dropout=dropout, use_flash=use_flash) # start with model_args from command line\n","if init_from == 'scratch':\n","    # init a new model from scratch\n","    print(\"Initializing a new model from scratch\")\n","    # determine the vocab size we'll use for from-scratch training\n","    if meta_vocab_size is None:\n","        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n","    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    model.to(device)\n","elif init_from == 'resume':\n","    if resume_dir:\n","        print(f\"Resuming training from {resume_dir}\")\n","        checkpoint = torch.load(resume_dir, map_location=device)\n","    else:\n","        print(f\"Resuming training from {out_dir}\")\n","        # resume training from a checkpoint.\n","        ckpt_path = os.path.join(out_dir, ckpt_path_name)\n","        checkpoint = torch.load(ckpt_path, map_location=device)\n","    checkpoint_model_args = checkpoint['model_args']\n","    # force these config attributes to be equal otherwise we can't even resume training\n","    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n","    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n","        model_args[k] = checkpoint_model_args[k]\n","    # create the model\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    model.to(device)\n","    state_dict = checkpoint['model']\n","    # fix the keys of the state dictionary :(\n","    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)\n","    iter_num = checkpoint['iter_num'] if resume_iter else 0\n","    max_iters += iter_num\n","    best_val_loss = checkpoint['best_val_loss']\n","    if 'best_perplexity' in checkpoint.keys():\n","        best_perplexity = checkpoint['best_perplexity']\n","    if 'best_accuracy' in checkpoint.keys():\n","        best_accuracy = checkpoint['best_accuracy']\n","\n","scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","\n","optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n","\n","# compile the model\n","if compile:\n","    print(\"compiling the model... (takes a ~minute)\")\n","    unoptimized_model = model\n","    model = torch.compile(model) # requires PyTorch 2.0\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        # Get an iterator from the DataLoader\n","        dataloader = train_loader if split == 'train' else val_loader\n","        dataloader_iter = iter(dataloader)\n","\n","        for k in range(eval_iters):\n","            try:\n","                X, Y = next(dataloader_iter)\n","\n","            except StopIteration:\n","                # If we run out of batches, create a new iterator\n","                dataloader_iter = iter(dataloader)\n","                X, Y = next(dataloader_iter)\n","\n","            with ctx:\n","                X, Y = X.to(device), Y.to(device)\n","                logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","\n","def get_lr_for_iter(iter_num):\n","    \"\"\"Calculate learning rate based on iteration number using cosine decay with warmup.\"\"\"\n","    if iter_num < warmup_iters:\n","        return learning_rate * (iter_num + 1) / warmup_iters\n","    \n","    if iter_num >= lr_decay_iters:\n","        return min_lr\n","    \n","    decay_ratio = (iter_num - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n","    return min_lr + coeff * (learning_rate - min_lr)\n","\n","# logging\n","if wandb_log and master_process:\n","    import wandb\n","    wandb.init(project=wandb_project, name=wandb_run_name, config=config, dir = out_dir)\n","\n","\n","\n","\n","train_dataset = AdditionDataset(train_data_path, meta)\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    pin_memory=(device_type=='cuda')\n",")\n","\n","val_dataset = AdditionDataset(val_data_path, meta)\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    pin_memory=(device_type=='cuda')\n",")\n","\n","# encode, decode = get_encode_decode(meta_path, tokenizer=tokenizer)\n","\n","# Initialize result_dict with basic metrics\n","result_dict = {\n","    'iter': [],\n","    'train_loss': [],\n","    'val_loss': [],\n","    'test_acc': [],\n","    'train_acc': []\n","}\n","\n","# Initialize test accuracy keys for all test files\n","if eval_additional_test and test_dir:\n","    test_files = [f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))]\n","    for test_file in test_files:\n","        test_name = os.path.splitext(os.path.basename(test_file))[0]\n","        result_dict[f'test_acc_{test_name}'] = []\n","\n","result_dir = get_results_dir(config)\n","config['result_dir'] = result_dir\n","with open(os.path.join(result_dir, \"config.yaml\"), \"w\") as yaml_file:\n","    yaml.dump(config, yaml_file, default_flow_style=False)\n","\n","\n","# # build a dict of open file handles, one per dataset\n","# csv_writers = {}\n","# for dataset in stats_measurement_dataset_list:\n","#     name = dataset['name']\n","#     path = os.path.join(result_dir, f\"{name}_stats.csv\")\n","#     f = open(path, 'w', newline='')\n","#     writer = csv.DictWriter(f, fieldnames=[\n","#         'iter',\n","#         'ave_correct_probs',\n","#         'ave_correct_preds',\n","#         'ave_diff_probs_L1',\n","#         'ave_diff_probs_L2',\n","#         'ave_diff_probs_kl',\n","#         'ave_diff_logits_L1',\n","#         'ave_diff_logits_L2',\n","#         'ave_diff_preds',\n","#     ])\n","#     writer.writeheader()\n","#     csv_writers[name] = writer\n","\n","\n","# Initialize additional metrics for statistical measurements\n","stats_oo = [] # output-output mutual information\n","stats_io = [] # input-output mutual information\n","\n","\n","import time\n","t0 = time.time()\n","local_iter_num = 0 # number of iterations in the lifetime of this process\n","raw_model = model\n","running_mfu = -1.0\n","iter_num = 0\n","\n","max_iters = config.get('max_iters', 10000)\n"," # number of epochs to warm up learning rate\n","\n","# Initialize tracking variables\n","iter_num = 0\n","best_val_loss = 1e9\n","best_accuracy = -1\n","running_mfu = -1.0\n","\n","# Create infinite data loader\n","def get_infinite_dataloader(dataloader):\n","    while True:\n","        for batch in dataloader:\n","            yield batch\n","\n","train_loader_iter = get_infinite_dataloader(train_loader)\n","if 'max_new_tokens' in config.keys():\n","    print(f\"max_new_tokens: {config['max_new_tokens']}\")\n","else:\n","    print(f\"max_new_tokens used: {num_digit+2}\")\n","\n","# Training loop - iteration based\n","while iter_num < max_iters:\n","    model.train()\n","    \n","    # Get learning rate for current iteration\n","    if decay_lr:\n","        lr = get_lr_for_iter(iter_num)\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","    \n","    # Get next batch\n","    X, Y = next(train_loader_iter)\n","    X, Y = X.to(device), Y.to(device)\n","    \n","    # Forward pass\n","    with ctx:\n","        logits, loss = model(X, Y)\n","    \n","    # Backward pass\n","    scaler.scale(loss).backward()\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    scaler.step(optimizer)\n","    scaler.update()\n","    optimizer.zero_grad(set_to_none=True)\n","    \n","    # Do additional statistical measurements\n","    if iter_num in measure_iters:\n","        model.eval()\n","        \n","        with torch.no_grad():\n","            # eval_res = eval_model(model, meta, stats_measurement_dataset_list, digits_per_num=num_digit, batch_size=test_batch_size)\n","            mi_stats = calc_model_dataset_mi(\n","                model = model,\n","                metadata = meta,\n","                data = stats_measurement_data,\n","                digits_per_num = num_digit,\n","                batch_size = test_batch_size,\n","                drop_leading_digit = drop_leading_digit\n","            )\n","\n","        # for name, stats in eval_res.items():\n","        #     if name == \"model_embeddings\":\n","        #         continue\n","        #     if name == 'base':\n","        #         row = {\n","        #             'iter': iter_num,\n","        #             'ave_correct_probs': stats['ave_correct_probs'],\n","        #             'ave_correct_preds': stats['ave_correct_preds'],\n","        #         }\n","        #     else:\n","        #         row = {\n","        #             'iter': iter_num,\n","        #             'ave_correct_probs': stats['ave_correct_probs'],\n","        #             'ave_correct_preds': stats['ave_correct_preds'],\n","        #             'ave_diff_probs_L1': stats['ave_diff_probs_L1'],\n","        #             'ave_diff_probs_L2': stats['ave_diff_probs_L2'],\n","        #             'ave_diff_probs_kl': stats['ave_diff_probs_kl'],\n","        #             'ave_diff_logits_L1': stats['ave_diff_logits_L1'],\n","        #             'ave_diff_logits_L2': stats['ave_diff_logits_L2'],\n","        #             'ave_diff_preds': stats['ave_diff_preds'],\n","        #         }\n","        #     # Write to the CSV file for this dataset\n","        #     csv_writers[name].writerow(row)\n","\n","        \n","        # Calculate output-output mutual information\n","        mi_mat = mi_stats['output-output']['mutual_info']\n","        nmi_mat = mi_stats['output-output']['normalized_mutual_info']\n","        for i in range(mi_mat.shape[0]):\n","            for j in range(i, mi_mat.shape[1]):\n","                stats_oo.append({\n","                    'iter': iter_num,\n","                    'i': i,\n","                    'j': j,\n","                    'mi': mi_mat[i, j].item(),\n","                    'nmi': nmi_mat[i, j].item()\n","                })\n","\n","        # also calculate input-output mutual information\n","        mi_mat_io = mi_stats['input-output']['mutual_info']\n","        nmi_mat_io = mi_stats['input-output']['normalized_mutual_info']\n","        for i in range(mi_mat_io.shape[0]):\n","            for j in range(mi_mat_io.shape[1]):\n","                stats_io.append({\n","                    'iter': iter_num,\n","                    'i': i,\n","                    'j': j,\n","                    'mi': mi_mat_io[i, j].item(),\n","                    'nmi': nmi_mat_io[i, j].item()\n","                })\n","\n","        # **NOW write out the two MI CSVs immediately:**\n","        stats_oo_df = pd.DataFrame(stats_oo)\n","        stats_oo_df.to_csv(os.path.join(result_dir, 'output_output_mi.csv'), index=False)\n","\n","        stats_io_df = pd.DataFrame(stats_io)\n","        stats_io_df.to_csv(os.path.join(result_dir, 'input_output_mi.csv'), index=False)\n","\n","        model.train()\n","        \n","    # Evaluation\n","    if iter_num % eval_interval == 0 or (more_early_eval1 and iter_num <= early_eval_iters1 and iter_num % early_eval_interval1 == 0) or (more_early_eval2 and iter_num <= early_eval_iters2 and iter_num % early_eval_interval2 == 0):\n","        losses = estimate_loss()\n","        print(f\"iter {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        \n","        # Initialize wandb_dict for this iteration\n","        wandb_dict = {\n","            \"iter\": iter_num,\n","            \"train/loss\": losses['train'],\n","            \"val/loss\": losses['val'],\n","            \"lr\": lr,\n","        }\n","\n","        if losses['val'] < best_val_loss:\n","            best_val_loss = losses['val']\n","        \n","        # Regular test evaluation\n","        test_accuracy = None\n","        if eval_addition:\n","            config['start'] = start\n","            test_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","                config, model, ctx, \n","                encode=lambda x: encode_addition(x, meta),\n","                decode=lambda x: decode_addition(x, meta), \n","                verbose=False, \n","                num_digit=num_digit, \n","                zero_pad=zero_pad,\n","                reverse_ab=reverse_ab, \n","                reverse_c=reverse_c,\n","                data_type=data_type, \n","                operator=operator, \n","                data_format=data_format, \n","                analyze=True\n","            )\n","            \n","            # Add test accuracy to wandb_dict\n","            wandb_dict[\"test/accuracy\"] = test_accuracy\n","            \n","            if test_accuracy > best_accuracy and iter_num % 5 * eval_interval == 0:\n","                best_accuracy = test_accuracy\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'best_accuracy': best_accuracy,\n","                    'config': config,\n","                    'meta': meta,\n","                }\n","                torch.save(checkpoint, os.path.join(out_dir, f'ckpt_iter_{iter_num}_acc.pt'))\n","        \n","        # Training data evaluation\n","        train_accuracy = None\n","        if eval_addition_train:\n","            config['start'] = start_train\n","            train_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","                config, model, ctx, \n","                encode=lambda x: encode_addition(x, meta),\n","                decode=lambda x: decode_addition(x, meta), \n","                verbose=False, \n","                num_digit=num_digit, \n","                zero_pad=zero_pad,\n","                reverse_ab=reverse_ab, \n","                reverse_c=reverse_c,\n","                data_type=data_type, \n","                operator=operator, \n","                data_format=data_format\n","            )\n","            \n","            # Add train accuracy to wandb_dict\n","            wandb_dict[\"train/accuracy\"] = train_accuracy\n","        \n","        # Additional test files evaluation\n","        if eval_additional_test and test_dir:\n","            test_files = []\n","            for file in os.listdir(test_dir):\n","                if os.path.isfile(os.path.join(test_dir, file)):\n","                    test_files.append(os.path.join(test_dir, file))\n","            \n","            if not test_files:\n","                print(f\"Warning: No files found in test directory: {test_dir}\")\n","            else:\n","                # Evaluate on all test files\n","                test_results = evaluate_multiple_files(\n","                    config, model, ctx,\n","                    encode=lambda x: encode_addition(x, meta),\n","                    decode=lambda x: decode_addition(x, meta),\n","                    test_files=test_files,\n","                    iter_num=iter_num,\n","                    result_dir=result_dir,\n","                    verbose=False,\n","                    num_digit=num_digit,\n","                    zero_pad=zero_pad,\n","                    reverse_ab=reverse_ab,\n","                    reverse_c=reverse_c,\n","                    data_type=data_type,\n","                    operator=operator,\n","                    data_format=data_format,\n","                    analyze=True\n","                )\n","                \n","                # Log results\n","                print(\"\\nTest Results:\")\n","                for test_name, accuracy in test_results.items():\n","                    print(f\"{test_name}: {accuracy:.2f}%\")\n","                    # Add accuracy to result_dict (key was initialized at start)\n","                    result_dict[f'test_acc_{test_name}'].append(accuracy)\n","                    # Add to wandb_dict\n","                    wandb_dict[f\"test/accuracy_{test_name}\"] = accuracy\n","                print()\n","        \n","        # Update and save basic metrics\n","        result_dict['iter'].append(iter_num)\n","        result_dict['train_loss'].append(losses['train'].item())\n","        result_dict['val_loss'].append(losses['val'].item())\n","        result_dict['test_acc'].append(test_accuracy)\n","        result_dict['train_acc'].append(train_accuracy)\n","\n","        # For any test file that wasn't evaluated this iteration, add None to maintain array lengths\n","        if eval_additional_test and test_dir:\n","            for test_file in test_files:\n","                test_name = os.path.splitext(os.path.basename(test_file))[0]\n","                if test_name not in test_results:\n","                    result_dict[f'test_acc_{test_name}'].append(None)\n","        \n","        # Save results to CSV after each evaluation\n","        result_df = pd.DataFrame(result_dict)\n","        result_df.to_csv(os.path.join(result_dir, 'training_metrics.csv'), index=False)\n","        \n","        # Single wandb log per iteration with all metrics\n","        if wandb_log:\n","            wandb.log(wandb_dict)\n","    \n","    iter_num += 1\n","\n","# Save final checkpoint\n","checkpoint = {\n","    'model': raw_model.state_dict(),\n","    'optimizer': optimizer.state_dict(),\n","    'model_args': model_args,\n","    'iter_num': iter_num,\n","    'best_val_loss': best_val_loss,\n","    'best_accuracy': best_accuracy,\n","    'config': config,\n","    'meta': meta,\n","}\n","torch.save(checkpoint, os.path.join(out_dir, f'ckpt_final.pt'))\n","\n","\n","losses = estimate_loss()\n","\n","if eval_addition:\n","    config['start'] = start\n","    test_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","        config, model, ctx, \n","        encode=lambda x: encode_addition(x, meta),\n","        decode=lambda x: decode_addition(x, meta), \n","        verbose=False, \n","        num_digit=num_digit, \n","        zero_pad=zero_pad,\n","        reverse_ab=reverse_ab, \n","        reverse_c=reverse_c,\n","        data_type=data_type, \n","        operator=operator, \n","        data_format=data_format, \n","        analyze=True\n","    )\n","    import csv\n","    # Save correct examples\n","    correct_path = os.path.join(result_dir, 'correct_examples.csv')\n","    with open(correct_path, 'w', newline='') as csvfile:\n","        fieldnames = ['operands', 'result', 'outcome', 'c_hat2']\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","        writer.writeheader()\n","        for i, nums in enumerate(correct):\n","            operands, result, outcome, c_hat2 = nums\n","            writer.writerow({'operands': operands, 'result': result, 'outcome': outcome, 'c_hat2': c_hat2})\n","    \n","    # Save incorrect examples\n","    incorrect_path = os.path.join(result_dir, 'incorrect_examples.csv')\n","    with open(incorrect_path, 'w', newline='') as csvfile:\n","        fieldnames = ['operands', 'result', 'outcome', 'c_hat2']\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","        writer.writeheader()\n","        for i, nums in enumerate(incorrect):\n","            operands, result, outcome, c_hat2 = nums\n","            writer.writerow({'operands': operands, 'result': result, 'outcome': outcome, 'c_hat2': c_hat2})\n","\n","if eval_addition_train:\n","    config['start'] = start_train\n","    train_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","        config, model, ctx, \n","        encode=lambda x: encode_addition(x, meta),\n","        decode=lambda x: decode_addition(x, meta), \n","        verbose=False, \n","        num_digit=num_digit, \n","        zero_pad=zero_pad,\n","        reverse_ab=reverse_ab, \n","        reverse_c=reverse_c,\n","        data_type=data_type, \n","        operator=operator, \n","        data_format=data_format\n","    )\n","    \n","    \n","if eval_additional_test and test_dir:\n","    # Get all files from the test directory\n","    test_files = []\n","    for file in os.listdir(test_dir):\n","        if os.path.isfile(os.path.join(test_dir, file)):\n","            test_files.append(os.path.join(test_dir, file))\n","    \n","    if not test_files:\n","        print(f\"Warning: No files found in test directory: {test_dir}\")\n","    else:\n","        final_results = evaluate_multiple_files(\n","            config, model, ctx,\n","            encode=lambda x: encode_addition(x, meta),\n","            decode=lambda x: decode_addition(x, meta),\n","            test_files=test_files,\n","            iter_num='final',\n","            result_dir=result_dir,\n","            verbose=False,\n","            num_digit=num_digit,\n","            zero_pad=zero_pad,\n","            reverse_ab=reverse_ab,\n","            reverse_c=reverse_c,\n","            data_type=data_type,\n","            operator=operator,\n","            data_format=data_format,\n","            analyze=True\n","        )\n","        \n","        print(\"\\nFinal Test Results:\")\n","        for test_name, accuracy in final_results.items():\n","            print(f\"{test_name}: {accuracy:.2f}%\")\n","        print()\n","\n","\n","# Final wandb logging\n","if wandb_log:\n","    final_dict = {\n","        \"iter\": iter_num,\n","        \"train/loss\": losses['train'],\n","        \"val/loss\": losses['val'],\n","        \"lr\": lr,\n","        \"test/accuracy\": test_accuracy if eval_addition else None,\n","        \"train/accuracy\": train_accuracy if eval_addition_train else None,\n","    }\n","    if eval_additional_test and test_dir:\n","        for test_name, accuracy in final_results.items():\n","            final_dict[f\"final_test/accuracy_{test_name}\"] = accuracy\n","    wandb.log(final_dict)\n","\n","# Save final DataFrame\n","result_df = pd.DataFrame(result_dict)\n","result_df.to_csv(os.path.join(result_dir, 'training_metrics.csv'), index=False)\n"]}],"source":["%cat train_additional_measurement.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":664,"status":"ok","timestamp":1753557193637,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"Y9wu_se2bn14","outputId":"77ce9768-760f-4a1f-c281-cebcc339a56a"},"outputs":[{"name":"stdout","output_type":"stream","text":["# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 2000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_0_to_999_output_wo_leading_digit_mi_reverse'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'algo_reasoning'\n","data_format='reverse'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: whether the result is reversed\n","reverse_c = True\n","eval_addition = True\n","\n","analysis = False\n","\n","# to edit: num of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 4\n","\n","drop_leading_digit = True\n","\n","# whether need zero‐padding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 400000\n","lr_decay_iters = 400000 # make equal to max_iters usually (300000)\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory\n","out_dir = '/content/drive/MyDrive/addition/4_operands_0_to_999_output_wo_leading_digit/reverse_out_with_mi'\n","\n","# to edit: training data\n","train_data_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","start_train = \"FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/train_eval_reverse.txt\"\n","\n","# to edit: valuation data\n","val_data_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/val_reverse.txt'\n","\n","# to edit: test data (start is just the test file)\n","start = 'FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/test_reverse/test_reverse.txt'\n","\n","# (optional param) to_edit: whether to enable detailed metric recording at each eval_interval\n","# test_dir: the directory storing test files\n","test_dir = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/test_reverse'\n","eval_additional_test = True  # whether to evaluate on additional test files \n","\n","# (optional) data for additional statistical measurement\n","stats_measurement_data_file_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/4_operand_addition_stats_measurement_data_reversed.txt'\n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval1 = False\n","early_eval_interval1 = 5\n","early_eval_iters1 = 1000\n","\n","more_early_eval2 = False\n","early_eval_interval2 = 5\n","early_eval_iters2 = 750"]}],"source":["%cat 4_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1282,"status":"ok","timestamp":1753635053884,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"c-Hd5HcdQeM2","outputId":"3ae29b50-3d69-4aa1-8713-37741379e7ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["import os\n","import pickle\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","import copy\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import yaml\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","import math\n","import sys\n","\n","def encode_addition(text, meta):\n","    \"\"\"Encode text to tensor using the metadata.\"\"\"\n","    return torch.tensor([meta['stoi'][c] for c in text], dtype=torch.long)\n","\n","def decode_addition(tensor, meta):\n","    \"\"\"Decode tensor to text using the metadata.\"\"\"\n","    if isinstance(tensor, torch.Tensor):\n","        return ''.join([meta['itos'][i.item()] for i in tensor])\n","    else:\n","        return ''.join([meta['itos'][i] for i in tensor])\n","\n","def token_to_numeric(tensor, meta):\n","    \"\"\"Convert tensor to numeric digits.\"\"\"\n","    # Build lookup tensor\n","    lookup_tensor = torch.empty(len(meta[\"vocab\"]), dtype=torch.long)\n","    for i, s in enumerate(meta[\"vocab\"]):\n","        if s.isdigit():\n","            lookup_tensor[i] = int(s)\n","    return lookup_tensor[tensor]  # Same shape as tensor\n","\n","\n","def calc_embed_scores(model, diffs=list(range(5))):\n","    \"\"\"\n","    calc_embed_scores uses the model's embedding and unembedding weight matrices to calculate scores,\n","        which measures the variance of cosine similarity between two embedding vetors E(n), E(m) when n-m is a constant\n","    \"\"\"\n","    W0 = model.transformer.wte.weight.to(\"cpu\") # (vocab_size, n_embd)\n","    W_out = model.lm_head.weight.to(\"cpu\") # (vocab_size, n_embd)\n","    G0 = F.normalize(W0, dim=-1) @ F.normalize(W0, dim=-1).T\n","    G_out = F.normalize(W_out, dim=-1) @ F.normalize(W_out, dim=-1).T\n","\n","    out = []\n","    for G in [G0, G_out]:\n","        score = torch.zeros(len(diffs))\n","        for k, diag_id in enumerate(diffs):\n","            score[k] = torch.var(torch.diag(G, diagonal=diag_id))\n","        out.append(score.mean().item())\n","    return out\n","\n","def randomize_test_data(data: torch.Tensor, metadata, digits_per_num=3, randomize_digit_place=[0,1], seed=2025,\n","                        randomize=\"input\", valid_carry=False, reverse_input=False, reverse_output=False) -> torch.Tensor:\n","    \"\"\"\n","    randomize_test_data randomizes a part of the test data by keeping some digits and randomizing the other digits\n","    Arguments:\n","        data is a 2-order tensor of shape (sample_size, seq_len), representing tokenized inputs (padded right to the same length) such as\n","        '437+357+579+984=7532' and '932+084+230+349=5951' (reverse output)\n","        digits_per_num is the number of digits in a number\n","        randomize_digit_place is a list indicating which digits are to be randomized. [0, 1] means the least two digits are to be randomized\n","        randomize: if \"input\" then the input numbers are randomized, if \"output\" then the output number is randomized\n","        valid_carry is a boolean indicating whether randomization keeps carry valid (carry operation before randomization remains so)\n","    \"\"\"\n","    assert isinstance(randomize_digit_place, list)\n","    L = len(randomize_digit_place)\n","    n, T = data.shape\n","    S = digits_per_num + 1\n","    assert (T - S) % S == 0, \"data format not conform to expectation, e.g., '437+357+579+984=7532'. \"\n","    assert randomize in [\"input\", \"output\"], \"randomize is either `input` or `output`.\"\n","    num_op = (T - S) // S\n","    torch.manual_seed(seed)\n","\n","    ids0 = [digits_per_num-1-id for id in randomize_digit_place] if not reverse_input else randomize_digit_place\n","    ids1 = [digits_per_num-id for id in randomize_digit_place] if not reverse_output else randomize_digit_place\n","    ids_rand_input = torch.cat([torch.arange(num_op).long() * S + j for j in ids0])\n","    ids_rand_output = torch.tensor(ids1).long() + S*num_op\n","    new_data = copy.deepcopy(data)\n","    ids2 = []\n","    if randomize == \"output\":\n","        for col_id in ids_rand_output:\n","            new_data[:,col_id] = data[torch.randperm(n),col_id]\n","        return new_data\n","    if valid_carry: # if control for valid carry\n","        if 0 in randomize_digit_place: # if least significant digit is randomized\n","            J = max(randomize_digit_place) if reverse_input else digits_per_num-1-max(randomize_digit_place) #\n","            ids2 = torch.arange(num_op).long() * S + J\n","            all_carry = token_to_numeric(data[:,ids2], meta=metadata).sum(dim=1) // 10\n","            unique_carry = torch.unique(all_carry)\n","            for carry in unique_carry:\n","                ids_rand = (all_carry == carry)\n","                n_rand = ids_rand.sum().item()\n","                subset_ids = ids_rand.nonzero(as_tuple=True)[0]\n","                subset_data = new_data[ids_rand, :][:, ids2]\n","                subset_data = subset_data[torch.randperm(n_rand), :]\n","                ii, jj = torch.meshgrid(subset_ids, ids2, indexing='ij')\n","                new_data[ii, jj] = subset_data\n","        # randomize other digits independently\n","        for col_id in ids_rand_input:\n","            if col_id not in ids2:\n","                new_data[:,col_id] = data[torch.randperm(n),col_id]\n","    else: # if disregard carry\n","        for col_id in ids_rand_input:\n","            new_data[:,col_id] = data[torch.randperm(n),col_id]\n","    return new_data\n","\n","def _model_forward(model, metadata, data, digits_per_num=3, batch_size=128):\n","    n, T = data.shape\n","    vocab_size = len(metadata[\"vocab\"])\n","    device = next(model.parameters()).device\n","    res = {\"logits\": torch.empty(n, digits_per_num+1, vocab_size, dtype=torch.float),\n","           \"probs\": torch.empty(n, digits_per_num+1, vocab_size, dtype=torch.float),\n","           \"pred_ids\": torch.empty(n, digits_per_num+1, dtype=torch.long)}\n","    num_batches = np.ceil(n / batch_size).astype(int)\n","    with torch.no_grad():\n","        for b in range(num_batches):\n","            if b < num_batches - 1:\n","                samp_ids = list(range(b*batch_size, b*batch_size+batch_size))\n","            else:\n","                samp_ids = list(range(b*batch_size, n))\n","            input_ids, targets = data[samp_ids, :-1].to(device), data[samp_ids, 1:].to(device)\n","            logits, _ = model(input_ids, targets)  # (batch_size, T-1, vocab_size)\n","            logits = logits[:, -(digits_per_num+1):, :] # (batch_size, digits_per_num+1, vocab_size)\n","            probs = torch.softmax(logits, dim=-1) # (batch_size, digits_per_num+1, vocab_size)\n","            pred_ids = torch.argmax(probs, dim=-1) # (batch_size, digits_per_num+1)\n","            res[\"logits\"][samp_ids], res[\"probs\"][samp_ids], res[\"pred_ids\"][samp_ids] = logits.to(\"cpu\"), probs.to(\"cpu\"), pred_ids.to(\"cpu\")\n","    return res\n","\n","def gen_randomized_datasets(base_data, metadata, digits_per_num=3, base_seed=2005, reverse_input=False, reverse_output=False):\n","    \"\"\"Generate a list of randomized datasets\"\"\"\n","    base_dataset = {\"name\": \"base\", \"data\": base_data}\n","    dataset_list = [base_dataset]\n","    # generate different datasets by randomizing input integers of the base dataset\n","    for is_carry in [True, False]:\n","        for increasing in [True, False]:\n","            for k in range(digits_per_num):\n","                randomize_digit_place = list(range(0,k+1)) if increasing else list(range(digits_per_num-1-k,digits_per_num))\n","                seed = base_seed+k+is_carry*100+increasing*10+k\n","                name = f\"carry_{is_carry}_\" + \"_\".join(map(str, randomize_digit_place))\n","                data = randomize_test_data(base_data, metadata, digits_per_num, randomize_digit_place, seed,\n","                        \"input\", is_carry, reverse_input, reverse_output)\n","                dataset = {\"name\": name, \"data\": data, \"is_carry\": is_carry, \"randomize_digit_place\": randomize_digit_place, \"randomize\": \"input\"}\n","                dataset_list.append(dataset)\n","\n","    # generate different datasets by randomizing output integers of the base dataset\n","    for k in range(digits_per_num):\n","        randomize_digit_place = list(range(0,k+1))\n","        seed = base_seed + k\n","        name = \"output_randomize_\" + \"_\".join(map(str, randomize_digit_place))\n","        data = randomize_test_data(base_data, metadata, digits_per_num, randomize_digit_place, seed,\n","                \"output\", False, reverse_input, reverse_output)\n","        dataset = {\"name\": name, \"data\": data, \"is_carry\": None, \"randomize_digit_place\": randomize_digit_place, \"randomize\": \"output\"}\n","        dataset_list.append(dataset)\n","    return dataset_list\n","\n","\n","def eval_model(model, metadata, dataset_list, digits_per_num=3, batch_size=128):\n","    \"\"\"\n","    eval_model evaluates a model on a list of datasets, including the baseset (testset) and randomized datasets\n","    Returns:\n","        eval_res: a dictionary of evaluation results with dataset names as keys, and values are again a dictionary of different evaluation metrics\n","    \"\"\"\n","\n","    dataset_names = [dataset[\"name\"] for dataset in dataset_list]\n","    k0 = dataset_names.index(\"base\")\n","    base_data = dataset_list[k0][\"data\"]\n","    n = base_data.shape[0]\n","    S = digits_per_num + 1\n","    vocab_size = len(metadata[\"vocab\"])\n","    eval_res = {}\n","\n","    base_res = _model_forward(model, metadata, base_data, digits_per_num=digits_per_num, batch_size=batch_size)\n","    batch_idx = torch.arange(n).unsqueeze(1)  # shape: (batch_size, 1)\n","    seq_idx = torch.arange(S).unsqueeze(0)       # shape: (1, S)\n","    eval_res[\"base\"] = {}\n","    eval_res[\"base\"][\"ave_correct_probs\"] = base_res[\"probs\"][batch_idx, seq_idx, base_data[:, -(digits_per_num+1):]].mean(0).tolist()\n","    eval_res[\"base\"][\"ave_correct_preds\"] = torch.mean((base_res[\"pred_ids\"] == base_data[:, -(digits_per_num+1):]).float(), dim=0).tolist()\n","\n","    for k, dataset in tqdm(enumerate(dataset_list)):\n","        if k == k0:\n","            continue\n","        eval_res[dataset[\"name\"]] = {}\n","        res = _model_forward(model, metadata, dataset[\"data\"], digits_per_num=digits_per_num, batch_size=batch_size)\n","        eval_res[dataset[\"name\"]][\"ave_correct_probs\"] = res[\"probs\"][batch_idx, seq_idx, base_data[:, -(digits_per_num+1):]].mean(0).tolist()\n","        eval_res[dataset[\"name\"]][\"ave_correct_preds\"] = torch.mean((res[\"pred_ids\"] == base_data[:, -(digits_per_num+1):]).float(), dim=0).tolist()\n","        eval_res[dataset[\"name\"]][\"ave_diff_probs_L1\"] = torch.sum(torch.abs(res[\"probs\"] - base_res[\"probs\"]), dim=-1).mean(0).tolist()\n","        eval_res[dataset[\"name\"]][\"ave_diff_probs_L2\"] = torch.sum((res[\"probs\"] - base_res[\"probs\"])**2, dim=-1).sqrt().mean(0).tolist()\n","        eval_res[dataset[\"name\"]][\"ave_diff_probs_kl\"] = F.kl_div(F.log_softmax(res[\"logits\"], dim=-1), base_res[\"probs\"], reduction=\"none\").sum(-1).mean(0).tolist()\n","        eval_res[dataset[\"name\"]][\"ave_diff_logits_L1\"] = torch.sum(torch.abs(res[\"logits\"] - base_res[\"logits\"]), dim=-1).mean(0).tolist()\n","        eval_res[dataset[\"name\"]][\"ave_diff_logits_L2\"] = torch.sum((res[\"logits\"] - base_res[\"logits\"])**2, dim=-1).sqrt().mean(0).tolist()\n","        eval_res[dataset[\"name\"]][\"ave_diff_preds\"] = torch.mean((res[\"pred_ids\"] == base_res[\"pred_ids\"]).float(), dim=0).tolist()\n","\n","    eval_res[\"model_embeddings\"] = calc_embed_scores(model)\n","    return eval_res\n","\n","def calc_mi_x_p(x, py_given_x):\n","    \"\"\"\n","    Estimate mutual information I(X; Y) from:\n","    - x: 1D tensor of n samples from X (discrete values)\n","    - py_given_x: 2D tensor of shape (n, k), where each row is p(y | x_i)\n","\n","    Assumes:\n","    - Each row of py_given_x is a valid probability distribution (sums to 1)\n","    - y takes k possible values\n","    \"\"\"\n","    n, k = py_given_x.shape\n","    assert x.shape[0] == n, \"x and py_given_x must have the same number of samples\"\n","\n","    # Compute empirical p(x)\n","    x_unique, x_counts = torch.unique(x, return_counts=True)\n","    px_dict = dict(zip(x_unique.tolist(), (x_counts / n).tolist()))\n","\n","    # Aggregate by unique x values\n","    # Create mapping from each unique x to its p(y | x)\n","    py_given_x_dict = {}\n","    for val in x_unique:\n","        mask = (x == val)\n","        py_given_x_dict[val.item()] = py_given_x[mask].mean(dim=0)\n","\n","    # Compute marginal p(y)\n","    py = sum(px_dict[val] * py_given_x_dict[val] for val in px_dict)\n","    py = py / py.sum()  # normalize for safety\n","    E_y = -torch.sum(py * torch.log(py))\n","\n","    # Compute KL divergence for each unique x\n","    mi = 0.0\n","    for val in px_dict:\n","        pxy = py_given_x_dict[val]\n","        kl = (pxy * (pxy / py).log()).sum()\n","        mi += px_dict[val] * kl\n","\n","    return {\"mutual_info\": float(mi), \"normalized_mutual_info\": float(mi/E_y)}\n","\n","\n","def calc_model_dataset_mi(model, metadata, data, digits_per_num=3, batch_size=128, drop_leading_digit=False):\n","    \"\"\"\n","    calc_model_dataset_mi estimate mutual information I(X; Y) for various pairs of digits X and Y from both data and model prediction probs\n","    res1---X is taken to be one of the digits in the first number, Y is one of the digits in the output number\n","    res2---both X and Y are taken to be one of the digits in the output number\n","    \"\"\"\n","    n, T = data.shape\n","    vocab_size = len(metadata[\"vocab\"])\n","    device = next(model.parameters()).device\n","    if drop_leading_digit:\n","        S = digits_per_num\n","    else:\n","        S = digits_per_num + 1\n","    S = int(S)\n","    if drop_leading_digit:\n","        assert (T - S) % (S + 1) == 0, \"data format not conform to expectation, e.g., '437+357+579+984=7532'. \"\n","    else:\n","        assert (T - S) % S == 0, \"data format not conform to expectation, e.g., '437+357+579+984=7532'. \"\n","    num_op = (T - S) // S\n","    num_batches = np.ceil(n / batch_size).astype(int)\n","    #\n","    res1 = {\"mutual_info\": np.empty((digits_per_num, S)),\n","            \"normalized_mutual_info\": np.empty((digits_per_num, S))}\n","    probs = torch.empty(n, S, vocab_size, dtype=torch.float)\n","    with torch.no_grad():\n","        for b in range(num_batches):\n","            if b < num_batches - 1:\n","                samp_ids = list(range(b*batch_size, b*batch_size+batch_size))\n","            else:\n","                samp_ids = list(range(b*batch_size, n))\n","            input_ids, targets = data[samp_ids, :-1].to(device), data[samp_ids, 1:].to(device)\n","            logits, _ = model(input_ids, targets)  # (batch_size, T-1, vocab_size)\n","            logits = logits[:, -S:, :] # (batch_size, digits_per_num+1, vocab_size)\n","            probs[samp_ids] = torch.softmax(logits, dim=-1).to(\"cpu\") # (batch_size, digits_per_num+1, vocab_size)\n","    for i1 in range(digits_per_num):\n","        for i2 in range(S):\n","            out = calc_mi_x_p(data[:,i1], probs[:,i2])\n","            res1[\"mutual_info\"][i1, i2] = out[\"mutual_info\"]\n","            res1[\"normalized_mutual_info\"][i1, i2] = out[\"normalized_mutual_info\"]\n","    #\n","    res2 = {\"mutual_info\": np.empty((S-1, S-1)),\n","            \"normalized_mutual_info\": np.empty((S-1, S-1))}\n","\n","    # first calculate joint probs of every pair (X, Y) of output digits, each joint probs is of size (vocab_size, vocab_size)\n","    # where X is one of the first digits_per_num digits in output number\n","    # and Y is one of the last digits_per_num digits in output number\n","    joint_probs_all = torch.empty(S-1, S-1, vocab_size, vocab_size, dtype=torch.float)\n","    with torch.no_grad():\n","        for i3 in range(S-1):\n","            for z in tqdm(range(vocab_size)):\n","                new_data = copy.deepcopy(data)\n","                new_data[:, T-S+i3] = z # replace the digit X at index T-S+i3 with a fixed value z\n","                probs3 = torch.empty(n, S-1, vocab_size, dtype=torch.float)\n","                for b in range(num_batches):\n","                    if b < num_batches - 1:\n","                        samp_ids = list(range(b*batch_size, b*batch_size+batch_size))\n","                    else:\n","                        samp_ids = list(range(b*batch_size, n))\n","                    input_ids, targets = new_data[samp_ids, :-1].to(device), new_data[samp_ids, 1:].to(device)\n","                    logits, _ = model(input_ids, targets)  # (batch_size, T-1, vocab_size)\n","                    logits = logits[:, -(S-1):, :] # (batch_size, digits_per_num, vocab_size)\n","                    probs3[samp_ids] = torch.softmax(logits, dim=-1).to(\"cpu\") # (batch_size, digits_per_num, vocab_size) representing Pr(Y|X=z)\n","                joint_probs_all[i3,:,z,:] = torch.mean(probs3 * probs[:,i3,z].unsqueeze(1).unsqueeze(1), dim=0) # Pr(Y|X=z) * Pr(X=z)\n","\n","    for j1 in range(S-1):\n","        for j2 in range(S-1):\n","            if j1 > j2:\n","                res2[\"mutual_info\"][j1, j2] = None\n","                res2[\"normalized_mutual_info\"][j1, j2] = None\n","                continue\n","            pxy = joint_probs_all[j1, j2] / joint_probs_all[j1, j2].sum() # P(x, y), normalized for safety\n","            px = pxy.sum(dim=1, keepdim=True)  # shape (num_x, 1)\n","            py = pxy.sum(dim=0, keepdim=True)  # shape (1, num_y)\n","            E_y = -torch.sum(py * torch.log(py))\n","            px_py = px @ py  # outer product, shape (num_x, num_y)\n","            mask = pxy > 0\n","            mi = torch.sum(pxy[mask] * torch.log(pxy[mask] / px_py[mask]))\n","            res2[\"mutual_info\"][j1, j2] = float(mi)\n","            res2[\"normalized_mutual_info\"][j1, j2] = float(mi/E_y)\n","\n","    return {\"input-output\":res1, \"output-output\":res2}"]}],"source":["%cat statistical_measurements.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eezrmAW0c8I_"},"outputs":[],"source":["!python train_additional_measurement.py 2_operands_addition_plain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"GZuxFM6fc_wA","outputId":"0de8365f-2b18-4e13-c1a0-2a444adcf0e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=3601\n","Skipping y_hat=3601\n","Skipping y_hat=3601\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=6740\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=5711\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=4561\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=9180\n","Skipping y_hat=9180\n","Skipping y_hat=9180\n","Skipping y_hat=9180\n","Skipping y_hat=9180\n","Skipping y_hat=9180\n","Skipping y_hat=9180\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=7011\n","Skipping y_hat=9841\n","Skipping y_hat=9841\n","Skipping y_hat=9841\n","Skipping y_hat=9841\n","Skipping y_hat=9841\n","Skipping y_hat=9841\n","Skipping y_hat=9841\n","Skipping y_hat=0311\n","Skipping y_hat=0311\n","Skipping y_hat=0311\n","Skipping y_hat=0311\n","Skipping y_hat=0311\n","Skipping y_hat=0311\n","Skipping y_hat=0311\n","Skipping y_hat=8680\n","Skipping y_hat=8680\n","Skipping y_hat=8680\n","Skipping y_hat=8680\n","Skipping y_hat=8680\n","Skipping y_hat=8680\n","Skipping y_hat=8680\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=9001\n","Skipping y_hat=9001\n","Skipping y_hat=9001\n","Skipping y_hat=9001\n","Skipping y_hat=9001\n","Skipping y_hat=9001\n","Skipping y_hat=9001\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=5840\n","Skipping y_hat=5840\n","Skipping y_hat=5840\n","Skipping y_hat=5840\n","Skipping y_hat=5840\n","Skipping y_hat=5840\n","Skipping y_hat=5840\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=5511\n","Skipping y_hat=5511\n","Skipping y_hat=5511\n","Skipping y_hat=5511\n","Skipping y_hat=5511\n","Skipping y_hat=5511\n","Skipping y_hat=5511\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n"," 75% 18/24 [00:02<00:00,  6.66it/s]Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=9331\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=5261\n","Skipping y_hat=5261\n","Skipping y_hat=5261\n","Skipping y_hat=5261\n","Skipping y_hat=5261\n","Skipping y_hat=5261\n","Skipping y_hat=5261\n","Skipping y_hat=0490\n","Skipping y_hat=0490\n","Skipping y_hat=0490\n","Skipping y_hat=0490\n","Skipping y_hat=0490\n","Skipping y_hat=0490\n","Skipping y_hat=0490\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=8430\n","Skipping y_hat=8430\n","Skipping y_hat=8430\n","Skipping y_hat=8430\n","Skipping y_hat=8430\n","Skipping y_hat=8430\n","Skipping y_hat=8430\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=4310\n","Skipping y_hat=4310\n","Skipping y_hat=4310\n","Skipping y_hat=4310\n","Skipping y_hat=4310\n","Skipping y_hat=4310\n","Skipping y_hat=4310\n","Skipping y_hat=4950\n","Skipping y_hat=4950\n","Skipping y_hat=4950\n","Skipping y_hat=4950\n","Skipping y_hat=4950\n","Skipping y_hat=4950\n","Skipping y_hat=4950\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=1331\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=8531\n","Skipping y_hat=7760\n","Skipping y_hat=7760\n","Skipping y_hat=7760\n","Skipping y_hat=7760\n","Skipping y_hat=7760\n","Skipping y_hat=7760\n","Skipping y_hat=7760\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=1481\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=4251\n","Skipping y_hat=4430\n","Skipping y_hat=4430\n","Skipping y_hat=4430\n","Skipping y_hat=4430\n","Skipping y_hat=4430\n","Skipping y_hat=4430\n","Skipping y_hat=4430\n","Skipping y_hat=2811\n","Skipping y_hat=2811\n","Skipping y_hat=2811\n","Skipping y_hat=2811\n","Skipping y_hat=2811\n","Skipping y_hat=2811\n","Skipping y_hat=2811\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=4260\n","Skipping y_hat=4260\n","Skipping y_hat=4260\n","Skipping y_hat=4260\n","Skipping y_hat=4260\n","Skipping y_hat=4260\n","Skipping y_hat=4260\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=5831\n","Skipping y_hat=5831\n","Skipping y_hat=5831\n","Skipping y_hat=5831\n","Skipping y_hat=5831\n","Skipping y_hat=5831\n","Skipping y_hat=5831\n","Skipping y_hat=4840\n","Skipping y_hat=4840\n","Skipping y_hat=4840\n","Skipping y_hat=4840\n","Skipping y_hat=4840\n","Skipping y_hat=4840\n","Skipping y_hat=4840\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=2050\n","Skipping y_hat=2050\n","Skipping y_hat=2050\n","Skipping y_hat=2050\n","Skipping y_hat=2050\n","Skipping y_hat=2050\n","Skipping y_hat=2050\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=4460\n","Skipping y_hat=4460\n","Skipping y_hat=4460\n","Skipping y_hat=4460\n","Skipping y_hat=4460\n","Skipping y_hat=4460\n","Skipping y_hat=4460\n","Skipping y_hat=5160\n","Skipping y_hat=5160\n","Skipping y_hat=5160\n","Skipping y_hat=5160\n","Skipping y_hat=5160\n","Skipping y_hat=5160\n","Skipping y_hat=5160\n","Skipping y_hat=8640\n","Skipping y_hat=8640\n","Skipping y_hat=8640\n","Skipping y_hat=8640\n","Skipping y_hat=8640\n","Skipping y_hat=8640\n","Skipping y_hat=8640\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=7501\n","Skipping y_hat=7501\n","Skipping y_hat=7501\n","Skipping y_hat=7501\n","Skipping y_hat=7501\n","Skipping y_hat=7501\n","Skipping y_hat=7501\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=9400\n","Skipping y_hat=9400\n","Skipping y_hat=9400\n","Skipping y_hat=9400\n","Skipping y_hat=9400\n","Skipping y_hat=9400\n","Skipping y_hat=9400\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=2780\n","Skipping y_hat=9270\n","Skipping y_hat=9270\n","Skipping y_hat=9270\n","Skipping y_hat=9270\n","Skipping y_hat=9270\n","Skipping y_hat=9270\n","Skipping y_hat=9270\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=4081\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=9341\n","Skipping y_hat=9341\n","Skipping y_hat=9341\n","Skipping y_hat=9341\n","Skipping y_hat=9341\n","Skipping y_hat=9341\n","Skipping y_hat=9341\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=2460\n","Skipping y_hat=2460\n","Skipping y_hat=2460\n","Skipping y_hat=2460\n","Skipping y_hat=2460\n","Skipping y_hat=2460\n","Skipping y_hat=2460\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=0121\n","Skipping y_hat=0121\n","Skipping y_hat=0121\n","Skipping y_hat=0121\n","Skipping y_hat=0121\n","Skipping y_hat=0121\n","Skipping y_hat=0121\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=1590\n","Skipping y_hat=1590\n","Skipping y_hat=1590\n","Skipping y_hat=1590\n","Skipping y_hat=1590\n","Skipping y_hat=1590\n","Skipping y_hat=1590\n","Skipping y_hat=0081\n","Skipping y_hat=0081\n","Skipping y_hat=0081\n","Skipping y_hat=0081\n","Skipping y_hat=0081\n","Skipping y_hat=0081\n","Skipping y_hat=0081\n","Skipping y_hat=6670\n","Skipping y_hat=6670\n","Skipping y_hat=6670\n","Skipping y_hat=6670\n","Skipping y_hat=6670\n","Skipping y_hat=6670\n","Skipping y_hat=6670\n","Skipping y_hat=9880\n","Skipping y_hat=9880\n","Skipping y_hat=9880\n","Skipping y_hat=9880\n","Skipping y_hat=9880\n","Skipping y_hat=9880\n","Skipping y_hat=9880\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=4041\n","Skipping y_hat=4041\n","Skipping y_hat=4041\n","Skipping y_hat=4041\n","Skipping y_hat=4041\n","Skipping y_hat=4041\n","Skipping y_hat=4041\n","Skipping y_hat=9930\n","Skipping y_hat=9930\n","Skipping y_hat=9930\n","Skipping y_hat=9930\n","Skipping y_hat=9930\n","Skipping y_hat=9930\n","Skipping y_hat=9930\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=2690\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=6941\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=9611\n","Skipping y_hat=9611\n","Skipping y_hat=9611\n","Skipping y_hat=9611\n","Skipping y_hat=9611\n","Skipping y_hat=9611\n","Skipping y_hat=9611\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=7190\n","Skipping y_hat=7190\n","Skipping y_hat=7190\n","Skipping y_hat=7190\n","Skipping y_hat=7190\n","Skipping y_hat=7190\n","Skipping y_hat=7190\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n","Skipping y_hat=9021\n","Skipping y_hat=8741\n","Skipping y_hat=8741\n","Skipping y_hat=8741\n","Skipping y_hat=8741\n","Skipping y_hat=8741\n","Skipping y_hat=8741\n","Skipping y_hat=8741\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=3571\n","Skipping y_hat=6031\n","Skipping y_hat=6031\n","Skipping y_hat=6031\n","Skipping y_hat=6031\n","Skipping y_hat=6031\n","Skipping y_hat=6031\n","Skipping y_hat=6031\n","Skipping y_hat=6880\n","Skipping y_hat=6880\n","Skipping y_hat=6880\n","Skipping y_hat=6880\n","Skipping y_hat=6880\n","Skipping y_hat=6880\n","Skipping y_hat=6880\n","Skipping y_hat=0211\n","Skipping y_hat=0211\n","Skipping y_hat=0211\n","Skipping y_hat=0211\n","Skipping y_hat=0211\n","Skipping y_hat=0211\n","Skipping y_hat=0211\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=3950\n","Skipping y_hat=0210\n","Skipping y_hat=0210\n","Skipping y_hat=0210\n","Skipping y_hat=0210\n","Skipping y_hat=0210\n","Skipping y_hat=0210\n","Skipping y_hat=0210\n","Skipping y_hat=6540\n","Skipping y_hat=6540\n","Skipping y_hat=6540\n","Skipping y_hat=6540\n","Skipping y_hat=6540\n","Skipping y_hat=6540\n","Skipping y_hat=6540\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=8431\n","Skipping y_hat=8431\n","Skipping y_hat=8431\n","Skipping y_hat=8431\n","Skipping y_hat=8431\n","Skipping y_hat=8431\n","Skipping y_hat=8431\n","Skipping y_hat=0380\n","Skipping y_hat=0380\n","Skipping y_hat=0380\n","Skipping y_hat=0380\n","Skipping y_hat=0380\n","Skipping y_hat=0380\n","Skipping y_hat=0380\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=6611\n","Skipping y_hat=6611\n","Skipping y_hat=6611\n","Skipping y_hat=6611\n","Skipping y_hat=6611\n","Skipping y_hat=6611\n","Skipping y_hat=6611\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=0941\n","Skipping y_hat=0941\n","Skipping y_hat=0941\n","Skipping y_hat=0941\n","Skipping y_hat=0941\n","Skipping y_hat=0941\n","Skipping y_hat=0941\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=9881\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=3601\n","Skipping y_hat=3601\n","Skipping y_hat=3601\n","Skipping y_hat=3601\n","Skipping y_hat=3601\n","Skipping y_hat=3601\n","Skipping y_hat=3601\n","Skipping y_hat=6821\n","Skipping y_hat=6821\n","Skipping y_hat=6821\n","Skipping y_hat=6821\n","Skipping y_hat=6821\n","Skipping y_hat=6821\n","Skipping y_hat=6821\n","Skipping y_hat=7601\n","Skipping y_hat=7601\n","Skipping y_hat=7601\n","Skipping y_hat=7601\n","Skipping y_hat=7601\n","Skipping y_hat=7601\n","Skipping y_hat=7601\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=5071\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=0950\n","Skipping y_hat=0950\n","Skipping y_hat=0950\n","Skipping y_hat=0950\n","Skipping y_hat=0950\n","Skipping y_hat=0950\n","Skipping y_hat=0950\n","Skipping y_hat=0621\n","Skipping y_hat=0621\n","Skipping y_hat=0621\n","Skipping y_hat=0621\n","Skipping y_hat=0621\n","Skipping y_hat=0621\n","Skipping y_hat=0621\n","Skipping y_hat=5190\n","Skipping y_hat=5190\n","Skipping y_hat=5190\n","Skipping y_hat=5190\n","Skipping y_hat=5190\n","Skipping y_hat=5190\n","Skipping y_hat=5190\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=2911\n","Skipping y_hat=5171\n","Skipping y_hat=5171\n","Skipping y_hat=5171\n","Skipping y_hat=5171\n","Skipping y_hat=5171\n","Skipping y_hat=5171\n","Skipping y_hat=5171\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=0671\n","Skipping y_hat=7780\n","Skipping y_hat=7780\n","Skipping y_hat=7780\n","Skipping y_hat=7780\n","Skipping y_hat=7780\n","Skipping y_hat=7780\n","Skipping y_hat=7780\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=1490\n","Skipping y_hat=4011\n","Skipping y_hat=4011\n","Skipping y_hat=4011\n","Skipping y_hat=4011\n","Skipping y_hat=4011\n","Skipping y_hat=4011\n","Skipping y_hat=4011\n","Skipping y_hat=4841\n","Skipping y_hat=4841\n","Skipping y_hat=4841\n","Skipping y_hat=4841\n","Skipping y_hat=4841\n","Skipping y_hat=4841\n","Skipping y_hat=4841\n","Skipping y_hat=6711\n","Skipping y_hat=6711\n","Skipping y_hat=6711\n","Skipping y_hat=6711\n","Skipping y_hat=6711\n","Skipping y_hat=6711\n","Skipping y_hat=6711\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=7710\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=5040\n","Skipping y_hat=5040\n","Skipping y_hat=5040\n","Skipping y_hat=5040\n","Skipping y_hat=5040\n","Skipping y_hat=5040\n","Skipping y_hat=5040\n"," 79% 19/24 [00:02<00:00,  7.01it/s]Skipping y_hat=0361\n","Skipping y_hat=0361\n","Skipping y_hat=0361\n","Skipping y_hat=0361\n","Skipping y_hat=0361\n","Skipping y_hat=0361\n","Skipping y_hat=0361\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=2831\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=1950\n","Skipping y_hat=8760\n","Skipping y_hat=8760\n","Skipping y_hat=8760\n","Skipping y_hat=8760\n","Skipping y_hat=8760\n","Skipping y_hat=8760\n","Skipping y_hat=8760\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=9650\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=2290\n","Skipping y_hat=8021\n","Skipping y_hat=8021\n","Skipping y_hat=8021\n","Skipping y_hat=8021\n","Skipping y_hat=8021\n","Skipping y_hat=8021\n","Skipping y_hat=8021\n","Skipping y_hat=4160\n","Skipping y_hat=4160\n","Skipping y_hat=4160\n","Skipping y_hat=4160\n","Skipping y_hat=4160\n","Skipping y_hat=4160\n","Skipping y_hat=4160\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=9751\n","Skipping y_hat=2650\n","Skipping y_hat=2650\n","Skipping y_hat=2650\n","Skipping y_hat=2650\n","Skipping y_hat=2650\n","Skipping y_hat=2650\n","Skipping y_hat=2650\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=3071\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=4960\n","Skipping y_hat=1701\n","Skipping y_hat=1701\n","Skipping y_hat=1701\n","Skipping y_hat=1701\n","Skipping y_hat=1701\n","Skipping y_hat=1701\n","Skipping y_hat=1701\n","Skipping y_hat=8770\n","Skipping y_hat=8770\n","Skipping y_hat=8770\n","Skipping y_hat=8770\n","Skipping y_hat=8770\n","Skipping y_hat=8770\n","Skipping y_hat=8770\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=8921\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=6070\n","Skipping y_hat=6070\n","Skipping y_hat=6070\n","Skipping y_hat=6070\n","Skipping y_hat=6070\n","Skipping y_hat=6070\n","Skipping y_hat=6070\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6830\n","Skipping y_hat=6830\n","Skipping y_hat=6830\n","Skipping y_hat=6830\n","Skipping y_hat=6830\n","Skipping y_hat=6830\n","Skipping y_hat=6830\n","Skipping y_hat=9120\n","Skipping y_hat=9120\n","Skipping y_hat=9120\n","Skipping y_hat=9120\n","Skipping y_hat=9120\n","Skipping y_hat=9120\n","Skipping y_hat=9120\n","Skipping y_hat=8350\n","Skipping y_hat=8350\n","Skipping y_hat=8350\n","Skipping y_hat=8350\n","Skipping y_hat=8350\n","Skipping y_hat=8350\n","Skipping y_hat=8350\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=6180\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=6380\n","Skipping y_hat=3131\n","Skipping y_hat=3131\n","Skipping y_hat=3131\n","Skipping y_hat=3131\n","Skipping y_hat=3131\n","Skipping y_hat=3131\n","Skipping y_hat=3131\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=4551\n","Skipping y_hat=4551\n","Skipping y_hat=4551\n","Skipping y_hat=4551\n","Skipping y_hat=4551\n","Skipping y_hat=4551\n","Skipping y_hat=4551\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=8521\n","Skipping y_hat=5730\n","Skipping y_hat=5730\n","Skipping y_hat=5730\n","Skipping y_hat=5730\n","Skipping y_hat=5730\n","Skipping y_hat=5730\n","Skipping y_hat=5730\n","Skipping y_hat=8910\n","Skipping y_hat=8910\n","Skipping y_hat=8910\n","Skipping y_hat=8910\n","Skipping y_hat=8910\n","Skipping y_hat=8910\n","Skipping y_hat=8910\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=3901\n","Skipping y_hat=3901\n","Skipping y_hat=3901\n","Skipping y_hat=3901\n","Skipping y_hat=3901\n","Skipping y_hat=3901\n","Skipping y_hat=3901\n","Skipping y_hat=5331\n","Skipping y_hat=5331\n","Skipping y_hat=5331\n","Skipping y_hat=5331\n","Skipping y_hat=5331\n","Skipping y_hat=5331\n","Skipping y_hat=5331\n","Skipping y_hat=5821\n","Skipping y_hat=5821\n","Skipping y_hat=5821\n","Skipping y_hat=5821\n","Skipping y_hat=5821\n","Skipping y_hat=5821\n","Skipping y_hat=5821\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=7890\n","Skipping y_hat=7890\n","Skipping y_hat=7890\n","Skipping y_hat=7890\n","Skipping y_hat=7890\n","Skipping y_hat=7890\n","Skipping y_hat=7890\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=9831\n","Skipping y_hat=9831\n","Skipping y_hat=9831\n","Skipping y_hat=9831\n","Skipping y_hat=9831\n","Skipping y_hat=9831\n","Skipping y_hat=9831\n","Skipping y_hat=3460\n","Skipping y_hat=3460\n","Skipping y_hat=3460\n","Skipping y_hat=3460\n","Skipping y_hat=3460\n","Skipping y_hat=3460\n","Skipping y_hat=3460\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=5301\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=5890\n","Skipping y_hat=7230\n","Skipping y_hat=7230\n","Skipping y_hat=7230\n","Skipping y_hat=7230\n","Skipping y_hat=7230\n","Skipping y_hat=7230\n","Skipping y_hat=7230\n","Skipping y_hat=3580\n","Skipping y_hat=3580\n","Skipping y_hat=3580\n","Skipping y_hat=3580\n","Skipping y_hat=3580\n","Skipping y_hat=3580\n","Skipping y_hat=3580\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=6930\n","Skipping y_hat=6930\n","Skipping y_hat=6930\n","Skipping y_hat=6930\n","Skipping y_hat=6930\n","Skipping y_hat=6930\n","Skipping y_hat=6930\n","Skipping y_hat=0960\n","Skipping y_hat=0960\n","Skipping y_hat=0960\n","Skipping y_hat=0960\n","Skipping y_hat=0960\n","Skipping y_hat=0960\n","Skipping y_hat=0960\n","Skipping y_hat=9230\n","Skipping y_hat=9230\n","Skipping y_hat=9230\n","Skipping y_hat=9230\n","Skipping y_hat=9230\n","Skipping y_hat=9230\n","Skipping y_hat=9230\n","Skipping y_hat=5030\n","Skipping y_hat=5030\n","Skipping y_hat=5030\n","Skipping y_hat=5030\n","Skipping y_hat=5030\n","Skipping y_hat=5030\n","Skipping y_hat=5030\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=1600\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=2860\n","Skipping y_hat=2860\n","Skipping y_hat=2860\n","Skipping y_hat=2860\n","Skipping y_hat=2860\n","Skipping y_hat=2860\n","Skipping y_hat=2860\n","Skipping y_hat=8660\n","Skipping y_hat=8660\n","Skipping y_hat=8660\n","Skipping y_hat=8660\n","Skipping y_hat=8660\n","Skipping y_hat=8660\n","Skipping y_hat=8660\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=5480\n","Skipping y_hat=5480\n","Skipping y_hat=5480\n","Skipping y_hat=5480\n","Skipping y_hat=5480\n","Skipping y_hat=5480\n","Skipping y_hat=5480\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=7381\n","Skipping y_hat=9740\n","Skipping y_hat=9740\n","Skipping y_hat=9740\n","Skipping y_hat=9740\n","Skipping y_hat=9740\n","Skipping y_hat=9740\n","Skipping y_hat=9740\n","Skipping y_hat=1660\n","Skipping y_hat=1660\n","Skipping y_hat=1660\n","Skipping y_hat=1660\n","Skipping y_hat=1660\n","Skipping y_hat=1660\n","Skipping y_hat=1660\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=9131\n","Skipping y_hat=9131\n","Skipping y_hat=9131\n","Skipping y_hat=9131\n","Skipping y_hat=9131\n","Skipping y_hat=9131\n","Skipping y_hat=9131\n","Skipping y_hat=1501\n","Skipping y_hat=1501\n","Skipping y_hat=1501\n","Skipping y_hat=1501\n","Skipping y_hat=1501\n","Skipping y_hat=1501\n","Skipping y_hat=1501\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=5670\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=9190\n","Skipping y_hat=9190\n","Skipping y_hat=9190\n","Skipping y_hat=9190\n","Skipping y_hat=9190\n","Skipping y_hat=9190\n","Skipping y_hat=9190\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=7350\n","Skipping y_hat=7350\n","Skipping y_hat=7350\n","Skipping y_hat=7350\n","Skipping y_hat=7350\n","Skipping y_hat=7350\n","Skipping y_hat=7350\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=1570\n","Skipping y_hat=1570\n","Skipping y_hat=1570\n","Skipping y_hat=1570\n","Skipping y_hat=1570\n","Skipping y_hat=1570\n","Skipping y_hat=1570\n","Skipping y_hat=3610\n","Skipping y_hat=3610\n","Skipping y_hat=3610\n","Skipping y_hat=3610\n","Skipping y_hat=3610\n","Skipping y_hat=3610\n","Skipping y_hat=3610\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=6160\n","Skipping y_hat=6160\n","Skipping y_hat=6160\n","Skipping y_hat=6160\n","Skipping y_hat=6160\n","Skipping y_hat=6160\n","Skipping y_hat=6160\n","Skipping y_hat=5560\n","Skipping y_hat=5560\n","Skipping y_hat=5560\n","Skipping y_hat=5560\n","Skipping y_hat=5560\n","Skipping y_hat=5560\n","Skipping y_hat=5560\n","Skipping y_hat=6570\n","Skipping y_hat=6570\n","Skipping y_hat=6570\n","Skipping y_hat=6570\n","Skipping y_hat=6570\n","Skipping y_hat=6570\n","Skipping y_hat=6570\n","Skipping y_hat=4270\n","Skipping y_hat=4270\n","Skipping y_hat=4270\n","Skipping y_hat=4270\n","Skipping y_hat=4270\n","Skipping y_hat=4270\n","Skipping y_hat=4270\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=0190\n","Skipping y_hat=2301\n","Skipping y_hat=2301\n","Skipping y_hat=2301\n","Skipping y_hat=2301\n","Skipping y_hat=2301\n","Skipping y_hat=2301\n","Skipping y_hat=2301\n","Skipping y_hat=4590\n","Skipping y_hat=4590\n","Skipping y_hat=4590\n","Skipping y_hat=4590\n","Skipping y_hat=4590\n","Skipping y_hat=4590\n","Skipping y_hat=4590\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=2131\n","Skipping y_hat=6940\n","Skipping y_hat=6940\n","Skipping y_hat=6940\n","Skipping y_hat=6940\n","Skipping y_hat=6940\n","Skipping y_hat=6940\n","Skipping y_hat=6940\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=6060\n","Skipping y_hat=6060\n","Skipping y_hat=6060\n","Skipping y_hat=6060\n","Skipping y_hat=6060\n","Skipping y_hat=6060\n","Skipping y_hat=6060\n","Skipping y_hat=8490\n","Skipping y_hat=8490\n","Skipping y_hat=8490\n","Skipping y_hat=8490\n","Skipping y_hat=8490\n","Skipping y_hat=8490\n","Skipping y_hat=8490\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=7960\n","Skipping y_hat=0741\n","Skipping y_hat=0741\n","Skipping y_hat=0741\n","Skipping y_hat=0741\n","Skipping y_hat=0741\n","Skipping y_hat=0741\n","Skipping y_hat=0741\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=6071\n","Skipping y_hat=8701\n","Skipping y_hat=8701\n","Skipping y_hat=8701\n","Skipping y_hat=8701\n","Skipping y_hat=8701\n","Skipping y_hat=8701\n","Skipping y_hat=8701\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=9760\n","Skipping y_hat=3551\n","Skipping y_hat=3551\n","Skipping y_hat=3551\n","Skipping y_hat=3551\n","Skipping y_hat=3551\n","Skipping y_hat=3551\n","Skipping y_hat=3551\n","Skipping y_hat=3701\n","Skipping y_hat=3701\n","Skipping y_hat=3701\n","Skipping y_hat=3701\n","Skipping y_hat=3701\n","Skipping y_hat=3701\n","Skipping y_hat=3701\n","Skipping y_hat=4401\n","Skipping y_hat=4401\n","Skipping y_hat=4401\n","Skipping y_hat=4401\n","Skipping y_hat=4401\n","Skipping y_hat=4401\n","Skipping y_hat=4401\n","Skipping y_hat=9901\n","Skipping y_hat=9901\n","Skipping y_hat=9901\n","Skipping y_hat=9901\n","Skipping y_hat=9901\n","Skipping y_hat=9901\n","Skipping y_hat=9901\n","Skipping y_hat=0961\n","Skipping y_hat=0961\n","Skipping y_hat=0961\n","Skipping y_hat=0961\n","Skipping y_hat=0961\n","Skipping y_hat=0961\n","Skipping y_hat=0961\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=7641\n","Skipping y_hat=9491\n","Skipping y_hat=9491\n","Skipping y_hat=9491\n","Skipping y_hat=9491\n","Skipping y_hat=9491\n","Skipping y_hat=9491\n","Skipping y_hat=9491\n","Skipping y_hat=9631\n","Skipping y_hat=9631\n","Skipping y_hat=9631\n","Skipping y_hat=9631\n","Skipping y_hat=9631\n","Skipping y_hat=9631\n","Skipping y_hat=9631\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9501\n","Skipping y_hat=9321\n","Skipping y_hat=9321\n","Skipping y_hat=9321\n","Skipping y_hat=9321\n","Skipping y_hat=9321\n","Skipping y_hat=9321\n","Skipping y_hat=9321\n","Skipping y_hat=3760\n","Skipping y_hat=3760\n","Skipping y_hat=3760\n","Skipping y_hat=3760\n","Skipping y_hat=3760\n","Skipping y_hat=3760\n","Skipping y_hat=3760\n","Skipping y_hat=5070\n","Skipping y_hat=5070\n","Skipping y_hat=5070\n","Skipping y_hat=5070\n","Skipping y_hat=5070\n","Skipping y_hat=5070\n","Skipping y_hat=5070\n","Skipping y_hat=6990\n","Skipping y_hat=6990\n","Skipping y_hat=6990\n","Skipping y_hat=6990\n","Skipping y_hat=6990\n","Skipping y_hat=6990\n","Skipping y_hat=6990\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=9490\n","Skipping y_hat=0531\n","Skipping y_hat=0531\n","Skipping y_hat=0531\n","Skipping y_hat=0531\n","Skipping y_hat=0531\n","Skipping y_hat=0531\n","Skipping y_hat=0531\n","Skipping y_hat=7090\n","Skipping y_hat=7090\n","Skipping y_hat=7090\n","Skipping y_hat=7090\n","Skipping y_hat=7090\n","Skipping y_hat=7090\n","Skipping y_hat=7090\n","Skipping y_hat=7431\n","Skipping y_hat=7431\n","Skipping y_hat=7431\n","Skipping y_hat=7431\n","Skipping y_hat=7431\n","Skipping y_hat=7431\n","Skipping y_hat=7431\n","Skipping y_hat=0260\n","Skipping y_hat=0260\n","Skipping y_hat=0260\n","Skipping y_hat=0260\n","Skipping y_hat=0260\n","Skipping y_hat=0260\n","Skipping y_hat=0260\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=4090\n","Skipping y_hat=6871\n","Skipping y_hat=6871\n","Skipping y_hat=6871\n","Skipping y_hat=6871\n","Skipping y_hat=6871\n","Skipping y_hat=6871\n","Skipping y_hat=6871\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=4990\n","Skipping y_hat=4990\n","Skipping y_hat=4990\n","Skipping y_hat=4990\n","Skipping y_hat=4990\n","Skipping y_hat=4990\n","Skipping y_hat=4990\n"," 83% 20/24 [00:02<00:00,  7.36it/s]Skipping y_hat=0681\n","Skipping y_hat=0681\n","Skipping y_hat=0681\n","Skipping y_hat=0681\n","Skipping y_hat=0681\n","Skipping y_hat=0681\n","Skipping y_hat=0681\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=5231\n","Skipping y_hat=0150\n","Skipping y_hat=0150\n","Skipping y_hat=0150\n","Skipping y_hat=0150\n","Skipping y_hat=0150\n","Skipping y_hat=0150\n","Skipping y_hat=0150\n","Skipping y_hat=7160\n","Skipping y_hat=7160\n","Skipping y_hat=7160\n","Skipping y_hat=7160\n","Skipping y_hat=7160\n","Skipping y_hat=7160\n","Skipping y_hat=7160\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=5850\n","Skipping y_hat=5850\n","Skipping y_hat=5850\n","Skipping y_hat=5850\n","Skipping y_hat=5850\n","Skipping y_hat=5850\n","Skipping y_hat=5850\n","Skipping y_hat=5961\n","Skipping y_hat=5961\n","Skipping y_hat=5961\n","Skipping y_hat=5961\n","Skipping y_hat=5961\n","Skipping y_hat=5961\n","Skipping y_hat=5961\n","Skipping y_hat=0601\n","Skipping y_hat=0601\n","Skipping y_hat=0601\n","Skipping y_hat=0601\n","Skipping y_hat=0601\n","Skipping y_hat=0601\n","Skipping y_hat=0601\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=2680\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=3490\n","Skipping y_hat=3490\n","Skipping y_hat=3490\n","Skipping y_hat=3490\n","Skipping y_hat=3490\n","Skipping y_hat=3490\n","Skipping y_hat=3490\n","Skipping y_hat=9031\n","Skipping y_hat=9031\n","Skipping y_hat=9031\n","Skipping y_hat=9031\n","Skipping y_hat=9031\n","Skipping y_hat=9031\n","Skipping y_hat=9031\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=5191\n","Skipping y_hat=6460\n","Skipping y_hat=6460\n","Skipping y_hat=6460\n","Skipping y_hat=6460\n","Skipping y_hat=6460\n","Skipping y_hat=6460\n","Skipping y_hat=6460\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=9301\n","Skipping y_hat=9301\n","Skipping y_hat=9301\n","Skipping y_hat=9301\n","Skipping y_hat=9301\n","Skipping y_hat=9301\n","Skipping y_hat=9301\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=7341\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=5950\n","Skipping y_hat=5950\n","Skipping y_hat=5950\n","Skipping y_hat=5950\n","Skipping y_hat=5950\n","Skipping y_hat=5950\n","Skipping y_hat=5950\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=4711\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=7380\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=5551\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=8621\n","Skipping y_hat=2330\n","Skipping y_hat=2330\n","Skipping y_hat=2330\n","Skipping y_hat=2330\n","Skipping y_hat=2330\n","Skipping y_hat=2330\n","Skipping y_hat=2330\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=3001\n","Skipping y_hat=0980\n","Skipping y_hat=0980\n","Skipping y_hat=0980\n","Skipping y_hat=0980\n","Skipping y_hat=0980\n","Skipping y_hat=0980\n","Skipping y_hat=0980\n","Skipping y_hat=3801\n","Skipping y_hat=3801\n","Skipping y_hat=3801\n","Skipping y_hat=3801\n","Skipping y_hat=3801\n","Skipping y_hat=3801\n","Skipping y_hat=3801\n","Skipping y_hat=4221\n","Skipping y_hat=4221\n","Skipping y_hat=4221\n","Skipping y_hat=4221\n","Skipping y_hat=4221\n","Skipping y_hat=4221\n","Skipping y_hat=4221\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=0141\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=3380\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=6690\n","Skipping y_hat=6690\n","Skipping y_hat=6690\n","Skipping y_hat=6690\n","Skipping y_hat=6690\n","Skipping y_hat=6690\n","Skipping y_hat=6690\n","Skipping y_hat=7851\n","Skipping y_hat=7851\n","Skipping y_hat=7851\n","Skipping y_hat=7851\n","Skipping y_hat=7851\n","Skipping y_hat=7851\n","Skipping y_hat=7851\n","Skipping y_hat=8011\n","Skipping y_hat=8011\n","Skipping y_hat=8011\n","Skipping y_hat=8011\n","Skipping y_hat=8011\n","Skipping y_hat=8011\n","Skipping y_hat=8011\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=3360\n","Skipping y_hat=3360\n","Skipping y_hat=3360\n","Skipping y_hat=3360\n","Skipping y_hat=3360\n","Skipping y_hat=3360\n","Skipping y_hat=3360\n","Skipping y_hat=0310\n","Skipping y_hat=0310\n","Skipping y_hat=0310\n","Skipping y_hat=0310\n","Skipping y_hat=0310\n","Skipping y_hat=0310\n","Skipping y_hat=0310\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=7021\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=8601\n","Skipping y_hat=8601\n","Skipping y_hat=8601\n","Skipping y_hat=8601\n","Skipping y_hat=8601\n","Skipping y_hat=8601\n","Skipping y_hat=8601\n","Skipping y_hat=4111\n","Skipping y_hat=4111\n","Skipping y_hat=4111\n","Skipping y_hat=4111\n","Skipping y_hat=4111\n","Skipping y_hat=4111\n","Skipping y_hat=4111\n","Skipping y_hat=0401\n","Skipping y_hat=0401\n","Skipping y_hat=0401\n","Skipping y_hat=0401\n","Skipping y_hat=0401\n","Skipping y_hat=0401\n","Skipping y_hat=0401\n","Skipping y_hat=3070\n","Skipping y_hat=3070\n","Skipping y_hat=3070\n","Skipping y_hat=3070\n","Skipping y_hat=3070\n","Skipping y_hat=3070\n","Skipping y_hat=3070\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=0940\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=9960\n","Skipping y_hat=9171\n","Skipping y_hat=9171\n","Skipping y_hat=9171\n","Skipping y_hat=9171\n","Skipping y_hat=9171\n","Skipping y_hat=9171\n","Skipping y_hat=9171\n","Skipping y_hat=8850\n","Skipping y_hat=8850\n","Skipping y_hat=8850\n","Skipping y_hat=8850\n","Skipping y_hat=8850\n","Skipping y_hat=8850\n","Skipping y_hat=8850\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=7490\n","Skipping y_hat=6721\n","Skipping y_hat=6721\n","Skipping y_hat=6721\n","Skipping y_hat=6721\n","Skipping y_hat=6721\n","Skipping y_hat=6721\n","Skipping y_hat=6721\n","Skipping y_hat=9850\n","Skipping y_hat=9850\n","Skipping y_hat=9850\n","Skipping y_hat=9850\n","Skipping y_hat=9850\n","Skipping y_hat=9850\n","Skipping y_hat=9850\n","Skipping y_hat=9360\n","Skipping y_hat=9360\n","Skipping y_hat=9360\n","Skipping y_hat=9360\n","Skipping y_hat=9360\n","Skipping y_hat=9360\n","Skipping y_hat=9360\n","Skipping y_hat=3121\n","Skipping y_hat=3121\n","Skipping y_hat=3121\n","Skipping y_hat=3121\n","Skipping y_hat=3121\n","Skipping y_hat=3121\n","Skipping y_hat=3121\n","Skipping y_hat=8460\n","Skipping y_hat=8460\n","Skipping y_hat=8460\n","Skipping y_hat=8460\n","Skipping y_hat=8460\n","Skipping y_hat=8460\n","Skipping y_hat=8460\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=1141\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=7730\n","Skipping y_hat=7730\n","Skipping y_hat=7730\n","Skipping y_hat=7730\n","Skipping y_hat=7730\n","Skipping y_hat=7730\n","Skipping y_hat=7730\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=1921\n","Skipping y_hat=5460\n","Skipping y_hat=5460\n","Skipping y_hat=5460\n","Skipping y_hat=5460\n","Skipping y_hat=5460\n","Skipping y_hat=5460\n","Skipping y_hat=5460\n","Skipping y_hat=0550\n","Skipping y_hat=0550\n","Skipping y_hat=0550\n","Skipping y_hat=0550\n","Skipping y_hat=0550\n","Skipping y_hat=0550\n","Skipping y_hat=0550\n","Skipping y_hat=5221\n","Skipping y_hat=5221\n","Skipping y_hat=5221\n","Skipping y_hat=5221\n","Skipping y_hat=5221\n","Skipping y_hat=5221\n","Skipping y_hat=5221\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=3370\n","Skipping y_hat=3370\n","Skipping y_hat=3370\n","Skipping y_hat=3370\n","Skipping y_hat=3370\n","Skipping y_hat=3370\n","Skipping y_hat=3370\n","Skipping y_hat=0231\n","Skipping y_hat=0231\n","Skipping y_hat=0231\n","Skipping y_hat=0231\n","Skipping y_hat=0231\n","Skipping y_hat=0231\n","Skipping y_hat=0231\n","Skipping y_hat=5141\n","Skipping y_hat=5141\n","Skipping y_hat=5141\n","Skipping y_hat=5141\n","Skipping y_hat=5141\n","Skipping y_hat=5141\n","Skipping y_hat=5141\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=6001\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=3120\n","Skipping y_hat=3120\n","Skipping y_hat=3120\n","Skipping y_hat=3120\n","Skipping y_hat=3120\n","Skipping y_hat=3120\n","Skipping y_hat=3120\n","Skipping y_hat=6541\n","Skipping y_hat=6541\n","Skipping y_hat=6541\n","Skipping y_hat=6541\n","Skipping y_hat=6541\n","Skipping y_hat=6541\n","Skipping y_hat=6541\n","Skipping y_hat=0911\n","Skipping y_hat=0911\n","Skipping y_hat=0911\n","Skipping y_hat=0911\n","Skipping y_hat=0911\n","Skipping y_hat=0911\n","Skipping y_hat=0911\n","Skipping y_hat=6810\n","Skipping y_hat=6810\n","Skipping y_hat=6810\n","Skipping y_hat=6810\n","Skipping y_hat=6810\n","Skipping y_hat=6810\n","Skipping y_hat=6810\n","Skipping y_hat=7470\n","Skipping y_hat=7470\n","Skipping y_hat=7470\n","Skipping y_hat=7470\n","Skipping y_hat=7470\n","Skipping y_hat=7470\n","Skipping y_hat=7470\n","Skipping y_hat=5580\n","Skipping y_hat=5580\n","Skipping y_hat=5580\n","Skipping y_hat=5580\n","Skipping y_hat=5580\n","Skipping y_hat=5580\n","Skipping y_hat=5580\n","Skipping y_hat=4080\n","Skipping y_hat=4080\n","Skipping y_hat=4080\n","Skipping y_hat=4080\n","Skipping y_hat=4080\n","Skipping y_hat=4080\n","Skipping y_hat=4080\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=8301\n","Skipping y_hat=4060\n","Skipping y_hat=4060\n","Skipping y_hat=4060\n","Skipping y_hat=4060\n","Skipping y_hat=4060\n","Skipping y_hat=4060\n","Skipping y_hat=4060\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=1411\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=0001\n","Skipping y_hat=1130\n","Skipping y_hat=1130\n","Skipping y_hat=1130\n","Skipping y_hat=1130\n","Skipping y_hat=1130\n","Skipping y_hat=1130\n","Skipping y_hat=1130\n","Skipping y_hat=2870\n","Skipping y_hat=2870\n","Skipping y_hat=2870\n","Skipping y_hat=2870\n","Skipping y_hat=2870\n","Skipping y_hat=2870\n","Skipping y_hat=2870\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=1870\n","Skipping y_hat=1870\n","Skipping y_hat=1870\n","Skipping y_hat=1870\n","Skipping y_hat=1870\n","Skipping y_hat=1870\n","Skipping y_hat=1870\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=6951\n","Skipping y_hat=2901\n","Skipping y_hat=2901\n","Skipping y_hat=2901\n","Skipping y_hat=2901\n","Skipping y_hat=2901\n","Skipping y_hat=2901\n","Skipping y_hat=2901\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=0620\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=9810\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=8290\n","Skipping y_hat=5320\n","Skipping y_hat=5320\n","Skipping y_hat=5320\n","Skipping y_hat=5320\n","Skipping y_hat=5320\n","Skipping y_hat=5320\n","Skipping y_hat=5320\n","Skipping y_hat=7871\n","Skipping y_hat=7871\n","Skipping y_hat=7871\n","Skipping y_hat=7871\n","Skipping y_hat=7871\n","Skipping y_hat=7871\n","Skipping y_hat=7871\n","Skipping y_hat=9770\n","Skipping y_hat=9770\n","Skipping y_hat=9770\n","Skipping y_hat=9770\n","Skipping y_hat=9770\n","Skipping y_hat=9770\n","Skipping y_hat=9770\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=5431\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=1171\n","Skipping y_hat=5401\n","Skipping y_hat=5401\n","Skipping y_hat=5401\n","Skipping y_hat=5401\n","Skipping y_hat=5401\n","Skipping y_hat=5401\n","Skipping y_hat=5401\n","Skipping y_hat=3931\n","Skipping y_hat=3931\n","Skipping y_hat=3931\n","Skipping y_hat=3931\n","Skipping y_hat=3931\n","Skipping y_hat=3931\n","Skipping y_hat=3931\n","Skipping y_hat=6901\n","Skipping y_hat=6901\n","Skipping y_hat=6901\n","Skipping y_hat=6901\n","Skipping y_hat=6901\n","Skipping y_hat=6901\n","Skipping y_hat=6901\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=5931\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=2280\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=1090\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=8611\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=1230\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=7101\n","Skipping y_hat=4620\n","Skipping y_hat=4620\n","Skipping y_hat=4620\n","Skipping y_hat=4620\n","Skipping y_hat=4620\n","Skipping y_hat=4620\n","Skipping y_hat=4620\n","Skipping y_hat=8880\n","Skipping y_hat=8880\n","Skipping y_hat=8880\n","Skipping y_hat=8880\n","Skipping y_hat=8880\n","Skipping y_hat=8880\n","Skipping y_hat=8880\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=5471\n","Skipping y_hat=0390\n","Skipping y_hat=0390\n","Skipping y_hat=0390\n","Skipping y_hat=0390\n","Skipping y_hat=0390\n","Skipping y_hat=0390\n","Skipping y_hat=0390\n","Skipping y_hat=6471\n","Skipping y_hat=6471\n","Skipping y_hat=6471\n","Skipping y_hat=6471\n","Skipping y_hat=6471\n","Skipping y_hat=6471\n","Skipping y_hat=6471\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=0571\n","Skipping y_hat=8201\n","Skipping y_hat=8201\n","Skipping y_hat=8201\n","Skipping y_hat=8201\n","Skipping y_hat=8201\n","Skipping y_hat=8201\n","Skipping y_hat=8201\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=9090\n","Skipping y_hat=9090\n","Skipping y_hat=9090\n","Skipping y_hat=9090\n","Skipping y_hat=9090\n","Skipping y_hat=9090\n","Skipping y_hat=9090\n","Skipping y_hat=9990\n","Skipping y_hat=9990\n","Skipping y_hat=9990\n","Skipping y_hat=9990\n","Skipping y_hat=9990\n","Skipping y_hat=9990\n","Skipping y_hat=9990\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n"," 88% 21/24 [00:02<00:00,  7.79it/s]Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=0641\n","Skipping y_hat=0641\n","Skipping y_hat=0641\n","Skipping y_hat=0641\n","Skipping y_hat=0641\n","Skipping y_hat=0641\n","Skipping y_hat=0641\n","Skipping y_hat=4321\n","Skipping y_hat=4321\n","Skipping y_hat=4321\n","Skipping y_hat=4321\n","Skipping y_hat=4321\n","Skipping y_hat=4321\n","Skipping y_hat=4321\n","Skipping y_hat=9390\n","Skipping y_hat=9390\n","Skipping y_hat=9390\n","Skipping y_hat=9390\n","Skipping y_hat=9390\n","Skipping y_hat=9390\n","Skipping y_hat=9390\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=4560\n","Skipping y_hat=7861\n","Skipping y_hat=7861\n","Skipping y_hat=7861\n","Skipping y_hat=7861\n","Skipping y_hat=7861\n","Skipping y_hat=7861\n","Skipping y_hat=7861\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2621\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=2590\n","Skipping y_hat=4201\n","Skipping y_hat=4201\n","Skipping y_hat=4201\n","Skipping y_hat=4201\n","Skipping y_hat=4201\n","Skipping y_hat=4201\n","Skipping y_hat=4201\n","Skipping y_hat=3881\n","Skipping y_hat=3881\n","Skipping y_hat=3881\n","Skipping y_hat=3881\n","Skipping y_hat=3881\n","Skipping y_hat=3881\n","Skipping y_hat=3881\n","Skipping y_hat=9020\n","Skipping y_hat=9020\n","Skipping y_hat=9020\n","Skipping y_hat=9020\n","Skipping y_hat=9020\n","Skipping y_hat=9020\n","Skipping y_hat=9020\n","Skipping y_hat=2220\n","Skipping y_hat=2220\n","Skipping y_hat=2220\n","Skipping y_hat=2220\n","Skipping y_hat=2220\n","Skipping y_hat=2220\n","Skipping y_hat=2220\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=2461\n","Skipping y_hat=6701\n","Skipping y_hat=6701\n","Skipping y_hat=6701\n","Skipping y_hat=6701\n","Skipping y_hat=6701\n","Skipping y_hat=6701\n","Skipping y_hat=6701\n","Skipping y_hat=1851\n","Skipping y_hat=1851\n","Skipping y_hat=1851\n","Skipping y_hat=1851\n","Skipping y_hat=1851\n","Skipping y_hat=1851\n","Skipping y_hat=1851\n","Skipping y_hat=2820\n","Skipping y_hat=2820\n","Skipping y_hat=2820\n","Skipping y_hat=2820\n","Skipping y_hat=2820\n","Skipping y_hat=2820\n","Skipping y_hat=2820\n","Skipping y_hat=6150\n","Skipping y_hat=6150\n","Skipping y_hat=6150\n","Skipping y_hat=6150\n","Skipping y_hat=6150\n","Skipping y_hat=6150\n","Skipping y_hat=6150\n","Skipping y_hat=7901\n","Skipping y_hat=7901\n","Skipping y_hat=7901\n","Skipping y_hat=7901\n","Skipping y_hat=7901\n","Skipping y_hat=7901\n","Skipping y_hat=7901\n","Skipping y_hat=0091\n","Skipping y_hat=0091\n","Skipping y_hat=0091\n","Skipping y_hat=0091\n","Skipping y_hat=0091\n","Skipping y_hat=0091\n","Skipping y_hat=0091\n","Skipping y_hat=3441\n","Skipping y_hat=3441\n","Skipping y_hat=3441\n","Skipping y_hat=3441\n","Skipping y_hat=3441\n","Skipping y_hat=3441\n","Skipping y_hat=3441\n","Skipping y_hat=7801\n","Skipping y_hat=7801\n","Skipping y_hat=7801\n","Skipping y_hat=7801\n","Skipping y_hat=7801\n","Skipping y_hat=7801\n","Skipping y_hat=7801\n","Skipping y_hat=3231\n","Skipping y_hat=3231\n","Skipping y_hat=3231\n","Skipping y_hat=3231\n","Skipping y_hat=3231\n","Skipping y_hat=3231\n","Skipping y_hat=3231\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=3521\n","Skipping y_hat=3521\n","Skipping y_hat=3521\n","Skipping y_hat=3521\n","Skipping y_hat=3521\n","Skipping y_hat=3521\n","Skipping y_hat=3521\n","Skipping y_hat=2090\n","Skipping y_hat=2090\n","Skipping y_hat=2090\n","Skipping y_hat=2090\n","Skipping y_hat=2090\n","Skipping y_hat=2090\n","Skipping y_hat=2090\n","Skipping y_hat=7950\n","Skipping y_hat=7950\n","Skipping y_hat=7950\n","Skipping y_hat=7950\n","Skipping y_hat=7950\n","Skipping y_hat=7950\n","Skipping y_hat=7950\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=5611\n","Skipping y_hat=8570\n","Skipping y_hat=8570\n","Skipping y_hat=8570\n","Skipping y_hat=8570\n","Skipping y_hat=8570\n","Skipping y_hat=8570\n","Skipping y_hat=8570\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=3621\n","Skipping y_hat=9350\n","Skipping y_hat=9350\n","Skipping y_hat=9350\n","Skipping y_hat=9350\n","Skipping y_hat=9350\n","Skipping y_hat=9350\n","Skipping y_hat=9350\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=8811\n","Skipping y_hat=3730\n","Skipping y_hat=3730\n","Skipping y_hat=3730\n","Skipping y_hat=3730\n","Skipping y_hat=3730\n","Skipping y_hat=3730\n","Skipping y_hat=3730\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2201\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=2641\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=2560\n","Skipping y_hat=8110\n","Skipping y_hat=8110\n","Skipping y_hat=8110\n","Skipping y_hat=8110\n","Skipping y_hat=8110\n","Skipping y_hat=8110\n","Skipping y_hat=8110\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=8560\n","Skipping y_hat=8560\n","Skipping y_hat=8560\n","Skipping y_hat=8560\n","Skipping y_hat=8560\n","Skipping y_hat=8560\n","Skipping y_hat=8560\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=1771\n","Skipping y_hat=0790\n","Skipping y_hat=0790\n","Skipping y_hat=0790\n","Skipping y_hat=0790\n","Skipping y_hat=0790\n","Skipping y_hat=0790\n","Skipping y_hat=0790\n","Skipping y_hat=8720\n","Skipping y_hat=8720\n","Skipping y_hat=8720\n","Skipping y_hat=8720\n","Skipping y_hat=8720\n","Skipping y_hat=8720\n","Skipping y_hat=8720\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=5011\n","Skipping y_hat=5011\n","Skipping y_hat=5011\n","Skipping y_hat=5011\n","Skipping y_hat=5011\n","Skipping y_hat=5011\n","Skipping y_hat=5011\n","Skipping y_hat=7210\n","Skipping y_hat=7210\n","Skipping y_hat=7210\n","Skipping y_hat=7210\n","Skipping y_hat=7210\n","Skipping y_hat=7210\n","Skipping y_hat=7210\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=7860\n","Skipping y_hat=7860\n","Skipping y_hat=7860\n","Skipping y_hat=7860\n","Skipping y_hat=7860\n","Skipping y_hat=7860\n","Skipping y_hat=7860\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=7250\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=8321\n","Skipping y_hat=0290\n","Skipping y_hat=0290\n","Skipping y_hat=0290\n","Skipping y_hat=0290\n","Skipping y_hat=0290\n","Skipping y_hat=0290\n","Skipping y_hat=0290\n","Skipping y_hat=7360\n","Skipping y_hat=7360\n","Skipping y_hat=7360\n","Skipping y_hat=7360\n","Skipping y_hat=7360\n","Skipping y_hat=7360\n","Skipping y_hat=7360\n","Skipping y_hat=1450\n","Skipping y_hat=1450\n","Skipping y_hat=1450\n","Skipping y_hat=1450\n","Skipping y_hat=1450\n","Skipping y_hat=1450\n","Skipping y_hat=1450\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=9661\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=8590\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=5960\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=8220\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=7790\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=3331\n","Skipping y_hat=3331\n","Skipping y_hat=3331\n","Skipping y_hat=3331\n","Skipping y_hat=3331\n","Skipping y_hat=3331\n","Skipping y_hat=3331\n","Skipping y_hat=0480\n","Skipping y_hat=0480\n","Skipping y_hat=0480\n","Skipping y_hat=0480\n","Skipping y_hat=0480\n","Skipping y_hat=0480\n","Skipping y_hat=0480\n","Skipping y_hat=7401\n","Skipping y_hat=7401\n","Skipping y_hat=7401\n","Skipping y_hat=7401\n","Skipping y_hat=7401\n","Skipping y_hat=7401\n","Skipping y_hat=7401\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=5121\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=2480\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0580\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=0031\n","Skipping y_hat=3770\n","Skipping y_hat=3770\n","Skipping y_hat=3770\n","Skipping y_hat=3770\n","Skipping y_hat=3770\n","Skipping y_hat=3770\n","Skipping y_hat=3770\n","Skipping y_hat=3970\n","Skipping y_hat=3970\n","Skipping y_hat=3970\n","Skipping y_hat=3970\n","Skipping y_hat=3970\n","Skipping y_hat=3970\n","Skipping y_hat=3970\n","Skipping y_hat=4770\n","Skipping y_hat=4770\n","Skipping y_hat=4770\n","Skipping y_hat=4770\n","Skipping y_hat=4770\n","Skipping y_hat=4770\n","Skipping y_hat=4770\n","Skipping y_hat=6790\n","Skipping y_hat=6790\n","Skipping y_hat=6790\n","Skipping y_hat=6790\n","Skipping y_hat=6790\n","Skipping y_hat=6790\n","Skipping y_hat=6790\n","Skipping y_hat=0011\n","Skipping y_hat=0011\n","Skipping y_hat=0011\n","Skipping y_hat=0011\n","Skipping y_hat=0011\n","Skipping y_hat=0011\n","Skipping y_hat=0011\n","Skipping y_hat=8231\n","Skipping y_hat=8231\n","Skipping y_hat=8231\n","Skipping y_hat=8231\n","Skipping y_hat=8231\n","Skipping y_hat=8231\n","Skipping y_hat=8231\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=4670\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=3060\n","Skipping y_hat=4161\n","Skipping y_hat=4161\n","Skipping y_hat=4161\n","Skipping y_hat=4161\n","Skipping y_hat=4161\n","Skipping y_hat=4161\n","Skipping y_hat=4161\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=2111\n","Skipping y_hat=0411\n","Skipping y_hat=0411\n","Skipping y_hat=0411\n","Skipping y_hat=0411\n","Skipping y_hat=0411\n","Skipping y_hat=0411\n","Skipping y_hat=0411\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=8890\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=9040\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=5390\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=3971\n","Skipping y_hat=9401\n","Skipping y_hat=9401\n","Skipping y_hat=9401\n","Skipping y_hat=9401\n","Skipping y_hat=9401\n","Skipping y_hat=9401\n","Skipping y_hat=9401\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=7721\n","Skipping y_hat=2951\n","Skipping y_hat=2951\n","Skipping y_hat=2951\n","Skipping y_hat=2951\n","Skipping y_hat=2951\n","Skipping y_hat=2951\n","Skipping y_hat=2951\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=7970\n","Skipping y_hat=5420\n","Skipping y_hat=5420\n","Skipping y_hat=5420\n","Skipping y_hat=5420\n","Skipping y_hat=5420\n","Skipping y_hat=5420\n","Skipping y_hat=5420\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=6960\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=0251\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=5541\n","Skipping y_hat=5541\n","Skipping y_hat=5541\n","Skipping y_hat=5541\n","Skipping y_hat=5541\n","Skipping y_hat=5541\n","Skipping y_hat=5541\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=1690\n","Skipping y_hat=4640\n","Skipping y_hat=4640\n","Skipping y_hat=4640\n","Skipping y_hat=4640\n","Skipping y_hat=4640\n","Skipping y_hat=4640\n","Skipping y_hat=4640\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=3711\n","Skipping y_hat=4361\n","Skipping y_hat=4361\n","Skipping y_hat=4361\n","Skipping y_hat=4361\n","Skipping y_hat=4361\n","Skipping y_hat=4361\n","Skipping y_hat=4361\n","Skipping y_hat=0901\n","Skipping y_hat=0901\n","Skipping y_hat=0901\n","Skipping y_hat=0901\n","Skipping y_hat=0901\n","Skipping y_hat=0901\n","Skipping y_hat=0901\n","Skipping y_hat=4001\n","Skipping y_hat=4001\n","Skipping y_hat=4001\n","Skipping y_hat=4001\n","Skipping y_hat=4001\n","Skipping y_hat=4001\n","Skipping y_hat=4001\n","Skipping y_hat=0541\n","Skipping y_hat=0541\n","Skipping y_hat=0541\n","Skipping y_hat=0541\n","Skipping y_hat=0541\n","Skipping y_hat=0541\n","Skipping y_hat=0541\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=7151\n","Skipping y_hat=3570\n","Skipping y_hat=3570\n","Skipping y_hat=3570\n","Skipping y_hat=3570\n","Skipping y_hat=3570\n","Skipping y_hat=3570\n","Skipping y_hat=3570\n","Skipping y_hat=3150\n","Skipping y_hat=3150\n","Skipping y_hat=3150\n","Skipping y_hat=3150\n","Skipping y_hat=3150\n","Skipping y_hat=3150\n","Skipping y_hat=3150\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=5921\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=2331\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=9470\n","Skipping y_hat=3650\n","Skipping y_hat=3650\n","Skipping y_hat=3650\n","Skipping y_hat=3650\n","Skipping y_hat=3650\n","Skipping y_hat=3650\n","Skipping y_hat=3650\n","Skipping y_hat=5370\n","Skipping y_hat=5370\n","Skipping y_hat=5370\n","Skipping y_hat=5370\n","Skipping y_hat=5370\n","Skipping y_hat=5370\n","Skipping y_hat=5370\n","Skipping y_hat=4291\n","Skipping y_hat=4291\n","Skipping y_hat=4291\n","Skipping y_hat=4291\n","Skipping y_hat=4291\n","Skipping y_hat=4291\n","Skipping y_hat=4291\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=3351\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=9701\n","Skipping y_hat=4140\n","Skipping y_hat=4140\n","Skipping y_hat=4140\n","Skipping y_hat=4140\n","Skipping y_hat=4140\n","Skipping y_hat=4140\n","Skipping y_hat=4140\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=8500\n","Skipping y_hat=8500\n","Skipping y_hat=8500\n","Skipping y_hat=8500\n","Skipping y_hat=8500\n","Skipping y_hat=8500\n","Skipping y_hat=8500\n","Skipping y_hat=6340\n","Skipping y_hat=6340\n","Skipping y_hat=6340\n","Skipping y_hat=6340\n","Skipping y_hat=6340\n","Skipping y_hat=6340\n","Skipping y_hat=6340\n","Skipping y_hat=9460\n","Skipping y_hat=9460\n","Skipping y_hat=9460\n","Skipping y_hat=9460\n","Skipping y_hat=9460\n","Skipping y_hat=9460\n","Skipping y_hat=9460\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=0061\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=0821\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=9680\n","Skipping y_hat=9680\n","Skipping y_hat=9680\n","Skipping y_hat=9680\n","Skipping y_hat=9680\n","Skipping y_hat=9680\n","Skipping y_hat=9680\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n"," 92% 22/24 [00:02<00:00,  7.93it/s]Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=8261\n","Skipping y_hat=8261\n","Skipping y_hat=8261\n","Skipping y_hat=8261\n","Skipping y_hat=8261\n","Skipping y_hat=8261\n","Skipping y_hat=8261\n","Skipping y_hat=8441\n","Skipping y_hat=8441\n","Skipping y_hat=8441\n","Skipping y_hat=8441\n","Skipping y_hat=8441\n","Skipping y_hat=8441\n","Skipping y_hat=8441\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=1911\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=6770\n","Skipping y_hat=4980\n","Skipping y_hat=4980\n","Skipping y_hat=4980\n","Skipping y_hat=4980\n","Skipping y_hat=4980\n","Skipping y_hat=4980\n","Skipping y_hat=4980\n","Skipping y_hat=3851\n","Skipping y_hat=3851\n","Skipping y_hat=3851\n","Skipping y_hat=3851\n","Skipping y_hat=3851\n","Skipping y_hat=3851\n","Skipping y_hat=3851\n","Skipping y_hat=9480\n","Skipping y_hat=9480\n","Skipping y_hat=9480\n","Skipping y_hat=9480\n","Skipping y_hat=9480\n","Skipping y_hat=9480\n","Skipping y_hat=9480\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=2390\n","Skipping y_hat=0111\n","Skipping y_hat=0111\n","Skipping y_hat=0111\n","Skipping y_hat=0111\n","Skipping y_hat=0111\n","Skipping y_hat=0111\n","Skipping y_hat=0111\n","Skipping y_hat=4880\n","Skipping y_hat=4880\n","Skipping y_hat=4880\n","Skipping y_hat=4880\n","Skipping y_hat=4880\n","Skipping y_hat=4880\n","Skipping y_hat=4880\n","Skipping y_hat=1830\n","Skipping y_hat=1830\n","Skipping y_hat=1830\n","Skipping y_hat=1830\n","Skipping y_hat=1830\n","Skipping y_hat=1830\n","Skipping y_hat=1830\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=5060\n","Skipping y_hat=1220\n","Skipping y_hat=1220\n","Skipping y_hat=1220\n","Skipping y_hat=1220\n","Skipping y_hat=1220\n","Skipping y_hat=1220\n","Skipping y_hat=1220\n","Skipping y_hat=7670\n","Skipping y_hat=7670\n","Skipping y_hat=7670\n","Skipping y_hat=7670\n","Skipping y_hat=7670\n","Skipping y_hat=7670\n","Skipping y_hat=7670\n","Skipping y_hat=8340\n","Skipping y_hat=8340\n","Skipping y_hat=8340\n","Skipping y_hat=8340\n","Skipping y_hat=8340\n","Skipping y_hat=8340\n","Skipping y_hat=8340\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=2701\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=6221\n","Skipping y_hat=6221\n","Skipping y_hat=6221\n","Skipping y_hat=6221\n","Skipping y_hat=6221\n","Skipping y_hat=6221\n","Skipping y_hat=6221\n","Skipping y_hat=3761\n","Skipping y_hat=3761\n","Skipping y_hat=3761\n","Skipping y_hat=3761\n","Skipping y_hat=3761\n","Skipping y_hat=3761\n","Skipping y_hat=3761\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=1860\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=2140\n","Skipping y_hat=2140\n","Skipping y_hat=2140\n","Skipping y_hat=2140\n","Skipping y_hat=2140\n","Skipping y_hat=2140\n","Skipping y_hat=2140\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=1890\n","Skipping y_hat=4521\n","Skipping y_hat=4521\n","Skipping y_hat=4521\n","Skipping y_hat=4521\n","Skipping y_hat=4521\n","Skipping y_hat=4521\n","Skipping y_hat=4521\n","Skipping y_hat=6410\n","Skipping y_hat=6410\n","Skipping y_hat=6410\n","Skipping y_hat=6410\n","Skipping y_hat=6410\n","Skipping y_hat=6410\n","Skipping y_hat=6410\n","Skipping y_hat=0530\n","Skipping y_hat=0530\n","Skipping y_hat=0530\n","Skipping y_hat=0530\n","Skipping y_hat=0530\n","Skipping y_hat=0530\n","Skipping y_hat=0530\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=4921\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=2421\n","Skipping y_hat=5820\n","Skipping y_hat=5820\n","Skipping y_hat=5820\n","Skipping y_hat=5820\n","Skipping y_hat=5820\n","Skipping y_hat=5820\n","Skipping y_hat=5820\n","Skipping y_hat=7370\n","Skipping y_hat=7370\n","Skipping y_hat=7370\n","Skipping y_hat=7370\n","Skipping y_hat=7370\n","Skipping y_hat=7370\n","Skipping y_hat=7370\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=5421\n","Skipping y_hat=5421\n","Skipping y_hat=5421\n","Skipping y_hat=5421\n","Skipping y_hat=5421\n","Skipping y_hat=5421\n","Skipping y_hat=5421\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=1361\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=5201\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=2380\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=4870\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=1711\n","Skipping y_hat=3651\n","Skipping y_hat=3651\n","Skipping y_hat=3651\n","Skipping y_hat=3651\n","Skipping y_hat=3651\n","Skipping y_hat=3651\n","Skipping y_hat=3651\n","Skipping y_hat=6141\n","Skipping y_hat=6141\n","Skipping y_hat=6141\n","Skipping y_hat=6141\n","Skipping y_hat=6141\n","Skipping y_hat=6141\n","Skipping y_hat=6141\n","Skipping y_hat=3780\n","Skipping y_hat=3780\n","Skipping y_hat=3780\n","Skipping y_hat=3780\n","Skipping y_hat=3780\n","Skipping y_hat=3780\n","Skipping y_hat=3780\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=7911\n","Skipping y_hat=0280\n","Skipping y_hat=0280\n","Skipping y_hat=0280\n","Skipping y_hat=0280\n","Skipping y_hat=0280\n","Skipping y_hat=0280\n","Skipping y_hat=0280\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=2120\n","Skipping y_hat=0650\n","Skipping y_hat=0650\n","Skipping y_hat=0650\n","Skipping y_hat=0650\n","Skipping y_hat=0650\n","Skipping y_hat=0650\n","Skipping y_hat=0650\n","Skipping y_hat=0600\n","Skipping y_hat=0600\n","Skipping y_hat=0600\n","Skipping y_hat=0600\n","Skipping y_hat=0600\n","Skipping y_hat=0600\n","Skipping y_hat=0600\n","Skipping y_hat=6981\n","Skipping y_hat=6981\n","Skipping y_hat=6981\n","Skipping y_hat=6981\n","Skipping y_hat=6981\n","Skipping y_hat=6981\n","Skipping y_hat=6981\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=2180\n","Skipping y_hat=0170\n","Skipping y_hat=0170\n","Skipping y_hat=0170\n","Skipping y_hat=0170\n","Skipping y_hat=0170\n","Skipping y_hat=0170\n","Skipping y_hat=0170\n","Skipping y_hat=8831\n","Skipping y_hat=8831\n","Skipping y_hat=8831\n","Skipping y_hat=8831\n","Skipping y_hat=8831\n","Skipping y_hat=8831\n","Skipping y_hat=8831\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=4411\n","Skipping y_hat=8140\n","Skipping y_hat=8140\n","Skipping y_hat=8140\n","Skipping y_hat=8140\n","Skipping y_hat=8140\n","Skipping y_hat=8140\n","Skipping y_hat=8140\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=7251\n","Skipping y_hat=0350\n","Skipping y_hat=0350\n","Skipping y_hat=0350\n","Skipping y_hat=0350\n","Skipping y_hat=0350\n","Skipping y_hat=0350\n","Skipping y_hat=0350\n","Skipping y_hat=6750\n","Skipping y_hat=6750\n","Skipping y_hat=6750\n","Skipping y_hat=6750\n","Skipping y_hat=6750\n","Skipping y_hat=6750\n","Skipping y_hat=6750\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=9561\n","Skipping y_hat=9561\n","Skipping y_hat=9561\n","Skipping y_hat=9561\n","Skipping y_hat=9561\n","Skipping y_hat=9561\n","Skipping y_hat=9561\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=3540\n","Skipping y_hat=3540\n","Skipping y_hat=3540\n","Skipping y_hat=3540\n","Skipping y_hat=3540\n","Skipping y_hat=3540\n","Skipping y_hat=3540\n","Skipping y_hat=2841\n","Skipping y_hat=2841\n","Skipping y_hat=2841\n","Skipping y_hat=2841\n","Skipping y_hat=2841\n","Skipping y_hat=2841\n","Skipping y_hat=2841\n","Skipping y_hat=5601\n","Skipping y_hat=5601\n","Skipping y_hat=5601\n","Skipping y_hat=5601\n","Skipping y_hat=5601\n","Skipping y_hat=5601\n","Skipping y_hat=5601\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=7070\n","Skipping y_hat=5531\n","Skipping y_hat=5531\n","Skipping y_hat=5531\n","Skipping y_hat=5531\n","Skipping y_hat=5531\n","Skipping y_hat=5531\n","Skipping y_hat=5531\n","Skipping y_hat=2510\n","Skipping y_hat=2510\n","Skipping y_hat=2510\n","Skipping y_hat=2510\n","Skipping y_hat=2510\n","Skipping y_hat=2510\n","Skipping y_hat=2510\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=0740\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=2670\n","Skipping y_hat=1400\n","Skipping y_hat=1400\n","Skipping y_hat=1400\n","Skipping y_hat=1400\n","Skipping y_hat=1400\n","Skipping y_hat=1400\n","Skipping y_hat=1400\n","Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=0560\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=8001\n","Skipping y_hat=8001\n","Skipping y_hat=8001\n","Skipping y_hat=8001\n","Skipping y_hat=8001\n","Skipping y_hat=8001\n","Skipping y_hat=8001\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=4311\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=3251\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=3541\n","Skipping y_hat=4780\n","Skipping y_hat=4780\n","Skipping y_hat=4780\n","Skipping y_hat=4780\n","Skipping y_hat=4780\n","Skipping y_hat=4780\n","Skipping y_hat=4780\n","Skipping y_hat=6431\n","Skipping y_hat=6431\n","Skipping y_hat=6431\n","Skipping y_hat=6431\n","Skipping y_hat=6431\n","Skipping y_hat=6431\n","Skipping y_hat=6431\n","Skipping y_hat=7261\n","Skipping y_hat=7261\n","Skipping y_hat=7261\n","Skipping y_hat=7261\n","Skipping y_hat=7261\n","Skipping y_hat=7261\n","Skipping y_hat=7261\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=5790\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=8931\n","Skipping y_hat=7031\n","Skipping y_hat=7031\n","Skipping y_hat=7031\n","Skipping y_hat=7031\n","Skipping y_hat=7031\n","Skipping y_hat=7031\n","Skipping y_hat=7031\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=7921\n","Skipping y_hat=9570\n","Skipping y_hat=9570\n","Skipping y_hat=9570\n","Skipping y_hat=9570\n","Skipping y_hat=9570\n","Skipping y_hat=9570\n","Skipping y_hat=9570\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=8761\n","Skipping y_hat=5641\n","Skipping y_hat=5641\n","Skipping y_hat=5641\n","Skipping y_hat=5641\n","Skipping y_hat=5641\n","Skipping y_hat=5641\n","Skipping y_hat=5641\n","Skipping y_hat=6370\n","Skipping y_hat=6370\n","Skipping y_hat=6370\n","Skipping y_hat=6370\n","Skipping y_hat=6370\n","Skipping y_hat=6370\n","Skipping y_hat=6370\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=1170\n","Skipping y_hat=9870\n","Skipping y_hat=9870\n","Skipping y_hat=9870\n","Skipping y_hat=9870\n","Skipping y_hat=9870\n","Skipping y_hat=9870\n","Skipping y_hat=9870\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=3531\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=6661\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=1201\n","Skipping y_hat=3640\n","Skipping y_hat=3640\n","Skipping y_hat=3640\n","Skipping y_hat=3640\n","Skipping y_hat=3640\n","Skipping y_hat=3640\n","Skipping y_hat=3640\n","Skipping y_hat=5481\n","Skipping y_hat=5481\n","Skipping y_hat=5481\n","Skipping y_hat=5481\n","Skipping y_hat=5481\n","Skipping y_hat=5481\n","Skipping y_hat=5481\n","Skipping y_hat=9590\n","Skipping y_hat=9590\n","Skipping y_hat=9590\n","Skipping y_hat=9590\n","Skipping y_hat=9590\n","Skipping y_hat=9590\n","Skipping y_hat=9590\n","Skipping y_hat=0291\n","Skipping y_hat=0291\n","Skipping y_hat=0291\n","Skipping y_hat=0291\n","Skipping y_hat=0291\n","Skipping y_hat=0291\n","Skipping y_hat=0291\n","Skipping y_hat=3850\n","Skipping y_hat=3850\n","Skipping y_hat=3850\n","Skipping y_hat=3850\n","Skipping y_hat=3850\n","Skipping y_hat=3850\n","Skipping y_hat=3850\n","Skipping y_hat=3751\n","Skipping y_hat=3751\n","Skipping y_hat=3751\n","Skipping y_hat=3751\n","Skipping y_hat=3751\n","Skipping y_hat=3751\n","Skipping y_hat=3751\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=3870\n","Skipping y_hat=6040\n","Skipping y_hat=6040\n","Skipping y_hat=6040\n","Skipping y_hat=6040\n","Skipping y_hat=6040\n","Skipping y_hat=6040\n","Skipping y_hat=6040\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=5041\n","Skipping y_hat=5041\n","Skipping y_hat=5041\n","Skipping y_hat=5041\n","Skipping y_hat=5041\n","Skipping y_hat=5041\n","Skipping y_hat=5041\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=8651\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=9821\n","Skipping y_hat=8061\n","Skipping y_hat=8061\n","Skipping y_hat=8061\n","Skipping y_hat=8061\n","Skipping y_hat=8061\n","Skipping y_hat=8061\n","Skipping y_hat=8061\n","Skipping y_hat=3160\n","Skipping y_hat=3160\n","Skipping y_hat=3160\n","Skipping y_hat=3160\n","Skipping y_hat=3160\n","Skipping y_hat=3160\n","Skipping y_hat=3160\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=3431\n","Skipping y_hat=3431\n","Skipping y_hat=3431\n","Skipping y_hat=3431\n","Skipping y_hat=3431\n","Skipping y_hat=3431\n","Skipping y_hat=3431\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=9651\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=6590\n","Skipping y_hat=0840\n","Skipping y_hat=0840\n","Skipping y_hat=0840\n","Skipping y_hat=0840\n","Skipping y_hat=0840\n","Skipping y_hat=0840\n","Skipping y_hat=0840\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=3220\n","Skipping y_hat=3220\n","Skipping y_hat=3220\n","Skipping y_hat=3220\n","Skipping y_hat=3220\n","Skipping y_hat=3220\n","Skipping y_hat=3220\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=4641\n","Skipping y_hat=9721\n","Skipping y_hat=9721\n","Skipping y_hat=9721\n","Skipping y_hat=9721\n","Skipping y_hat=9721\n","Skipping y_hat=9721\n","Skipping y_hat=9721\n","Skipping y_hat=9741\n","Skipping y_hat=9741\n","Skipping y_hat=9741\n","Skipping y_hat=9741\n","Skipping y_hat=9741\n","Skipping y_hat=9741\n","Skipping y_hat=9741\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=1390\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=7280\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1880\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=4281\n","Skipping y_hat=7910\n","Skipping y_hat=7910\n","Skipping y_hat=7910\n","Skipping y_hat=7910\n","Skipping y_hat=7910\n","Skipping y_hat=7910\n","Skipping y_hat=7910\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=9921\n","Skipping y_hat=0160\n","Skipping y_hat=0160\n","Skipping y_hat=0160\n","Skipping y_hat=0160\n","Skipping y_hat=0160\n","Skipping y_hat=0160\n","Skipping y_hat=0160\n","Skipping y_hat=1480\n","Skipping y_hat=1480\n","Skipping y_hat=1480\n","Skipping y_hat=1480\n","Skipping y_hat=1480\n","Skipping y_hat=1480\n","Skipping y_hat=1480\n"," 96% 23/24 [00:02<00:00,  7.91it/s]Skipping y_hat=3271\n","Skipping y_hat=3271\n","Skipping y_hat=3271\n","Skipping y_hat=3271\n","Skipping y_hat=3271\n","Skipping y_hat=3271\n","Skipping y_hat=3271\n","Skipping y_hat=4671\n","Skipping y_hat=4671\n","Skipping y_hat=4671\n","Skipping y_hat=4671\n","Skipping y_hat=4671\n","Skipping y_hat=4671\n","Skipping y_hat=4671\n","Skipping y_hat=9941\n","Skipping y_hat=9941\n","Skipping y_hat=9941\n","Skipping y_hat=9941\n","Skipping y_hat=9941\n","Skipping y_hat=9941\n","Skipping y_hat=9941\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=1370\n","Skipping y_hat=8051\n","Skipping y_hat=8051\n","Skipping y_hat=8051\n","Skipping y_hat=8051\n","Skipping y_hat=8051\n","Skipping y_hat=8051\n","Skipping y_hat=8051\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=8351\n","Skipping y_hat=4790\n","Skipping y_hat=4790\n","Skipping y_hat=4790\n","Skipping y_hat=4790\n","Skipping y_hat=4790\n","Skipping y_hat=4790\n","Skipping y_hat=4790\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=3550\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=6521\n","Skipping y_hat=3930\n","Skipping y_hat=3930\n","Skipping y_hat=3930\n","Skipping y_hat=3930\n","Skipping y_hat=3930\n","Skipping y_hat=3930\n","Skipping y_hat=3930\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=8830\n","Skipping y_hat=6450\n","Skipping y_hat=6450\n","Skipping y_hat=6450\n","Skipping y_hat=6450\n","Skipping y_hat=6450\n","Skipping y_hat=6450\n","Skipping y_hat=6450\n","Skipping y_hat=5780\n","Skipping y_hat=5780\n","Skipping y_hat=5780\n","Skipping y_hat=5780\n","Skipping y_hat=5780\n","Skipping y_hat=5780\n","Skipping y_hat=5780\n","Skipping y_hat=7990\n","Skipping y_hat=7990\n","Skipping y_hat=7990\n","Skipping y_hat=7990\n","Skipping y_hat=7990\n","Skipping y_hat=7990\n","Skipping y_hat=7990\n","Skipping y_hat=6421\n","Skipping y_hat=6421\n","Skipping y_hat=6421\n","Skipping y_hat=6421\n","Skipping y_hat=6421\n","Skipping y_hat=6421\n","Skipping y_hat=6421\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=4360\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=7241\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=8121\n","Skipping y_hat=0331\n","Skipping y_hat=0331\n","Skipping y_hat=0331\n","Skipping y_hat=0331\n","Skipping y_hat=0331\n","Skipping y_hat=0331\n","Skipping y_hat=0331\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=3340\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=3201\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=0761\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=8221\n","Skipping y_hat=7400\n","Skipping y_hat=7400\n","Skipping y_hat=7400\n","Skipping y_hat=7400\n","Skipping y_hat=7400\n","Skipping y_hat=7400\n","Skipping y_hat=7400\n","Skipping y_hat=0501\n","Skipping y_hat=0501\n","Skipping y_hat=0501\n","Skipping y_hat=0501\n","Skipping y_hat=0501\n","Skipping y_hat=0501\n","Skipping y_hat=0501\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=5210\n","Skipping y_hat=5530\n","Skipping y_hat=5530\n","Skipping y_hat=5530\n","Skipping y_hat=5530\n","Skipping y_hat=5530\n","Skipping y_hat=5530\n","Skipping y_hat=5530\n","Skipping y_hat=3811\n","Skipping y_hat=3811\n","Skipping y_hat=3811\n","Skipping y_hat=3811\n","Skipping y_hat=3811\n","Skipping y_hat=3811\n","Skipping y_hat=3811\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=4490\n","Skipping y_hat=8381\n","Skipping y_hat=8381\n","Skipping y_hat=8381\n","Skipping y_hat=8381\n","Skipping y_hat=8381\n","Skipping y_hat=8381\n","Skipping y_hat=8381\n","Skipping y_hat=2150\n","Skipping y_hat=2150\n","Skipping y_hat=2150\n","Skipping y_hat=2150\n","Skipping y_hat=2150\n","Skipping y_hat=2150\n","Skipping y_hat=2150\n","Skipping y_hat=0801\n","Skipping y_hat=0801\n","Skipping y_hat=0801\n","Skipping y_hat=0801\n","Skipping y_hat=0801\n","Skipping y_hat=0801\n","Skipping y_hat=0801\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=5761\n","Skipping y_hat=8631\n","Skipping y_hat=8631\n","Skipping y_hat=8631\n","Skipping y_hat=8631\n","Skipping y_hat=8631\n","Skipping y_hat=8631\n","Skipping y_hat=8631\n","Skipping y_hat=0731\n","Skipping y_hat=0731\n","Skipping y_hat=0731\n","Skipping y_hat=0731\n","Skipping y_hat=0731\n","Skipping y_hat=0731\n","Skipping y_hat=0731\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=3421\n","Skipping y_hat=5830\n","Skipping y_hat=5830\n","Skipping y_hat=5830\n","Skipping y_hat=5830\n","Skipping y_hat=5830\n","Skipping y_hat=5830\n","Skipping y_hat=5830\n","Skipping y_hat=7560\n","Skipping y_hat=7560\n","Skipping y_hat=7560\n","Skipping y_hat=7560\n","Skipping y_hat=7560\n","Skipping y_hat=7560\n","Skipping y_hat=7560\n","Skipping y_hat=4331\n","Skipping y_hat=4331\n","Skipping y_hat=4331\n","Skipping y_hat=4331\n","Skipping y_hat=4331\n","Skipping y_hat=4331\n","Skipping y_hat=4331\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=7211\n","Skipping y_hat=8650\n","Skipping y_hat=8650\n","Skipping y_hat=8650\n","Skipping y_hat=8650\n","Skipping y_hat=8650\n","Skipping y_hat=8650\n","Skipping y_hat=8650\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=7811\n","Skipping y_hat=8801\n","Skipping y_hat=8801\n","Skipping y_hat=8801\n","Skipping y_hat=8801\n","Skipping y_hat=8801\n","Skipping y_hat=8801\n","Skipping y_hat=8801\n","Skipping y_hat=6980\n","Skipping y_hat=6980\n","Skipping y_hat=6980\n","Skipping y_hat=6980\n","Skipping y_hat=6980\n","Skipping y_hat=6980\n","Skipping y_hat=6980\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=3041\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=7701\n","Skipping y_hat=5340\n","Skipping y_hat=5340\n","Skipping y_hat=5340\n","Skipping y_hat=5340\n","Skipping y_hat=5340\n","Skipping y_hat=5340\n","Skipping y_hat=5340\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=2260\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=8311\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=3630\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=6131\n","Skipping y_hat=0711\n","Skipping y_hat=0711\n","Skipping y_hat=0711\n","Skipping y_hat=0711\n","Skipping y_hat=0711\n","Skipping y_hat=0711\n","Skipping y_hat=0711\n","Skipping y_hat=6290\n","Skipping y_hat=6290\n","Skipping y_hat=6290\n","Skipping y_hat=6290\n","Skipping y_hat=6290\n","Skipping y_hat=6290\n","Skipping y_hat=6290\n","100% 24/24 [00:02<00:00,  8.34it/s]\n","accuracy of 3000 examples: 3000/3000 (100.0%)\n","\n","Test Results:\n","test_reverse: 100.00%\n","\n"]}],"source":["!python train_additional_measurement.py 2_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1787187,"status":"ok","timestamp":1753662627433,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":300},"id":"DaeI9kfRz7Uw","outputId":"ce2d7462-5d88-484a-a4ef-178254725538"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=454\n","Skipping y_hat=454\n","Skipping y_hat=454\n","Skipping y_hat=454\n","Skipping y_hat=454\n","Skipping y_hat=454\n","Skipping y_hat=454\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=119\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=104\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=994\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=243\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=962\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=982\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=120\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=957\n","Skipping y_hat=402\n","Skipping y_hat=402\n","Skipping y_hat=402\n","Skipping y_hat=402\n","Skipping y_hat=402\n","Skipping y_hat=402\n","Skipping y_hat=402\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=640\n","Skipping y_hat=640\n","Skipping y_hat=640\n","Skipping y_hat=640\n","Skipping y_hat=640\n","Skipping y_hat=640\n","Skipping y_hat=640\n","Skipping y_hat=180\n","Skipping y_hat=180\n","Skipping y_hat=180\n","Skipping y_hat=180\n","Skipping y_hat=180\n","Skipping y_hat=180\n","Skipping y_hat=180\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=021\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=572\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=306\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=477\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=147\n","Skipping y_hat=147\n","Skipping y_hat=147\n","Skipping y_hat=147\n","Skipping y_hat=147\n","Skipping y_hat=147\n","Skipping y_hat=147\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=610\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=289\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=117\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=279\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=301\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=259\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=377\n","Skipping y_hat=199\n","Skipping y_hat=199\n","Skipping y_hat=199\n","Skipping y_hat=199\n","Skipping y_hat=199\n","Skipping y_hat=199\n","Skipping y_hat=199\n","Skipping y_hat=081\n","Skipping y_hat=081\n","Skipping y_hat=081\n","Skipping y_hat=081\n","Skipping y_hat=081\n","Skipping y_hat=081\n","Skipping y_hat=081\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=956\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=649\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=211\n","Skipping y_hat=292\n","Skipping y_hat=292\n","Skipping y_hat=292\n","Skipping y_hat=292\n","Skipping y_hat=292\n","Skipping y_hat=292\n","Skipping y_hat=292\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=800\n","Skipping y_hat=240\n","Skipping y_hat=240\n","Skipping y_hat=240\n","Skipping y_hat=240\n","Skipping y_hat=240\n","Skipping y_hat=240\n","Skipping y_hat=240\n"," 92% 73/79 [00:08<00:00,  8.88it/s]Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=915\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=258\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=952\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=390\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=903\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=569\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=145\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=813\n","Skipping y_hat=813\n","Skipping y_hat=813\n","Skipping y_hat=813\n","Skipping y_hat=813\n","Skipping y_hat=813\n","Skipping y_hat=813\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=669\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=093\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=582\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=445\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=055\n","Skipping y_hat=739\n","Skipping y_hat=739\n","Skipping y_hat=739\n","Skipping y_hat=739\n","Skipping y_hat=739\n","Skipping y_hat=739\n","Skipping y_hat=739\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=874\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=463\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=294\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=123\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=165\n","Skipping y_hat=825\n","Skipping y_hat=825\n","Skipping y_hat=825\n","Skipping y_hat=825\n","Skipping y_hat=825\n","Skipping y_hat=825\n","Skipping y_hat=825\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=858\n","Skipping y_hat=858\n","Skipping y_hat=858\n","Skipping y_hat=858\n","Skipping y_hat=858\n","Skipping y_hat=858\n","Skipping y_hat=858\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=615\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=497\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=050\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=906\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=196\n","Skipping y_hat=196\n","Skipping y_hat=196\n","Skipping y_hat=196\n","Skipping y_hat=196\n","Skipping y_hat=196\n","Skipping y_hat=196\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=430\n","Skipping y_hat=430\n","Skipping y_hat=430\n","Skipping y_hat=430\n","Skipping y_hat=430\n","Skipping y_hat=430\n","Skipping y_hat=430\n","Skipping y_hat=557\n","Skipping y_hat=557\n","Skipping y_hat=557\n","Skipping y_hat=557\n","Skipping y_hat=557\n","Skipping y_hat=557\n","Skipping y_hat=557\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=890\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=474\n","Skipping y_hat=474\n","Skipping y_hat=474\n","Skipping y_hat=474\n","Skipping y_hat=474\n","Skipping y_hat=474\n","Skipping y_hat=474\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=567\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=393\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=137\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=204\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=219\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=677\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=984\n","Skipping y_hat=953\n","Skipping y_hat=953\n","Skipping y_hat=953\n","Skipping y_hat=953\n","Skipping y_hat=953\n","Skipping y_hat=953\n","Skipping y_hat=953\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=086\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=835\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=064\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=167\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=501\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=781\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=025\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=718\n","Skipping y_hat=718\n","Skipping y_hat=718\n","Skipping y_hat=718\n","Skipping y_hat=718\n","Skipping y_hat=718\n","Skipping y_hat=718\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=655\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=399\n","Skipping y_hat=713\n","Skipping y_hat=713\n","Skipping y_hat=713\n","Skipping y_hat=713\n","Skipping y_hat=713\n","Skipping y_hat=713\n","Skipping y_hat=713\n","Skipping y_hat=680\n","Skipping y_hat=680\n","Skipping y_hat=680\n","Skipping y_hat=680\n","Skipping y_hat=680\n","Skipping y_hat=680\n","Skipping y_hat=680\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=647\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=517\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=361\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n","Skipping y_hat=551\n"," 94% 74/79 [00:08<00:00,  8.74it/s]Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=109\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=264\n","Skipping y_hat=809\n","Skipping y_hat=809\n","Skipping y_hat=809\n","Skipping y_hat=809\n","Skipping y_hat=809\n","Skipping y_hat=809\n","Skipping y_hat=809\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=550\n","Skipping y_hat=018\n","Skipping y_hat=018\n","Skipping y_hat=018\n","Skipping y_hat=018\n","Skipping y_hat=018\n","Skipping y_hat=018\n","Skipping y_hat=018\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=837\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=365\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=838\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=898\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=843\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=531\n","Skipping y_hat=531\n","Skipping y_hat=531\n","Skipping y_hat=531\n","Skipping y_hat=531\n","Skipping y_hat=531\n","Skipping y_hat=531\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=833\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=881\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=503\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=878\n","Skipping y_hat=878\n","Skipping y_hat=878\n","Skipping y_hat=878\n","Skipping y_hat=878\n","Skipping y_hat=878\n","Skipping y_hat=878\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=639\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=161\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=944\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=668\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=400\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=685\n","Skipping y_hat=273\n","Skipping y_hat=273\n","Skipping y_hat=273\n","Skipping y_hat=273\n","Skipping y_hat=273\n","Skipping y_hat=273\n","Skipping y_hat=273\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=263\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=017\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=146\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=000\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=141\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=836\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=446\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=692\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=231\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=200\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=974\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=656\n","Skipping y_hat=656\n","Skipping y_hat=656\n","Skipping y_hat=656\n","Skipping y_hat=656\n","Skipping y_hat=656\n","Skipping y_hat=656\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=579\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=152\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=210\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=490\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=820\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=645\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=754\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=197\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=213\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=686\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=519\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=202\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=659\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=185\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=142\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=591\n","Skipping y_hat=216\n","Skipping y_hat=216\n","Skipping y_hat=216\n","Skipping y_hat=216\n","Skipping y_hat=216\n","Skipping y_hat=216\n","Skipping y_hat=216\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=203\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=545\n","Skipping y_hat=545\n","Skipping y_hat=545\n","Skipping y_hat=545\n","Skipping y_hat=545\n","Skipping y_hat=545\n","Skipping y_hat=545\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=624\n","Skipping y_hat=052\n","Skipping y_hat=052\n","Skipping y_hat=052\n","Skipping y_hat=052\n","Skipping y_hat=052\n","Skipping y_hat=052\n","Skipping y_hat=052\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=221\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=031\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=148\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=534\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=546\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=646\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=175\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=305\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=864\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=337\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=140\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=300\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=251\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=416\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=041\n","Skipping y_hat=132\n","Skipping y_hat=132\n","Skipping y_hat=132\n","Skipping y_hat=132\n","Skipping y_hat=132\n","Skipping y_hat=132\n","Skipping y_hat=132\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=034\n","Skipping y_hat=034\n","Skipping y_hat=034\n","Skipping y_hat=034\n","Skipping y_hat=034\n","Skipping y_hat=034\n","Skipping y_hat=034\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=097\n","Skipping y_hat=024\n","Skipping y_hat=024\n","Skipping y_hat=024\n","Skipping y_hat=024\n","Skipping y_hat=024\n","Skipping y_hat=024\n","Skipping y_hat=024\n"," 95% 75/79 [00:08<00:00,  8.87it/s]Skipping y_hat=636\n","Skipping y_hat=636\n","Skipping y_hat=636\n","Skipping y_hat=636\n","Skipping y_hat=636\n","Skipping y_hat=636\n","Skipping y_hat=636\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=749\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=282\n","Skipping y_hat=266\n","Skipping y_hat=266\n","Skipping y_hat=266\n","Skipping y_hat=266\n","Skipping y_hat=266\n","Skipping y_hat=266\n","Skipping y_hat=266\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=015\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=003\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=581\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=401\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=252\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=149\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=170\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=618\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=392\n","Skipping y_hat=392\n","Skipping y_hat=392\n","Skipping y_hat=392\n","Skipping y_hat=392\n","Skipping y_hat=392\n","Skipping y_hat=392\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=187\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=709\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=158\n","Skipping y_hat=158\n","Skipping y_hat=158\n","Skipping y_hat=158\n","Skipping y_hat=158\n","Skipping y_hat=158\n","Skipping y_hat=158\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=509\n","Skipping y_hat=598\n","Skipping y_hat=598\n","Skipping y_hat=598\n","Skipping y_hat=598\n","Skipping y_hat=598\n","Skipping y_hat=598\n","Skipping y_hat=598\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=660\n","Skipping y_hat=660\n","Skipping y_hat=660\n","Skipping y_hat=660\n","Skipping y_hat=660\n","Skipping y_hat=660\n","Skipping y_hat=660\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=212\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=229\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=302\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=583\n","Skipping y_hat=528\n","Skipping y_hat=528\n","Skipping y_hat=528\n","Skipping y_hat=528\n","Skipping y_hat=528\n","Skipping y_hat=528\n","Skipping y_hat=528\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=895\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=151\n","Skipping y_hat=151\n","Skipping y_hat=151\n","Skipping y_hat=151\n","Skipping y_hat=151\n","Skipping y_hat=151\n","Skipping y_hat=151\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=561\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=191\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=459\n","Skipping y_hat=549\n","Skipping y_hat=549\n","Skipping y_hat=549\n","Skipping y_hat=549\n","Skipping y_hat=549\n","Skipping y_hat=549\n","Skipping y_hat=549\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=239\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=135\n","Skipping y_hat=135\n","Skipping y_hat=135\n","Skipping y_hat=135\n","Skipping y_hat=135\n","Skipping y_hat=135\n","Skipping y_hat=135\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=316\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=666\n","Skipping y_hat=042\n","Skipping y_hat=042\n","Skipping y_hat=042\n","Skipping y_hat=042\n","Skipping y_hat=042\n","Skipping y_hat=042\n","Skipping y_hat=042\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=281\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=755\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=378\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=876\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=092\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=241\n","Skipping y_hat=241\n","Skipping y_hat=241\n","Skipping y_hat=241\n","Skipping y_hat=241\n","Skipping y_hat=241\n","Skipping y_hat=241\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=410\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=850\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=683\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=792\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=777\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=009\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=795\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=633\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=465\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=125\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=970\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=784\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=734\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=045\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=099\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=331\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=071\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=658\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=215\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=779\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=280\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=095\n","Skipping y_hat=383\n","Skipping y_hat=383\n","Skipping y_hat=383\n","Skipping y_hat=383\n","Skipping y_hat=383\n","Skipping y_hat=383\n","Skipping y_hat=383\n","Skipping y_hat=314\n","Skipping y_hat=314\n","Skipping y_hat=314\n","Skipping y_hat=314\n","Skipping y_hat=314\n","Skipping y_hat=314\n","Skipping y_hat=314\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=744\n","Skipping y_hat=744\n","Skipping y_hat=744\n","Skipping y_hat=744\n","Skipping y_hat=744\n","Skipping y_hat=744\n","Skipping y_hat=744\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=091\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=274\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=275\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=193\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=447\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=458\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=885\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=054\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=006\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=398\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=076\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=637\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=319\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=347\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n"," 96% 76/79 [00:08<00:00,  8.96it/s]Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=127\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=565\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=773\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=207\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=133\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=461\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=048\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=840\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=491\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=271\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=950\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=335\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=673\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=371\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=013\n","Skipping y_hat=013\n","Skipping y_hat=013\n","Skipping y_hat=013\n","Skipping y_hat=013\n","Skipping y_hat=013\n","Skipping y_hat=013\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=859\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=100\n","Skipping y_hat=619\n","Skipping y_hat=619\n","Skipping y_hat=619\n","Skipping y_hat=619\n","Skipping y_hat=619\n","Skipping y_hat=619\n","Skipping y_hat=619\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=245\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=526\n","Skipping y_hat=732\n","Skipping y_hat=732\n","Skipping y_hat=732\n","Skipping y_hat=732\n","Skipping y_hat=732\n","Skipping y_hat=732\n","Skipping y_hat=732\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=338\n","Skipping y_hat=442\n","Skipping y_hat=442\n","Skipping y_hat=442\n","Skipping y_hat=442\n","Skipping y_hat=442\n","Skipping y_hat=442\n","Skipping y_hat=442\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=373\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=376\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=924\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=555\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=729\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=307\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=159\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=058\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=315\n","Skipping y_hat=247\n","Skipping y_hat=247\n","Skipping y_hat=247\n","Skipping y_hat=247\n","Skipping y_hat=247\n","Skipping y_hat=247\n","Skipping y_hat=247\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=357\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=059\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=726\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=218\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=642\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=724\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=516\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=285\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=694\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=107\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=523\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=943\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=278\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=759\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=573\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=169\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=077\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=324\n","Skipping y_hat=324\n","Skipping y_hat=324\n","Skipping y_hat=324\n","Skipping y_hat=324\n","Skipping y_hat=324\n","Skipping y_hat=324\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=866\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=128\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=011\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=672\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=407\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=586\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=065\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=403\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=817\n","Skipping y_hat=255\n","Skipping y_hat=255\n","Skipping y_hat=255\n","Skipping y_hat=255\n","Skipping y_hat=255\n","Skipping y_hat=255\n","Skipping y_hat=255\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=236\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=763\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=959\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=220\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=846\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=082\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=473\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=233\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=544\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=112\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=179\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=452\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=032\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=847\n","Skipping y_hat=847\n","Skipping y_hat=847\n","Skipping y_hat=847\n","Skipping y_hat=847\n","Skipping y_hat=847\n","Skipping y_hat=847\n","Skipping y_hat=481\n","Skipping y_hat=481\n","Skipping y_hat=481\n","Skipping y_hat=481\n","Skipping y_hat=481\n","Skipping y_hat=481\n","Skipping y_hat=481\n","Skipping y_hat=341\n","Skipping y_hat=341\n","Skipping y_hat=341\n","Skipping y_hat=341\n","Skipping y_hat=341\n","Skipping y_hat=341\n","Skipping y_hat=341\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=177\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=114\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=352\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=287\n","Skipping y_hat=287\n","Skipping y_hat=287\n","Skipping y_hat=287\n","Skipping y_hat=287\n","Skipping y_hat=287\n","Skipping y_hat=287\n"," 97% 77/79 [00:08<00:00,  8.97it/s]Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=638\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=414\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=564\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=761\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=883\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=422\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=110\n","Skipping y_hat=980\n","Skipping y_hat=980\n","Skipping y_hat=980\n","Skipping y_hat=980\n","Skipping y_hat=980\n","Skipping y_hat=980\n","Skipping y_hat=980\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=593\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=253\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=488\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=087\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=590\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=267\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=068\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=291\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=766\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=733\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=620\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=035\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=131\n","Skipping y_hat=131\n","Skipping y_hat=131\n","Skipping y_hat=131\n","Skipping y_hat=131\n","Skipping y_hat=131\n","Skipping y_hat=131\n","Skipping y_hat=379\n","Skipping y_hat=379\n","Skipping y_hat=379\n","Skipping y_hat=379\n","Skipping y_hat=379\n","Skipping y_hat=379\n","Skipping y_hat=379\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=332\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=424\n","Skipping y_hat=424\n","Skipping y_hat=424\n","Skipping y_hat=424\n","Skipping y_hat=424\n","Skipping y_hat=424\n","Skipping y_hat=424\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=485\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=367\n","Skipping y_hat=367\n","Skipping y_hat=367\n","Skipping y_hat=367\n","Skipping y_hat=367\n","Skipping y_hat=367\n","Skipping y_hat=367\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=828\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=026\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=046\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=156\n","Skipping y_hat=156\n","Skipping y_hat=156\n","Skipping y_hat=156\n","Skipping y_hat=156\n","Skipping y_hat=156\n","Skipping y_hat=156\n","Skipping y_hat=162\n","Skipping y_hat=162\n","Skipping y_hat=162\n","Skipping y_hat=162\n","Skipping y_hat=162\n","Skipping y_hat=162\n","Skipping y_hat=162\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=190\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=616\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=368\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=608\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=937\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=456\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=040\n","Skipping y_hat=914\n","Skipping y_hat=914\n","Skipping y_hat=914\n","Skipping y_hat=914\n","Skipping y_hat=914\n","Skipping y_hat=914\n","Skipping y_hat=914\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=330\n","Skipping y_hat=810\n","Skipping y_hat=810\n","Skipping y_hat=810\n","Skipping y_hat=810\n","Skipping y_hat=810\n","Skipping y_hat=810\n","Skipping y_hat=810\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=884\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=535\n","Skipping y_hat=115\n","Skipping y_hat=115\n","Skipping y_hat=115\n","Skipping y_hat=115\n","Skipping y_hat=115\n","Skipping y_hat=115\n","Skipping y_hat=115\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=949\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=470\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=172\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=750\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=079\n","Skipping y_hat=979\n","Skipping y_hat=979\n","Skipping y_hat=979\n","Skipping y_hat=979\n","Skipping y_hat=979\n","Skipping y_hat=979\n","Skipping y_hat=979\n","Skipping y_hat=967\n","Skipping y_hat=967\n","Skipping y_hat=967\n","Skipping y_hat=967\n","Skipping y_hat=967\n","Skipping y_hat=967\n","Skipping y_hat=967\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=986\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=706\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=222\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=130\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=644\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=717\n","Skipping y_hat=419\n","Skipping y_hat=419\n","Skipping y_hat=419\n","Skipping y_hat=419\n","Skipping y_hat=419\n","Skipping y_hat=419\n","Skipping y_hat=419\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=793\n","Skipping y_hat=237\n","Skipping y_hat=237\n","Skipping y_hat=237\n","Skipping y_hat=237\n","Skipping y_hat=237\n","Skipping y_hat=237\n","Skipping y_hat=237\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=033\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=395\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=265\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=411\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=770\n","Skipping y_hat=511\n","Skipping y_hat=511\n","Skipping y_hat=511\n","Skipping y_hat=511\n","Skipping y_hat=511\n","Skipping y_hat=511\n","Skipping y_hat=511\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=078\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=594\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=556\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=747\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=867\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=894\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=604\n","Skipping y_hat=143\n","Skipping y_hat=143\n","Skipping y_hat=143\n","Skipping y_hat=143\n","Skipping y_hat=143\n","Skipping y_hat=143\n","Skipping y_hat=143\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=083\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=515\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=257\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=662\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=719\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=297\n","Skipping y_hat=374\n","Skipping y_hat=374\n","Skipping y_hat=374\n","Skipping y_hat=374\n","Skipping y_hat=374\n","Skipping y_hat=374\n","Skipping y_hat=374\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=600\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=063\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=804\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=183\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=936\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=966\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=286\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=118\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=070\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=854\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=194\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=518\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=061\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=417\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=495\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=588\n","Skipping y_hat=588\n","Skipping y_hat=588\n","Skipping y_hat=588\n","Skipping y_hat=588\n","Skipping y_hat=588\n","Skipping y_hat=588\n"," 99% 78/79 [00:09<00:00,  8.55it/s]Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=918\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=577\n","Skipping y_hat=577\n","Skipping y_hat=577\n","Skipping y_hat=577\n","Skipping y_hat=577\n","Skipping y_hat=577\n","Skipping y_hat=577\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=225\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=740\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=192\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=355\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=084\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=321\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=129\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=075\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=796\n","Skipping y_hat=181\n","Skipping y_hat=181\n","Skipping y_hat=181\n","Skipping y_hat=181\n","Skipping y_hat=181\n","Skipping y_hat=181\n","Skipping y_hat=181\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=440\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=771\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","Skipping y_hat=327\n","100% 79/79 [00:09<00:00,  8.69it/s]\n","accuracy of 10000 examples: 14/10000 (0.13999999999999999%)\n","\n","Final Test Results:\n","test_reverse: 0.14%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m4_operands_0_to_999_output_wo_leading_digit_mi_reverse\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/66wftg6m\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m4_operands_0_to_999_output_wo_leading_digit/reverse_out_with_mi/wandb/run-20250727_165113-66wftg6m/logs\u001b[0m\n"]}],"source":["!python train_additional_measurement.py 4_operands_addition_reversed.txt"]},{"cell_type":"markdown","source":["# Subtraction"],"metadata":{"id":"nERObTRTDfMF"}},{"cell_type":"code","source":["%cat train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"oirCmufV347v","executionInfo":{"status":"ok","timestamp":1757075726323,"user_tz":300,"elapsed":632,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"f28e4dc8-25fe-4fac-c295-6e5fc6b34e6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["import os\n","import pickle\n","import requests\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","import copy\n","import time\n","\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import yaml\n","from torch.utils.data import Dataset, DataLoader\n","import wandb\n","import torch.nn.functional as F\n","import math\n","\n","from model import GPTConfig, GPT\n","from main_utilities import *\n","from evaluation import *\n","from statistical_measurements import *\n","\n","import re\n","\n","def create_meta_for_addition(data):\n","    \"\"\"Create metadata for addition data.\"\"\"\n","    # Define the vocabulary for addition problems\n","    # This includes digits, operators, equals sign, and newline\n","    chars = sorted(list(set(data)))\n","    vocab_size = len(chars)\n","    # Create encoder and decoder dictionaries\n","    stoi = {ch: i for i, ch in enumerate(chars)}\n","    itos = {i: ch for i, ch in enumerate(chars)}\n","    \n","    meta = {\n","        'vocab_size': vocab_size,\n","        'vocab': chars,\n","        'stoi': stoi,\n","        'itos': itos\n","    }\n","    return meta\n","\n","def encode_addition(text, meta):\n","    \"\"\"Encode text to tensor using the metadata.\"\"\"\n","    return torch.tensor([meta['stoi'][c] for c in text], dtype=torch.long)\n","\n","def decode_addition(tensor, meta):\n","    \"\"\"Decode tensor to text using the metadata.\"\"\"\n","    if isinstance(tensor, torch.Tensor):\n","        return ''.join([meta['itos'][i.item()] for i in tensor])\n","    else:\n","        return ''.join([meta['itos'][i] for i in tensor])\n","    \n","def pad_sequence(x: torch.Tensor, length: int, pad_value: int):\n","    if x.size(0) < length:\n","        padding = torch.full((length - x.size(0),), pad_value, dtype=torch.long)\n","        return torch.cat([x, padding], dim=0)\n","    else:\n","        return x\n","\n","class AdditionDataset(Dataset):\n","    def __init__(self, file_path, meta):\n","        self.meta = meta\n","        # Read the text file\n","        with open(file_path, 'r') as f:\n","            self.lines = f.readlines()\n","        # Remove any empty lines and strip whitespace\n","        self.lines = [line.strip() for line in self.lines if line.strip()]\n","        self.block_size = block_size  # from your config\n","        \n","    def __len__(self):\n","        return len(self.lines)\n","    \n","    def __getitem__(self, idx):\n","        line = self.lines[idx]\n","        # Convert the line to tensor using our encoder\n","        raw = encode_addition(line, self.meta)\n","        x = pad_sequence(raw[:-1], self.block_size, pad_value=meta['stoi']['$'])  # all but last char\n","        y = pad_sequence(raw[1:], self.block_size, pad_value=-1)   # all but first char\n","        return x, y\n","\n","# I/O\n","\n","out_dir = '/drive/MyDrive/addition/plain_no_pad/out'\n","resume_dir = None\n","resume_iter = False # if True, resume from saved iter_num, otherwise resume from iter_num 0\n","eval_interval = 2000\n","log_interval = 1\n","eval_iters = 200\n","eval_only = False # if True, script exits right after the first eval\n","always_save_checkpoint = True # if True, always save a checkpoint after each eval\n","init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n","\n","# wandb logging\n","wandb_entity = 'ssdd'\n","wandb_log = False # disabled by default\n","wandb_project = 'owt'\n","wandb_run_name = 'gpt2' # 'run' + str(time.time())\n","exp_name = 'default_exp_name'\n","\n","# data\n","dataset = 'bal'\n","gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n","test_batch_size = 128\n","batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n","block_size = 1024\n","train_data_path = 'train.bin'\n","val_data_path = 'val.bin'\n","multi_digit = False\n","num_digit = 3\n","max_new_tokens = 5\n","binary = False\n","\n","# using two data - data1 = text / data2 = addition\n","train_both = False # use seperate text/add data for train/val (get_batch uses this to sample from two differernt datasets)\n","data_ratio = 0.2 # ratio of data_path2 compared with data_path1\n","train_data_path2 = 'train_addition.bin' # only used when train_both = True\n","val_data_path2 = 'val_addition.bin'\n","\n","# evaluation\n","eval_text = False # if True get perplexity using eval_text_data_path\n","eval_text_data_path = None # directory to text data (.bin file) - ex. 'data/shakespeare_add_ar_mixed/val_text.bin'\n","eval_addition = False # if True compute test accuracy of \"a+b=\"\n","test_file_path = None\n","eval_addition_ar = False\n","start_ar = None\n","eval_other = False # use this to evaluate other operations (ex. train on operator '-' but evaluate on other_operator '+')\n","start_other = None\n","other_operator = '+'\n","eval_addition_train = False\n","start_train = None\n","reverse_ab = False\n","reverse_c = False\n","zero_pad = False\n","algo_reason = False\n","add_space = False\n","analysis = False\n","\n","# model\n","n_layer = 6\n","n_head = 6\n","n_embd = 768\n","dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n","bias = False # do we use bias inside LayerNorm and Linear layers?\n","ckpt_path_name = 'ckpt.pt'\n","save_final = True\n","\n","# adamw optimizer\n","learning_rate = 6e-4 # max learning rate\n","max_iters = 600000 # total number of training iterations\n","weight_decay = 1e-1\n","beta1 = 0.9\n","beta2 = 0.95\n","grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n","# learning rate decay settings\n","decay_lr = True # whether to decay the learning rate\n","warmup_iters = 2000 # how many steps to warm up for\n","lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n","min_lr = None # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n","\n","# DDP settings\n","backend = 'nccl' # 'nccl', 'gloo', etc.\n","# system\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n","dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n","compile = True # use PyTorch 2.0 to compile the model to be faster\n","use_flash = True\n","data_type = 'binary' # 'binary' by default, can be 'text'\n","operator = '+' # can be '+', '-', '*', 'sin', 'sqrt'\n","data_shuffle = True\n","data_format = 'plain' # 'plain' or 'reverse' or 'algo_reasoning'\n","vocabulary = 'all_ascii_chars' # can be 'all_ascii_chars' or 'numbers_only' or 'custom_input_data'\n","meta_path_specified = True # use saved meta_file (False if data_type='text')\n","eps = 0\n","tokenizer = 'char' # by default, use char level tokenizer. but for pretrained models, use openai tokenizer eg: 'gpt2'\n","\n","simple=False\n","random_A=False\n","random_C=False\n","\n","use_lora = False # use lora (from minLoRA)\n","print_interval = 2  # if we're using gpt-2 model, I want to see it prompted on text\n","\n","mode = \"compute_gold\"  # Mode for evaluation: \"compute_gold\" or \"read_gold_as_str\"\n","\n","more_early_eval1 = False # if True, do early, more frequent eval on train and val data\n","early_eval_interval1 = 25\n","early_eval_border1 = 1000\n","\n","more_early_eval2 = False # if True, do even earlier, even more frequent eval on train and val data\n","early_eval_interval2 = 5\n","early_eval_border2 = 500\n","\n","stats_measurement_data_file_path = \"\"\n","\n","drop_leading_digit = False\n","\n","config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str, type(None)))]\n","exec(open('configurator.py').read()) # overrides from command line or config file\n","config = {k: globals()[k] for k in config_keys} # will be useful for logging\n","\n","# additional statistical measurements\n","mi_measurement = False # whether to do mutual information measurement\n","early_mi_measure_border = 20000 # border for early mutual information measurement\n","early_mi_measure_interval = 1000 # interval for early mutual information measurement\n","final_mi_measure_interval = 5000 # interval for final mutual information measurement\n","\n","mi_measure_iters = set(\n","    list(range(0,  early_mi_measure_border, early_mi_measure_interval)) +    # every 20 steps before 200\n","    # list(range(100000, 100000, 20)) +   # every 50 steps from 200 up to 1500\n","    list(range(early_mi_measure_border, max_iters+1, final_mi_measure_interval))  # every 100 steps thereafter\n",")\n","\n","# function to set seed for all random number generators\n","def set_seed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    # to make sure GPU runs are deterministic even if they are slower set this to True\n","    torch.backends.cudnn.deterministic = False\n","    # warning: this causes the code to vary across runs\n","    torch.backends.cudnn.benchmark = True\n","    print(\"Seeded everything: {}\".format(seed))\n","\n","if min_lr == None:\n","    min_lr = learning_rate/10\n","master_process = True\n","seed_offset = 0\n","if master_process:\n","  os.makedirs(out_dir, exist_ok=True)\n","torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","torch.backends.cudnn.benchmark = True # cudnn auto-tuner\n","torch.backends.cudnn.deterministic = False # cudnn auto-tuner\n","# this is probably overkill but seed everything again\n","set_seed(1337 + seed_offset)\n","\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","# note: float16 data type will automatically use a GradScaler\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# Read the data files\n","with open(train_data_path, 'r') as f:\n","    train_data = f.read()\n","with open(val_data_path, 'r') as f:\n","    val_data = f.read()\n","\n","# Create metadata from the combined data\n","all_data = train_data + val_data\n","meta = create_meta_for_addition(train_data)\n","meta_vocab_size = meta['vocab_size']\n","print(f\"Using vocabulary size: {meta_vocab_size}\")\n","\n","config['eos_id'] = meta['stoi']['$']\n","\n","if mi_measurement:\n","    with open(stats_measurement_data_file_path, 'r', encoding='utf-8') as f:\n","        lines = [line.rstrip() for line in f]\n","\n","    if drop_leading_digit:\n","            S = num_digit\n","    else:\n","        S = num_digit + 1\n","    # a simple way to parse test strings\n","    padded_lines = [] # add 0 padding, remove $; an example padded_lines[6] is '932+084+230+349=5951'\n","    for i in range(len(lines)):\n","        numbers = re.split(r'[+=]', lines[i])\n","        numbers[-1] = numbers[-1][:-1]\n","        for k, number in enumerate(numbers[:-1]):\n","            numbers[k] = '0' * (3-len(number)) + number\n","        numbers[-1] = numbers[-1] + '0' * (S-len(numbers[-1]))\n","        padded_lines.append(\"+\".join(numbers[:-1]) + \"=\" + numbers[-1])\n","\n","    stats_measurement_data = torch.cat([encode_addition(padded_lines[i], meta).unsqueeze(0) for i in range(len(padded_lines))], dim=0)\n","\n","# # get 16 different datasets (including the base dataset) by randomizing input/output integers of the base dataset\n","# stats_measurement_dataset_list = gen_randomized_datasets(\n","#     stats_measurement_data,\n","#     meta,\n","#     digits_per_num=num_digit,\n","#     base_seed=2005,\n","#     reverse_input=reverse_ab,\n","#     reverse_output=reverse_c\n","# )\n","\n","# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n","iter_num = 0\n","best_val_loss = 1e9\n","best_perplexity = 1e9 # on text data\n","best_accuracy = -1 # on addition data\n","\n","\n","model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n","                  bias=bias, vocab_size=None, dropout=dropout, use_flash=use_flash) # start with model_args from command line\n","if init_from == 'scratch':\n","    # init a new model from scratch\n","    print(\"Initializing a new model from scratch\")\n","    # determine the vocab size we'll use for from-scratch training\n","    if meta_vocab_size is None:\n","        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n","    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    model.to(device)\n","elif init_from == 'resume':\n","    if resume_dir:\n","        print(f\"Resuming training from {resume_dir}\")\n","        checkpoint = torch.load(resume_dir, map_location=device)\n","    else:\n","        print(f\"Resuming training from {out_dir}\")\n","        # resume training from a checkpoint.\n","        ckpt_path = os.path.join(out_dir, ckpt_path_name)\n","        checkpoint = torch.load(ckpt_path, map_location=device)\n","    checkpoint_model_args = checkpoint['model_args']\n","    # force these config attributes to be equal otherwise we can't even resume training\n","    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n","    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n","        model_args[k] = checkpoint_model_args[k]\n","    # create the model\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    model.to(device)\n","    state_dict = checkpoint['model']\n","    # fix the keys of the state dictionary :(\n","    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)\n","    iter_num = checkpoint['iter_num'] if resume_iter else 0\n","    max_iters += iter_num\n","    best_val_loss = checkpoint['best_val_loss']\n","    if 'best_perplexity' in checkpoint.keys():\n","        best_perplexity = checkpoint['best_perplexity']\n","    if 'best_accuracy' in checkpoint.keys():\n","        best_accuracy = checkpoint['best_accuracy']\n","\n","scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","\n","optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n","\n","# compile the model\n","if compile:\n","    print(\"compiling the model... (takes a ~minute)\")\n","    unoptimized_model = model\n","    model = torch.compile(model) # requires PyTorch 2.0\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        # Get an iterator from the DataLoader\n","        dataloader = train_loader if split == 'train' else val_loader\n","        dataloader_iter = iter(dataloader)\n","\n","        for k in range(eval_iters):\n","            try:\n","                X, Y = next(dataloader_iter)\n","\n","            except StopIteration:\n","                # If we run out of batches, create a new iterator\n","                dataloader_iter = iter(dataloader)\n","                X, Y = next(dataloader_iter)\n","\n","            with ctx:\n","                X, Y = X.to(device), Y.to(device)\n","                logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","\n","def get_lr_for_iter(iter_num):\n","    \"\"\"Calculate learning rate based on iteration number using cosine decay with warmup.\"\"\"\n","    if iter_num < warmup_iters:\n","        return learning_rate * (iter_num + 1) / warmup_iters\n","    \n","    if iter_num >= lr_decay_iters:\n","        return min_lr\n","    \n","    decay_ratio = (iter_num - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n","    return min_lr + coeff * (learning_rate - min_lr)\n","\n","# logging\n","if wandb_log and master_process:\n","    import wandb\n","    wandb.init(project=wandb_project, name=wandb_run_name, config=config, dir = out_dir)\n","\n","\n","\n","\n","train_dataset = AdditionDataset(train_data_path, meta)\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    pin_memory=(device_type=='cuda')\n",")\n","\n","val_dataset = AdditionDataset(val_data_path, meta)\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    pin_memory=(device_type=='cuda')\n",")\n","\n","# encode, decode = get_encode_decode(meta_path, tokenizer=tokenizer)\n","\n","# Initialize result_dict with basic metrics\n","result_dict = {\n","    'iter': [],\n","    'train_loss': [],\n","    'val_loss': [],\n","    'test_acc': [],\n","    'train_acc': []\n","}\n","\n","# Initialize test accuracy keys for all test files\n","result_dict[f'test_acc'] = []\n","\n","result_dir = get_results_dir(config)\n","config['result_dir'] = result_dir\n","with open(os.path.join(result_dir, \"config.yaml\"), \"w\") as yaml_file:\n","    yaml.dump(config, yaml_file, default_flow_style=False)\n","\n","\n","# # build a dict of open file handles, one per dataset\n","# csv_writers = {}\n","# for dataset in stats_measurement_dataset_list:\n","#     name = dataset['name']\n","#     path = os.path.join(result_dir, f\"{name}_stats.csv\")\n","#     f = open(path, 'w', newline='')\n","#     writer = csv.DictWriter(f, fieldnames=[\n","#         'iter',\n","#         'ave_correct_probs',\n","#         'ave_correct_preds',\n","#         'ave_diff_probs_L1',\n","#         'ave_diff_probs_L2',\n","#         'ave_diff_probs_kl',\n","#         'ave_diff_logits_L1',\n","#         'ave_diff_logits_L2',\n","#         'ave_diff_preds',\n","#     ])\n","#     writer.writeheader()\n","#     csv_writers[name] = writer\n","\n","\n","# Initialize additional metrics for statistical measurements\n","stats_oo = [] # output-output mutual information\n","stats_io = [] # input-output mutual information\n","\n","\n","import time\n","t0 = time.time()\n","local_iter_num = 0 # number of iterations in the lifetime of this process\n","raw_model = model\n","running_mfu = -1.0\n","iter_num = 0\n","\n","max_iters = config.get('max_iters', 10000)\n"," # number of epochs to warm up learning rate\n","\n","# Initialize tracking variables\n","iter_num = 0\n","best_val_loss = 1e9\n","best_accuracy = -1\n","running_mfu = -1.0\n","\n","# Create infinite data loader\n","def get_infinite_dataloader(dataloader):\n","    while True:\n","        for batch in dataloader:\n","            yield batch\n","\n","train_loader_iter = get_infinite_dataloader(train_loader)\n","if 'max_new_tokens' in config.keys():\n","    print(f\"max_new_tokens: {config['max_new_tokens']}\")\n","else:\n","    print(f\"max_new_tokens used: {num_digit+2}\")\n","\n","# Training loop - iteration based\n","while iter_num < max_iters:\n","    model.train()\n","    \n","    # Get learning rate for current iteration\n","    if decay_lr:\n","        lr = get_lr_for_iter(iter_num)\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","    \n","    # Get next batch\n","    X, Y = next(train_loader_iter)\n","    X, Y = X.to(device), Y.to(device)\n","    \n","    # Forward pass\n","    with ctx:\n","        logits, loss = model(X, Y)\n","    \n","    # Backward pass\n","    scaler.scale(loss).backward()\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    scaler.step(optimizer)\n","    scaler.update()\n","    optimizer.zero_grad(set_to_none=True)\n","    \n","    # Do additional statistical measurements\n","    if mi_measurement:\n","        if iter_num in mi_measure_iters:\n","            model.eval()\n","            \n","            with torch.no_grad():\n","                # eval_res = eval_model(model, meta, stats_measurement_dataset_list, digits_per_num=num_digit, batch_size=test_batch_size)\n","                mi_stats = calc_model_dataset_mi(\n","                    model = model,\n","                    metadata = meta,\n","                    data = stats_measurement_data,\n","                    digits_per_num = num_digit,\n","                    batch_size = test_batch_size,\n","                    drop_leading_digit = drop_leading_digit\n","                )\n","\n","            # for name, stats in eval_res.items():\n","            #     if name == \"model_embeddings\":\n","            #         continue\n","            #     if name == 'base':\n","            #         row = {\n","            #             'iter': iter_num,\n","            #             'ave_correct_probs': stats['ave_correct_probs'],\n","            #             'ave_correct_preds': stats['ave_correct_preds'],\n","            #         }\n","            #     else:\n","            #         row = {\n","            #             'iter': iter_num,\n","            #             'ave_correct_probs': stats['ave_correct_probs'],\n","            #             'ave_correct_preds': stats['ave_correct_preds'],\n","            #             'ave_diff_probs_L1': stats['ave_diff_probs_L1'],\n","            #             'ave_diff_probs_L2': stats['ave_diff_probs_L2'],\n","            #             'ave_diff_probs_kl': stats['ave_diff_probs_kl'],\n","            #             'ave_diff_logits_L1': stats['ave_diff_logits_L1'],\n","            #             'ave_diff_logits_L2': stats['ave_diff_logits_L2'],\n","            #             'ave_diff_preds': stats['ave_diff_preds'],\n","            #         }\n","            #     # Write to the CSV file for this dataset\n","            #     csv_writers[name].writerow(row)\n","\n","            \n","            # Calculate output-output mutual information\n","            mi_mat = mi_stats['output-output']['mutual_info']\n","            nmi_mat = mi_stats['output-output']['normalized_mutual_info']\n","            for i in range(mi_mat.shape[0]):\n","                for j in range(i, mi_mat.shape[1]):\n","                    stats_oo.append({\n","                        'iter': iter_num,\n","                        'i': i,\n","                        'j': j,\n","                        'mi': mi_mat[i, j].item(),\n","                        'nmi': nmi_mat[i, j].item()\n","                    })\n","\n","            # also calculate input-output mutual information\n","            mi_mat_io = mi_stats['input-output']['mutual_info']\n","            nmi_mat_io = mi_stats['input-output']['normalized_mutual_info']\n","            for i in range(mi_mat_io.shape[0]):\n","                for j in range(mi_mat_io.shape[1]):\n","                    stats_io.append({\n","                        'iter': iter_num,\n","                        'i': i,\n","                        'j': j,\n","                        'mi': mi_mat_io[i, j].item(),\n","                        'nmi': nmi_mat_io[i, j].item()\n","                    })\n","\n","            # **NOW write out the two MI CSVs immediately:**\n","            stats_oo_df = pd.DataFrame(stats_oo)\n","            stats_oo_df.to_csv(os.path.join(result_dir, 'output_output_mi.csv'), index=False)\n","\n","            stats_io_df = pd.DataFrame(stats_io)\n","            stats_io_df.to_csv(os.path.join(result_dir, 'input_output_mi.csv'), index=False)\n","\n","            model.train()\n","        \n","    # Evaluation\n","    if iter_num % eval_interval == 0 or (more_early_eval1 and iter_num <= early_eval_border1 and iter_num % early_eval_interval1 == 0) or (more_early_eval2 and iter_num <= early_eval_border2 and iter_num % early_eval_interval2 == 0):\n","        losses = estimate_loss()\n","        print(f\"iter {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        \n","        # Initialize wandb_dict for this iteration\n","        wandb_dict = {\n","            \"iter\": iter_num,\n","            \"train/loss\": losses['train'],\n","            \"val/loss\": losses['val'],\n","            \"lr\": lr,\n","        }\n","\n","        if losses['val'] < best_val_loss:\n","            best_val_loss = losses['val']\n","        \n","        # Regular test evaluation\n","        test_accuracy = None\n","        if eval_addition:\n","            test_name, test_accuracy, _ , correct, incorrect = evaluate_multiple_files(\n","                config, model, ctx,\n","                encode=lambda x: encode_addition(x, meta),\n","                decode=lambda x: decode_addition(x, meta),\n","                test_file=test_file_path,\n","                iter_num=iter_num,\n","                result_dir=result_dir,\n","                verbose=False,\n","                num_digit=num_digit,\n","                zero_pad=zero_pad,\n","                reverse_ab=reverse_ab,\n","                reverse_c=reverse_c,\n","                data_type=data_type,\n","                operator=operator,\n","                data_format=data_format,\n","                analyze=True,\n","                mode=mode\n","            )\n","\n","            # Log results\n","            print(\"\\nTest Results:\")\n","            print(f\"{test_name}: {test_accuracy:.2f}%\")\n","\n","            print()\n","            \n","            # Add test accuracy to wandb_dict\n","            wandb_dict[\"test/accuracy\"] = test_accuracy\n","            \n","            if test_accuracy > best_accuracy and iter_num % 5 * eval_interval == 0:\n","                best_accuracy = test_accuracy\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'best_accuracy': best_accuracy,\n","                    'config': config,\n","                    'meta': meta,\n","                }\n","                torch.save(checkpoint, os.path.join(out_dir, f'ckpt_iter_{iter_num}_acc.pt'))\n","        \n","        # Training data evaluation\n","        train_accuracy = None\n","        if eval_addition_train:\n","            config['start'] = f\"FILE:{start_train}\"\n","            train_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","                config, model, ctx, \n","                encode=lambda x: encode_addition(x, meta),\n","                decode=lambda x: decode_addition(x, meta), \n","                verbose=False, \n","                num_digit=num_digit, \n","                zero_pad=zero_pad,\n","                reverse_ab=reverse_ab, \n","                reverse_c=reverse_c,\n","                data_type=data_type, \n","                operator=operator, \n","                data_format=data_format,\n","                mode=mode\n","            )\n","            \n","            # Add train accuracy to wandb_dict\n","            wandb_dict[\"train/accuracy\"] = train_accuracy\n","        \n","        # Update and save basic metrics\n","        result_dict['iter'].append(iter_num)\n","        result_dict['train_loss'].append(losses['train'].item())\n","        result_dict['val_loss'].append(losses['val'].item())\n","        result_dict['test_acc'].append(test_accuracy)\n","        result_dict['train_acc'].append(train_accuracy)\n","        \n","        # Save results to CSV after each evaluation\n","        result_df = pd.DataFrame(result_dict)\n","        result_df.to_csv(os.path.join(result_dir, 'training_metrics.csv'), index=False)\n","        \n","        # Single wandb log per iteration with all metrics\n","        if wandb_log:\n","            wandb.log(wandb_dict)\n","    \n","    iter_num += 1\n","\n","# Save final checkpoint\n","checkpoint = {\n","    'model': raw_model.state_dict(),\n","    'optimizer': optimizer.state_dict(),\n","    'model_args': model_args,\n","    'iter_num': iter_num,\n","    'best_val_loss': best_val_loss,\n","    'best_accuracy': best_accuracy,\n","    'config': config,\n","    'meta': meta,\n","}\n","torch.save(checkpoint, os.path.join(out_dir, f'ckpt_final.pt'))\n","\n","\n","losses = estimate_loss()\n","\n","if eval_addition:\n","    config['start'] = f\"FILE:{test_file_path}\"\n","    test_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","        config, model, ctx, \n","        encode=lambda x: encode_addition(x, meta),\n","        decode=lambda x: decode_addition(x, meta), \n","        verbose=False, \n","        num_digit=num_digit, \n","        zero_pad=zero_pad,\n","        reverse_ab=reverse_ab, \n","        reverse_c=reverse_c,\n","        data_type=data_type, \n","        operator=operator, \n","        data_format=data_format, \n","        analyze=True,\n","        mode=mode\n","    )\n","    import csv\n","    # Save correct examples\n","    correct_path = os.path.join(result_dir, 'correct_examples.csv')\n","    with open(correct_path, 'w', newline='') as csvfile:\n","        fieldnames = ['operands', 'result', 'outcome', 'c_hat2']\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","        writer.writeheader()\n","        for i, nums in enumerate(correct):\n","            operands, result, outcome, c_hat2 = nums\n","            writer.writerow({'operands': operands, 'result': result, 'outcome': outcome, 'c_hat2': c_hat2})\n","    \n","    # Save incorrect examples\n","    incorrect_path = os.path.join(result_dir, 'incorrect_examples.csv')\n","    with open(incorrect_path, 'w', newline='') as csvfile:\n","        fieldnames = ['operands', 'result', 'outcome', 'c_hat2']\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","        writer.writeheader()\n","        for i, nums in enumerate(incorrect):\n","            operands, result, outcome, c_hat2 = nums\n","            writer.writerow({'operands': operands, 'result': result, 'outcome': outcome, 'c_hat2': c_hat2})\n","\n","if eval_addition_train:\n","    config['start'] = f\"FILE:{start_train}\"\n","    train_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","        config, model, ctx, \n","        encode=lambda x: encode_addition(x, meta),\n","        decode=lambda x: decode_addition(x, meta), \n","        verbose=False, \n","        num_digit=num_digit, \n","        zero_pad=zero_pad,\n","        reverse_ab=reverse_ab, \n","        reverse_c=reverse_c,\n","        data_type=data_type, \n","        operator=operator, \n","        data_format=data_format,\n","        mode=mode\n","    )\n","    \n","    \n","test_name, accuracy, metrics, correct, incorrect = evaluate_multiple_files(\n","    config, model, ctx,\n","    encode=lambda x: encode_addition(x, meta),\n","    decode=lambda x: decode_addition(x, meta),\n","    test_file=test_file_path,\n","    iter_num='final',\n","    result_dir=result_dir,\n","    verbose=False,\n","    num_digit=num_digit,\n","    zero_pad=zero_pad,\n","    reverse_ab=reverse_ab,\n","    reverse_c=reverse_c,\n","    data_type=data_type,\n","    operator=operator,\n","    data_format=data_format,\n","    analyze=True,\n","    mode=mode\n",")\n","\n","print(\"\\nFinal Test Results:\")\n","print(f\"{test_name}: {accuracy:.2f}%\")\n","print()\n","\n","\n","# Final wandb logging\n","if wandb_log:\n","    final_dict = {\n","        \"iter\": iter_num,\n","        \"train/loss\": losses['train'],\n","        \"val/loss\": losses['val'],\n","        \"lr\": lr,\n","        \"test/accuracy\": test_accuracy if eval_addition else None,\n","        \"train/accuracy\": train_accuracy if eval_addition_train else None,\n","    }\n","    final_dict[f\"final_test/accuracy\"] = accuracy\n","    wandb.log(final_dict)\n","\n","# Save final DataFrame\n","result_df = pd.DataFrame(result_dict)\n","result_df.to_csv(os.path.join(result_dir, 'training_metrics.csv'), index=False)\n"]}]},{"cell_type":"code","source":["!python train.py 2_operands_subtraction_reversed.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CN0FYmVoDiA6","executionInfo":{"status":"ok","timestamp":1757163958142,"user_tz":300,"elapsed":1041772,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"590eef41-1784-496a-8684-198cdaabe560","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=-206\n","Skipping y_hat=-206\n","Skipping y_hat=-206\n","Skipping y_hat=-206\n","Skipping y_hat=-206\n","Skipping y_hat=-206\n","Skipping y_hat=-206\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+415\n","Skipping y_hat=+415\n","Skipping y_hat=+415\n","Skipping y_hat=+415\n","Skipping y_hat=+415\n","Skipping y_hat=+415\n","Skipping y_hat=+415\n","Skipping y_hat=-418\n","Skipping y_hat=-418\n","Skipping y_hat=-418\n","Skipping y_hat=-418\n","Skipping y_hat=-418\n","Skipping y_hat=-418\n","Skipping y_hat=-418\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=+1\n","Skipping y_hat=+1\n","Skipping y_hat=+1\n","Skipping y_hat=+1\n","Skipping y_hat=+1\n","Skipping y_hat=+1\n","Skipping y_hat=+1\n","Skipping y_hat=-603\n","Skipping y_hat=-603\n","Skipping y_hat=-603\n","Skipping y_hat=-603\n","Skipping y_hat=-603\n","Skipping y_hat=-603\n","Skipping y_hat=-603\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=+144\n","Skipping y_hat=+144\n","Skipping y_hat=+144\n","Skipping y_hat=+144\n","Skipping y_hat=+144\n","Skipping y_hat=+144\n","Skipping y_hat=+144\n","Skipping y_hat=+280\n","Skipping y_hat=+280\n","Skipping y_hat=+280\n","Skipping y_hat=+280\n","Skipping y_hat=+280\n","Skipping y_hat=+280\n","Skipping y_hat=+280\n","Skipping y_hat=-17\n","Skipping y_hat=-17\n","Skipping y_hat=-17\n","Skipping y_hat=-17\n","Skipping y_hat=-17\n","Skipping y_hat=-17\n","Skipping y_hat=-17\n","Skipping y_hat=+347\n","Skipping y_hat=+347\n","Skipping y_hat=+347\n","Skipping y_hat=+347\n","Skipping y_hat=+347\n","Skipping y_hat=+347\n","Skipping y_hat=+347\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n","Skipping y_hat=+558\n","Skipping y_hat=+558\n","Skipping y_hat=+558\n","Skipping y_hat=+558\n","Skipping y_hat=+558\n","Skipping y_hat=+558\n","Skipping y_hat=+558\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=-161\n","Skipping y_hat=-161\n","Skipping y_hat=-161\n","Skipping y_hat=-161\n","Skipping y_hat=-161\n","Skipping y_hat=-161\n","Skipping y_hat=-161\n"," 72% 18/25 [00:07<00:02,  3.15it/s]Skipping y_hat=-363\n","Skipping y_hat=-363\n","Skipping y_hat=-363\n","Skipping y_hat=-363\n","Skipping y_hat=-363\n","Skipping y_hat=-363\n","Skipping y_hat=-363\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=+576\n","Skipping y_hat=+576\n","Skipping y_hat=+576\n","Skipping y_hat=+576\n","Skipping y_hat=+576\n","Skipping y_hat=+576\n","Skipping y_hat=+576\n","Skipping y_hat=+394\n","Skipping y_hat=+394\n","Skipping y_hat=+394\n","Skipping y_hat=+394\n","Skipping y_hat=+394\n","Skipping y_hat=+394\n","Skipping y_hat=+394\n","Skipping y_hat=+216\n","Skipping y_hat=+216\n","Skipping y_hat=+216\n","Skipping y_hat=+216\n","Skipping y_hat=+216\n","Skipping y_hat=+216\n","Skipping y_hat=+216\n","Skipping y_hat=+77\n","Skipping y_hat=+77\n","Skipping y_hat=+77\n","Skipping y_hat=+77\n","Skipping y_hat=+77\n","Skipping y_hat=+77\n","Skipping y_hat=+77\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-273\n","Skipping y_hat=-273\n","Skipping y_hat=-273\n","Skipping y_hat=-273\n","Skipping y_hat=-273\n","Skipping y_hat=-273\n","Skipping y_hat=-273\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=+385\n","Skipping y_hat=+385\n","Skipping y_hat=+385\n","Skipping y_hat=+385\n","Skipping y_hat=+385\n","Skipping y_hat=+385\n","Skipping y_hat=+385\n","Skipping y_hat=+208\n","Skipping y_hat=+208\n","Skipping y_hat=+208\n","Skipping y_hat=+208\n","Skipping y_hat=+208\n","Skipping y_hat=+208\n","Skipping y_hat=+208\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+384\n","Skipping y_hat=+384\n","Skipping y_hat=+384\n","Skipping y_hat=+384\n","Skipping y_hat=+384\n","Skipping y_hat=+384\n","Skipping y_hat=+384\n","Skipping y_hat=+559\n","Skipping y_hat=+559\n","Skipping y_hat=+559\n","Skipping y_hat=+559\n","Skipping y_hat=+559\n","Skipping y_hat=+559\n","Skipping y_hat=+559\n","Skipping y_hat=+56\n","Skipping y_hat=+56\n","Skipping y_hat=+56\n","Skipping y_hat=+56\n","Skipping y_hat=+56\n","Skipping y_hat=+56\n","Skipping y_hat=+56\n","Skipping y_hat=-10\n","Skipping y_hat=-10\n","Skipping y_hat=-10\n","Skipping y_hat=-10\n","Skipping y_hat=-10\n","Skipping y_hat=-10\n","Skipping y_hat=-10\n","Skipping y_hat=+182\n","Skipping y_hat=+182\n","Skipping y_hat=+182\n","Skipping y_hat=+182\n","Skipping y_hat=+182\n","Skipping y_hat=+182\n","Skipping y_hat=+182\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=-146\n","Skipping y_hat=-146\n","Skipping y_hat=-146\n","Skipping y_hat=-146\n","Skipping y_hat=-146\n","Skipping y_hat=-146\n","Skipping y_hat=-146\n","Skipping y_hat=-48\n","Skipping y_hat=-48\n","Skipping y_hat=-48\n","Skipping y_hat=-48\n","Skipping y_hat=-48\n","Skipping y_hat=-48\n","Skipping y_hat=-48\n","Skipping y_hat=+132\n","Skipping y_hat=+132\n","Skipping y_hat=+132\n","Skipping y_hat=+132\n","Skipping y_hat=+132\n","Skipping y_hat=+132\n","Skipping y_hat=+132\n","Skipping y_hat=+329\n","Skipping y_hat=+329\n","Skipping y_hat=+329\n","Skipping y_hat=+329\n","Skipping y_hat=+329\n","Skipping y_hat=+329\n","Skipping y_hat=+329\n","Skipping y_hat=+764\n","Skipping y_hat=+764\n","Skipping y_hat=+764\n","Skipping y_hat=+764\n","Skipping y_hat=+764\n","Skipping y_hat=+764\n","Skipping y_hat=+764\n","Skipping y_hat=-159\n","Skipping y_hat=-159\n","Skipping y_hat=-159\n","Skipping y_hat=-159\n","Skipping y_hat=-159\n","Skipping y_hat=-159\n","Skipping y_hat=-159\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=+223\n","Skipping y_hat=+223\n","Skipping y_hat=+223\n","Skipping y_hat=+223\n","Skipping y_hat=+223\n","Skipping y_hat=+223\n","Skipping y_hat=+223\n","Skipping y_hat=-382\n","Skipping y_hat=-382\n","Skipping y_hat=-382\n","Skipping y_hat=-382\n","Skipping y_hat=-382\n","Skipping y_hat=-382\n","Skipping y_hat=-382\n","Skipping y_hat=-145\n","Skipping y_hat=-145\n","Skipping y_hat=-145\n","Skipping y_hat=-145\n","Skipping y_hat=-145\n","Skipping y_hat=-145\n","Skipping y_hat=-145\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=+491\n","Skipping y_hat=+491\n","Skipping y_hat=+491\n","Skipping y_hat=+491\n","Skipping y_hat=+491\n","Skipping y_hat=+491\n","Skipping y_hat=+491\n","Skipping y_hat=-140\n","Skipping y_hat=-140\n","Skipping y_hat=-140\n","Skipping y_hat=-140\n","Skipping y_hat=-140\n","Skipping y_hat=-140\n","Skipping y_hat=-140\n","Skipping y_hat=+475\n","Skipping y_hat=+475\n","Skipping y_hat=+475\n","Skipping y_hat=+475\n","Skipping y_hat=+475\n","Skipping y_hat=+475\n","Skipping y_hat=+475\n","Skipping y_hat=-366\n","Skipping y_hat=-366\n","Skipping y_hat=-366\n","Skipping y_hat=-366\n","Skipping y_hat=-366\n","Skipping y_hat=-366\n","Skipping y_hat=-366\n","Skipping y_hat=+220\n","Skipping y_hat=+220\n","Skipping y_hat=+220\n","Skipping y_hat=+220\n","Skipping y_hat=+220\n","Skipping y_hat=+220\n","Skipping y_hat=+220\n","Skipping y_hat=+11\n","Skipping y_hat=+11\n","Skipping y_hat=+11\n","Skipping y_hat=+11\n","Skipping y_hat=+11\n","Skipping y_hat=+11\n","Skipping y_hat=+11\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=-504\n","Skipping y_hat=-504\n","Skipping y_hat=-504\n","Skipping y_hat=-504\n","Skipping y_hat=-504\n","Skipping y_hat=-504\n","Skipping y_hat=-504\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=-310\n","Skipping y_hat=+303\n","Skipping y_hat=+303\n","Skipping y_hat=+303\n","Skipping y_hat=+303\n","Skipping y_hat=+303\n","Skipping y_hat=+303\n","Skipping y_hat=+303\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=+330\n","Skipping y_hat=+330\n","Skipping y_hat=+330\n","Skipping y_hat=+330\n","Skipping y_hat=+330\n","Skipping y_hat=+330\n","Skipping y_hat=+330\n","Skipping y_hat=+471\n","Skipping y_hat=+471\n","Skipping y_hat=+471\n","Skipping y_hat=+471\n","Skipping y_hat=+471\n","Skipping y_hat=+471\n","Skipping y_hat=+471\n","Skipping y_hat=-651\n","Skipping y_hat=-651\n","Skipping y_hat=-651\n","Skipping y_hat=-651\n","Skipping y_hat=-651\n","Skipping y_hat=-651\n","Skipping y_hat=-651\n","Skipping y_hat=-493\n","Skipping y_hat=-493\n","Skipping y_hat=-493\n","Skipping y_hat=-493\n","Skipping y_hat=-493\n","Skipping y_hat=-493\n","Skipping y_hat=-493\n","Skipping y_hat=+204\n","Skipping y_hat=+204\n","Skipping y_hat=+204\n","Skipping y_hat=+204\n","Skipping y_hat=+204\n","Skipping y_hat=+204\n","Skipping y_hat=+204\n","Skipping y_hat=-143\n","Skipping y_hat=-143\n","Skipping y_hat=-143\n","Skipping y_hat=-143\n","Skipping y_hat=-143\n","Skipping y_hat=-143\n","Skipping y_hat=-143\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=-433\n","Skipping y_hat=-433\n","Skipping y_hat=-433\n","Skipping y_hat=-433\n","Skipping y_hat=-433\n","Skipping y_hat=-433\n","Skipping y_hat=-433\n","Skipping y_hat=+447\n","Skipping y_hat=+447\n","Skipping y_hat=+447\n","Skipping y_hat=+447\n","Skipping y_hat=+447\n","Skipping y_hat=+447\n","Skipping y_hat=+447\n","Skipping y_hat=-37\n","Skipping y_hat=-37\n","Skipping y_hat=-37\n","Skipping y_hat=-37\n","Skipping y_hat=-37\n","Skipping y_hat=-37\n","Skipping y_hat=-37\n","Skipping y_hat=-102\n","Skipping y_hat=-102\n","Skipping y_hat=-102\n","Skipping y_hat=-102\n","Skipping y_hat=-102\n","Skipping y_hat=-102\n","Skipping y_hat=-102\n","Skipping y_hat=-601\n","Skipping y_hat=-601\n","Skipping y_hat=-601\n","Skipping y_hat=-601\n","Skipping y_hat=-601\n","Skipping y_hat=-601\n","Skipping y_hat=-601\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=+393\n","Skipping y_hat=+393\n","Skipping y_hat=+393\n","Skipping y_hat=+393\n","Skipping y_hat=+393\n","Skipping y_hat=+393\n","Skipping y_hat=+393\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-23\n","Skipping y_hat=-23\n","Skipping y_hat=-23\n","Skipping y_hat=-23\n","Skipping y_hat=-23\n","Skipping y_hat=-23\n","Skipping y_hat=-23\n","Skipping y_hat=+367\n","Skipping y_hat=+367\n","Skipping y_hat=+367\n","Skipping y_hat=+367\n","Skipping y_hat=+367\n","Skipping y_hat=+367\n","Skipping y_hat=+367\n","Skipping y_hat=+110\n","Skipping y_hat=+110\n","Skipping y_hat=+110\n","Skipping y_hat=+110\n","Skipping y_hat=+110\n","Skipping y_hat=+110\n","Skipping y_hat=+110\n","Skipping y_hat=-668\n","Skipping y_hat=-668\n","Skipping y_hat=-668\n","Skipping y_hat=-668\n","Skipping y_hat=-668\n","Skipping y_hat=-668\n","Skipping y_hat=-668\n","Skipping y_hat=-160\n","Skipping y_hat=-160\n","Skipping y_hat=-160\n","Skipping y_hat=-160\n","Skipping y_hat=-160\n","Skipping y_hat=-160\n","Skipping y_hat=-160\n","Skipping y_hat=+425\n","Skipping y_hat=+425\n","Skipping y_hat=+425\n","Skipping y_hat=+425\n","Skipping y_hat=+425\n","Skipping y_hat=+425\n","Skipping y_hat=+425\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=+266\n","Skipping y_hat=+266\n","Skipping y_hat=+266\n","Skipping y_hat=+266\n","Skipping y_hat=+266\n","Skipping y_hat=+266\n","Skipping y_hat=+266\n","Skipping y_hat=-215\n","Skipping y_hat=-215\n","Skipping y_hat=-215\n","Skipping y_hat=-215\n","Skipping y_hat=-215\n","Skipping y_hat=-215\n","Skipping y_hat=-215\n","Skipping y_hat=+481\n","Skipping y_hat=+481\n","Skipping y_hat=+481\n","Skipping y_hat=+481\n","Skipping y_hat=+481\n","Skipping y_hat=+481\n","Skipping y_hat=+481\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+33\n","Skipping y_hat=+33\n","Skipping y_hat=+33\n","Skipping y_hat=+33\n","Skipping y_hat=+33\n","Skipping y_hat=+33\n","Skipping y_hat=+33\n","Skipping y_hat=+414\n","Skipping y_hat=+414\n","Skipping y_hat=+414\n","Skipping y_hat=+414\n","Skipping y_hat=+414\n","Skipping y_hat=+414\n","Skipping y_hat=+414\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+190\n","Skipping y_hat=+190\n","Skipping y_hat=+190\n","Skipping y_hat=+190\n","Skipping y_hat=+190\n","Skipping y_hat=+190\n","Skipping y_hat=+190\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=-41\n","Skipping y_hat=-469\n","Skipping y_hat=-469\n","Skipping y_hat=-469\n","Skipping y_hat=-469\n","Skipping y_hat=-469\n","Skipping y_hat=-469\n","Skipping y_hat=-469\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=+122\n","Skipping y_hat=+122\n","Skipping y_hat=+122\n","Skipping y_hat=+122\n","Skipping y_hat=+122\n","Skipping y_hat=+122\n","Skipping y_hat=+122\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+9\n","Skipping y_hat=+192\n","Skipping y_hat=+192\n","Skipping y_hat=+192\n","Skipping y_hat=+192\n","Skipping y_hat=+192\n","Skipping y_hat=+192\n","Skipping y_hat=+192\n","Skipping y_hat=-47\n","Skipping y_hat=-47\n","Skipping y_hat=-47\n","Skipping y_hat=-47\n","Skipping y_hat=-47\n","Skipping y_hat=-47\n","Skipping y_hat=-47\n","Skipping y_hat=+197\n","Skipping y_hat=+197\n","Skipping y_hat=+197\n","Skipping y_hat=+197\n","Skipping y_hat=+197\n","Skipping y_hat=+197\n","Skipping y_hat=+197\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=+170\n","Skipping y_hat=-247\n","Skipping y_hat=-247\n","Skipping y_hat=-247\n","Skipping y_hat=-247\n","Skipping y_hat=-247\n","Skipping y_hat=-247\n","Skipping y_hat=-247\n","Skipping y_hat=-345\n","Skipping y_hat=-345\n","Skipping y_hat=-345\n","Skipping y_hat=-345\n","Skipping y_hat=-345\n","Skipping y_hat=-345\n","Skipping y_hat=-345\n","Skipping y_hat=+134\n","Skipping y_hat=+134\n","Skipping y_hat=+134\n","Skipping y_hat=+134\n","Skipping y_hat=+134\n","Skipping y_hat=+134\n","Skipping y_hat=+134\n","Skipping y_hat=+296\n","Skipping y_hat=+296\n","Skipping y_hat=+296\n","Skipping y_hat=+296\n","Skipping y_hat=+296\n","Skipping y_hat=+296\n","Skipping y_hat=+296\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-316\n","Skipping y_hat=-316\n","Skipping y_hat=-316\n","Skipping y_hat=-316\n","Skipping y_hat=-316\n","Skipping y_hat=-316\n","Skipping y_hat=-316\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=+30\n","Skipping y_hat=-106\n","Skipping y_hat=-106\n","Skipping y_hat=-106\n","Skipping y_hat=-106\n","Skipping y_hat=-106\n","Skipping y_hat=-106\n","Skipping y_hat=-106\n","Skipping y_hat=-463\n","Skipping y_hat=-463\n","Skipping y_hat=-463\n","Skipping y_hat=-463\n","Skipping y_hat=-463\n","Skipping y_hat=-463\n","Skipping y_hat=-463\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=-133\n","Skipping y_hat=-133\n","Skipping y_hat=-133\n","Skipping y_hat=-133\n","Skipping y_hat=-133\n","Skipping y_hat=-133\n","Skipping y_hat=-133\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=-46\n","Skipping y_hat=-46\n","Skipping y_hat=-46\n","Skipping y_hat=-46\n","Skipping y_hat=-46\n","Skipping y_hat=-46\n","Skipping y_hat=-46\n","Skipping y_hat=-130\n","Skipping y_hat=-130\n","Skipping y_hat=-130\n","Skipping y_hat=-130\n","Skipping y_hat=-130\n","Skipping y_hat=-130\n","Skipping y_hat=-130\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+457\n","Skipping y_hat=+457\n","Skipping y_hat=+457\n","Skipping y_hat=+457\n","Skipping y_hat=+457\n","Skipping y_hat=+457\n","Skipping y_hat=+457\n","Skipping y_hat=-190\n","Skipping y_hat=-190\n","Skipping y_hat=-190\n","Skipping y_hat=-190\n","Skipping y_hat=-190\n","Skipping y_hat=-190\n","Skipping y_hat=-190\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=+285\n","Skipping y_hat=+285\n","Skipping y_hat=+285\n","Skipping y_hat=+285\n","Skipping y_hat=+285\n","Skipping y_hat=+285\n","Skipping y_hat=+285\n","Skipping y_hat=-444\n","Skipping y_hat=-444\n","Skipping y_hat=-444\n","Skipping y_hat=-444\n","Skipping y_hat=-444\n","Skipping y_hat=-444\n","Skipping y_hat=-444\n","Skipping y_hat=-855\n","Skipping y_hat=-855\n","Skipping y_hat=-855\n","Skipping y_hat=-855\n","Skipping y_hat=-855\n","Skipping y_hat=-855\n","Skipping y_hat=-855\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=+291\n","Skipping y_hat=+291\n","Skipping y_hat=+291\n","Skipping y_hat=+291\n","Skipping y_hat=+291\n","Skipping y_hat=+291\n","Skipping y_hat=+291\n","Skipping y_hat=-838\n","Skipping y_hat=-838\n","Skipping y_hat=-838\n","Skipping y_hat=-838\n","Skipping y_hat=-838\n","Skipping y_hat=-838\n","Skipping y_hat=-838\n","Skipping y_hat=+727\n","Skipping y_hat=+727\n","Skipping y_hat=+727\n","Skipping y_hat=+727\n","Skipping y_hat=+727\n","Skipping y_hat=+727\n","Skipping y_hat=+727\n","Skipping y_hat=-521\n","Skipping y_hat=-521\n","Skipping y_hat=-521\n","Skipping y_hat=-521\n","Skipping y_hat=-521\n","Skipping y_hat=-521\n","Skipping y_hat=-521\n","Skipping y_hat=+377\n","Skipping y_hat=+377\n","Skipping y_hat=+377\n","Skipping y_hat=+377\n","Skipping y_hat=+377\n","Skipping y_hat=+377\n","Skipping y_hat=+377\n","Skipping y_hat=-189\n","Skipping y_hat=-189\n","Skipping y_hat=-189\n","Skipping y_hat=-189\n","Skipping y_hat=-189\n","Skipping y_hat=-189\n","Skipping y_hat=-189\n","Skipping y_hat=-19\n","Skipping y_hat=-19\n","Skipping y_hat=-19\n","Skipping y_hat=-19\n","Skipping y_hat=-19\n","Skipping y_hat=-19\n","Skipping y_hat=-19\n","Skipping y_hat=-652\n","Skipping y_hat=-652\n","Skipping y_hat=-652\n","Skipping y_hat=-652\n","Skipping y_hat=-652\n","Skipping y_hat=-652\n","Skipping y_hat=-652\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=-97\n","Skipping y_hat=-514\n","Skipping y_hat=-514\n","Skipping y_hat=-514\n","Skipping y_hat=-514\n","Skipping y_hat=-514\n","Skipping y_hat=-514\n","Skipping y_hat=-514\n","Skipping y_hat=-612\n","Skipping y_hat=-612\n","Skipping y_hat=-612\n","Skipping y_hat=-612\n","Skipping y_hat=-612\n","Skipping y_hat=-612\n","Skipping y_hat=-612\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n","Skipping y_hat=-132\n"," 76% 19/25 [00:08<00:02,  2.56it/s]Skipping y_hat=+852\n","Skipping y_hat=+852\n","Skipping y_hat=+852\n","Skipping y_hat=+852\n","Skipping y_hat=+852\n","Skipping y_hat=+852\n","Skipping y_hat=+852\n","Skipping y_hat=+487\n","Skipping y_hat=+487\n","Skipping y_hat=+487\n","Skipping y_hat=+487\n","Skipping y_hat=+487\n","Skipping y_hat=+487\n","Skipping y_hat=+487\n","Skipping y_hat=+328\n","Skipping y_hat=+328\n","Skipping y_hat=+328\n","Skipping y_hat=+328\n","Skipping y_hat=+328\n","Skipping y_hat=+328\n","Skipping y_hat=+328\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=+823\n","Skipping y_hat=+823\n","Skipping y_hat=+823\n","Skipping y_hat=+823\n","Skipping y_hat=+823\n","Skipping y_hat=+823\n","Skipping y_hat=+823\n","Skipping y_hat=-629\n","Skipping y_hat=-629\n","Skipping y_hat=-629\n","Skipping y_hat=-629\n","Skipping y_hat=-629\n","Skipping y_hat=-629\n","Skipping y_hat=-629\n","Skipping y_hat=+812\n","Skipping y_hat=+812\n","Skipping y_hat=+812\n","Skipping y_hat=+812\n","Skipping y_hat=+812\n","Skipping y_hat=+812\n","Skipping y_hat=+812\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+580\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=-448\n","Skipping y_hat=-448\n","Skipping y_hat=-448\n","Skipping y_hat=-448\n","Skipping y_hat=-448\n","Skipping y_hat=-448\n","Skipping y_hat=-448\n","Skipping y_hat=+739\n","Skipping y_hat=+739\n","Skipping y_hat=+739\n","Skipping y_hat=+739\n","Skipping y_hat=+739\n","Skipping y_hat=+739\n","Skipping y_hat=+739\n","Skipping y_hat=+416\n","Skipping y_hat=+416\n","Skipping y_hat=+416\n","Skipping y_hat=+416\n","Skipping y_hat=+416\n","Skipping y_hat=+416\n","Skipping y_hat=+416\n","Skipping y_hat=-274\n","Skipping y_hat=-274\n","Skipping y_hat=-274\n","Skipping y_hat=-274\n","Skipping y_hat=-274\n","Skipping y_hat=-274\n","Skipping y_hat=-274\n","Skipping y_hat=-63\n","Skipping y_hat=-63\n","Skipping y_hat=-63\n","Skipping y_hat=-63\n","Skipping y_hat=-63\n","Skipping y_hat=-63\n","Skipping y_hat=-63\n","Skipping y_hat=-750\n","Skipping y_hat=-750\n","Skipping y_hat=-750\n","Skipping y_hat=-750\n","Skipping y_hat=-750\n","Skipping y_hat=-750\n","Skipping y_hat=-750\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=-402\n","Skipping y_hat=-402\n","Skipping y_hat=-402\n","Skipping y_hat=-402\n","Skipping y_hat=-402\n","Skipping y_hat=-402\n","Skipping y_hat=-402\n","Skipping y_hat=-53\n","Skipping y_hat=-53\n","Skipping y_hat=-53\n","Skipping y_hat=-53\n","Skipping y_hat=-53\n","Skipping y_hat=-53\n","Skipping y_hat=-53\n","Skipping y_hat=-594\n","Skipping y_hat=-594\n","Skipping y_hat=-594\n","Skipping y_hat=-594\n","Skipping y_hat=-594\n","Skipping y_hat=-594\n","Skipping y_hat=-594\n","Skipping y_hat=+763\n","Skipping y_hat=+763\n","Skipping y_hat=+763\n","Skipping y_hat=+763\n","Skipping y_hat=+763\n","Skipping y_hat=+763\n","Skipping y_hat=+763\n","Skipping y_hat=+358\n","Skipping y_hat=+358\n","Skipping y_hat=+358\n","Skipping y_hat=+358\n","Skipping y_hat=+358\n","Skipping y_hat=+358\n","Skipping y_hat=+358\n","Skipping y_hat=-265\n","Skipping y_hat=-265\n","Skipping y_hat=-265\n","Skipping y_hat=-265\n","Skipping y_hat=-265\n","Skipping y_hat=-265\n","Skipping y_hat=-265\n","Skipping y_hat=-590\n","Skipping y_hat=-590\n","Skipping y_hat=-590\n","Skipping y_hat=-590\n","Skipping y_hat=-590\n","Skipping y_hat=-590\n","Skipping y_hat=-590\n","Skipping y_hat=-387\n","Skipping y_hat=-387\n","Skipping y_hat=-387\n","Skipping y_hat=-387\n","Skipping y_hat=-387\n","Skipping y_hat=-387\n","Skipping y_hat=-387\n","Skipping y_hat=+173\n","Skipping y_hat=+173\n","Skipping y_hat=+173\n","Skipping y_hat=+173\n","Skipping y_hat=+173\n","Skipping y_hat=+173\n","Skipping y_hat=+173\n","Skipping y_hat=+657\n","Skipping y_hat=+657\n","Skipping y_hat=+657\n","Skipping y_hat=+657\n","Skipping y_hat=+657\n","Skipping y_hat=+657\n","Skipping y_hat=+657\n","Skipping y_hat=-740\n","Skipping y_hat=-740\n","Skipping y_hat=-740\n","Skipping y_hat=-740\n","Skipping y_hat=-740\n","Skipping y_hat=-740\n","Skipping y_hat=-740\n","Skipping y_hat=-609\n","Skipping y_hat=-609\n","Skipping y_hat=-609\n","Skipping y_hat=-609\n","Skipping y_hat=-609\n","Skipping y_hat=-609\n","Skipping y_hat=-609\n","Skipping y_hat=-344\n","Skipping y_hat=-344\n","Skipping y_hat=-344\n","Skipping y_hat=-344\n","Skipping y_hat=-344\n","Skipping y_hat=-344\n","Skipping y_hat=-344\n","Skipping y_hat=-527\n","Skipping y_hat=-527\n","Skipping y_hat=-527\n","Skipping y_hat=-527\n","Skipping y_hat=-527\n","Skipping y_hat=-527\n","Skipping y_hat=-527\n","Skipping y_hat=+640\n","Skipping y_hat=+640\n","Skipping y_hat=+640\n","Skipping y_hat=+640\n","Skipping y_hat=+640\n","Skipping y_hat=+640\n","Skipping y_hat=+640\n","Skipping y_hat=+232\n","Skipping y_hat=+232\n","Skipping y_hat=+232\n","Skipping y_hat=+232\n","Skipping y_hat=+232\n","Skipping y_hat=+232\n","Skipping y_hat=+232\n","Skipping y_hat=-711\n","Skipping y_hat=-711\n","Skipping y_hat=-711\n","Skipping y_hat=-711\n","Skipping y_hat=-711\n","Skipping y_hat=-711\n","Skipping y_hat=-711\n","Skipping y_hat=-489\n","Skipping y_hat=-489\n","Skipping y_hat=-489\n","Skipping y_hat=-489\n","Skipping y_hat=-489\n","Skipping y_hat=-489\n","Skipping y_hat=-489\n","Skipping y_hat=-100\n","Skipping y_hat=-100\n","Skipping y_hat=-100\n","Skipping y_hat=-100\n","Skipping y_hat=-100\n","Skipping y_hat=-100\n","Skipping y_hat=-100\n","Skipping y_hat=+914\n","Skipping y_hat=+914\n","Skipping y_hat=+914\n","Skipping y_hat=+914\n","Skipping y_hat=+914\n","Skipping y_hat=+914\n","Skipping y_hat=+914\n","Skipping y_hat=+590\n","Skipping y_hat=+590\n","Skipping y_hat=+590\n","Skipping y_hat=+590\n","Skipping y_hat=+590\n","Skipping y_hat=+590\n","Skipping y_hat=+590\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-692\n","Skipping y_hat=-692\n","Skipping y_hat=-692\n","Skipping y_hat=-692\n","Skipping y_hat=-692\n","Skipping y_hat=-692\n","Skipping y_hat=-692\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=+114\n","Skipping y_hat=+114\n","Skipping y_hat=+114\n","Skipping y_hat=+114\n","Skipping y_hat=+114\n","Skipping y_hat=+114\n","Skipping y_hat=+114\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-586\n","Skipping y_hat=-586\n","Skipping y_hat=-586\n","Skipping y_hat=-586\n","Skipping y_hat=-586\n","Skipping y_hat=-586\n","Skipping y_hat=-586\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=-205\n","Skipping y_hat=+21\n","Skipping y_hat=+21\n","Skipping y_hat=+21\n","Skipping y_hat=+21\n","Skipping y_hat=+21\n","Skipping y_hat=+21\n","Skipping y_hat=+21\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-746\n","Skipping y_hat=-746\n","Skipping y_hat=-746\n","Skipping y_hat=-746\n","Skipping y_hat=-746\n","Skipping y_hat=-746\n","Skipping y_hat=-746\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=-812\n","Skipping y_hat=-812\n","Skipping y_hat=-812\n","Skipping y_hat=-812\n","Skipping y_hat=-812\n","Skipping y_hat=-812\n","Skipping y_hat=-812\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-228\n","Skipping y_hat=-228\n","Skipping y_hat=-228\n","Skipping y_hat=-228\n","Skipping y_hat=-228\n","Skipping y_hat=-228\n","Skipping y_hat=-228\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=+213\n","Skipping y_hat=+213\n","Skipping y_hat=+213\n","Skipping y_hat=+213\n","Skipping y_hat=+213\n","Skipping y_hat=+213\n","Skipping y_hat=+213\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=-400\n","Skipping y_hat=-400\n","Skipping y_hat=-400\n","Skipping y_hat=-400\n","Skipping y_hat=-400\n","Skipping y_hat=-400\n","Skipping y_hat=-400\n","Skipping y_hat=+410\n","Skipping y_hat=+410\n","Skipping y_hat=+410\n","Skipping y_hat=+410\n","Skipping y_hat=+410\n","Skipping y_hat=+410\n","Skipping y_hat=+410\n","Skipping y_hat=+357\n","Skipping y_hat=+357\n","Skipping y_hat=+357\n","Skipping y_hat=+357\n","Skipping y_hat=+357\n","Skipping y_hat=+357\n","Skipping y_hat=+357\n","Skipping y_hat=+339\n","Skipping y_hat=+339\n","Skipping y_hat=+339\n","Skipping y_hat=+339\n","Skipping y_hat=+339\n","Skipping y_hat=+339\n","Skipping y_hat=+339\n","Skipping y_hat=-377\n","Skipping y_hat=-377\n","Skipping y_hat=-377\n","Skipping y_hat=-377\n","Skipping y_hat=-377\n","Skipping y_hat=-377\n","Skipping y_hat=-377\n","Skipping y_hat=+194\n","Skipping y_hat=+194\n","Skipping y_hat=+194\n","Skipping y_hat=+194\n","Skipping y_hat=+194\n","Skipping y_hat=+194\n","Skipping y_hat=+194\n","Skipping y_hat=-971\n","Skipping y_hat=-971\n","Skipping y_hat=-971\n","Skipping y_hat=-971\n","Skipping y_hat=-971\n","Skipping y_hat=-971\n","Skipping y_hat=-971\n","Skipping y_hat=-309\n","Skipping y_hat=-309\n","Skipping y_hat=-309\n","Skipping y_hat=-309\n","Skipping y_hat=-309\n","Skipping y_hat=-309\n","Skipping y_hat=-309\n","Skipping y_hat=-422\n","Skipping y_hat=-422\n","Skipping y_hat=-422\n","Skipping y_hat=-422\n","Skipping y_hat=-422\n","Skipping y_hat=-422\n","Skipping y_hat=-422\n","Skipping y_hat=-164\n","Skipping y_hat=-164\n","Skipping y_hat=-164\n","Skipping y_hat=-164\n","Skipping y_hat=-164\n","Skipping y_hat=-164\n","Skipping y_hat=-164\n","Skipping y_hat=-203\n","Skipping y_hat=-203\n","Skipping y_hat=-203\n","Skipping y_hat=-203\n","Skipping y_hat=-203\n","Skipping y_hat=-203\n","Skipping y_hat=-203\n","Skipping y_hat=-735\n","Skipping y_hat=-735\n","Skipping y_hat=-735\n","Skipping y_hat=-735\n","Skipping y_hat=-735\n","Skipping y_hat=-735\n","Skipping y_hat=-735\n","Skipping y_hat=+877\n","Skipping y_hat=+877\n","Skipping y_hat=+877\n","Skipping y_hat=+877\n","Skipping y_hat=+877\n","Skipping y_hat=+877\n","Skipping y_hat=+877\n","Skipping y_hat=-791\n","Skipping y_hat=-791\n","Skipping y_hat=-791\n","Skipping y_hat=-791\n","Skipping y_hat=-791\n","Skipping y_hat=-791\n","Skipping y_hat=-791\n","Skipping y_hat=-619\n","Skipping y_hat=-619\n","Skipping y_hat=-619\n","Skipping y_hat=-619\n","Skipping y_hat=-619\n","Skipping y_hat=-619\n","Skipping y_hat=-619\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-457\n","Skipping y_hat=-457\n","Skipping y_hat=-457\n","Skipping y_hat=-457\n","Skipping y_hat=-457\n","Skipping y_hat=-457\n","Skipping y_hat=-457\n","Skipping y_hat=-696\n","Skipping y_hat=-696\n","Skipping y_hat=-696\n","Skipping y_hat=-696\n","Skipping y_hat=-696\n","Skipping y_hat=-696\n","Skipping y_hat=-696\n","Skipping y_hat=+511\n","Skipping y_hat=+511\n","Skipping y_hat=+511\n","Skipping y_hat=+511\n","Skipping y_hat=+511\n","Skipping y_hat=+511\n","Skipping y_hat=+511\n","Skipping y_hat=-378\n","Skipping y_hat=-378\n","Skipping y_hat=-378\n","Skipping y_hat=-378\n","Skipping y_hat=-378\n","Skipping y_hat=-378\n","Skipping y_hat=-378\n","Skipping y_hat=-180\n","Skipping y_hat=-180\n","Skipping y_hat=-180\n","Skipping y_hat=-180\n","Skipping y_hat=-180\n","Skipping y_hat=-180\n","Skipping y_hat=-180\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=+623\n","Skipping y_hat=+623\n","Skipping y_hat=+623\n","Skipping y_hat=+623\n","Skipping y_hat=+623\n","Skipping y_hat=+623\n","Skipping y_hat=+623\n","Skipping y_hat=-172\n","Skipping y_hat=-172\n","Skipping y_hat=-172\n","Skipping y_hat=-172\n","Skipping y_hat=-172\n","Skipping y_hat=-172\n","Skipping y_hat=-172\n","Skipping y_hat=-742\n","Skipping y_hat=-742\n","Skipping y_hat=-742\n","Skipping y_hat=-742\n","Skipping y_hat=-742\n","Skipping y_hat=-742\n","Skipping y_hat=-742\n","Skipping y_hat=-921\n","Skipping y_hat=-921\n","Skipping y_hat=-921\n","Skipping y_hat=-921\n","Skipping y_hat=-921\n","Skipping y_hat=-921\n","Skipping y_hat=-921\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=-659\n","Skipping y_hat=-659\n","Skipping y_hat=-659\n","Skipping y_hat=-659\n","Skipping y_hat=-659\n","Skipping y_hat=-659\n","Skipping y_hat=-659\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=-625\n","Skipping y_hat=+599\n","Skipping y_hat=+599\n","Skipping y_hat=+599\n","Skipping y_hat=+599\n","Skipping y_hat=+599\n","Skipping y_hat=+599\n","Skipping y_hat=+599\n","Skipping y_hat=+817\n","Skipping y_hat=+817\n","Skipping y_hat=+817\n","Skipping y_hat=+817\n","Skipping y_hat=+817\n","Skipping y_hat=+817\n","Skipping y_hat=+817\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=+146\n","Skipping y_hat=-384\n","Skipping y_hat=-384\n","Skipping y_hat=-384\n","Skipping y_hat=-384\n","Skipping y_hat=-384\n","Skipping y_hat=-384\n","Skipping y_hat=-384\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=+304\n","Skipping y_hat=+304\n","Skipping y_hat=+304\n","Skipping y_hat=+304\n","Skipping y_hat=+304\n","Skipping y_hat=+304\n","Skipping y_hat=+304\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=-641\n","Skipping y_hat=+910\n","Skipping y_hat=+910\n","Skipping y_hat=+910\n","Skipping y_hat=+910\n","Skipping y_hat=+910\n","Skipping y_hat=+910\n","Skipping y_hat=+910\n","Skipping y_hat=+290\n","Skipping y_hat=+290\n","Skipping y_hat=+290\n","Skipping y_hat=+290\n","Skipping y_hat=+290\n","Skipping y_hat=+290\n","Skipping y_hat=+290\n","Skipping y_hat=-411\n","Skipping y_hat=-411\n","Skipping y_hat=-411\n","Skipping y_hat=-411\n","Skipping y_hat=-411\n","Skipping y_hat=-411\n","Skipping y_hat=-411\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=+535\n","Skipping y_hat=+535\n","Skipping y_hat=+535\n","Skipping y_hat=+535\n","Skipping y_hat=+535\n","Skipping y_hat=+535\n","Skipping y_hat=+535\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=+585\n","Skipping y_hat=+585\n","Skipping y_hat=+585\n","Skipping y_hat=+585\n","Skipping y_hat=+585\n","Skipping y_hat=+585\n","Skipping y_hat=+585\n","Skipping y_hat=-699\n","Skipping y_hat=-699\n","Skipping y_hat=-699\n","Skipping y_hat=-699\n","Skipping y_hat=-699\n","Skipping y_hat=-699\n","Skipping y_hat=-699\n","Skipping y_hat=-284\n","Skipping y_hat=-284\n","Skipping y_hat=-284\n","Skipping y_hat=-284\n","Skipping y_hat=-284\n","Skipping y_hat=-284\n","Skipping y_hat=-284\n","Skipping y_hat=+151\n","Skipping y_hat=+151\n","Skipping y_hat=+151\n","Skipping y_hat=+151\n","Skipping y_hat=+151\n","Skipping y_hat=+151\n","Skipping y_hat=+151\n","Skipping y_hat=-877\n","Skipping y_hat=-877\n","Skipping y_hat=-877\n","Skipping y_hat=-877\n","Skipping y_hat=-877\n","Skipping y_hat=-877\n","Skipping y_hat=-877\n","Skipping y_hat=+224\n","Skipping y_hat=+224\n","Skipping y_hat=+224\n","Skipping y_hat=+224\n","Skipping y_hat=+224\n","Skipping y_hat=+224\n","Skipping y_hat=+224\n","Skipping y_hat=+99\n","Skipping y_hat=+99\n","Skipping y_hat=+99\n","Skipping y_hat=+99\n","Skipping y_hat=+99\n","Skipping y_hat=+99\n","Skipping y_hat=+99\n","Skipping y_hat=+355\n","Skipping y_hat=+355\n","Skipping y_hat=+355\n","Skipping y_hat=+355\n","Skipping y_hat=+355\n","Skipping y_hat=+355\n","Skipping y_hat=+355\n","Skipping y_hat=-796\n","Skipping y_hat=-796\n","Skipping y_hat=-796\n","Skipping y_hat=-796\n","Skipping y_hat=-796\n","Skipping y_hat=-796\n","Skipping y_hat=-796\n","Skipping y_hat=+265\n","Skipping y_hat=+265\n","Skipping y_hat=+265\n","Skipping y_hat=+265\n","Skipping y_hat=+265\n","Skipping y_hat=+265\n","Skipping y_hat=+265\n","Skipping y_hat=+584\n","Skipping y_hat=+584\n","Skipping y_hat=+584\n","Skipping y_hat=+584\n","Skipping y_hat=+584\n","Skipping y_hat=+584\n","Skipping y_hat=+584\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-368\n","Skipping y_hat=-368\n","Skipping y_hat=-368\n","Skipping y_hat=-368\n","Skipping y_hat=-368\n","Skipping y_hat=-368\n","Skipping y_hat=-368\n","Skipping y_hat=-922\n","Skipping y_hat=-922\n","Skipping y_hat=-922\n","Skipping y_hat=-922\n","Skipping y_hat=-922\n","Skipping y_hat=-922\n","Skipping y_hat=-922\n","Skipping y_hat=-86\n","Skipping y_hat=-86\n","Skipping y_hat=-86\n","Skipping y_hat=-86\n","Skipping y_hat=-86\n","Skipping y_hat=-86\n","Skipping y_hat=-86\n","Skipping y_hat=-785\n","Skipping y_hat=-785\n","Skipping y_hat=-785\n","Skipping y_hat=-785\n","Skipping y_hat=-785\n","Skipping y_hat=-785\n","Skipping y_hat=-785\n","Skipping y_hat=+888\n","Skipping y_hat=+888\n","Skipping y_hat=+888\n","Skipping y_hat=+888\n","Skipping y_hat=+888\n","Skipping y_hat=+888\n","Skipping y_hat=+888\n","Skipping y_hat=-883\n","Skipping y_hat=-883\n","Skipping y_hat=-883\n","Skipping y_hat=-883\n","Skipping y_hat=-883\n","Skipping y_hat=-883\n","Skipping y_hat=-883\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=+79\n","Skipping y_hat=-539\n","Skipping y_hat=-539\n","Skipping y_hat=-539\n","Skipping y_hat=-539\n","Skipping y_hat=-539\n","Skipping y_hat=-539\n","Skipping y_hat=-539\n","Skipping y_hat=+185\n","Skipping y_hat=+185\n","Skipping y_hat=+185\n","Skipping y_hat=+185\n","Skipping y_hat=+185\n","Skipping y_hat=+185\n","Skipping y_hat=+185\n","Skipping y_hat=+548\n","Skipping y_hat=+548\n","Skipping y_hat=+548\n","Skipping y_hat=+548\n","Skipping y_hat=+548\n","Skipping y_hat=+548\n","Skipping y_hat=+548\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n","Skipping y_hat=-884\n"," 80% 20/25 [00:08<00:01,  2.85it/s]Skipping y_hat=+480\n","Skipping y_hat=+480\n","Skipping y_hat=+480\n","Skipping y_hat=+480\n","Skipping y_hat=+480\n","Skipping y_hat=+480\n","Skipping y_hat=+480\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-442\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-811\n","Skipping y_hat=-732\n","Skipping y_hat=-732\n","Skipping y_hat=-732\n","Skipping y_hat=-732\n","Skipping y_hat=-732\n","Skipping y_hat=-732\n","Skipping y_hat=-732\n","Skipping y_hat=-546\n","Skipping y_hat=-546\n","Skipping y_hat=-546\n","Skipping y_hat=-546\n","Skipping y_hat=-546\n","Skipping y_hat=-546\n","Skipping y_hat=-546\n","Skipping y_hat=+332\n","Skipping y_hat=+332\n","Skipping y_hat=+332\n","Skipping y_hat=+332\n","Skipping y_hat=+332\n","Skipping y_hat=+332\n","Skipping y_hat=+332\n","Skipping y_hat=-646\n","Skipping y_hat=-646\n","Skipping y_hat=-646\n","Skipping y_hat=-646\n","Skipping y_hat=-646\n","Skipping y_hat=-646\n","Skipping y_hat=-646\n","Skipping y_hat=+430\n","Skipping y_hat=+430\n","Skipping y_hat=+430\n","Skipping y_hat=+430\n","Skipping y_hat=+430\n","Skipping y_hat=+430\n","Skipping y_hat=+430\n","Skipping y_hat=+675\n","Skipping y_hat=+675\n","Skipping y_hat=+675\n","Skipping y_hat=+675\n","Skipping y_hat=+675\n","Skipping y_hat=+675\n","Skipping y_hat=+675\n","Skipping y_hat=-665\n","Skipping y_hat=-665\n","Skipping y_hat=-665\n","Skipping y_hat=-665\n","Skipping y_hat=-665\n","Skipping y_hat=-665\n","Skipping y_hat=-665\n","Skipping y_hat=-116\n","Skipping y_hat=-116\n","Skipping y_hat=-116\n","Skipping y_hat=-116\n","Skipping y_hat=-116\n","Skipping y_hat=-116\n","Skipping y_hat=-116\n","Skipping y_hat=+717\n","Skipping y_hat=+717\n","Skipping y_hat=+717\n","Skipping y_hat=+717\n","Skipping y_hat=+717\n","Skipping y_hat=+717\n","Skipping y_hat=+717\n","Skipping y_hat=-483\n","Skipping y_hat=-483\n","Skipping y_hat=-483\n","Skipping y_hat=-483\n","Skipping y_hat=-483\n","Skipping y_hat=-483\n","Skipping y_hat=-483\n","Skipping y_hat=+588\n","Skipping y_hat=+588\n","Skipping y_hat=+588\n","Skipping y_hat=+588\n","Skipping y_hat=+588\n","Skipping y_hat=+588\n","Skipping y_hat=+588\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+687\n","Skipping y_hat=+687\n","Skipping y_hat=+687\n","Skipping y_hat=+687\n","Skipping y_hat=+687\n","Skipping y_hat=+687\n","Skipping y_hat=+687\n","Skipping y_hat=-751\n","Skipping y_hat=-751\n","Skipping y_hat=-751\n","Skipping y_hat=-751\n","Skipping y_hat=-751\n","Skipping y_hat=-751\n","Skipping y_hat=-751\n","Skipping y_hat=-113\n","Skipping y_hat=-113\n","Skipping y_hat=-113\n","Skipping y_hat=-113\n","Skipping y_hat=-113\n","Skipping y_hat=-113\n","Skipping y_hat=-113\n","Skipping y_hat=+682\n","Skipping y_hat=+682\n","Skipping y_hat=+682\n","Skipping y_hat=+682\n","Skipping y_hat=+682\n","Skipping y_hat=+682\n","Skipping y_hat=+682\n","Skipping y_hat=-781\n","Skipping y_hat=-781\n","Skipping y_hat=-781\n","Skipping y_hat=-781\n","Skipping y_hat=-781\n","Skipping y_hat=-781\n","Skipping y_hat=-781\n","Skipping y_hat=+777\n","Skipping y_hat=+777\n","Skipping y_hat=+777\n","Skipping y_hat=+777\n","Skipping y_hat=+777\n","Skipping y_hat=+777\n","Skipping y_hat=+777\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=+434\n","Skipping y_hat=+363\n","Skipping y_hat=+363\n","Skipping y_hat=+363\n","Skipping y_hat=+363\n","Skipping y_hat=+363\n","Skipping y_hat=+363\n","Skipping y_hat=+363\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-201\n","Skipping y_hat=-201\n","Skipping y_hat=-201\n","Skipping y_hat=-201\n","Skipping y_hat=-201\n","Skipping y_hat=-201\n","Skipping y_hat=-201\n","Skipping y_hat=-553\n","Skipping y_hat=-553\n","Skipping y_hat=-553\n","Skipping y_hat=-553\n","Skipping y_hat=-553\n","Skipping y_hat=-553\n","Skipping y_hat=-553\n","Skipping y_hat=-733\n","Skipping y_hat=-733\n","Skipping y_hat=-733\n","Skipping y_hat=-733\n","Skipping y_hat=-733\n","Skipping y_hat=-733\n","Skipping y_hat=-733\n","Skipping y_hat=+231\n","Skipping y_hat=+231\n","Skipping y_hat=+231\n","Skipping y_hat=+231\n","Skipping y_hat=+231\n","Skipping y_hat=+231\n","Skipping y_hat=+231\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=-580\n","Skipping y_hat=+49\n","Skipping y_hat=+49\n","Skipping y_hat=+49\n","Skipping y_hat=+49\n","Skipping y_hat=+49\n","Skipping y_hat=+49\n","Skipping y_hat=+49\n","Skipping y_hat=+970\n","Skipping y_hat=+970\n","Skipping y_hat=+970\n","Skipping y_hat=+970\n","Skipping y_hat=+970\n","Skipping y_hat=+970\n","Skipping y_hat=+970\n","Skipping y_hat=-556\n","Skipping y_hat=-556\n","Skipping y_hat=-556\n","Skipping y_hat=-556\n","Skipping y_hat=-556\n","Skipping y_hat=-556\n","Skipping y_hat=-556\n","Skipping y_hat=-208\n","Skipping y_hat=-208\n","Skipping y_hat=-208\n","Skipping y_hat=-208\n","Skipping y_hat=-208\n","Skipping y_hat=-208\n","Skipping y_hat=-208\n","Skipping y_hat=-75\n","Skipping y_hat=-75\n","Skipping y_hat=-75\n","Skipping y_hat=-75\n","Skipping y_hat=-75\n","Skipping y_hat=-75\n","Skipping y_hat=-75\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=+710\n","Skipping y_hat=+710\n","Skipping y_hat=+710\n","Skipping y_hat=+710\n","Skipping y_hat=+710\n","Skipping y_hat=+710\n","Skipping y_hat=+710\n","Skipping y_hat=-192\n","Skipping y_hat=-192\n","Skipping y_hat=-192\n","Skipping y_hat=-192\n","Skipping y_hat=-192\n","Skipping y_hat=-192\n","Skipping y_hat=-192\n","Skipping y_hat=-899\n","Skipping y_hat=-899\n","Skipping y_hat=-899\n","Skipping y_hat=-899\n","Skipping y_hat=-899\n","Skipping y_hat=-899\n","Skipping y_hat=-899\n","Skipping y_hat=-207\n","Skipping y_hat=-207\n","Skipping y_hat=-207\n","Skipping y_hat=-207\n","Skipping y_hat=-207\n","Skipping y_hat=-207\n","Skipping y_hat=-207\n","Skipping y_hat=+378\n","Skipping y_hat=+378\n","Skipping y_hat=+378\n","Skipping y_hat=+378\n","Skipping y_hat=+378\n","Skipping y_hat=+378\n","Skipping y_hat=+378\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=+282\n","Skipping y_hat=+282\n","Skipping y_hat=+282\n","Skipping y_hat=+282\n","Skipping y_hat=+282\n","Skipping y_hat=+282\n","Skipping y_hat=+282\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-567\n","Skipping y_hat=-567\n","Skipping y_hat=-567\n","Skipping y_hat=-567\n","Skipping y_hat=-567\n","Skipping y_hat=-567\n","Skipping y_hat=-567\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=-235\n","Skipping y_hat=-291\n","Skipping y_hat=-291\n","Skipping y_hat=-291\n","Skipping y_hat=-291\n","Skipping y_hat=-291\n","Skipping y_hat=-291\n","Skipping y_hat=-291\n","Skipping y_hat=-170\n","Skipping y_hat=-170\n","Skipping y_hat=-170\n","Skipping y_hat=-170\n","Skipping y_hat=-170\n","Skipping y_hat=-170\n","Skipping y_hat=-170\n","Skipping y_hat=+188\n","Skipping y_hat=+188\n","Skipping y_hat=+188\n","Skipping y_hat=+188\n","Skipping y_hat=+188\n","Skipping y_hat=+188\n","Skipping y_hat=+188\n","Skipping y_hat=+836\n","Skipping y_hat=+836\n","Skipping y_hat=+836\n","Skipping y_hat=+836\n","Skipping y_hat=+836\n","Skipping y_hat=+836\n","Skipping y_hat=+836\n","Skipping y_hat=-101\n","Skipping y_hat=-101\n","Skipping y_hat=-101\n","Skipping y_hat=-101\n","Skipping y_hat=-101\n","Skipping y_hat=-101\n","Skipping y_hat=-101\n","Skipping y_hat=-218\n","Skipping y_hat=-218\n","Skipping y_hat=-218\n","Skipping y_hat=-218\n","Skipping y_hat=-218\n","Skipping y_hat=-218\n","Skipping y_hat=-218\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+133\n","Skipping y_hat=+449\n","Skipping y_hat=+449\n","Skipping y_hat=+449\n","Skipping y_hat=+449\n","Skipping y_hat=+449\n","Skipping y_hat=+449\n","Skipping y_hat=+449\n","Skipping y_hat=-57\n","Skipping y_hat=-57\n","Skipping y_hat=-57\n","Skipping y_hat=-57\n","Skipping y_hat=-57\n","Skipping y_hat=-57\n","Skipping y_hat=-57\n","Skipping y_hat=+895\n","Skipping y_hat=+895\n","Skipping y_hat=+895\n","Skipping y_hat=+895\n","Skipping y_hat=+895\n","Skipping y_hat=+895\n","Skipping y_hat=+895\n","Skipping y_hat=+112\n","Skipping y_hat=+112\n","Skipping y_hat=+112\n","Skipping y_hat=+112\n","Skipping y_hat=+112\n","Skipping y_hat=+112\n","Skipping y_hat=+112\n","Skipping y_hat=+663\n","Skipping y_hat=+663\n","Skipping y_hat=+663\n","Skipping y_hat=+663\n","Skipping y_hat=+663\n","Skipping y_hat=+663\n","Skipping y_hat=+663\n","Skipping y_hat=+689\n","Skipping y_hat=+689\n","Skipping y_hat=+689\n","Skipping y_hat=+689\n","Skipping y_hat=+689\n","Skipping y_hat=+689\n","Skipping y_hat=+689\n","Skipping y_hat=-81\n","Skipping y_hat=-81\n","Skipping y_hat=-81\n","Skipping y_hat=-81\n","Skipping y_hat=-81\n","Skipping y_hat=-81\n","Skipping y_hat=-81\n","Skipping y_hat=+578\n","Skipping y_hat=+578\n","Skipping y_hat=+578\n","Skipping y_hat=+578\n","Skipping y_hat=+578\n","Skipping y_hat=+578\n","Skipping y_hat=+578\n","Skipping y_hat=+24\n","Skipping y_hat=+24\n","Skipping y_hat=+24\n","Skipping y_hat=+24\n","Skipping y_hat=+24\n","Skipping y_hat=+24\n","Skipping y_hat=+24\n","Skipping y_hat=-232\n","Skipping y_hat=-232\n","Skipping y_hat=-232\n","Skipping y_hat=-232\n","Skipping y_hat=-232\n","Skipping y_hat=-232\n","Skipping y_hat=-232\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=+565\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=-147\n","Skipping y_hat=+147\n","Skipping y_hat=+147\n","Skipping y_hat=+147\n","Skipping y_hat=+147\n","Skipping y_hat=+147\n","Skipping y_hat=+147\n","Skipping y_hat=+147\n","Skipping y_hat=-689\n","Skipping y_hat=-689\n","Skipping y_hat=-689\n","Skipping y_hat=-689\n","Skipping y_hat=-689\n","Skipping y_hat=-689\n","Skipping y_hat=-689\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+206\n","Skipping y_hat=+906\n","Skipping y_hat=+906\n","Skipping y_hat=+906\n","Skipping y_hat=+906\n","Skipping y_hat=+906\n","Skipping y_hat=+906\n","Skipping y_hat=+906\n","Skipping y_hat=+532\n","Skipping y_hat=+532\n","Skipping y_hat=+532\n","Skipping y_hat=+532\n","Skipping y_hat=+532\n","Skipping y_hat=+532\n","Skipping y_hat=+532\n","Skipping y_hat=+595\n","Skipping y_hat=+595\n","Skipping y_hat=+595\n","Skipping y_hat=+595\n","Skipping y_hat=+595\n","Skipping y_hat=+595\n","Skipping y_hat=+595\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=-42\n","Skipping y_hat=-42\n","Skipping y_hat=-42\n","Skipping y_hat=-42\n","Skipping y_hat=-42\n","Skipping y_hat=-42\n","Skipping y_hat=-42\n","Skipping y_hat=+118\n","Skipping y_hat=+118\n","Skipping y_hat=+118\n","Skipping y_hat=+118\n","Skipping y_hat=+118\n","Skipping y_hat=+118\n","Skipping y_hat=+118\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=+494\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-720\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=+562\n","Skipping y_hat=+562\n","Skipping y_hat=+562\n","Skipping y_hat=+562\n","Skipping y_hat=+562\n","Skipping y_hat=+562\n","Skipping y_hat=+562\n","Skipping y_hat=+766\n","Skipping y_hat=+766\n","Skipping y_hat=+766\n","Skipping y_hat=+766\n","Skipping y_hat=+766\n","Skipping y_hat=+766\n","Skipping y_hat=+766\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-157\n","Skipping y_hat=-157\n","Skipping y_hat=-157\n","Skipping y_hat=-157\n","Skipping y_hat=-157\n","Skipping y_hat=-157\n","Skipping y_hat=-157\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=-167\n","Skipping y_hat=-167\n","Skipping y_hat=-167\n","Skipping y_hat=-167\n","Skipping y_hat=-167\n","Skipping y_hat=-167\n","Skipping y_hat=-167\n","Skipping y_hat=-684\n","Skipping y_hat=-684\n","Skipping y_hat=-684\n","Skipping y_hat=-684\n","Skipping y_hat=-684\n","Skipping y_hat=-684\n","Skipping y_hat=-684\n","Skipping y_hat=+738\n","Skipping y_hat=+738\n","Skipping y_hat=+738\n","Skipping y_hat=+738\n","Skipping y_hat=+738\n","Skipping y_hat=+738\n","Skipping y_hat=+738\n","Skipping y_hat=-285\n","Skipping y_hat=-285\n","Skipping y_hat=-285\n","Skipping y_hat=-285\n","Skipping y_hat=-285\n","Skipping y_hat=-285\n","Skipping y_hat=-285\n","Skipping y_hat=-202\n","Skipping y_hat=-202\n","Skipping y_hat=-202\n","Skipping y_hat=-202\n","Skipping y_hat=-202\n","Skipping y_hat=-202\n","Skipping y_hat=-202\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=-294\n","Skipping y_hat=+302\n","Skipping y_hat=+302\n","Skipping y_hat=+302\n","Skipping y_hat=+302\n","Skipping y_hat=+302\n","Skipping y_hat=+302\n","Skipping y_hat=+302\n","Skipping y_hat=-461\n","Skipping y_hat=-461\n","Skipping y_hat=-461\n","Skipping y_hat=-461\n","Skipping y_hat=-461\n","Skipping y_hat=-461\n","Skipping y_hat=-461\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=-416\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=+545\n","Skipping y_hat=-163\n","Skipping y_hat=-163\n","Skipping y_hat=-163\n","Skipping y_hat=-163\n","Skipping y_hat=-163\n","Skipping y_hat=-163\n","Skipping y_hat=-163\n","Skipping y_hat=+632\n","Skipping y_hat=+632\n","Skipping y_hat=+632\n","Skipping y_hat=+632\n","Skipping y_hat=+632\n","Skipping y_hat=+632\n","Skipping y_hat=+632\n","Skipping y_hat=-283\n","Skipping y_hat=-283\n","Skipping y_hat=-283\n","Skipping y_hat=-283\n","Skipping y_hat=-283\n","Skipping y_hat=-283\n","Skipping y_hat=-283\n","Skipping y_hat=-430\n","Skipping y_hat=-430\n","Skipping y_hat=-430\n","Skipping y_hat=-430\n","Skipping y_hat=-430\n","Skipping y_hat=-430\n","Skipping y_hat=-430\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+488\n","Skipping y_hat=+515\n","Skipping y_hat=+515\n","Skipping y_hat=+515\n","Skipping y_hat=+515\n","Skipping y_hat=+515\n","Skipping y_hat=+515\n","Skipping y_hat=+515\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=-74\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=+719\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=-325\n","Skipping y_hat=+441\n","Skipping y_hat=+441\n","Skipping y_hat=+441\n","Skipping y_hat=+441\n","Skipping y_hat=+441\n","Skipping y_hat=+441\n","Skipping y_hat=+441\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-104\n","Skipping y_hat=-104\n","Skipping y_hat=-104\n","Skipping y_hat=-104\n","Skipping y_hat=-104\n","Skipping y_hat=-104\n","Skipping y_hat=-104\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=-245\n","Skipping y_hat=+660\n","Skipping y_hat=+660\n","Skipping y_hat=+660\n","Skipping y_hat=+660\n","Skipping y_hat=+660\n","Skipping y_hat=+660\n","Skipping y_hat=+660\n","Skipping y_hat=-373\n","Skipping y_hat=-373\n","Skipping y_hat=-373\n","Skipping y_hat=-373\n","Skipping y_hat=-373\n","Skipping y_hat=-373\n","Skipping y_hat=-373\n","Skipping y_hat=+145\n","Skipping y_hat=+145\n","Skipping y_hat=+145\n","Skipping y_hat=+145\n","Skipping y_hat=+145\n","Skipping y_hat=+145\n","Skipping y_hat=+145\n","Skipping y_hat=-386\n","Skipping y_hat=-386\n","Skipping y_hat=-386\n","Skipping y_hat=-386\n","Skipping y_hat=-386\n","Skipping y_hat=-386\n","Skipping y_hat=-386\n","Skipping y_hat=-196\n","Skipping y_hat=-196\n","Skipping y_hat=-196\n","Skipping y_hat=-196\n","Skipping y_hat=-196\n","Skipping y_hat=-196\n","Skipping y_hat=-196\n","Skipping y_hat=+650\n","Skipping y_hat=+650\n","Skipping y_hat=+650\n","Skipping y_hat=+650\n","Skipping y_hat=+650\n","Skipping y_hat=+650\n","Skipping y_hat=+650\n","Skipping y_hat=+912\n","Skipping y_hat=+912\n","Skipping y_hat=+912\n","Skipping y_hat=+912\n","Skipping y_hat=+912\n","Skipping y_hat=+912\n","Skipping y_hat=+912\n","Skipping y_hat=+58\n","Skipping y_hat=+58\n","Skipping y_hat=+58\n","Skipping y_hat=+58\n","Skipping y_hat=+58\n","Skipping y_hat=+58\n","Skipping y_hat=+58\n","Skipping y_hat=-547\n","Skipping y_hat=-547\n","Skipping y_hat=-547\n","Skipping y_hat=-547\n","Skipping y_hat=-547\n","Skipping y_hat=-547\n","Skipping y_hat=-547\n","Skipping y_hat=+402\n","Skipping y_hat=+402\n","Skipping y_hat=+402\n","Skipping y_hat=+402\n","Skipping y_hat=+402\n","Skipping y_hat=+402\n","Skipping y_hat=+402\n","Skipping y_hat=-572\n","Skipping y_hat=-572\n","Skipping y_hat=-572\n","Skipping y_hat=-572\n","Skipping y_hat=-572\n","Skipping y_hat=-572\n","Skipping y_hat=-572\n","Skipping y_hat=+547\n","Skipping y_hat=+547\n","Skipping y_hat=+547\n","Skipping y_hat=+547\n","Skipping y_hat=+547\n","Skipping y_hat=+547\n","Skipping y_hat=+547\n","Skipping y_hat=+680\n","Skipping y_hat=+680\n","Skipping y_hat=+680\n","Skipping y_hat=+680\n","Skipping y_hat=+680\n","Skipping y_hat=+680\n","Skipping y_hat=+680\n","Skipping y_hat=-185\n","Skipping y_hat=-185\n","Skipping y_hat=-185\n","Skipping y_hat=-185\n","Skipping y_hat=-185\n","Skipping y_hat=-185\n","Skipping y_hat=-185\n","Skipping y_hat=+555\n","Skipping y_hat=+555\n","Skipping y_hat=+555\n","Skipping y_hat=+555\n","Skipping y_hat=+555\n","Skipping y_hat=+555\n","Skipping y_hat=+555\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+606\n","Skipping y_hat=+606\n","Skipping y_hat=+606\n","Skipping y_hat=+606\n","Skipping y_hat=+606\n","Skipping y_hat=+606\n","Skipping y_hat=+606\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n"," 84% 21/25 [00:08<00:01,  3.19it/s]Skipping y_hat=-593\n","Skipping y_hat=-593\n","Skipping y_hat=-593\n","Skipping y_hat=-593\n","Skipping y_hat=-593\n","Skipping y_hat=-593\n","Skipping y_hat=-593\n","Skipping y_hat=+749\n","Skipping y_hat=+749\n","Skipping y_hat=+749\n","Skipping y_hat=+749\n","Skipping y_hat=+749\n","Skipping y_hat=+749\n","Skipping y_hat=+749\n","Skipping y_hat=+624\n","Skipping y_hat=+624\n","Skipping y_hat=+624\n","Skipping y_hat=+624\n","Skipping y_hat=+624\n","Skipping y_hat=+624\n","Skipping y_hat=+624\n","Skipping y_hat=+131\n","Skipping y_hat=+131\n","Skipping y_hat=+131\n","Skipping y_hat=+131\n","Skipping y_hat=+131\n","Skipping y_hat=+131\n","Skipping y_hat=+131\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=-166\n","Skipping y_hat=-166\n","Skipping y_hat=-166\n","Skipping y_hat=-166\n","Skipping y_hat=-166\n","Skipping y_hat=-166\n","Skipping y_hat=-166\n","Skipping y_hat=+331\n","Skipping y_hat=+331\n","Skipping y_hat=+331\n","Skipping y_hat=+331\n","Skipping y_hat=+331\n","Skipping y_hat=+331\n","Skipping y_hat=+331\n","Skipping y_hat=+506\n","Skipping y_hat=+506\n","Skipping y_hat=+506\n","Skipping y_hat=+506\n","Skipping y_hat=+506\n","Skipping y_hat=+506\n","Skipping y_hat=+506\n","Skipping y_hat=+718\n","Skipping y_hat=+718\n","Skipping y_hat=+718\n","Skipping y_hat=+718\n","Skipping y_hat=+718\n","Skipping y_hat=+718\n","Skipping y_hat=+718\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+541\n","Skipping y_hat=+841\n","Skipping y_hat=+841\n","Skipping y_hat=+841\n","Skipping y_hat=+841\n","Skipping y_hat=+841\n","Skipping y_hat=+841\n","Skipping y_hat=+841\n","Skipping y_hat=-35\n","Skipping y_hat=-35\n","Skipping y_hat=-35\n","Skipping y_hat=-35\n","Skipping y_hat=-35\n","Skipping y_hat=-35\n","Skipping y_hat=-35\n","Skipping y_hat=-188\n","Skipping y_hat=-188\n","Skipping y_hat=-188\n","Skipping y_hat=-188\n","Skipping y_hat=-188\n","Skipping y_hat=-188\n","Skipping y_hat=-188\n","Skipping y_hat=-128\n","Skipping y_hat=-128\n","Skipping y_hat=-128\n","Skipping y_hat=-128\n","Skipping y_hat=-128\n","Skipping y_hat=-128\n","Skipping y_hat=-128\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+907\n","Skipping y_hat=+907\n","Skipping y_hat=+907\n","Skipping y_hat=+907\n","Skipping y_hat=+907\n","Skipping y_hat=+907\n","Skipping y_hat=+907\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=-60\n","Skipping y_hat=+881\n","Skipping y_hat=+881\n","Skipping y_hat=+881\n","Skipping y_hat=+881\n","Skipping y_hat=+881\n","Skipping y_hat=+881\n","Skipping y_hat=+881\n","Skipping y_hat=+242\n","Skipping y_hat=+242\n","Skipping y_hat=+242\n","Skipping y_hat=+242\n","Skipping y_hat=+242\n","Skipping y_hat=+242\n","Skipping y_hat=+242\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=+838\n","Skipping y_hat=+838\n","Skipping y_hat=+838\n","Skipping y_hat=+838\n","Skipping y_hat=+838\n","Skipping y_hat=+838\n","Skipping y_hat=+838\n","Skipping y_hat=+335\n","Skipping y_hat=+335\n","Skipping y_hat=+335\n","Skipping y_hat=+335\n","Skipping y_hat=+335\n","Skipping y_hat=+335\n","Skipping y_hat=+335\n","Skipping y_hat=+713\n","Skipping y_hat=+713\n","Skipping y_hat=+713\n","Skipping y_hat=+713\n","Skipping y_hat=+713\n","Skipping y_hat=+713\n","Skipping y_hat=+713\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=+142\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=+464\n","Skipping y_hat=-486\n","Skipping y_hat=-486\n","Skipping y_hat=-486\n","Skipping y_hat=-486\n","Skipping y_hat=-486\n","Skipping y_hat=-486\n","Skipping y_hat=-486\n","Skipping y_hat=+512\n","Skipping y_hat=+512\n","Skipping y_hat=+512\n","Skipping y_hat=+512\n","Skipping y_hat=+512\n","Skipping y_hat=+512\n","Skipping y_hat=+512\n","Skipping y_hat=-827\n","Skipping y_hat=-827\n","Skipping y_hat=-827\n","Skipping y_hat=-827\n","Skipping y_hat=-827\n","Skipping y_hat=-827\n","Skipping y_hat=-827\n","Skipping y_hat=+785\n","Skipping y_hat=+785\n","Skipping y_hat=+785\n","Skipping y_hat=+785\n","Skipping y_hat=+785\n","Skipping y_hat=+785\n","Skipping y_hat=+785\n","Skipping y_hat=-813\n","Skipping y_hat=-813\n","Skipping y_hat=-813\n","Skipping y_hat=-813\n","Skipping y_hat=-813\n","Skipping y_hat=-813\n","Skipping y_hat=-813\n","Skipping y_hat=-920\n","Skipping y_hat=-920\n","Skipping y_hat=-920\n","Skipping y_hat=-920\n","Skipping y_hat=-920\n","Skipping y_hat=-920\n","Skipping y_hat=-920\n","Skipping y_hat=+431\n","Skipping y_hat=+431\n","Skipping y_hat=+431\n","Skipping y_hat=+431\n","Skipping y_hat=+431\n","Skipping y_hat=+431\n","Skipping y_hat=+431\n","Skipping y_hat=-395\n","Skipping y_hat=-395\n","Skipping y_hat=-395\n","Skipping y_hat=-395\n","Skipping y_hat=-395\n","Skipping y_hat=-395\n","Skipping y_hat=-395\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=+379\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=-569\n","Skipping y_hat=+729\n","Skipping y_hat=+729\n","Skipping y_hat=+729\n","Skipping y_hat=+729\n","Skipping y_hat=+729\n","Skipping y_hat=+729\n","Skipping y_hat=+729\n","Skipping y_hat=+439\n","Skipping y_hat=+439\n","Skipping y_hat=+439\n","Skipping y_hat=+439\n","Skipping y_hat=+439\n","Skipping y_hat=+439\n","Skipping y_hat=+439\n","Skipping y_hat=+903\n","Skipping y_hat=+903\n","Skipping y_hat=+903\n","Skipping y_hat=+903\n","Skipping y_hat=+903\n","Skipping y_hat=+903\n","Skipping y_hat=+903\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+734\n","Skipping y_hat=+734\n","Skipping y_hat=+734\n","Skipping y_hat=+734\n","Skipping y_hat=+734\n","Skipping y_hat=+734\n","Skipping y_hat=+734\n","Skipping y_hat=+804\n","Skipping y_hat=+804\n","Skipping y_hat=+804\n","Skipping y_hat=+804\n","Skipping y_hat=+804\n","Skipping y_hat=+804\n","Skipping y_hat=+804\n","Skipping y_hat=-79\n","Skipping y_hat=-79\n","Skipping y_hat=-79\n","Skipping y_hat=-79\n","Skipping y_hat=-79\n","Skipping y_hat=-79\n","Skipping y_hat=-79\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+293\n","Skipping y_hat=+572\n","Skipping y_hat=+572\n","Skipping y_hat=+572\n","Skipping y_hat=+572\n","Skipping y_hat=+572\n","Skipping y_hat=+572\n","Skipping y_hat=+572\n","Skipping y_hat=-353\n","Skipping y_hat=-353\n","Skipping y_hat=-353\n","Skipping y_hat=-353\n","Skipping y_hat=-353\n","Skipping y_hat=-353\n","Skipping y_hat=-353\n","Skipping y_hat=+902\n","Skipping y_hat=+902\n","Skipping y_hat=+902\n","Skipping y_hat=+902\n","Skipping y_hat=+902\n","Skipping y_hat=+902\n","Skipping y_hat=+902\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=+163\n","Skipping y_hat=-381\n","Skipping y_hat=-381\n","Skipping y_hat=-381\n","Skipping y_hat=-381\n","Skipping y_hat=-381\n","Skipping y_hat=-381\n","Skipping y_hat=-381\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=+283\n","Skipping y_hat=+283\n","Skipping y_hat=+283\n","Skipping y_hat=+283\n","Skipping y_hat=+283\n","Skipping y_hat=+283\n","Skipping y_hat=+283\n","Skipping y_hat=+629\n","Skipping y_hat=+629\n","Skipping y_hat=+629\n","Skipping y_hat=+629\n","Skipping y_hat=+629\n","Skipping y_hat=+629\n","Skipping y_hat=+629\n","Skipping y_hat=-758\n","Skipping y_hat=-758\n","Skipping y_hat=-758\n","Skipping y_hat=-758\n","Skipping y_hat=-758\n","Skipping y_hat=-758\n","Skipping y_hat=-758\n","Skipping y_hat=+573\n","Skipping y_hat=+573\n","Skipping y_hat=+573\n","Skipping y_hat=+573\n","Skipping y_hat=+573\n","Skipping y_hat=+573\n","Skipping y_hat=+573\n","Skipping y_hat=+653\n","Skipping y_hat=+653\n","Skipping y_hat=+653\n","Skipping y_hat=+653\n","Skipping y_hat=+653\n","Skipping y_hat=+653\n","Skipping y_hat=+653\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=+661\n","Skipping y_hat=+661\n","Skipping y_hat=+661\n","Skipping y_hat=+661\n","Skipping y_hat=+661\n","Skipping y_hat=+661\n","Skipping y_hat=+661\n","Skipping y_hat=+539\n","Skipping y_hat=+539\n","Skipping y_hat=+539\n","Skipping y_hat=+539\n","Skipping y_hat=+539\n","Skipping y_hat=+539\n","Skipping y_hat=+539\n","Skipping y_hat=-55\n","Skipping y_hat=-55\n","Skipping y_hat=-55\n","Skipping y_hat=-55\n","Skipping y_hat=-55\n","Skipping y_hat=-55\n","Skipping y_hat=-55\n","Skipping y_hat=-419\n","Skipping y_hat=-419\n","Skipping y_hat=-419\n","Skipping y_hat=-419\n","Skipping y_hat=-419\n","Skipping y_hat=-419\n","Skipping y_hat=-419\n","Skipping y_hat=-897\n","Skipping y_hat=-897\n","Skipping y_hat=-897\n","Skipping y_hat=-897\n","Skipping y_hat=-897\n","Skipping y_hat=-897\n","Skipping y_hat=-897\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=-73\n","Skipping y_hat=-828\n","Skipping y_hat=-828\n","Skipping y_hat=-828\n","Skipping y_hat=-828\n","Skipping y_hat=-828\n","Skipping y_hat=-828\n","Skipping y_hat=-828\n","Skipping y_hat=-817\n","Skipping y_hat=-817\n","Skipping y_hat=-817\n","Skipping y_hat=-817\n","Skipping y_hat=-817\n","Skipping y_hat=-817\n","Skipping y_hat=-817\n","Skipping y_hat=-525\n","Skipping y_hat=-525\n","Skipping y_hat=-525\n","Skipping y_hat=-525\n","Skipping y_hat=-525\n","Skipping y_hat=-525\n","Skipping y_hat=-525\n","Skipping y_hat=-503\n","Skipping y_hat=-503\n","Skipping y_hat=-503\n","Skipping y_hat=-503\n","Skipping y_hat=-503\n","Skipping y_hat=-503\n","Skipping y_hat=-503\n","Skipping y_hat=+668\n","Skipping y_hat=+668\n","Skipping y_hat=+668\n","Skipping y_hat=+668\n","Skipping y_hat=+668\n","Skipping y_hat=+668\n","Skipping y_hat=+668\n","Skipping y_hat=+879\n","Skipping y_hat=+879\n","Skipping y_hat=+879\n","Skipping y_hat=+879\n","Skipping y_hat=+879\n","Skipping y_hat=+879\n","Skipping y_hat=+879\n","Skipping y_hat=-961\n","Skipping y_hat=-961\n","Skipping y_hat=-961\n","Skipping y_hat=-961\n","Skipping y_hat=-961\n","Skipping y_hat=-961\n","Skipping y_hat=-961\n","Skipping y_hat=+543\n","Skipping y_hat=+543\n","Skipping y_hat=+543\n","Skipping y_hat=+543\n","Skipping y_hat=+543\n","Skipping y_hat=+543\n","Skipping y_hat=+543\n","Skipping y_hat=+750\n","Skipping y_hat=+750\n","Skipping y_hat=+750\n","Skipping y_hat=+750\n","Skipping y_hat=+750\n","Skipping y_hat=+750\n","Skipping y_hat=+750\n","Skipping y_hat=+873\n","Skipping y_hat=+873\n","Skipping y_hat=+873\n","Skipping y_hat=+873\n","Skipping y_hat=+873\n","Skipping y_hat=+873\n","Skipping y_hat=+873\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-300\n","Skipping y_hat=-305\n","Skipping y_hat=-305\n","Skipping y_hat=-305\n","Skipping y_hat=-305\n","Skipping y_hat=-305\n","Skipping y_hat=-305\n","Skipping y_hat=-305\n","Skipping y_hat=-122\n","Skipping y_hat=-122\n","Skipping y_hat=-122\n","Skipping y_hat=-122\n","Skipping y_hat=-122\n","Skipping y_hat=-122\n","Skipping y_hat=-122\n","Skipping y_hat=+526\n","Skipping y_hat=+526\n","Skipping y_hat=+526\n","Skipping y_hat=+526\n","Skipping y_hat=+526\n","Skipping y_hat=+526\n","Skipping y_hat=+526\n","Skipping y_hat=-614\n","Skipping y_hat=-614\n","Skipping y_hat=-614\n","Skipping y_hat=-614\n","Skipping y_hat=-614\n","Skipping y_hat=-614\n","Skipping y_hat=-614\n","Skipping y_hat=+269\n","Skipping y_hat=+269\n","Skipping y_hat=+269\n","Skipping y_hat=+269\n","Skipping y_hat=+269\n","Skipping y_hat=+269\n","Skipping y_hat=+269\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+264\n","Skipping y_hat=+66\n","Skipping y_hat=+66\n","Skipping y_hat=+66\n","Skipping y_hat=+66\n","Skipping y_hat=+66\n","Skipping y_hat=+66\n","Skipping y_hat=+66\n","Skipping y_hat=-21\n","Skipping y_hat=-21\n","Skipping y_hat=-21\n","Skipping y_hat=-21\n","Skipping y_hat=-21\n","Skipping y_hat=-21\n","Skipping y_hat=-21\n","Skipping y_hat=-268\n","Skipping y_hat=-268\n","Skipping y_hat=-268\n","Skipping y_hat=-268\n","Skipping y_hat=-268\n","Skipping y_hat=-268\n","Skipping y_hat=-268\n","Skipping y_hat=-403\n","Skipping y_hat=-403\n","Skipping y_hat=-403\n","Skipping y_hat=-403\n","Skipping y_hat=-403\n","Skipping y_hat=-403\n","Skipping y_hat=-403\n","Skipping y_hat=-304\n","Skipping y_hat=-304\n","Skipping y_hat=-304\n","Skipping y_hat=-304\n","Skipping y_hat=-304\n","Skipping y_hat=-304\n","Skipping y_hat=-304\n","Skipping y_hat=-456\n","Skipping y_hat=-456\n","Skipping y_hat=-456\n","Skipping y_hat=-456\n","Skipping y_hat=-456\n","Skipping y_hat=-456\n","Skipping y_hat=-456\n","Skipping y_hat=-843\n","Skipping y_hat=-843\n","Skipping y_hat=-843\n","Skipping y_hat=-843\n","Skipping y_hat=-843\n","Skipping y_hat=-843\n","Skipping y_hat=-843\n","Skipping y_hat=-54\n","Skipping y_hat=-54\n","Skipping y_hat=-54\n","Skipping y_hat=-54\n","Skipping y_hat=-54\n","Skipping y_hat=-54\n","Skipping y_hat=-54\n","Skipping y_hat=+149\n","Skipping y_hat=+149\n","Skipping y_hat=+149\n","Skipping y_hat=+149\n","Skipping y_hat=+149\n","Skipping y_hat=+149\n","Skipping y_hat=+149\n","Skipping y_hat=-945\n","Skipping y_hat=-945\n","Skipping y_hat=-945\n","Skipping y_hat=-945\n","Skipping y_hat=-945\n","Skipping y_hat=-945\n","Skipping y_hat=-945\n","Skipping y_hat=-404\n","Skipping y_hat=-404\n","Skipping y_hat=-404\n","Skipping y_hat=-404\n","Skipping y_hat=-404\n","Skipping y_hat=-404\n","Skipping y_hat=-404\n","Skipping y_hat=+175\n","Skipping y_hat=+175\n","Skipping y_hat=+175\n","Skipping y_hat=+175\n","Skipping y_hat=+175\n","Skipping y_hat=+175\n","Skipping y_hat=+175\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=+525\n","Skipping y_hat=+525\n","Skipping y_hat=+525\n","Skipping y_hat=+525\n","Skipping y_hat=+525\n","Skipping y_hat=+525\n","Skipping y_hat=+525\n","Skipping y_hat=+932\n","Skipping y_hat=+932\n","Skipping y_hat=+932\n","Skipping y_hat=+932\n","Skipping y_hat=+932\n","Skipping y_hat=+932\n","Skipping y_hat=+932\n","Skipping y_hat=+826\n","Skipping y_hat=+826\n","Skipping y_hat=+826\n","Skipping y_hat=+826\n","Skipping y_hat=+826\n","Skipping y_hat=+826\n","Skipping y_hat=+826\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=+705\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=-437\n","Skipping y_hat=+198\n","Skipping y_hat=+198\n","Skipping y_hat=+198\n","Skipping y_hat=+198\n","Skipping y_hat=+198\n","Skipping y_hat=+198\n","Skipping y_hat=+198\n","Skipping y_hat=+837\n","Skipping y_hat=+837\n","Skipping y_hat=+837\n","Skipping y_hat=+837\n","Skipping y_hat=+837\n","Skipping y_hat=+837\n","Skipping y_hat=+837\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=+417\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=+88\n","Skipping y_hat=-767\n","Skipping y_hat=-767\n","Skipping y_hat=-767\n","Skipping y_hat=-767\n","Skipping y_hat=-767\n","Skipping y_hat=-767\n","Skipping y_hat=-767\n","Skipping y_hat=+167\n","Skipping y_hat=+167\n","Skipping y_hat=+167\n","Skipping y_hat=+167\n","Skipping y_hat=+167\n","Skipping y_hat=+167\n","Skipping y_hat=+167\n","Skipping y_hat=-718\n","Skipping y_hat=-718\n","Skipping y_hat=-718\n","Skipping y_hat=-718\n","Skipping y_hat=-718\n","Skipping y_hat=-718\n","Skipping y_hat=-718\n","Skipping y_hat=-938\n","Skipping y_hat=-938\n","Skipping y_hat=-938\n","Skipping y_hat=-938\n","Skipping y_hat=-938\n","Skipping y_hat=-938\n","Skipping y_hat=-938\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=+840\n","Skipping y_hat=+840\n","Skipping y_hat=+840\n","Skipping y_hat=+840\n","Skipping y_hat=+840\n","Skipping y_hat=+840\n","Skipping y_hat=+840\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=-168\n","Skipping y_hat=-168\n","Skipping y_hat=-168\n","Skipping y_hat=-168\n","Skipping y_hat=-168\n","Skipping y_hat=-168\n","Skipping y_hat=-168\n","Skipping y_hat=-518\n","Skipping y_hat=-518\n","Skipping y_hat=-518\n","Skipping y_hat=-518\n","Skipping y_hat=-518\n","Skipping y_hat=-518\n","Skipping y_hat=-518\n","Skipping y_hat=+605\n","Skipping y_hat=+605\n","Skipping y_hat=+605\n","Skipping y_hat=+605\n","Skipping y_hat=+605\n","Skipping y_hat=+605\n","Skipping y_hat=+605\n","Skipping y_hat=+448\n","Skipping y_hat=+448\n","Skipping y_hat=+448\n","Skipping y_hat=+448\n","Skipping y_hat=+448\n","Skipping y_hat=+448\n","Skipping y_hat=+448\n","Skipping y_hat=+135\n","Skipping y_hat=+135\n","Skipping y_hat=+135\n","Skipping y_hat=+135\n","Skipping y_hat=+135\n","Skipping y_hat=+135\n","Skipping y_hat=+135\n","Skipping y_hat=+370\n","Skipping y_hat=+370\n","Skipping y_hat=+370\n","Skipping y_hat=+370\n","Skipping y_hat=+370\n","Skipping y_hat=+370\n","Skipping y_hat=+370\n","Skipping y_hat=-605\n","Skipping y_hat=-605\n","Skipping y_hat=-605\n","Skipping y_hat=-605\n","Skipping y_hat=-605\n","Skipping y_hat=-605\n","Skipping y_hat=-605\n","Skipping y_hat=+433\n","Skipping y_hat=+433\n","Skipping y_hat=+433\n","Skipping y_hat=+433\n","Skipping y_hat=+433\n","Skipping y_hat=+433\n","Skipping y_hat=+433\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=+113\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=+596\n","Skipping y_hat=-360\n","Skipping y_hat=-360\n","Skipping y_hat=-360\n","Skipping y_hat=-360\n","Skipping y_hat=-360\n","Skipping y_hat=-360\n","Skipping y_hat=-360\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=+227\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=-598\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=-660\n","Skipping y_hat=+654\n","Skipping y_hat=+654\n","Skipping y_hat=+654\n","Skipping y_hat=+654\n","Skipping y_hat=+654\n","Skipping y_hat=+654\n","Skipping y_hat=+654\n","Skipping y_hat=-502\n","Skipping y_hat=-502\n","Skipping y_hat=-502\n","Skipping y_hat=-502\n","Skipping y_hat=-502\n","Skipping y_hat=-502\n","Skipping y_hat=-502\n","Skipping y_hat=-71\n","Skipping y_hat=-71\n","Skipping y_hat=-71\n","Skipping y_hat=-71\n","Skipping y_hat=-71\n","Skipping y_hat=-71\n","Skipping y_hat=-71\n","Skipping y_hat=+483\n","Skipping y_hat=+483\n","Skipping y_hat=+483\n","Skipping y_hat=+483\n","Skipping y_hat=+483\n","Skipping y_hat=+483\n","Skipping y_hat=+483\n","Skipping y_hat=-810\n","Skipping y_hat=-810\n","Skipping y_hat=-810\n","Skipping y_hat=-810\n","Skipping y_hat=-810\n","Skipping y_hat=-810\n","Skipping y_hat=-810\n"," 88% 22/25 [00:09<00:00,  3.28it/s]Skipping y_hat=+436\n","Skipping y_hat=+436\n","Skipping y_hat=+436\n","Skipping y_hat=+436\n","Skipping y_hat=+436\n","Skipping y_hat=+436\n","Skipping y_hat=+436\n","Skipping y_hat=+604\n","Skipping y_hat=+604\n","Skipping y_hat=+604\n","Skipping y_hat=+604\n","Skipping y_hat=+604\n","Skipping y_hat=+604\n","Skipping y_hat=+604\n","Skipping y_hat=+648\n","Skipping y_hat=+648\n","Skipping y_hat=+648\n","Skipping y_hat=+648\n","Skipping y_hat=+648\n","Skipping y_hat=+648\n","Skipping y_hat=+648\n","Skipping y_hat=+111\n","Skipping y_hat=+111\n","Skipping y_hat=+111\n","Skipping y_hat=+111\n","Skipping y_hat=+111\n","Skipping y_hat=+111\n","Skipping y_hat=+111\n","Skipping y_hat=-768\n","Skipping y_hat=-768\n","Skipping y_hat=-768\n","Skipping y_hat=-768\n","Skipping y_hat=-768\n","Skipping y_hat=-768\n","Skipping y_hat=-768\n","Skipping y_hat=+514\n","Skipping y_hat=+514\n","Skipping y_hat=+514\n","Skipping y_hat=+514\n","Skipping y_hat=+514\n","Skipping y_hat=+514\n","Skipping y_hat=+514\n","Skipping y_hat=-774\n","Skipping y_hat=-774\n","Skipping y_hat=-774\n","Skipping y_hat=-774\n","Skipping y_hat=-774\n","Skipping y_hat=-774\n","Skipping y_hat=-774\n","Skipping y_hat=+08\n","Skipping y_hat=+08\n","Skipping y_hat=+08\n","Skipping y_hat=+08\n","Skipping y_hat=+08\n","Skipping y_hat=+08\n","Skipping y_hat=+08\n","Skipping y_hat=+751\n","Skipping y_hat=+751\n","Skipping y_hat=+751\n","Skipping y_hat=+751\n","Skipping y_hat=+751\n","Skipping y_hat=+751\n","Skipping y_hat=+751\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-544\n","Skipping y_hat=-544\n","Skipping y_hat=-544\n","Skipping y_hat=-544\n","Skipping y_hat=-544\n","Skipping y_hat=-544\n","Skipping y_hat=-544\n","Skipping y_hat=-481\n","Skipping y_hat=-481\n","Skipping y_hat=-481\n","Skipping y_hat=-481\n","Skipping y_hat=-481\n","Skipping y_hat=-481\n","Skipping y_hat=-481\n","Skipping y_hat=+889\n","Skipping y_hat=+889\n","Skipping y_hat=+889\n","Skipping y_hat=+889\n","Skipping y_hat=+889\n","Skipping y_hat=+889\n","Skipping y_hat=+889\n","Skipping y_hat=+45\n","Skipping y_hat=+45\n","Skipping y_hat=+45\n","Skipping y_hat=+45\n","Skipping y_hat=+45\n","Skipping y_hat=+45\n","Skipping y_hat=+45\n","Skipping y_hat=+364\n","Skipping y_hat=+364\n","Skipping y_hat=+364\n","Skipping y_hat=+364\n","Skipping y_hat=+364\n","Skipping y_hat=+364\n","Skipping y_hat=+364\n","Skipping y_hat=-538\n","Skipping y_hat=-538\n","Skipping y_hat=-538\n","Skipping y_hat=-538\n","Skipping y_hat=-538\n","Skipping y_hat=-538\n","Skipping y_hat=-538\n","Skipping y_hat=+816\n","Skipping y_hat=+816\n","Skipping y_hat=+816\n","Skipping y_hat=+816\n","Skipping y_hat=+816\n","Skipping y_hat=+816\n","Skipping y_hat=+816\n","Skipping y_hat=+939\n","Skipping y_hat=+939\n","Skipping y_hat=+939\n","Skipping y_hat=+939\n","Skipping y_hat=+939\n","Skipping y_hat=+939\n","Skipping y_hat=+939\n","Skipping y_hat=+581\n","Skipping y_hat=+581\n","Skipping y_hat=+581\n","Skipping y_hat=+581\n","Skipping y_hat=+581\n","Skipping y_hat=+581\n","Skipping y_hat=+581\n","Skipping y_hat=-231\n","Skipping y_hat=-231\n","Skipping y_hat=-231\n","Skipping y_hat=-231\n","Skipping y_hat=-231\n","Skipping y_hat=-231\n","Skipping y_hat=-231\n","Skipping y_hat=+796\n","Skipping y_hat=+796\n","Skipping y_hat=+796\n","Skipping y_hat=+796\n","Skipping y_hat=+796\n","Skipping y_hat=+796\n","Skipping y_hat=+796\n","Skipping y_hat=+354\n","Skipping y_hat=+354\n","Skipping y_hat=+354\n","Skipping y_hat=+354\n","Skipping y_hat=+354\n","Skipping y_hat=+354\n","Skipping y_hat=+354\n","Skipping y_hat=-354\n","Skipping y_hat=-354\n","Skipping y_hat=-354\n","Skipping y_hat=-354\n","Skipping y_hat=-354\n","Skipping y_hat=-354\n","Skipping y_hat=-354\n","Skipping y_hat=-327\n","Skipping y_hat=-327\n","Skipping y_hat=-327\n","Skipping y_hat=-327\n","Skipping y_hat=-327\n","Skipping y_hat=-327\n","Skipping y_hat=-327\n","Skipping y_hat=-780\n","Skipping y_hat=-780\n","Skipping y_hat=-780\n","Skipping y_hat=-780\n","Skipping y_hat=-780\n","Skipping y_hat=-780\n","Skipping y_hat=-780\n","Skipping y_hat=+82\n","Skipping y_hat=+82\n","Skipping y_hat=+82\n","Skipping y_hat=+82\n","Skipping y_hat=+82\n","Skipping y_hat=+82\n","Skipping y_hat=+82\n","Skipping y_hat=+695\n","Skipping y_hat=+695\n","Skipping y_hat=+695\n","Skipping y_hat=+695\n","Skipping y_hat=+695\n","Skipping y_hat=+695\n","Skipping y_hat=+695\n","Skipping y_hat=+258\n","Skipping y_hat=+258\n","Skipping y_hat=+258\n","Skipping y_hat=+258\n","Skipping y_hat=+258\n","Skipping y_hat=+258\n","Skipping y_hat=+258\n","Skipping y_hat=-96\n","Skipping y_hat=-96\n","Skipping y_hat=-96\n","Skipping y_hat=-96\n","Skipping y_hat=-96\n","Skipping y_hat=-96\n","Skipping y_hat=-96\n","Skipping y_hat=-637\n","Skipping y_hat=-637\n","Skipping y_hat=-637\n","Skipping y_hat=-637\n","Skipping y_hat=-637\n","Skipping y_hat=-637\n","Skipping y_hat=-637\n","Skipping y_hat=+819\n","Skipping y_hat=+819\n","Skipping y_hat=+819\n","Skipping y_hat=+819\n","Skipping y_hat=+819\n","Skipping y_hat=+819\n","Skipping y_hat=+819\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-342\n","Skipping y_hat=-414\n","Skipping y_hat=-414\n","Skipping y_hat=-414\n","Skipping y_hat=-414\n","Skipping y_hat=-414\n","Skipping y_hat=-414\n","Skipping y_hat=-414\n","Skipping y_hat=-491\n","Skipping y_hat=-491\n","Skipping y_hat=-491\n","Skipping y_hat=-491\n","Skipping y_hat=-491\n","Skipping y_hat=-491\n","Skipping y_hat=-491\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=-464\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=+762\n","Skipping y_hat=+762\n","Skipping y_hat=+762\n","Skipping y_hat=+762\n","Skipping y_hat=+762\n","Skipping y_hat=+762\n","Skipping y_hat=+762\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=+419\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=+667\n","Skipping y_hat=-137\n","Skipping y_hat=-137\n","Skipping y_hat=-137\n","Skipping y_hat=-137\n","Skipping y_hat=-137\n","Skipping y_hat=-137\n","Skipping y_hat=-137\n","Skipping y_hat=+126\n","Skipping y_hat=+126\n","Skipping y_hat=+126\n","Skipping y_hat=+126\n","Skipping y_hat=+126\n","Skipping y_hat=+126\n","Skipping y_hat=+126\n","Skipping y_hat=-946\n","Skipping y_hat=-946\n","Skipping y_hat=-946\n","Skipping y_hat=-946\n","Skipping y_hat=-946\n","Skipping y_hat=-946\n","Skipping y_hat=-946\n","Skipping y_hat=+212\n","Skipping y_hat=+212\n","Skipping y_hat=+212\n","Skipping y_hat=+212\n","Skipping y_hat=+212\n","Skipping y_hat=+212\n","Skipping y_hat=+212\n","Skipping y_hat=-87\n","Skipping y_hat=-87\n","Skipping y_hat=-87\n","Skipping y_hat=-87\n","Skipping y_hat=-87\n","Skipping y_hat=-87\n","Skipping y_hat=-87\n","Skipping y_hat=-103\n","Skipping y_hat=-103\n","Skipping y_hat=-103\n","Skipping y_hat=-103\n","Skipping y_hat=-103\n","Skipping y_hat=-103\n","Skipping y_hat=-103\n","Skipping y_hat=-790\n","Skipping y_hat=-790\n","Skipping y_hat=-790\n","Skipping y_hat=-790\n","Skipping y_hat=-790\n","Skipping y_hat=-790\n","Skipping y_hat=-790\n","Skipping y_hat=+153\n","Skipping y_hat=+153\n","Skipping y_hat=+153\n","Skipping y_hat=+153\n","Skipping y_hat=+153\n","Skipping y_hat=+153\n","Skipping y_hat=+153\n","Skipping y_hat=+90\n","Skipping y_hat=+90\n","Skipping y_hat=+90\n","Skipping y_hat=+90\n","Skipping y_hat=+90\n","Skipping y_hat=+90\n","Skipping y_hat=+90\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+117\n","Skipping y_hat=+168\n","Skipping y_hat=+168\n","Skipping y_hat=+168\n","Skipping y_hat=+168\n","Skipping y_hat=+168\n","Skipping y_hat=+168\n","Skipping y_hat=+168\n","Skipping y_hat=+827\n","Skipping y_hat=+827\n","Skipping y_hat=+827\n","Skipping y_hat=+827\n","Skipping y_hat=+827\n","Skipping y_hat=+827\n","Skipping y_hat=+827\n","Skipping y_hat=+327\n","Skipping y_hat=+327\n","Skipping y_hat=+327\n","Skipping y_hat=+327\n","Skipping y_hat=+327\n","Skipping y_hat=+327\n","Skipping y_hat=+327\n","Skipping y_hat=-822\n","Skipping y_hat=-822\n","Skipping y_hat=-822\n","Skipping y_hat=-822\n","Skipping y_hat=-822\n","Skipping y_hat=-822\n","Skipping y_hat=-822\n","Skipping y_hat=+93\n","Skipping y_hat=+93\n","Skipping y_hat=+93\n","Skipping y_hat=+93\n","Skipping y_hat=+93\n","Skipping y_hat=+93\n","Skipping y_hat=+93\n","Skipping y_hat=+421\n","Skipping y_hat=+421\n","Skipping y_hat=+421\n","Skipping y_hat=+421\n","Skipping y_hat=+421\n","Skipping y_hat=+421\n","Skipping y_hat=+421\n","Skipping y_hat=-443\n","Skipping y_hat=-443\n","Skipping y_hat=-443\n","Skipping y_hat=-443\n","Skipping y_hat=-443\n","Skipping y_hat=-443\n","Skipping y_hat=-443\n","Skipping y_hat=-906\n","Skipping y_hat=-906\n","Skipping y_hat=-906\n","Skipping y_hat=-906\n","Skipping y_hat=-906\n","Skipping y_hat=-906\n","Skipping y_hat=-906\n","Skipping y_hat=-673\n","Skipping y_hat=-673\n","Skipping y_hat=-673\n","Skipping y_hat=-673\n","Skipping y_hat=-673\n","Skipping y_hat=-673\n","Skipping y_hat=-673\n","Skipping y_hat=+754\n","Skipping y_hat=+754\n","Skipping y_hat=+754\n","Skipping y_hat=+754\n","Skipping y_hat=+754\n","Skipping y_hat=+754\n","Skipping y_hat=+754\n","Skipping y_hat=-979\n","Skipping y_hat=-979\n","Skipping y_hat=-979\n","Skipping y_hat=-979\n","Skipping y_hat=-979\n","Skipping y_hat=-979\n","Skipping y_hat=-979\n","Skipping y_hat=-690\n","Skipping y_hat=-690\n","Skipping y_hat=-690\n","Skipping y_hat=-690\n","Skipping y_hat=-690\n","Skipping y_hat=-690\n","Skipping y_hat=-690\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=-211\n","Skipping y_hat=+123\n","Skipping y_hat=+123\n","Skipping y_hat=+123\n","Skipping y_hat=+123\n","Skipping y_hat=+123\n","Skipping y_hat=+123\n","Skipping y_hat=+123\n","Skipping y_hat=-415\n","Skipping y_hat=-415\n","Skipping y_hat=-415\n","Skipping y_hat=-415\n","Skipping y_hat=-415\n","Skipping y_hat=-415\n","Skipping y_hat=-415\n","Skipping y_hat=-679\n","Skipping y_hat=-679\n","Skipping y_hat=-679\n","Skipping y_hat=-679\n","Skipping y_hat=-679\n","Skipping y_hat=-679\n","Skipping y_hat=-679\n","Skipping y_hat=-729\n","Skipping y_hat=-729\n","Skipping y_hat=-729\n","Skipping y_hat=-729\n","Skipping y_hat=-729\n","Skipping y_hat=-729\n","Skipping y_hat=-729\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=-441\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=+643\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=-576\n","Skipping y_hat=+808\n","Skipping y_hat=+808\n","Skipping y_hat=+808\n","Skipping y_hat=+808\n","Skipping y_hat=+808\n","Skipping y_hat=+808\n","Skipping y_hat=+808\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=-429\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-149\n","Skipping y_hat=-717\n","Skipping y_hat=-717\n","Skipping y_hat=-717\n","Skipping y_hat=-717\n","Skipping y_hat=-717\n","Skipping y_hat=-717\n","Skipping y_hat=-717\n","Skipping y_hat=+268\n","Skipping y_hat=+268\n","Skipping y_hat=+268\n","Skipping y_hat=+268\n","Skipping y_hat=+268\n","Skipping y_hat=+268\n","Skipping y_hat=+268\n","Skipping y_hat=+116\n","Skipping y_hat=+116\n","Skipping y_hat=+116\n","Skipping y_hat=+116\n","Skipping y_hat=+116\n","Skipping y_hat=+116\n","Skipping y_hat=+116\n","Skipping y_hat=+246\n","Skipping y_hat=+246\n","Skipping y_hat=+246\n","Skipping y_hat=+246\n","Skipping y_hat=+246\n","Skipping y_hat=+246\n","Skipping y_hat=+246\n","Skipping y_hat=-643\n","Skipping y_hat=-643\n","Skipping y_hat=-643\n","Skipping y_hat=-643\n","Skipping y_hat=-643\n","Skipping y_hat=-643\n","Skipping y_hat=-643\n","Skipping y_hat=-869\n","Skipping y_hat=-869\n","Skipping y_hat=-869\n","Skipping y_hat=-869\n","Skipping y_hat=-869\n","Skipping y_hat=-869\n","Skipping y_hat=-869\n","Skipping y_hat=-728\n","Skipping y_hat=-728\n","Skipping y_hat=-728\n","Skipping y_hat=-728\n","Skipping y_hat=-728\n","Skipping y_hat=-728\n","Skipping y_hat=-728\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+120\n","Skipping y_hat=+322\n","Skipping y_hat=+322\n","Skipping y_hat=+322\n","Skipping y_hat=+322\n","Skipping y_hat=+322\n","Skipping y_hat=+322\n","Skipping y_hat=+322\n","Skipping y_hat=+446\n","Skipping y_hat=+446\n","Skipping y_hat=+446\n","Skipping y_hat=+446\n","Skipping y_hat=+446\n","Skipping y_hat=+446\n","Skipping y_hat=+446\n","Skipping y_hat=+468\n","Skipping y_hat=+468\n","Skipping y_hat=+468\n","Skipping y_hat=+468\n","Skipping y_hat=+468\n","Skipping y_hat=+468\n","Skipping y_hat=+468\n","Skipping y_hat=-82\n","Skipping y_hat=-82\n","Skipping y_hat=-82\n","Skipping y_hat=-82\n","Skipping y_hat=-82\n","Skipping y_hat=-82\n","Skipping y_hat=-82\n","Skipping y_hat=+466\n","Skipping y_hat=+466\n","Skipping y_hat=+466\n","Skipping y_hat=+466\n","Skipping y_hat=+466\n","Skipping y_hat=+466\n","Skipping y_hat=+466\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=+702\n","Skipping y_hat=+443\n","Skipping y_hat=+443\n","Skipping y_hat=+443\n","Skipping y_hat=+443\n","Skipping y_hat=+443\n","Skipping y_hat=+443\n","Skipping y_hat=+443\n","Skipping y_hat=-647\n","Skipping y_hat=-647\n","Skipping y_hat=-647\n","Skipping y_hat=-647\n","Skipping y_hat=-647\n","Skipping y_hat=-647\n","Skipping y_hat=-647\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-763\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n","Skipping y_hat=-109\n","Skipping y_hat=+815\n","Skipping y_hat=+815\n","Skipping y_hat=+815\n","Skipping y_hat=+815\n","Skipping y_hat=+815\n","Skipping y_hat=+815\n","Skipping y_hat=+815\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=+165\n","Skipping y_hat=-337\n","Skipping y_hat=-337\n","Skipping y_hat=-337\n","Skipping y_hat=-337\n","Skipping y_hat=-337\n","Skipping y_hat=-337\n","Skipping y_hat=-337\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=+405\n","Skipping y_hat=-875\n","Skipping y_hat=-875\n","Skipping y_hat=-875\n","Skipping y_hat=-875\n","Skipping y_hat=-875\n","Skipping y_hat=-875\n","Skipping y_hat=-875\n","Skipping y_hat=+205\n","Skipping y_hat=+205\n","Skipping y_hat=+205\n","Skipping y_hat=+205\n","Skipping y_hat=+205\n","Skipping y_hat=+205\n","Skipping y_hat=+205\n","Skipping y_hat=-635\n","Skipping y_hat=-635\n","Skipping y_hat=-635\n","Skipping y_hat=-635\n","Skipping y_hat=-635\n","Skipping y_hat=-635\n","Skipping y_hat=-635\n","Skipping y_hat=+412\n","Skipping y_hat=+412\n","Skipping y_hat=+412\n","Skipping y_hat=+412\n","Skipping y_hat=+412\n","Skipping y_hat=+412\n","Skipping y_hat=+412\n","Skipping y_hat=-832\n","Skipping y_hat=-832\n","Skipping y_hat=-832\n","Skipping y_hat=-832\n","Skipping y_hat=-832\n","Skipping y_hat=-832\n","Skipping y_hat=-832\n"," 92% 23/25 [00:09<00:00,  2.66it/s]Skipping y_hat=-281\n","Skipping y_hat=-281\n","Skipping y_hat=-281\n","Skipping y_hat=-281\n","Skipping y_hat=-281\n","Skipping y_hat=-281\n","Skipping y_hat=-281\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=-744\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=+440\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=-139\n","Skipping y_hat=+346\n","Skipping y_hat=+346\n","Skipping y_hat=+346\n","Skipping y_hat=+346\n","Skipping y_hat=+346\n","Skipping y_hat=+346\n","Skipping y_hat=+346\n","Skipping y_hat=-51\n","Skipping y_hat=-51\n","Skipping y_hat=-51\n","Skipping y_hat=-51\n","Skipping y_hat=-51\n","Skipping y_hat=-51\n","Skipping y_hat=-51\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=+574\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=-953\n","Skipping y_hat=-953\n","Skipping y_hat=-953\n","Skipping y_hat=-953\n","Skipping y_hat=-953\n","Skipping y_hat=-953\n","Skipping y_hat=-953\n","Skipping y_hat=+571\n","Skipping y_hat=+571\n","Skipping y_hat=+571\n","Skipping y_hat=+571\n","Skipping y_hat=+571\n","Skipping y_hat=+571\n","Skipping y_hat=+571\n","Skipping y_hat=-645\n","Skipping y_hat=-645\n","Skipping y_hat=-645\n","Skipping y_hat=-645\n","Skipping y_hat=-645\n","Skipping y_hat=-645\n","Skipping y_hat=-645\n","Skipping y_hat=-390\n","Skipping y_hat=-390\n","Skipping y_hat=-390\n","Skipping y_hat=-390\n","Skipping y_hat=-390\n","Skipping y_hat=-390\n","Skipping y_hat=-390\n","Skipping y_hat=-823\n","Skipping y_hat=-823\n","Skipping y_hat=-823\n","Skipping y_hat=-823\n","Skipping y_hat=-823\n","Skipping y_hat=-823\n","Skipping y_hat=-823\n","Skipping y_hat=+974\n","Skipping y_hat=+974\n","Skipping y_hat=+974\n","Skipping y_hat=+974\n","Skipping y_hat=+974\n","Skipping y_hat=+974\n","Skipping y_hat=+974\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=-362\n","Skipping y_hat=+15\n","Skipping y_hat=+15\n","Skipping y_hat=+15\n","Skipping y_hat=+15\n","Skipping y_hat=+15\n","Skipping y_hat=+15\n","Skipping y_hat=+15\n","Skipping y_hat=-0\n","Skipping y_hat=-0\n","Skipping y_hat=-0\n","Skipping y_hat=-0\n","Skipping y_hat=-0\n","Skipping y_hat=-0\n","Skipping y_hat=-0\n","Skipping y_hat=-602\n","Skipping y_hat=-602\n","Skipping y_hat=-602\n","Skipping y_hat=-602\n","Skipping y_hat=-602\n","Skipping y_hat=-602\n","Skipping y_hat=-602\n","Skipping y_hat=-158\n","Skipping y_hat=-158\n","Skipping y_hat=-158\n","Skipping y_hat=-158\n","Skipping y_hat=-158\n","Skipping y_hat=-158\n","Skipping y_hat=-158\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=-328\n","Skipping y_hat=+801\n","Skipping y_hat=+801\n","Skipping y_hat=+801\n","Skipping y_hat=+801\n","Skipping y_hat=+801\n","Skipping y_hat=+801\n","Skipping y_hat=+801\n","Skipping y_hat=+807\n","Skipping y_hat=+807\n","Skipping y_hat=+807\n","Skipping y_hat=+807\n","Skipping y_hat=+807\n","Skipping y_hat=+807\n","Skipping y_hat=+807\n","Skipping y_hat=-889\n","Skipping y_hat=-889\n","Skipping y_hat=-889\n","Skipping y_hat=-889\n","Skipping y_hat=-889\n","Skipping y_hat=-889\n","Skipping y_hat=-889\n","Skipping y_hat=-154\n","Skipping y_hat=-154\n","Skipping y_hat=-154\n","Skipping y_hat=-154\n","Skipping y_hat=-154\n","Skipping y_hat=-154\n","Skipping y_hat=-154\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+692\n","Skipping y_hat=+735\n","Skipping y_hat=+735\n","Skipping y_hat=+735\n","Skipping y_hat=+735\n","Skipping y_hat=+735\n","Skipping y_hat=+735\n","Skipping y_hat=+735\n","Skipping y_hat=-107\n","Skipping y_hat=-107\n","Skipping y_hat=-107\n","Skipping y_hat=-107\n","Skipping y_hat=-107\n","Skipping y_hat=-107\n","Skipping y_hat=-107\n","Skipping y_hat=-552\n","Skipping y_hat=-552\n","Skipping y_hat=-552\n","Skipping y_hat=-552\n","Skipping y_hat=-552\n","Skipping y_hat=-552\n","Skipping y_hat=-552\n","Skipping y_hat=-948\n","Skipping y_hat=-948\n","Skipping y_hat=-948\n","Skipping y_hat=-948\n","Skipping y_hat=-948\n","Skipping y_hat=-948\n","Skipping y_hat=-948\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=-39\n","Skipping y_hat=+200\n","Skipping y_hat=+200\n","Skipping y_hat=+200\n","Skipping y_hat=+200\n","Skipping y_hat=+200\n","Skipping y_hat=+200\n","Skipping y_hat=+200\n","Skipping y_hat=-224\n","Skipping y_hat=-224\n","Skipping y_hat=-224\n","Skipping y_hat=-224\n","Skipping y_hat=-224\n","Skipping y_hat=-224\n","Skipping y_hat=-224\n","Skipping y_hat=-542\n","Skipping y_hat=-542\n","Skipping y_hat=-542\n","Skipping y_hat=-542\n","Skipping y_hat=-542\n","Skipping y_hat=-542\n","Skipping y_hat=-542\n","Skipping y_hat=-770\n","Skipping y_hat=-770\n","Skipping y_hat=-770\n","Skipping y_hat=-770\n","Skipping y_hat=-770\n","Skipping y_hat=-770\n","Skipping y_hat=-770\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=-151\n","Skipping y_hat=+874\n","Skipping y_hat=+874\n","Skipping y_hat=+874\n","Skipping y_hat=+874\n","Skipping y_hat=+874\n","Skipping y_hat=+874\n","Skipping y_hat=+874\n","Skipping y_hat=+318\n","Skipping y_hat=+318\n","Skipping y_hat=+318\n","Skipping y_hat=+318\n","Skipping y_hat=+318\n","Skipping y_hat=+318\n","Skipping y_hat=+318\n","Skipping y_hat=-24\n","Skipping y_hat=-24\n","Skipping y_hat=-24\n","Skipping y_hat=-24\n","Skipping y_hat=-24\n","Skipping y_hat=-24\n","Skipping y_hat=-24\n","Skipping y_hat=+4\n","Skipping y_hat=+4\n","Skipping y_hat=+4\n","Skipping y_hat=+4\n","Skipping y_hat=+4\n","Skipping y_hat=+4\n","Skipping y_hat=+4\n","Skipping y_hat=-426\n","Skipping y_hat=-426\n","Skipping y_hat=-426\n","Skipping y_hat=-426\n","Skipping y_hat=-426\n","Skipping y_hat=-426\n","Skipping y_hat=-426\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+742\n","Skipping y_hat=+295\n","Skipping y_hat=+295\n","Skipping y_hat=+295\n","Skipping y_hat=+295\n","Skipping y_hat=+295\n","Skipping y_hat=+295\n","Skipping y_hat=+295\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=+37\n","Skipping y_hat=-40\n","Skipping y_hat=-40\n","Skipping y_hat=-40\n","Skipping y_hat=-40\n","Skipping y_hat=-40\n","Skipping y_hat=-40\n","Skipping y_hat=-40\n","Skipping y_hat=-506\n","Skipping y_hat=-506\n","Skipping y_hat=-506\n","Skipping y_hat=-506\n","Skipping y_hat=-506\n","Skipping y_hat=-506\n","Skipping y_hat=-506\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=+0\n","Skipping y_hat=-608\n","Skipping y_hat=-608\n","Skipping y_hat=-608\n","Skipping y_hat=-608\n","Skipping y_hat=-608\n","Skipping y_hat=-608\n","Skipping y_hat=-608\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-124\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=-216\n","Skipping y_hat=+103\n","Skipping y_hat=+103\n","Skipping y_hat=+103\n","Skipping y_hat=+103\n","Skipping y_hat=+103\n","Skipping y_hat=+103\n","Skipping y_hat=+103\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=+262\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=-148\n","Skipping y_hat=-5\n","Skipping y_hat=-5\n","Skipping y_hat=-5\n","Skipping y_hat=-5\n","Skipping y_hat=-5\n","Skipping y_hat=-5\n","Skipping y_hat=-5\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=-561\n","Skipping y_hat=+669\n","Skipping y_hat=+669\n","Skipping y_hat=+669\n","Skipping y_hat=+669\n","Skipping y_hat=+669\n","Skipping y_hat=+669\n","Skipping y_hat=+669\n","Skipping y_hat=+704\n","Skipping y_hat=+704\n","Skipping y_hat=+704\n","Skipping y_hat=+704\n","Skipping y_hat=+704\n","Skipping y_hat=+704\n","Skipping y_hat=+704\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-62\n","Skipping y_hat=-336\n","Skipping y_hat=-336\n","Skipping y_hat=-336\n","Skipping y_hat=-336\n","Skipping y_hat=-336\n","Skipping y_hat=-336\n","Skipping y_hat=-336\n","Skipping y_hat=-7\n","Skipping y_hat=-7\n","Skipping y_hat=-7\n","Skipping y_hat=-7\n","Skipping y_hat=-7\n","Skipping y_hat=-7\n","Skipping y_hat=-7\n","Skipping y_hat=-818\n","Skipping y_hat=-818\n","Skipping y_hat=-818\n","Skipping y_hat=-818\n","Skipping y_hat=-818\n","Skipping y_hat=-818\n","Skipping y_hat=-818\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=+38\n","Skipping y_hat=-824\n","Skipping y_hat=-824\n","Skipping y_hat=-824\n","Skipping y_hat=-824\n","Skipping y_hat=-824\n","Skipping y_hat=-824\n","Skipping y_hat=-824\n","Skipping y_hat=+19\n","Skipping y_hat=+19\n","Skipping y_hat=+19\n","Skipping y_hat=+19\n","Skipping y_hat=+19\n","Skipping y_hat=+19\n","Skipping y_hat=+19\n","Skipping y_hat=+121\n","Skipping y_hat=+121\n","Skipping y_hat=+121\n","Skipping y_hat=+121\n","Skipping y_hat=+121\n","Skipping y_hat=+121\n","Skipping y_hat=+121\n","Skipping y_hat=-634\n","Skipping y_hat=-634\n","Skipping y_hat=-634\n","Skipping y_hat=-634\n","Skipping y_hat=-634\n","Skipping y_hat=-634\n","Skipping y_hat=-634\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=+154\n","Skipping y_hat=+297\n","Skipping y_hat=+297\n","Skipping y_hat=+297\n","Skipping y_hat=+297\n","Skipping y_hat=+297\n","Skipping y_hat=+297\n","Skipping y_hat=+297\n","Skipping y_hat=-8\n","Skipping y_hat=-8\n","Skipping y_hat=-8\n","Skipping y_hat=-8\n","Skipping y_hat=-8\n","Skipping y_hat=-8\n","Skipping y_hat=-8\n","Skipping y_hat=+218\n","Skipping y_hat=+218\n","Skipping y_hat=+218\n","Skipping y_hat=+218\n","Skipping y_hat=+218\n","Skipping y_hat=+218\n","Skipping y_hat=+218\n","Skipping y_hat=-20\n","Skipping y_hat=-20\n","Skipping y_hat=-20\n","Skipping y_hat=-20\n","Skipping y_hat=-20\n","Skipping y_hat=-20\n","Skipping y_hat=-20\n","Skipping y_hat=+744\n","Skipping y_hat=+744\n","Skipping y_hat=+744\n","Skipping y_hat=+744\n","Skipping y_hat=+744\n","Skipping y_hat=+744\n","Skipping y_hat=+744\n","Skipping y_hat=-15\n","Skipping y_hat=-15\n","Skipping y_hat=-15\n","Skipping y_hat=-15\n","Skipping y_hat=-15\n","Skipping y_hat=-15\n","Skipping y_hat=-15\n","Skipping y_hat=-632\n","Skipping y_hat=-632\n","Skipping y_hat=-632\n","Skipping y_hat=-632\n","Skipping y_hat=-632\n","Skipping y_hat=-632\n","Skipping y_hat=-632\n","Skipping y_hat=-11\n","Skipping y_hat=-11\n","Skipping y_hat=-11\n","Skipping y_hat=-11\n","Skipping y_hat=-11\n","Skipping y_hat=-11\n","Skipping y_hat=-11\n","Skipping y_hat=+69\n","Skipping y_hat=+69\n","Skipping y_hat=+69\n","Skipping y_hat=+69\n","Skipping y_hat=+69\n","Skipping y_hat=+69\n","Skipping y_hat=+69\n","Skipping y_hat=-347\n","Skipping y_hat=-347\n","Skipping y_hat=-347\n","Skipping y_hat=-347\n","Skipping y_hat=-347\n","Skipping y_hat=-347\n","Skipping y_hat=-347\n","Skipping y_hat=-622\n","Skipping y_hat=-622\n","Skipping y_hat=-622\n","Skipping y_hat=-622\n","Skipping y_hat=-622\n","Skipping y_hat=-622\n","Skipping y_hat=-622\n"," 96% 24/25 [00:09<00:00,  3.21it/s]Skipping y_hat=+44\n","Skipping y_hat=+44\n","Skipping y_hat=+44\n","Skipping y_hat=+44\n","Skipping y_hat=+44\n","Skipping y_hat=+44\n","Skipping y_hat=+44\n","Skipping y_hat=+47\n","Skipping y_hat=+47\n","Skipping y_hat=+47\n","Skipping y_hat=+47\n","Skipping y_hat=+47\n","Skipping y_hat=+47\n","Skipping y_hat=+47\n","Skipping y_hat=+74\n","Skipping y_hat=+74\n","Skipping y_hat=+74\n","Skipping y_hat=+74\n","Skipping y_hat=+74\n","Skipping y_hat=+74\n","Skipping y_hat=+74\n","Skipping y_hat=+8\n","Skipping y_hat=+8\n","Skipping y_hat=+8\n","Skipping y_hat=+8\n","Skipping y_hat=+8\n","Skipping y_hat=+8\n","Skipping y_hat=+8\n","Skipping y_hat=+42\n","Skipping y_hat=+42\n","Skipping y_hat=+42\n","Skipping y_hat=+42\n","Skipping y_hat=+42\n","Skipping y_hat=+42\n","Skipping y_hat=+42\n","100% 25/25 [00:09<00:00,  2.55it/s]\n","accuracy of 3000 examples: 2990/3000 (99.66666666666667%)\n","\n","Final Test Results:\n","test_reverse: 99.67%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m2_operands_0_to_999_subtraction_balanced_digit_reverse\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/dn88fqxr\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mresults/2_operands_0_to_999_subtraction_balanced_digit/reverse_out/wandb/run-20250906_123815-dn88fqxr/logs\u001b[0m\n"]}]},{"cell_type":"markdown","source":["## 4 Operand Addition First Phase Change"],"metadata":{"id":"GwUiZn-_ov-1"}},{"cell_type":"code","source":["%cat configuration_files/4_operands_addition_reversed.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"znl0A3lAo5QU","executionInfo":{"status":"ok","timestamp":1757257920304,"user_tz":300,"elapsed":661,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"90838b4b-48ca-4ee0-c116-0dadec558c8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_0_to_999_first_phase_change'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'algo_reasoning'\n","data_format='reverse'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: whether the result is reversed\n","reverse_c = True\n","eval_addition = True\n","\n","analysis = False\n","\n","# to edit: num of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# whether need zero‐padding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 15000\n","lr_decay_iters = 15000 # make equal to max_iters usually (300000)\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory\n","out_dir = '/content/drive/MyDrive/addition/4_operands_0_to_999_first_phase_change/reverse_out'\n","\n","# to edit: training data\n","train_data_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_wo_padding/train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","start_train = \"/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_wo_padding/train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_wo_padding/val_reverse.txt'\n","\n","# to edit: test data; start is just the test file. It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","test_file_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform_wo_padding/test_reverse/test_reverse.txt'\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval1 = False\n","early_eval_interval1 = 25\n","early_eval_border1 = 1000\n","\n","more_early_eval2 = False\n","early_eval_interval2 = 5\n","early_eval_border2 = 750\n","\n","# to edit: whether do mutual information measurement between diffrent pairs of digits (include input-output and output-output)\n","mi_measurement = False\n","\n","# (optional) data for additional statistical measurement\n","stats_measurement_data_file_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/4_operand_addition_stats_measurement_data_reversed.txt'\n","\n","early_mi_measure_border = 200000\n","early_mi_measure_interval = 5000\n","final_mi_measure_interval = 5000"]}]},{"cell_type":"code","source":["%cd ../"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIXlBuUEtTQz","executionInfo":{"status":"ok","timestamp":1757257918397,"user_tz":300,"elapsed":52,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"ef1e247a-9449-4aeb-8d30-53399eda8964"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/addition\n"]}]},{"cell_type":"code","source":["!python train.py 4_operands_addition_reversed.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"N1J7QYGuo2JZ","executionInfo":{"status":"ok","timestamp":1757259291671,"user_tz":300,"elapsed":1135262,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"ab799db3-21fc-4fb1-bbf9-dfae2127d4c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=2058\n","Skipping y_hat=1603\n","Skipping y_hat=1603\n","Skipping y_hat=1603\n","Skipping y_hat=1603\n","Skipping y_hat=1603\n","Skipping y_hat=1603\n","Skipping y_hat=1603\n","Skipping y_hat=2364\n","Skipping y_hat=2364\n","Skipping y_hat=2364\n","Skipping y_hat=2364\n","Skipping y_hat=2364\n","Skipping y_hat=2364\n","Skipping y_hat=2364\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1187\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=1470\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=1797\n","Skipping y_hat=1646\n","Skipping y_hat=1646\n","Skipping y_hat=1646\n","Skipping y_hat=1646\n","Skipping y_hat=1646\n","Skipping y_hat=1646\n","Skipping y_hat=1646\n","Skipping y_hat=1177\n","Skipping y_hat=1177\n","Skipping y_hat=1177\n","Skipping y_hat=1177\n","Skipping y_hat=1177\n","Skipping y_hat=1177\n","Skipping y_hat=1177\n"," 91% 73/80 [00:24<00:02,  2.76it/s]Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=981\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1120\n","Skipping y_hat=1120\n","Skipping y_hat=1120\n","Skipping y_hat=1120\n","Skipping y_hat=1120\n","Skipping y_hat=1120\n","Skipping y_hat=1120\n","Skipping y_hat=882\n","Skipping y_hat=882\n","Skipping y_hat=882\n","Skipping y_hat=882\n","Skipping y_hat=882\n","Skipping y_hat=882\n","Skipping y_hat=882\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=664\n","Skipping y_hat=1903\n","Skipping y_hat=1903\n","Skipping y_hat=1903\n","Skipping y_hat=1903\n","Skipping y_hat=1903\n","Skipping y_hat=1903\n","Skipping y_hat=1903\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1482\n","Skipping y_hat=1482\n","Skipping y_hat=1482\n","Skipping y_hat=1482\n","Skipping y_hat=1482\n","Skipping y_hat=1482\n","Skipping y_hat=1482\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1175\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=1534\n","Skipping y_hat=848\n","Skipping y_hat=848\n","Skipping y_hat=848\n","Skipping y_hat=848\n","Skipping y_hat=848\n","Skipping y_hat=848\n","Skipping y_hat=848\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=1517\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=1751\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=1714\n","Skipping y_hat=1714\n","Skipping y_hat=1714\n","Skipping y_hat=1714\n","Skipping y_hat=1714\n","Skipping y_hat=1714\n","Skipping y_hat=1714\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=2374\n","Skipping y_hat=2374\n","Skipping y_hat=2374\n","Skipping y_hat=2374\n","Skipping y_hat=2374\n","Skipping y_hat=2374\n","Skipping y_hat=2374\n","Skipping y_hat=1538\n","Skipping y_hat=1538\n","Skipping y_hat=1538\n","Skipping y_hat=1538\n","Skipping y_hat=1538\n","Skipping y_hat=1538\n","Skipping y_hat=1538\n","Skipping y_hat=1882\n","Skipping y_hat=1882\n","Skipping y_hat=1882\n","Skipping y_hat=1882\n","Skipping y_hat=1882\n","Skipping y_hat=1882\n","Skipping y_hat=1882\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=1978\n","Skipping y_hat=1978\n","Skipping y_hat=1978\n","Skipping y_hat=1978\n","Skipping y_hat=1978\n","Skipping y_hat=1978\n","Skipping y_hat=1978\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=1664\n","Skipping y_hat=2393\n","Skipping y_hat=2393\n","Skipping y_hat=2393\n","Skipping y_hat=2393\n","Skipping y_hat=2393\n","Skipping y_hat=2393\n","Skipping y_hat=2393\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=831\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=716\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=1836\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=736\n","Skipping y_hat=1829\n","Skipping y_hat=1829\n","Skipping y_hat=1829\n","Skipping y_hat=1829\n","Skipping y_hat=1829\n","Skipping y_hat=1829\n","Skipping y_hat=1829\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1861\n","Skipping y_hat=1056\n","Skipping y_hat=1056\n","Skipping y_hat=1056\n","Skipping y_hat=1056\n","Skipping y_hat=1056\n","Skipping y_hat=1056\n","Skipping y_hat=1056\n","Skipping y_hat=1136\n","Skipping y_hat=1136\n","Skipping y_hat=1136\n","Skipping y_hat=1136\n","Skipping y_hat=1136\n","Skipping y_hat=1136\n","Skipping y_hat=1136\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=423\n","Skipping y_hat=423\n","Skipping y_hat=423\n","Skipping y_hat=423\n","Skipping y_hat=423\n","Skipping y_hat=423\n","Skipping y_hat=423\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=1418\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=889\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=896\n","Skipping y_hat=1703\n","Skipping y_hat=1703\n","Skipping y_hat=1703\n","Skipping y_hat=1703\n","Skipping y_hat=1703\n","Skipping y_hat=1703\n","Skipping y_hat=1703\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=449\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=921\n","Skipping y_hat=2122\n","Skipping y_hat=2122\n","Skipping y_hat=2122\n","Skipping y_hat=2122\n","Skipping y_hat=2122\n","Skipping y_hat=2122\n","Skipping y_hat=2122\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=1233\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=988\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=997\n","Skipping y_hat=1344\n","Skipping y_hat=1344\n","Skipping y_hat=1344\n","Skipping y_hat=1344\n","Skipping y_hat=1344\n","Skipping y_hat=1344\n","Skipping y_hat=1344\n","Skipping y_hat=2054\n","Skipping y_hat=2054\n","Skipping y_hat=2054\n","Skipping y_hat=2054\n","Skipping y_hat=2054\n","Skipping y_hat=2054\n","Skipping y_hat=2054\n","Skipping y_hat=1447\n","Skipping y_hat=1447\n","Skipping y_hat=1447\n","Skipping y_hat=1447\n","Skipping y_hat=1447\n","Skipping y_hat=1447\n","Skipping y_hat=1447\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1627\n","Skipping y_hat=1627\n","Skipping y_hat=1627\n","Skipping y_hat=1627\n","Skipping y_hat=1627\n","Skipping y_hat=1627\n","Skipping y_hat=1627\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=2346\n","Skipping y_hat=2346\n","Skipping y_hat=2346\n","Skipping y_hat=2346\n","Skipping y_hat=2346\n","Skipping y_hat=2346\n","Skipping y_hat=2346\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=1474\n","Skipping y_hat=1574\n","Skipping y_hat=1574\n","Skipping y_hat=1574\n","Skipping y_hat=1574\n","Skipping y_hat=1574\n","Skipping y_hat=1574\n","Skipping y_hat=1574\n","Skipping y_hat=2391\n","Skipping y_hat=2391\n","Skipping y_hat=2391\n","Skipping y_hat=2391\n","Skipping y_hat=2391\n","Skipping y_hat=2391\n","Skipping y_hat=2391\n","Skipping y_hat=1521\n","Skipping y_hat=1521\n","Skipping y_hat=1521\n","Skipping y_hat=1521\n","Skipping y_hat=1521\n","Skipping y_hat=1521\n","Skipping y_hat=1521\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=2402\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=731\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1678\n","Skipping y_hat=1678\n","Skipping y_hat=1678\n","Skipping y_hat=1678\n","Skipping y_hat=1678\n","Skipping y_hat=1678\n","Skipping y_hat=1678\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1902\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1437\n","Skipping y_hat=1437\n","Skipping y_hat=1437\n","Skipping y_hat=1437\n","Skipping y_hat=1437\n","Skipping y_hat=1437\n","Skipping y_hat=1437\n","Skipping y_hat=1299\n","Skipping y_hat=1299\n","Skipping y_hat=1299\n","Skipping y_hat=1299\n","Skipping y_hat=1299\n","Skipping y_hat=1299\n","Skipping y_hat=1299\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=978\n","Skipping y_hat=1265\n","Skipping y_hat=1265\n","Skipping y_hat=1265\n","Skipping y_hat=1265\n","Skipping y_hat=1265\n","Skipping y_hat=1265\n","Skipping y_hat=1265\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1724\n","Skipping y_hat=1724\n","Skipping y_hat=1724\n","Skipping y_hat=1724\n","Skipping y_hat=1724\n","Skipping y_hat=1724\n","Skipping y_hat=1724\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=904\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1778\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=815\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=860\n","Skipping y_hat=1368\n","Skipping y_hat=1368\n","Skipping y_hat=1368\n","Skipping y_hat=1368\n","Skipping y_hat=1368\n","Skipping y_hat=1368\n","Skipping y_hat=1368\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=961\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=1142\n","Skipping y_hat=1022\n","Skipping y_hat=1022\n","Skipping y_hat=1022\n","Skipping y_hat=1022\n","Skipping y_hat=1022\n","Skipping y_hat=1022\n","Skipping y_hat=1022\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=2037\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=420\n","Skipping y_hat=1560\n","Skipping y_hat=1560\n","Skipping y_hat=1560\n","Skipping y_hat=1560\n","Skipping y_hat=1560\n","Skipping y_hat=1560\n","Skipping y_hat=1560\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=805\n","Skipping y_hat=1014\n","Skipping y_hat=1014\n","Skipping y_hat=1014\n","Skipping y_hat=1014\n","Skipping y_hat=1014\n","Skipping y_hat=1014\n","Skipping y_hat=1014\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1088\n","Skipping y_hat=1088\n","Skipping y_hat=1088\n","Skipping y_hat=1088\n","Skipping y_hat=1088\n","Skipping y_hat=1088\n","Skipping y_hat=1088\n","Skipping y_hat=1873\n","Skipping y_hat=1873\n","Skipping y_hat=1873\n","Skipping y_hat=1873\n","Skipping y_hat=1873\n","Skipping y_hat=1873\n","Skipping y_hat=1873\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=502\n","Skipping y_hat=2048\n","Skipping y_hat=2048\n","Skipping y_hat=2048\n","Skipping y_hat=2048\n","Skipping y_hat=2048\n","Skipping y_hat=2048\n","Skipping y_hat=2048\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=803\n","Skipping y_hat=730\n","Skipping y_hat=730\n","Skipping y_hat=730\n","Skipping y_hat=730\n","Skipping y_hat=730\n","Skipping y_hat=730\n","Skipping y_hat=730\n","Skipping y_hat=1785\n","Skipping y_hat=1785\n","Skipping y_hat=1785\n","Skipping y_hat=1785\n","Skipping y_hat=1785\n","Skipping y_hat=1785\n","Skipping y_hat=1785\n","Skipping y_hat=788\n","Skipping y_hat=788\n","Skipping y_hat=788\n","Skipping y_hat=788\n","Skipping y_hat=788\n","Skipping y_hat=788\n","Skipping y_hat=788\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=807\n","Skipping y_hat=1527\n","Skipping y_hat=1527\n","Skipping y_hat=1527\n","Skipping y_hat=1527\n","Skipping y_hat=1527\n","Skipping y_hat=1527\n","Skipping y_hat=1527\n","Skipping y_hat=1545\n","Skipping y_hat=1545\n","Skipping y_hat=1545\n","Skipping y_hat=1545\n","Skipping y_hat=1545\n","Skipping y_hat=1545\n","Skipping y_hat=1545\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=2276\n","Skipping y_hat=1478\n","Skipping y_hat=1478\n","Skipping y_hat=1478\n","Skipping y_hat=1478\n","Skipping y_hat=1478\n","Skipping y_hat=1478\n","Skipping y_hat=1478\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=1965\n","Skipping y_hat=1965\n","Skipping y_hat=1965\n","Skipping y_hat=1965\n","Skipping y_hat=1965\n","Skipping y_hat=1965\n","Skipping y_hat=1965\n","Skipping y_hat=1767\n","Skipping y_hat=1767\n","Skipping y_hat=1767\n","Skipping y_hat=1767\n","Skipping y_hat=1767\n","Skipping y_hat=1767\n","Skipping y_hat=1767\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1936\n","Skipping y_hat=1936\n","Skipping y_hat=1936\n","Skipping y_hat=1936\n","Skipping y_hat=1936\n","Skipping y_hat=1936\n","Skipping y_hat=1936\n"," 92% 74/80 [00:24<00:01,  3.09it/s]Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1406\n","Skipping y_hat=1406\n","Skipping y_hat=1406\n","Skipping y_hat=1406\n","Skipping y_hat=1406\n","Skipping y_hat=1406\n","Skipping y_hat=1406\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1255\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=1558\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=1211\n","Skipping y_hat=1410\n","Skipping y_hat=1410\n","Skipping y_hat=1410\n","Skipping y_hat=1410\n","Skipping y_hat=1410\n","Skipping y_hat=1410\n","Skipping y_hat=1410\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=1027\n","Skipping y_hat=1027\n","Skipping y_hat=1027\n","Skipping y_hat=1027\n","Skipping y_hat=1027\n","Skipping y_hat=1027\n","Skipping y_hat=1027\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=1239\n","Skipping y_hat=1239\n","Skipping y_hat=1239\n","Skipping y_hat=1239\n","Skipping y_hat=1239\n","Skipping y_hat=1239\n","Skipping y_hat=1239\n","Skipping y_hat=1276\n","Skipping y_hat=1276\n","Skipping y_hat=1276\n","Skipping y_hat=1276\n","Skipping y_hat=1276\n","Skipping y_hat=1276\n","Skipping y_hat=1276\n","Skipping y_hat=1128\n","Skipping y_hat=1128\n","Skipping y_hat=1128\n","Skipping y_hat=1128\n","Skipping y_hat=1128\n","Skipping y_hat=1128\n","Skipping y_hat=1128\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1304\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1359\n","Skipping y_hat=1359\n","Skipping y_hat=1359\n","Skipping y_hat=1359\n","Skipping y_hat=1359\n","Skipping y_hat=1359\n","Skipping y_hat=1359\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1236\n","Skipping y_hat=1716\n","Skipping y_hat=1716\n","Skipping y_hat=1716\n","Skipping y_hat=1716\n","Skipping y_hat=1716\n","Skipping y_hat=1716\n","Skipping y_hat=1716\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=1693\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1251\n","Skipping y_hat=1979\n","Skipping y_hat=1979\n","Skipping y_hat=1979\n","Skipping y_hat=1979\n","Skipping y_hat=1979\n","Skipping y_hat=1979\n","Skipping y_hat=1979\n","Skipping y_hat=1154\n","Skipping y_hat=1154\n","Skipping y_hat=1154\n","Skipping y_hat=1154\n","Skipping y_hat=1154\n","Skipping y_hat=1154\n","Skipping y_hat=1154\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=1933\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1697\n","Skipping y_hat=1939\n","Skipping y_hat=1939\n","Skipping y_hat=1939\n","Skipping y_hat=1939\n","Skipping y_hat=1939\n","Skipping y_hat=1939\n","Skipping y_hat=1939\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=1392\n","Skipping y_hat=1795\n","Skipping y_hat=1795\n","Skipping y_hat=1795\n","Skipping y_hat=1795\n","Skipping y_hat=1795\n","Skipping y_hat=1795\n","Skipping y_hat=1795\n","Skipping y_hat=1583\n","Skipping y_hat=1583\n","Skipping y_hat=1583\n","Skipping y_hat=1583\n","Skipping y_hat=1583\n","Skipping y_hat=1583\n","Skipping y_hat=1583\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=1727\n","Skipping y_hat=1947\n","Skipping y_hat=1947\n","Skipping y_hat=1947\n","Skipping y_hat=1947\n","Skipping y_hat=1947\n","Skipping y_hat=1947\n","Skipping y_hat=1947\n","Skipping y_hat=1203\n","Skipping y_hat=1203\n","Skipping y_hat=1203\n","Skipping y_hat=1203\n","Skipping y_hat=1203\n","Skipping y_hat=1203\n","Skipping y_hat=1203\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1153\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1503\n","Skipping y_hat=1503\n","Skipping y_hat=1503\n","Skipping y_hat=1503\n","Skipping y_hat=1503\n","Skipping y_hat=1503\n","Skipping y_hat=1503\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=711\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=1763\n","Skipping y_hat=1535\n","Skipping y_hat=1535\n","Skipping y_hat=1535\n","Skipping y_hat=1535\n","Skipping y_hat=1535\n","Skipping y_hat=1535\n","Skipping y_hat=1535\n","Skipping y_hat=1628\n","Skipping y_hat=1628\n","Skipping y_hat=1628\n","Skipping y_hat=1628\n","Skipping y_hat=1628\n","Skipping y_hat=1628\n","Skipping y_hat=1628\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=1443\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=1987\n","Skipping y_hat=1987\n","Skipping y_hat=1987\n","Skipping y_hat=1987\n","Skipping y_hat=1987\n","Skipping y_hat=1987\n","Skipping y_hat=1987\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=900\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=427\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1818\n","Skipping y_hat=1472\n","Skipping y_hat=1472\n","Skipping y_hat=1472\n","Skipping y_hat=1472\n","Skipping y_hat=1472\n","Skipping y_hat=1472\n","Skipping y_hat=1472\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=785\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=1663\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=630\n","Skipping y_hat=1378\n","Skipping y_hat=1378\n","Skipping y_hat=1378\n","Skipping y_hat=1378\n","Skipping y_hat=1378\n","Skipping y_hat=1378\n","Skipping y_hat=1378\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1352\n","Skipping y_hat=1301\n","Skipping y_hat=1301\n","Skipping y_hat=1301\n","Skipping y_hat=1301\n","Skipping y_hat=1301\n","Skipping y_hat=1301\n","Skipping y_hat=1301\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=1877\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=1798\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=1531\n","Skipping y_hat=1341\n","Skipping y_hat=1341\n","Skipping y_hat=1341\n","Skipping y_hat=1341\n","Skipping y_hat=1341\n","Skipping y_hat=1341\n","Skipping y_hat=1341\n","Skipping y_hat=1938\n","Skipping y_hat=1938\n","Skipping y_hat=1938\n","Skipping y_hat=1938\n","Skipping y_hat=1938\n","Skipping y_hat=1938\n","Skipping y_hat=1938\n","Skipping y_hat=1399\n","Skipping y_hat=1399\n","Skipping y_hat=1399\n","Skipping y_hat=1399\n","Skipping y_hat=1399\n","Skipping y_hat=1399\n","Skipping y_hat=1399\n","Skipping y_hat=1362\n","Skipping y_hat=1362\n","Skipping y_hat=1362\n","Skipping y_hat=1362\n","Skipping y_hat=1362\n","Skipping y_hat=1362\n","Skipping y_hat=1362\n","Skipping y_hat=1753\n","Skipping y_hat=1753\n","Skipping y_hat=1753\n","Skipping y_hat=1753\n","Skipping y_hat=1753\n","Skipping y_hat=1753\n","Skipping y_hat=1753\n","Skipping y_hat=1916\n","Skipping y_hat=1916\n","Skipping y_hat=1916\n","Skipping y_hat=1916\n","Skipping y_hat=1916\n","Skipping y_hat=1916\n","Skipping y_hat=1916\n","Skipping y_hat=1712\n","Skipping y_hat=1712\n","Skipping y_hat=1712\n","Skipping y_hat=1712\n","Skipping y_hat=1712\n","Skipping y_hat=1712\n","Skipping y_hat=1712\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1601\n","Skipping y_hat=1601\n","Skipping y_hat=1601\n","Skipping y_hat=1601\n","Skipping y_hat=1601\n","Skipping y_hat=1601\n","Skipping y_hat=1601\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=1692\n","Skipping y_hat=1595\n","Skipping y_hat=1595\n","Skipping y_hat=1595\n","Skipping y_hat=1595\n","Skipping y_hat=1595\n","Skipping y_hat=1595\n","Skipping y_hat=1595\n","Skipping y_hat=1871\n","Skipping y_hat=1871\n","Skipping y_hat=1871\n","Skipping y_hat=1871\n","Skipping y_hat=1871\n","Skipping y_hat=1871\n","Skipping y_hat=1871\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=1131\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1994\n","Skipping y_hat=1994\n","Skipping y_hat=1994\n","Skipping y_hat=1994\n","Skipping y_hat=1994\n","Skipping y_hat=1994\n","Skipping y_hat=1994\n","Skipping y_hat=1834\n","Skipping y_hat=1834\n","Skipping y_hat=1834\n","Skipping y_hat=1834\n","Skipping y_hat=1834\n","Skipping y_hat=1834\n","Skipping y_hat=1834\n","Skipping y_hat=1028\n","Skipping y_hat=1028\n","Skipping y_hat=1028\n","Skipping y_hat=1028\n","Skipping y_hat=1028\n","Skipping y_hat=1028\n","Skipping y_hat=1028\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1990\n","Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1316\n","Skipping y_hat=1316\n","Skipping y_hat=1316\n","Skipping y_hat=1316\n","Skipping y_hat=1316\n","Skipping y_hat=1316\n","Skipping y_hat=1316\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=1743\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=907\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1780\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=1892\n","Skipping y_hat=1892\n","Skipping y_hat=1892\n","Skipping y_hat=1892\n","Skipping y_hat=1892\n","Skipping y_hat=1892\n","Skipping y_hat=1892\n","Skipping y_hat=1547\n","Skipping y_hat=1547\n","Skipping y_hat=1547\n","Skipping y_hat=1547\n","Skipping y_hat=1547\n","Skipping y_hat=1547\n","Skipping y_hat=1547\n","Skipping y_hat=1655\n","Skipping y_hat=1655\n","Skipping y_hat=1655\n","Skipping y_hat=1655\n","Skipping y_hat=1655\n","Skipping y_hat=1655\n","Skipping y_hat=1655\n","Skipping y_hat=1208\n","Skipping y_hat=1208\n","Skipping y_hat=1208\n","Skipping y_hat=1208\n","Skipping y_hat=1208\n","Skipping y_hat=1208\n","Skipping y_hat=1208\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=1319\n","Skipping y_hat=1319\n","Skipping y_hat=1319\n","Skipping y_hat=1319\n","Skipping y_hat=1319\n","Skipping y_hat=1319\n","Skipping y_hat=1319\n","Skipping y_hat=1017\n","Skipping y_hat=1017\n","Skipping y_hat=1017\n","Skipping y_hat=1017\n","Skipping y_hat=1017\n","Skipping y_hat=1017\n","Skipping y_hat=1017\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=1801\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=1215\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=1122\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1101\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=1820\n","Skipping y_hat=1288\n","Skipping y_hat=1288\n","Skipping y_hat=1288\n","Skipping y_hat=1288\n","Skipping y_hat=1288\n","Skipping y_hat=1288\n","Skipping y_hat=1288\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1052\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=480\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=2133\n","Skipping y_hat=1444\n","Skipping y_hat=1444\n","Skipping y_hat=1444\n","Skipping y_hat=1444\n","Skipping y_hat=1444\n","Skipping y_hat=1444\n","Skipping y_hat=1444\n","Skipping y_hat=1848\n","Skipping y_hat=1848\n","Skipping y_hat=1848\n","Skipping y_hat=1848\n","Skipping y_hat=1848\n","Skipping y_hat=1848\n","Skipping y_hat=1848\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=1550\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=1405\n","Skipping y_hat=1405\n","Skipping y_hat=1405\n","Skipping y_hat=1405\n","Skipping y_hat=1405\n","Skipping y_hat=1405\n","Skipping y_hat=1405\n","Skipping y_hat=818\n","Skipping y_hat=818\n","Skipping y_hat=818\n","Skipping y_hat=818\n","Skipping y_hat=818\n","Skipping y_hat=818\n","Skipping y_hat=818\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=1428\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=933\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=697\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=939\n","Skipping y_hat=1537\n","Skipping y_hat=1537\n","Skipping y_hat=1537\n","Skipping y_hat=1537\n","Skipping y_hat=1537\n","Skipping y_hat=1537\n","Skipping y_hat=1537\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=2412\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=1592\n","Skipping y_hat=1592\n","Skipping y_hat=1592\n","Skipping y_hat=1592\n","Skipping y_hat=1592\n","Skipping y_hat=1592\n","Skipping y_hat=1592\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=713\n","Skipping y_hat=713\n","Skipping y_hat=713\n","Skipping y_hat=713\n","Skipping y_hat=713\n","Skipping y_hat=713\n","Skipping y_hat=713\n"," 94% 75/80 [00:24<00:01,  3.29it/s]Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=1754\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=2123\n","Skipping y_hat=2123\n","Skipping y_hat=2123\n","Skipping y_hat=2123\n","Skipping y_hat=2123\n","Skipping y_hat=2123\n","Skipping y_hat=2123\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=1516\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=391\n","Skipping y_hat=2475\n","Skipping y_hat=2475\n","Skipping y_hat=2475\n","Skipping y_hat=2475\n","Skipping y_hat=2475\n","Skipping y_hat=2475\n","Skipping y_hat=2475\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1075\n","Skipping y_hat=1075\n","Skipping y_hat=1075\n","Skipping y_hat=1075\n","Skipping y_hat=1075\n","Skipping y_hat=1075\n","Skipping y_hat=1075\n","Skipping y_hat=1828\n","Skipping y_hat=1828\n","Skipping y_hat=1828\n","Skipping y_hat=1828\n","Skipping y_hat=1828\n","Skipping y_hat=1828\n","Skipping y_hat=1828\n","Skipping y_hat=2717\n","Skipping y_hat=2717\n","Skipping y_hat=2717\n","Skipping y_hat=2717\n","Skipping y_hat=2717\n","Skipping y_hat=2717\n","Skipping y_hat=2717\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1309\n","Skipping y_hat=1309\n","Skipping y_hat=1309\n","Skipping y_hat=1309\n","Skipping y_hat=1309\n","Skipping y_hat=1309\n","Skipping y_hat=1309\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=844\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=1241\n","Skipping y_hat=1241\n","Skipping y_hat=1241\n","Skipping y_hat=1241\n","Skipping y_hat=1241\n","Skipping y_hat=1241\n","Skipping y_hat=1241\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=1827\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=2368\n","Skipping y_hat=1733\n","Skipping y_hat=1733\n","Skipping y_hat=1733\n","Skipping y_hat=1733\n","Skipping y_hat=1733\n","Skipping y_hat=1733\n","Skipping y_hat=1733\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=983\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=612\n","Skipping y_hat=1196\n","Skipping y_hat=1196\n","Skipping y_hat=1196\n","Skipping y_hat=1196\n","Skipping y_hat=1196\n","Skipping y_hat=1196\n","Skipping y_hat=1196\n","Skipping y_hat=1297\n","Skipping y_hat=1297\n","Skipping y_hat=1297\n","Skipping y_hat=1297\n","Skipping y_hat=1297\n","Skipping y_hat=1297\n","Skipping y_hat=1297\n","Skipping y_hat=1269\n","Skipping y_hat=1269\n","Skipping y_hat=1269\n","Skipping y_hat=1269\n","Skipping y_hat=1269\n","Skipping y_hat=1269\n","Skipping y_hat=1269\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=2399\n","Skipping y_hat=2422\n","Skipping y_hat=2422\n","Skipping y_hat=2422\n","Skipping y_hat=2422\n","Skipping y_hat=2422\n","Skipping y_hat=2422\n","Skipping y_hat=2422\n","Skipping y_hat=1614\n","Skipping y_hat=1614\n","Skipping y_hat=1614\n","Skipping y_hat=1614\n","Skipping y_hat=1614\n","Skipping y_hat=1614\n","Skipping y_hat=1614\n","Skipping y_hat=1762\n","Skipping y_hat=1762\n","Skipping y_hat=1762\n","Skipping y_hat=1762\n","Skipping y_hat=1762\n","Skipping y_hat=1762\n","Skipping y_hat=1762\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=1415\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=2254\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=1391\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=2001\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1894\n","Skipping y_hat=1222\n","Skipping y_hat=1222\n","Skipping y_hat=1222\n","Skipping y_hat=1222\n","Skipping y_hat=1222\n","Skipping y_hat=1222\n","Skipping y_hat=1222\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=568\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=1949\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=1544\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=468\n","Skipping y_hat=1749\n","Skipping y_hat=1749\n","Skipping y_hat=1749\n","Skipping y_hat=1749\n","Skipping y_hat=1749\n","Skipping y_hat=1749\n","Skipping y_hat=1749\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=1671\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=751\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1445\n","Skipping y_hat=1223\n","Skipping y_hat=1223\n","Skipping y_hat=1223\n","Skipping y_hat=1223\n","Skipping y_hat=1223\n","Skipping y_hat=1223\n","Skipping y_hat=1223\n","Skipping y_hat=1977\n","Skipping y_hat=1977\n","Skipping y_hat=1977\n","Skipping y_hat=1977\n","Skipping y_hat=1977\n","Skipping y_hat=1977\n","Skipping y_hat=1977\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=975\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=2181\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=946\n","Skipping y_hat=2606\n","Skipping y_hat=2606\n","Skipping y_hat=2606\n","Skipping y_hat=2606\n","Skipping y_hat=2606\n","Skipping y_hat=2606\n","Skipping y_hat=2606\n","Skipping y_hat=1111\n","Skipping y_hat=1111\n","Skipping y_hat=1111\n","Skipping y_hat=1111\n","Skipping y_hat=1111\n","Skipping y_hat=1111\n","Skipping y_hat=1111\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=696\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=1710\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1126\n","Skipping y_hat=1546\n","Skipping y_hat=1546\n","Skipping y_hat=1546\n","Skipping y_hat=1546\n","Skipping y_hat=1546\n","Skipping y_hat=1546\n","Skipping y_hat=1546\n","Skipping y_hat=1246\n","Skipping y_hat=1246\n","Skipping y_hat=1246\n","Skipping y_hat=1246\n","Skipping y_hat=1246\n","Skipping y_hat=1246\n","Skipping y_hat=1246\n","Skipping y_hat=1452\n","Skipping y_hat=1452\n","Skipping y_hat=1452\n","Skipping y_hat=1452\n","Skipping y_hat=1452\n","Skipping y_hat=1452\n","Skipping y_hat=1452\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=958\n","Skipping y_hat=1620\n","Skipping y_hat=1620\n","Skipping y_hat=1620\n","Skipping y_hat=1620\n","Skipping y_hat=1620\n","Skipping y_hat=1620\n","Skipping y_hat=1620\n","Skipping y_hat=1099\n","Skipping y_hat=1099\n","Skipping y_hat=1099\n","Skipping y_hat=1099\n","Skipping y_hat=1099\n","Skipping y_hat=1099\n","Skipping y_hat=1099\n","Skipping y_hat=1067\n","Skipping y_hat=1067\n","Skipping y_hat=1067\n","Skipping y_hat=1067\n","Skipping y_hat=1067\n","Skipping y_hat=1067\n","Skipping y_hat=1067\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=1247\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=996\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=1047\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=954\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=554\n","Skipping y_hat=1565\n","Skipping y_hat=1565\n","Skipping y_hat=1565\n","Skipping y_hat=1565\n","Skipping y_hat=1565\n","Skipping y_hat=1565\n","Skipping y_hat=1565\n","Skipping y_hat=2558\n","Skipping y_hat=2558\n","Skipping y_hat=2558\n","Skipping y_hat=2558\n","Skipping y_hat=2558\n","Skipping y_hat=2558\n","Skipping y_hat=2558\n","Skipping y_hat=1849\n","Skipping y_hat=1849\n","Skipping y_hat=1849\n","Skipping y_hat=1849\n","Skipping y_hat=1849\n","Skipping y_hat=1849\n","Skipping y_hat=1849\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=1330\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=908\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1278\n","Skipping y_hat=1278\n","Skipping y_hat=1278\n","Skipping y_hat=1278\n","Skipping y_hat=1278\n","Skipping y_hat=1278\n","Skipping y_hat=1278\n","Skipping y_hat=1389\n","Skipping y_hat=1389\n","Skipping y_hat=1389\n","Skipping y_hat=1389\n","Skipping y_hat=1389\n","Skipping y_hat=1389\n","Skipping y_hat=1389\n","Skipping y_hat=1188\n","Skipping y_hat=1188\n","Skipping y_hat=1188\n","Skipping y_hat=1188\n","Skipping y_hat=1188\n","Skipping y_hat=1188\n","Skipping y_hat=1188\n","Skipping y_hat=1163\n","Skipping y_hat=1163\n","Skipping y_hat=1163\n","Skipping y_hat=1163\n","Skipping y_hat=1163\n","Skipping y_hat=1163\n","Skipping y_hat=1163\n","Skipping y_hat=1608\n","Skipping y_hat=1608\n","Skipping y_hat=1608\n","Skipping y_hat=1608\n","Skipping y_hat=1608\n","Skipping y_hat=1608\n","Skipping y_hat=1608\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=1275\n","Skipping y_hat=1275\n","Skipping y_hat=1275\n","Skipping y_hat=1275\n","Skipping y_hat=1275\n","Skipping y_hat=1275\n","Skipping y_hat=1275\n","Skipping y_hat=1258\n","Skipping y_hat=1258\n","Skipping y_hat=1258\n","Skipping y_hat=1258\n","Skipping y_hat=1258\n","Skipping y_hat=1258\n","Skipping y_hat=1258\n","Skipping y_hat=1197\n","Skipping y_hat=1197\n","Skipping y_hat=1197\n","Skipping y_hat=1197\n","Skipping y_hat=1197\n","Skipping y_hat=1197\n","Skipping y_hat=1197\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1398\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=2165\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1837\n","Skipping y_hat=1837\n","Skipping y_hat=1837\n","Skipping y_hat=1837\n","Skipping y_hat=1837\n","Skipping y_hat=1837\n","Skipping y_hat=1837\n","Skipping y_hat=1312\n","Skipping y_hat=1312\n","Skipping y_hat=1312\n","Skipping y_hat=1312\n","Skipping y_hat=1312\n","Skipping y_hat=1312\n","Skipping y_hat=1312\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1363\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=1467\n","Skipping y_hat=1272\n","Skipping y_hat=1272\n","Skipping y_hat=1272\n","Skipping y_hat=1272\n","Skipping y_hat=1272\n","Skipping y_hat=1272\n","Skipping y_hat=1272\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1183\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=1752\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=1041\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=880\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=1530\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=1799\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=1213\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=682\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1551\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1800\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=1640\n","Skipping y_hat=973\n","Skipping y_hat=973\n","Skipping y_hat=973\n","Skipping y_hat=973\n","Skipping y_hat=973\n","Skipping y_hat=973\n","Skipping y_hat=973\n","Skipping y_hat=1025\n","Skipping y_hat=1025\n","Skipping y_hat=1025\n","Skipping y_hat=1025\n","Skipping y_hat=1025\n","Skipping y_hat=1025\n","Skipping y_hat=1025\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1091\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=1114\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=500\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1984\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1078\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=1609\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=897\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=993\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=462\n","Skipping y_hat=2258\n","Skipping y_hat=2258\n","Skipping y_hat=2258\n","Skipping y_hat=2258\n","Skipping y_hat=2258\n","Skipping y_hat=2258\n","Skipping y_hat=2258\n","Skipping y_hat=2509\n","Skipping y_hat=2509\n","Skipping y_hat=2509\n","Skipping y_hat=2509\n","Skipping y_hat=2509\n","Skipping y_hat=2509\n","Skipping y_hat=2509\n"," 95% 76/80 [00:24<00:01,  3.55it/s]Skipping y_hat=1809\n","Skipping y_hat=1809\n","Skipping y_hat=1809\n","Skipping y_hat=1809\n","Skipping y_hat=1809\n","Skipping y_hat=1809\n","Skipping y_hat=1809\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=1182\n","Skipping y_hat=2156\n","Skipping y_hat=2156\n","Skipping y_hat=2156\n","Skipping y_hat=2156\n","Skipping y_hat=2156\n","Skipping y_hat=2156\n","Skipping y_hat=2156\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=1123\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=998\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=1841\n","Skipping y_hat=1077\n","Skipping y_hat=1077\n","Skipping y_hat=1077\n","Skipping y_hat=1077\n","Skipping y_hat=1077\n","Skipping y_hat=1077\n","Skipping y_hat=1077\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1003\n","Skipping y_hat=1003\n","Skipping y_hat=1003\n","Skipping y_hat=1003\n","Skipping y_hat=1003\n","Skipping y_hat=1003\n","Skipping y_hat=1003\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=853\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=1442\n","Skipping y_hat=1460\n","Skipping y_hat=1460\n","Skipping y_hat=1460\n","Skipping y_hat=1460\n","Skipping y_hat=1460\n","Skipping y_hat=1460\n","Skipping y_hat=1460\n","Skipping y_hat=2274\n","Skipping y_hat=2274\n","Skipping y_hat=2274\n","Skipping y_hat=2274\n","Skipping y_hat=2274\n","Skipping y_hat=2274\n","Skipping y_hat=2274\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=1621\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=631\n","Skipping y_hat=1005\n","Skipping y_hat=1005\n","Skipping y_hat=1005\n","Skipping y_hat=1005\n","Skipping y_hat=1005\n","Skipping y_hat=1005\n","Skipping y_hat=1005\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=901\n","Skipping y_hat=1681\n","Skipping y_hat=1681\n","Skipping y_hat=1681\n","Skipping y_hat=1681\n","Skipping y_hat=1681\n","Skipping y_hat=1681\n","Skipping y_hat=1681\n","Skipping y_hat=2293\n","Skipping y_hat=2293\n","Skipping y_hat=2293\n","Skipping y_hat=2293\n","Skipping y_hat=2293\n","Skipping y_hat=2293\n","Skipping y_hat=2293\n","Skipping y_hat=2311\n","Skipping y_hat=2311\n","Skipping y_hat=2311\n","Skipping y_hat=2311\n","Skipping y_hat=2311\n","Skipping y_hat=2311\n","Skipping y_hat=2311\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=2265\n","Skipping y_hat=2265\n","Skipping y_hat=2265\n","Skipping y_hat=2265\n","Skipping y_hat=2265\n","Skipping y_hat=2265\n","Skipping y_hat=2265\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=1081\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=667\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=1874\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=1457\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=2108\n","Skipping y_hat=1310\n","Skipping y_hat=1310\n","Skipping y_hat=1310\n","Skipping y_hat=1310\n","Skipping y_hat=1310\n","Skipping y_hat=1310\n","Skipping y_hat=1310\n","Skipping y_hat=1741\n","Skipping y_hat=1741\n","Skipping y_hat=1741\n","Skipping y_hat=1741\n","Skipping y_hat=1741\n","Skipping y_hat=1741\n","Skipping y_hat=1741\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=987\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1257\n","Skipping y_hat=1955\n","Skipping y_hat=1955\n","Skipping y_hat=1955\n","Skipping y_hat=1955\n","Skipping y_hat=1955\n","Skipping y_hat=1955\n","Skipping y_hat=1955\n","Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=1224\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=945\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=1018\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=764\n","Skipping y_hat=2068\n","Skipping y_hat=2068\n","Skipping y_hat=2068\n","Skipping y_hat=2068\n","Skipping y_hat=2068\n","Skipping y_hat=2068\n","Skipping y_hat=2068\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1459\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=1271\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=913\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=2060\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=814\n","Skipping y_hat=1720\n","Skipping y_hat=1720\n","Skipping y_hat=1720\n","Skipping y_hat=1720\n","Skipping y_hat=1720\n","Skipping y_hat=1720\n","Skipping y_hat=1720\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1334\n","Skipping y_hat=1824\n","Skipping y_hat=1824\n","Skipping y_hat=1824\n","Skipping y_hat=1824\n","Skipping y_hat=1824\n","Skipping y_hat=1824\n","Skipping y_hat=1824\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=671\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1057\n","Skipping y_hat=1207\n","Skipping y_hat=1207\n","Skipping y_hat=1207\n","Skipping y_hat=1207\n","Skipping y_hat=1207\n","Skipping y_hat=1207\n","Skipping y_hat=1207\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=2414\n","Skipping y_hat=1289\n","Skipping y_hat=1289\n","Skipping y_hat=1289\n","Skipping y_hat=1289\n","Skipping y_hat=1289\n","Skipping y_hat=1289\n","Skipping y_hat=1289\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1750\n","Skipping y_hat=1321\n","Skipping y_hat=1321\n","Skipping y_hat=1321\n","Skipping y_hat=1321\n","Skipping y_hat=1321\n","Skipping y_hat=1321\n","Skipping y_hat=1321\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=571\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=1407\n","Skipping y_hat=2119\n","Skipping y_hat=2119\n","Skipping y_hat=2119\n","Skipping y_hat=2119\n","Skipping y_hat=2119\n","Skipping y_hat=2119\n","Skipping y_hat=2119\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=1149\n","Skipping y_hat=1149\n","Skipping y_hat=1149\n","Skipping y_hat=1149\n","Skipping y_hat=1149\n","Skipping y_hat=1149\n","Skipping y_hat=1149\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=1709\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=1420\n","Skipping y_hat=2004\n","Skipping y_hat=2004\n","Skipping y_hat=2004\n","Skipping y_hat=2004\n","Skipping y_hat=2004\n","Skipping y_hat=2004\n","Skipping y_hat=2004\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=1509\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1329\n","Skipping y_hat=1659\n","Skipping y_hat=1659\n","Skipping y_hat=1659\n","Skipping y_hat=1659\n","Skipping y_hat=1659\n","Skipping y_hat=1659\n","Skipping y_hat=1659\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=919\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=1162\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=2463\n","Skipping y_hat=2463\n","Skipping y_hat=2463\n","Skipping y_hat=2463\n","Skipping y_hat=2463\n","Skipping y_hat=2463\n","Skipping y_hat=2463\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=991\n","Skipping y_hat=2024\n","Skipping y_hat=2024\n","Skipping y_hat=2024\n","Skipping y_hat=2024\n","Skipping y_hat=2024\n","Skipping y_hat=2024\n","Skipping y_hat=2024\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=830\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1232\n","Skipping y_hat=1579\n","Skipping y_hat=1579\n","Skipping y_hat=1579\n","Skipping y_hat=1579\n","Skipping y_hat=1579\n","Skipping y_hat=1579\n","Skipping y_hat=1579\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=1267\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=622\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=635\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=1507\n","Skipping y_hat=2147\n","Skipping y_hat=2147\n","Skipping y_hat=2147\n","Skipping y_hat=2147\n","Skipping y_hat=2147\n","Skipping y_hat=2147\n","Skipping y_hat=2147\n","Skipping y_hat=2077\n","Skipping y_hat=2077\n","Skipping y_hat=2077\n","Skipping y_hat=2077\n","Skipping y_hat=2077\n","Skipping y_hat=2077\n","Skipping y_hat=2077\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1423\n","Skipping y_hat=1423\n","Skipping y_hat=1423\n","Skipping y_hat=1423\n","Skipping y_hat=1423\n","Skipping y_hat=1423\n","Skipping y_hat=1423\n","Skipping y_hat=2187\n","Skipping y_hat=2187\n","Skipping y_hat=2187\n","Skipping y_hat=2187\n","Skipping y_hat=2187\n","Skipping y_hat=2187\n","Skipping y_hat=2187\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=1833\n","Skipping y_hat=2739\n","Skipping y_hat=2739\n","Skipping y_hat=2739\n","Skipping y_hat=2739\n","Skipping y_hat=2739\n","Skipping y_hat=2739\n","Skipping y_hat=2739\n","Skipping y_hat=1813\n","Skipping y_hat=1813\n","Skipping y_hat=1813\n","Skipping y_hat=1813\n","Skipping y_hat=1813\n","Skipping y_hat=1813\n","Skipping y_hat=1813\n","Skipping y_hat=1270\n","Skipping y_hat=1270\n","Skipping y_hat=1270\n","Skipping y_hat=1270\n","Skipping y_hat=1270\n","Skipping y_hat=1270\n","Skipping y_hat=1270\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=1731\n","Skipping y_hat=2789\n","Skipping y_hat=2789\n","Skipping y_hat=2789\n","Skipping y_hat=2789\n","Skipping y_hat=2789\n","Skipping y_hat=2789\n","Skipping y_hat=2789\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=1508\n","Skipping y_hat=1946\n","Skipping y_hat=1946\n","Skipping y_hat=1946\n","Skipping y_hat=1946\n","Skipping y_hat=1946\n","Skipping y_hat=1946\n","Skipping y_hat=1946\n","Skipping y_hat=1231\n","Skipping y_hat=1231\n","Skipping y_hat=1231\n","Skipping y_hat=1231\n","Skipping y_hat=1231\n","Skipping y_hat=1231\n","Skipping y_hat=1231\n","Skipping y_hat=1296\n","Skipping y_hat=1296\n","Skipping y_hat=1296\n","Skipping y_hat=1296\n","Skipping y_hat=1296\n","Skipping y_hat=1296\n","Skipping y_hat=1296\n","Skipping y_hat=1607\n","Skipping y_hat=1607\n","Skipping y_hat=1607\n","Skipping y_hat=1607\n","Skipping y_hat=1607\n","Skipping y_hat=1607\n","Skipping y_hat=1607\n","Skipping y_hat=1151\n","Skipping y_hat=1151\n","Skipping y_hat=1151\n","Skipping y_hat=1151\n","Skipping y_hat=1151\n","Skipping y_hat=1151\n","Skipping y_hat=1151\n","Skipping y_hat=1760\n","Skipping y_hat=1760\n","Skipping y_hat=1760\n","Skipping y_hat=1760\n","Skipping y_hat=1760\n","Skipping y_hat=1760\n","Skipping y_hat=1760\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1533\n","Skipping y_hat=1872\n","Skipping y_hat=1872\n","Skipping y_hat=1872\n","Skipping y_hat=1872\n","Skipping y_hat=1872\n","Skipping y_hat=1872\n","Skipping y_hat=1872\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=1360\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=1332\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1449\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1629\n","Skipping y_hat=1838\n","Skipping y_hat=1838\n","Skipping y_hat=1838\n","Skipping y_hat=1838\n","Skipping y_hat=1838\n","Skipping y_hat=1838\n","Skipping y_hat=1838\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=727\n","Skipping y_hat=1927\n","Skipping y_hat=1927\n","Skipping y_hat=1927\n","Skipping y_hat=1927\n","Skipping y_hat=1927\n","Skipping y_hat=1927\n","Skipping y_hat=1927\n","Skipping y_hat=1991\n","Skipping y_hat=1991\n","Skipping y_hat=1991\n","Skipping y_hat=1991\n","Skipping y_hat=1991\n","Skipping y_hat=1991\n","Skipping y_hat=1991\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=585\n","Skipping y_hat=1684\n","Skipping y_hat=1684\n","Skipping y_hat=1684\n","Skipping y_hat=1684\n","Skipping y_hat=1684\n","Skipping y_hat=1684\n","Skipping y_hat=1684\n","Skipping y_hat=1598\n","Skipping y_hat=1598\n","Skipping y_hat=1598\n","Skipping y_hat=1598\n","Skipping y_hat=1598\n","Skipping y_hat=1598\n","Skipping y_hat=1598\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=1280\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=1408\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=917\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n","Skipping y_hat=1842\n"," 96% 77/80 [00:25<00:01,  2.71it/s]Skipping y_hat=1291\n","Skipping y_hat=1291\n","Skipping y_hat=1291\n","Skipping y_hat=1291\n","Skipping y_hat=1291\n","Skipping y_hat=1291\n","Skipping y_hat=1291\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=690\n","Skipping y_hat=1554\n","Skipping y_hat=1554\n","Skipping y_hat=1554\n","Skipping y_hat=1554\n","Skipping y_hat=1554\n","Skipping y_hat=1554\n","Skipping y_hat=1554\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=1039\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=1596\n","Skipping y_hat=1580\n","Skipping y_hat=1580\n","Skipping y_hat=1580\n","Skipping y_hat=1580\n","Skipping y_hat=1580\n","Skipping y_hat=1580\n","Skipping y_hat=1580\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=909\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=512\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1380\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=1259\n","Skipping y_hat=1957\n","Skipping y_hat=1957\n","Skipping y_hat=1957\n","Skipping y_hat=1957\n","Skipping y_hat=1957\n","Skipping y_hat=1957\n","Skipping y_hat=1957\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=2211\n","Skipping y_hat=1656\n","Skipping y_hat=1656\n","Skipping y_hat=1656\n","Skipping y_hat=1656\n","Skipping y_hat=1656\n","Skipping y_hat=1656\n","Skipping y_hat=1656\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1863\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=1631\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=1419\n","Skipping y_hat=741\n","Skipping y_hat=741\n","Skipping y_hat=741\n","Skipping y_hat=741\n","Skipping y_hat=741\n","Skipping y_hat=741\n","Skipping y_hat=741\n","Skipping y_hat=1166\n","Skipping y_hat=1166\n","Skipping y_hat=1166\n","Skipping y_hat=1166\n","Skipping y_hat=1166\n","Skipping y_hat=1166\n","Skipping y_hat=1166\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=1038\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=2469\n","Skipping y_hat=1653\n","Skipping y_hat=1653\n","Skipping y_hat=1653\n","Skipping y_hat=1653\n","Skipping y_hat=1653\n","Skipping y_hat=1653\n","Skipping y_hat=1653\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=1206\n","Skipping y_hat=1206\n","Skipping y_hat=1206\n","Skipping y_hat=1206\n","Skipping y_hat=1206\n","Skipping y_hat=1206\n","Skipping y_hat=1206\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1500\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1625\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=1254\n","Skipping y_hat=1148\n","Skipping y_hat=1148\n","Skipping y_hat=1148\n","Skipping y_hat=1148\n","Skipping y_hat=1148\n","Skipping y_hat=1148\n","Skipping y_hat=1148\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1567\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=1511\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=1782\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=700\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=1888\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=930\n","Skipping y_hat=1669\n","Skipping y_hat=1669\n","Skipping y_hat=1669\n","Skipping y_hat=1669\n","Skipping y_hat=1669\n","Skipping y_hat=1669\n","Skipping y_hat=1669\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=1553\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=2101\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=1373\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=584\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=1519\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=504\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=2350\n","Skipping y_hat=1555\n","Skipping y_hat=1555\n","Skipping y_hat=1555\n","Skipping y_hat=1555\n","Skipping y_hat=1555\n","Skipping y_hat=1555\n","Skipping y_hat=1555\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1401\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=1559\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=826\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n","Skipping y_hat=823\n"," 98% 78/80 [00:25<00:00,  3.43it/s]Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=1340\n","Skipping y_hat=1150\n","Skipping y_hat=1150\n","Skipping y_hat=1150\n","Skipping y_hat=1150\n","Skipping y_hat=1150\n","Skipping y_hat=1150\n","Skipping y_hat=1150\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=969\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=802\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=553\n","Skipping y_hat=851\n","Skipping y_hat=851\n","Skipping y_hat=851\n","Skipping y_hat=851\n","Skipping y_hat=851\n","Skipping y_hat=851\n","Skipping y_hat=851\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=963\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=801\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=223\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=745\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1139\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=1904\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1004\n","Skipping y_hat=1004\n","Skipping y_hat=1004\n","Skipping y_hat=1004\n","Skipping y_hat=1004\n","Skipping y_hat=1004\n","Skipping y_hat=1004\n","Skipping y_hat=1060\n","Skipping y_hat=1060\n","Skipping y_hat=1060\n","Skipping y_hat=1060\n","Skipping y_hat=1060\n","Skipping y_hat=1060\n","Skipping y_hat=1060\n","Skipping y_hat=1665\n","Skipping y_hat=1665\n","Skipping y_hat=1665\n","Skipping y_hat=1665\n","Skipping y_hat=1665\n","Skipping y_hat=1665\n","Skipping y_hat=1665\n","Skipping y_hat=1353\n","Skipping y_hat=1353\n","Skipping y_hat=1353\n","Skipping y_hat=1353\n","Skipping y_hat=1353\n","Skipping y_hat=1353\n","Skipping y_hat=1353\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=822\n","Skipping y_hat=1792\n","Skipping y_hat=1792\n","Skipping y_hat=1792\n","Skipping y_hat=1792\n","Skipping y_hat=1792\n","Skipping y_hat=1792\n","Skipping y_hat=1792\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=1306\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=977\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=1012\n","Skipping y_hat=1012\n","Skipping y_hat=1012\n","Skipping y_hat=1012\n","Skipping y_hat=1012\n","Skipping y_hat=1012\n","Skipping y_hat=1012\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=972\n","Skipping y_hat=1209\n","Skipping y_hat=1209\n","Skipping y_hat=1209\n","Skipping y_hat=1209\n","Skipping y_hat=1209\n","Skipping y_hat=1209\n","Skipping y_hat=1209\n","Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=1256\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=530\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=786\n","Skipping y_hat=359\n","Skipping y_hat=359\n","Skipping y_hat=359\n","Skipping y_hat=359\n","Skipping y_hat=359\n","Skipping y_hat=359\n","Skipping y_hat=359\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1626\n","Skipping y_hat=1604\n","Skipping y_hat=1604\n","Skipping y_hat=1604\n","Skipping y_hat=1604\n","Skipping y_hat=1604\n","Skipping y_hat=1604\n","Skipping y_hat=1604\n","Skipping y_hat=1059\n","Skipping y_hat=1059\n","Skipping y_hat=1059\n","Skipping y_hat=1059\n","Skipping y_hat=1059\n","Skipping y_hat=1059\n","Skipping y_hat=1059\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=902\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=1026\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=487\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=862\n","Skipping y_hat=1448\n","Skipping y_hat=1448\n","Skipping y_hat=1448\n","Skipping y_hat=1448\n","Skipping y_hat=1448\n","Skipping y_hat=1448\n","Skipping y_hat=1448\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=1772\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=566\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=942\n","Skipping y_hat=942\n","Skipping y_hat=942\n","Skipping y_hat=942\n","Skipping y_hat=942\n","Skipping y_hat=942\n","Skipping y_hat=942\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=748\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=1561\n","Skipping y_hat=1572\n","Skipping y_hat=1572\n","Skipping y_hat=1572\n","Skipping y_hat=1572\n","Skipping y_hat=1572\n","Skipping y_hat=1572\n","Skipping y_hat=1572\n","Skipping y_hat=1383\n","Skipping y_hat=1383\n","Skipping y_hat=1383\n","Skipping y_hat=1383\n","Skipping y_hat=1383\n","Skipping y_hat=1383\n","Skipping y_hat=1383\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=999\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=436\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=772\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=288\n","Skipping y_hat=1314\n","Skipping y_hat=1314\n","Skipping y_hat=1314\n","Skipping y_hat=1314\n","Skipping y_hat=1314\n","Skipping y_hat=1314\n","Skipping y_hat=1314\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=1821\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=912\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=578\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=928\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=856\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=404\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=868\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=968\n","Skipping y_hat=879\n","Skipping y_hat=879\n","Skipping y_hat=879\n","Skipping y_hat=879\n","Skipping y_hat=879\n","Skipping y_hat=879\n","Skipping y_hat=879\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=333\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=460\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=1532\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=559\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=1326\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=861\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=797\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=812\n","Skipping y_hat=1104\n","Skipping y_hat=1104\n","Skipping y_hat=1104\n","Skipping y_hat=1104\n","Skipping y_hat=1104\n","Skipping y_hat=1104\n","Skipping y_hat=1104\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=1661\n","Skipping y_hat=629\n","Skipping y_hat=629\n","Skipping y_hat=629\n","Skipping y_hat=629\n","Skipping y_hat=629\n","Skipping y_hat=629\n","Skipping y_hat=629\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=688\n","Skipping y_hat=1287\n","Skipping y_hat=1287\n","Skipping y_hat=1287\n","Skipping y_hat=1287\n","Skipping y_hat=1287\n","Skipping y_hat=1287\n","Skipping y_hat=1287\n","Skipping y_hat=1676\n","Skipping y_hat=1676\n","Skipping y_hat=1676\n","Skipping y_hat=1676\n","Skipping y_hat=1676\n","Skipping y_hat=1676\n","Skipping y_hat=1676\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=990\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=871\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=676\n","Skipping y_hat=1168\n","Skipping y_hat=1168\n","Skipping y_hat=1168\n","Skipping y_hat=1168\n","Skipping y_hat=1168\n","Skipping y_hat=1168\n","Skipping y_hat=1168\n","Skipping y_hat=995\n","Skipping y_hat=995\n","Skipping y_hat=995\n","Skipping y_hat=995\n","Skipping y_hat=995\n","Skipping y_hat=995\n","Skipping y_hat=995\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=537\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=1458\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1355\n","Skipping y_hat=1395\n","Skipping y_hat=1395\n","Skipping y_hat=1395\n","Skipping y_hat=1395\n","Skipping y_hat=1395\n","Skipping y_hat=1395\n","Skipping y_hat=1395\n","Skipping y_hat=1132\n","Skipping y_hat=1132\n","Skipping y_hat=1132\n","Skipping y_hat=1132\n","Skipping y_hat=1132\n","Skipping y_hat=1132\n","Skipping y_hat=1132\n","Skipping y_hat=1779\n","Skipping y_hat=1779\n","Skipping y_hat=1779\n","Skipping y_hat=1779\n","Skipping y_hat=1779\n","Skipping y_hat=1779\n","Skipping y_hat=1779\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=702\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=589\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=539\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=1133\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1011\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1006\n","Skipping y_hat=1649\n","Skipping y_hat=1649\n","Skipping y_hat=1649\n","Skipping y_hat=1649\n","Skipping y_hat=1649\n","Skipping y_hat=1649\n","Skipping y_hat=1649\n","Skipping y_hat=899\n","Skipping y_hat=899\n","Skipping y_hat=899\n","Skipping y_hat=899\n","Skipping y_hat=899\n","Skipping y_hat=899\n","Skipping y_hat=899\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=1708\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1083\n","Skipping y_hat=1235\n","Skipping y_hat=1235\n","Skipping y_hat=1235\n","Skipping y_hat=1235\n","Skipping y_hat=1235\n","Skipping y_hat=1235\n","Skipping y_hat=1235\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1212\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1070\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=790\n","Skipping y_hat=1228\n","Skipping y_hat=1228\n","Skipping y_hat=1228\n","Skipping y_hat=1228\n","Skipping y_hat=1228\n","Skipping y_hat=1228\n","Skipping y_hat=1228\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=2019\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=409\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1622\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1198\n","Skipping y_hat=1439\n","Skipping y_hat=1439\n","Skipping y_hat=1439\n","Skipping y_hat=1439\n","Skipping y_hat=1439\n","Skipping y_hat=1439\n","Skipping y_hat=1439\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=370\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=1371\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=728\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=670\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=369\n","Skipping y_hat=1199\n","Skipping y_hat=1199\n","Skipping y_hat=1199\n","Skipping y_hat=1199\n","Skipping y_hat=1199\n","Skipping y_hat=1199\n","Skipping y_hat=1199\n","Skipping y_hat=1263\n","Skipping y_hat=1263\n","Skipping y_hat=1263\n","Skipping y_hat=1263\n","Skipping y_hat=1263\n","Skipping y_hat=1263\n","Skipping y_hat=1263\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=929\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=425\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=408\n","Skipping y_hat=1585\n","Skipping y_hat=1585\n","Skipping y_hat=1585\n","Skipping y_hat=1585\n","Skipping y_hat=1585\n","Skipping y_hat=1585\n","Skipping y_hat=1585\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=1008\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=435\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1744\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=1898\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=513\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=613\n","Skipping y_hat=1436\n","Skipping y_hat=1436\n","Skipping y_hat=1436\n","Skipping y_hat=1436\n","Skipping y_hat=1436\n","Skipping y_hat=1436\n","Skipping y_hat=1436\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=1112\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=570\n","Skipping y_hat=1145\n","Skipping y_hat=1145\n","Skipping y_hat=1145\n","Skipping y_hat=1145\n","Skipping y_hat=1145\n","Skipping y_hat=1145\n","Skipping y_hat=1145\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n","Skipping y_hat=1061\n"," 99% 79/80 [00:25<00:00,  3.72it/s]Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=757\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=597\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=1000\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=320\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=342\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=580\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=827\n","Skipping y_hat=1063\n","Skipping y_hat=1063\n","Skipping y_hat=1063\n","Skipping y_hat=1063\n","Skipping y_hat=1063\n","Skipping y_hat=1063\n","Skipping y_hat=1063\n","Skipping y_hat=1046\n","Skipping y_hat=1046\n","Skipping y_hat=1046\n","Skipping y_hat=1046\n","Skipping y_hat=1046\n","Skipping y_hat=1046\n","Skipping y_hat=1046\n","Skipping y_hat=483\n","Skipping y_hat=483\n","Skipping y_hat=483\n","Skipping y_hat=483\n","Skipping y_hat=483\n","Skipping y_hat=483\n","Skipping y_hat=483\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=345\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=887\n","Skipping y_hat=557\n","Skipping y_hat=557\n","Skipping y_hat=557\n","Skipping y_hat=557\n","Skipping y_hat=557\n","Skipping y_hat=557\n","Skipping y_hat=557\n","100% 80/80 [00:25<00:00,  3.09it/s]\n","accuracy of 10000 examples: 84/10000 (0.84%)\n","\n","Final Test Results:\n","test_reverse: 0.84%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m4_operands_0_to_999_first_phase_change\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/xhhfxa7x\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m4_operands_0_to_999_first_phase_change/reverse_out/wandb/run-20250907_151213-xhhfxa7x/logs\u001b[0m\n"]}]},{"cell_type":"markdown","source":["## 4 Operand Addition Scratchpad"],"metadata":{"id":"L0ttXoH9PGRV"}},{"cell_type":"code","source":["%cat configuration_files/4_operands_addition_scratchpad.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"3F4qYRSCQZhE","executionInfo":{"status":"ok","timestamp":1757518347294,"user_tz":300,"elapsed":844,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"48f7cd01-67b7-4d00-e723-ea3b0c6d3006"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_0_to_999_scratchpad'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad'\n","data_format='scratchpad'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 64 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: num of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 50\n","\n","drop_leading_digit = False\n","\n","# whether need zero‐padding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 300000\n","lr_decay_iters = 300000 # make equal to max_iters usually (300000)\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory\n","out_dir = '/content/drive/MyDrive/addition/results/4_operands_0_to_999_scratchpad/out'\n","\n","# to edit: training data\n","train_data_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_scratchpad/train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_path = \"/content/drive/MyDrive/addition/data/4_operands_0_to_999_scratchpad/train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_scratchpad/val.txt'\n","\n","# to edit: test data; start is just the test file. It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","eval_addition = True\n","test_file_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_scratchpad/test.txt'\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval1 = False\n","early_eval_interval1 = 25\n","early_eval_border1 = 1000\n","\n","more_early_eval2 = False\n","early_eval_interval2 = 5\n","early_eval_border2 = 750\n","\n","# to edit: whether do mutual information measurement between diffrent pairs of digits (include input-output and output-output)\n","mi_measurement = False\n","\n","# (optional) data for additional statistical measurement\n","stats_measurement_data_file_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/4_operand_addition_stats_measurement_data_reversed.txt'\n","\n","early_mi_measure_border = 200000\n","early_mi_measure_interval = 5000\n","final_mi_measure_interval = 5000"]}]},{"cell_type":"code","source":["%cat evaluation.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"ZEiUegj1UDm-","executionInfo":{"status":"ok","timestamp":1757519317743,"user_tz":300,"elapsed":629,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"5906439f-357f-4ef6-d57a-85e31a158e13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["from main_utilities import *\n","from tqdm.auto import tqdm\n","import torch\n","import numpy as np\n","import random\n","import math\n","import os\n","import pandas as pd\n","import csv\n","\n","\n","def get_abc_new(abc: str, zero_pad=False, data_format=\"plain\", binary=False, mode: str = \"compute_gold\"):\n","    \"\"\"Unified parser: mode='compute_gold' computes the groudtruth on the fly;\n","       mode='read_gold_as_str' reads the groundtruth from the evaluation files (testing, validation) to do string matching.\n","    Returns either\n","      (operands_str, result_int, operation)            # v1\n","    or\n","      (operands_str, result_int, result_str, operation)  # v2\n","    \"\"\"\n","    if '+' in abc:\n","        operation = '+'\n","    elif '-' in abc:\n","        operation = '-' \n","    elif '*' in abc:\n","        operation = '*'\n","    else:\n","        print(f'operation not found, abc: {abc}')\n","        return None, None, None\n","\n","    # Split the input string into parts\n","    parts = abc.split('=')\n","    if len(parts) != 2:\n","        print(f'Invalid format, expected \"a+b+c...=result\", got: {abc}')\n","        return None, None, None\n","\n","    # Get the operands part (before =)\n","    operands_str = parts[0]\n","    if operands_str[0] == '$':\n","        operands_str = operands_str[1:]\n","    if operands_str.startswith('Input:\\n'):\n","        operands_str = operands_str.split('Input:\\n')[-1]\n","    if 'Target' in operands_str:\n","        operands_str = operands_str.split('\\nTarget')[0]\n","\n","    # Split into individual operands\n","    operands = [op.strip() for op in operands_str.split(operation)]\n","    \n","    # Clean up operands\n","    operands = [op.replace(' ', '') for op in operands]\n","    \n","    if binary:\n","        # Convert all operands to binary and sum\n","        result = sum(int(op, 2) for op in operands)\n","        return operands_str, result, operation\n","\n","    if zero_pad:\n","        operands = [remove_zero_pad(op) for op in operands]\n","\n","    # version 1: compute the result\n","    if mode == \"compute_gold\":\n","        if operation == '+':\n","            result = sum(int(op) for op in operands)\n","        elif operation == '-':\n","            result = int(operands[0]) - sum(int(op) for op in operands[1:])\n","        elif operation == '*':\n","            result = 1\n","            for op in operands:\n","                result *= int(op)\n","        else:\n","            raise ValueError(f\"Unsupported operation: {operation}\")\n","\n","        return operands_str, result, operation\n","    # version 2: read the groundtruth from the evaluation files\n","    if mode == \"read_gold_as_str\":\n","        # parts[1] is the result part, which may contain a trailing '$' or newline\n","        result_str = parts[1].strip()\n","        if result_str.endswith('\\n'):\n","            result_str = result_str[:-1].strip()\n","        if result_str.endswith('$'):\n","            result_str = result_str[:-1].strip()\n","        if data_format == \"reverse\":\n","            sign = ''\n","            if result_str.startswith('-') or result_str.startswith('+'):\n","                sign = result_str[0]\n","                result_str = result_str[1:]\n","            result_str = sign + result_str[::-1]  # reverse the result string if needed\n","\n","        return operands_str, result_str, operation\n","\n","_precomputed_batches = {}\n","def prepare_addition_batches(config, encode, num_digit=3, zero_pad=False, binary=False,  data_type='binary', \n","                             operator='+', data_format='plain', add_space=False, simple=False, mode: str = \"compute_gold\"):\n","    device = config['device']\n","    test_batch_size = config['test_batch_size'] if 'test_batch_size' in config.keys() else 128\n","    start = config['start'] if 'start' in config.keys() else \"FILE:prompt/prompt_addition_pad_test_0.01.txt\"\n","    print(f\"Preparing batches from: {start}\")\n","    \n","    if start.startswith('FILE:'): # start is just the test file path\n","        with open(start[5:], 'r', encoding='utf-8') as f:\n","            lines = [line.rstrip() for line in f]\n","    else:\n","        lines = start.splitlines()\n","\n","    total = len(lines)\n","    print(f'Preparing batches for {total} examples from: {start}')\n","    \n","    # Process all lines and group by prompt length\n","    prompt_dict = {}\n","    for line in lines:\n","        # split off gold answer\n","        # e.g. line = \"123+456=579\"\n","        prompt_str = line.split('=')[0] + '='      # \"123+456=\"\n","        prompt_ids = encode(prompt_str)\n","        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","        prompt_length = x.size(1)\n","\n","        # parse out gold for evaluation later\n","        operands, result, op = get_abc_new(\n","            line,\n","            zero_pad=zero_pad,\n","            data_format=data_format,\n","            binary=binary,\n","            mode=mode\n","        )\n","\n","        entry = (x, operands, result)\n","        prompt_dict.setdefault(prompt_length, []).append(entry)\n","\n","    # Construct batches of prompts\n","    batch_list = []\n","    for prompt_length in prompt_dict.keys():\n","        input_tuple_list = prompt_dict[prompt_length]\n","        for batch_idx in range(math.ceil(len(input_tuple_list)/test_batch_size)):\n","            batch_list.append(input_tuple_list[batch_idx*test_batch_size:(batch_idx+1)*test_batch_size])\n","\n","    print(f'Created {len(batch_list)} batches')\n","    \n","    # Cache the batches using a hash of the configuration\n","    config_hash = hash(frozenset({k: str(v) for k, v in config.items() if k != 'device'}.items()))\n","    batch_key = f\"{config_hash}_{data_type}_{operator}_{num_digit}_{zero_pad}_{data_format}_{add_space}\"\n","    _precomputed_batches[batch_key] = (batch_list, total)\n","    \n","    return batch_list, total\n","\n","# Modified evaluation function that uses pre-created batches\n","def evaluate_addition_precomputed(config, model, ctx, decode, batch_list, total,\n","                                  verbose=False, num_digit=3, zero_pad=False, data_format='plain',\n","                                  add_space=False, operator='+', verbose_correct=False, analyze=False, mode: str = \"compute_gold\"):\n","    model.eval()\n","    device = config['device']\n","    max_new_tokens = config['max_new_tokens'] if 'max_new_tokens' in config.keys() else num_digit+2\n","    temperature = config['temperature'] if 'temperature' in config.keys() else 0.8\n","    top_k = config['top_k'] if 'top_k' in config.keys() else 200\n","\n","    if add_space:\n","        max_new_tokens = 2 * num_digit + 3\n","\n","    correct = 0\n","\n","    if analyze:\n","        # analyze various metrics\n","        error_dict = {'y': [], 'y_hat': [], 'accuracy_eps0': [], 'accuracy_eps5e-4': [],\n","                      'accuracy_eps5e-3': [], 'mse': [], 'normalized_mse': [],\n","                      'digit_wise_difference': [], 'incorrect_digit_count': []}\n","        list_not_num = []\n","        list_outlier_num = []\n","    op = operator\n","    correct_examples = []\n","    incorrect_examples = []\n","    print(f\"Max number of tokens {max_new_tokens}.\")\n","    for batch_idx in tqdm(range(len(batch_list))):\n","        batch = batch_list[batch_idx]\n","        x_list = [input_tuple[0] for input_tuple in batch]\n","        x = torch.cat(x_list, dim=0)\n","\n","        # Run generation\n","        with torch.no_grad():\n","            with ctx:\n","                eos_id = config['eos_id']\n","                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","                outcome_list = [decode(y_i.tolist()) for y_i in y]\n","\n","                for i, outcome in enumerate(outcome_list):\n","                    _, operands, result = batch[i]\n","                    \n","                    if mode == \"compute_gold\":\n","                        c_hat = outcome.split('=')[1].split('$')[0].strip()\n","\n","                        if zero_pad:\n","                            c_hat = remove_zero_pad(c_hat)\n","\n","                        # plain addition\n","                        c_hat = c_hat.split('\\n')[0]\n","\n","                        if data_format == \"reverse\":\n","                            c_hat = reverse_string(c_hat)\n","\n","                        if add_space:\n","                            c_hat = c_hat.replace(' ', '')\n","\n","                        if is_number(c_hat):\n","                            if '.' in c_hat:\n","                                c_hat = float(c_hat)\n","                            else:\n","                                c_hat = int(c_hat)\n","                        else:  # c_hat is not a number\n","                            result = str(result)\n","\n","                    if mode == \"read_gold_as_str\":\n","                        c_hat = outcome.split('=')[1].split('$')[0].strip()\n","\n","                        if data_format == \"reverse\":\n","                            sign = ''\n","                            if c_hat.startswith('-') or c_hat.startswith('+'):\n","                                sign = c_hat[0]\n","                                c_hat = c_hat[1:]\n","                            c_hat = sign + c_hat[::-1]\n","\n","                    # Check correctness\n","                    if result == c_hat:\n","                        correct += 1\n","                        correct_examples.append((operands, result, outcome, c_hat))\n","                        if verbose_correct:\n","                            print('outputs(o): ', outcome)\n","                            print(f'correct: {operands}={result}')\n","                    else:\n","                        incorrect_examples.append((operands, result, outcome, c_hat))\n","                        if verbose:\n","                            print('outputs(x): ', outcome)\n","                            print(f'wrong  : {operands}={c_hat}')\n","                            print(f'correct: {operands}={result}')\n","                    # Calculate metrics if analyzing\n","                    if analyze:\n","                        error_dict['y'].append(result)\n","                        error_dict['y_hat'].append(c_hat)\n","\n","                        metric_types = ['mse', 'normalized_mse', 'digit_wise_difference', 'incorrect_digit_count']\n","                        for metric_type in metric_types:\n","                            error, list_not_num, list_outlier_num = get_error_metric(result, c_hat, metric_type, eps=config.get('eps', 0),\n","                                                                                    list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                            error_dict[f'{metric_type}'].append(error)\n","\n","                        error, _, _ = get_error_metric(result, c_hat, 'accuracy', eps=0, list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                        error_dict[f'accuracy_eps0'].append(error * 100)\n","                        error, _, _ = get_error_metric(result, c_hat, 'accuracy', eps=5e-4, list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                        error_dict[f'accuracy_eps5e-4'].append(error * 100)\n","                        error, _, _ = get_error_metric(result, c_hat, 'accuracy', eps=5e-3, list_not_num=list_not_num, list_outlier_num=list_outlier_num)\n","                        error_dict[f'accuracy_eps5e-3'].append(error * 100)\n","\n","    accuracy = correct / total * 100\n","    print(f\"accuracy of {total} examples: {correct}/{total} ({accuracy}%)\")\n","\n","    accuracy_dictionary = {}\n","    if analyze:\n","        error_df = pd.DataFrame(error_dict)\n","        result_dir = config.get('result_dir')\n","        if result_dir is None:\n","            result_dir = get_results_dir(config)\n","        error_df.to_csv(os.path.join(result_dir, 'error_df.csv'), index=False)\n","\n","        error_mean_dict = {\n","            metric_type: np.nanmean(error_dict[f'{metric_type}'])\n","            for metric_type in ['accuracy_eps0', 'accuracy_eps5e-4', 'accuracy_eps5e-3',\n","                               'mse', 'normalized_mse', 'digit_wise_difference', 'incorrect_digit_count']\n","        }\n","        error_mean_dict['num_not_num'] = len(list_not_num) / len(metric_types)\n","        error_mean_dict['num_outlier_num'] = len(list_outlier_num) / len(metric_types)\n","        error_mean_dict['median_mse'] = error_df.mse.median()\n","        error_mean_dict['median_normalized_mse'] = error_df.normalized_mse.median()\n","        accuracy_dictionary.update(error_mean_dict)\n","\n","    model.train()\n","    return accuracy, accuracy_dictionary, correct_examples, incorrect_examples\n","\n","# Keep the original function for backward compatibility, but make it use the new functions\n","def evaluate_addition_batch(config, model, ctx, encode, decode, verbose=False, num_digit=3, zero_pad=False, \n","                          data_type='binary', operator='+', \n","                          data_format='plain', add_space=False, verbose_correct=False, analyze=False, mode: str = \"compute_gold\"):\n","    config_hash = hash(frozenset({k: str(v) for k, v in config.items() if k != 'device'}.items()))\n","    batch_key = f\"{config_hash}_{data_type}_{operator}_{num_digit}_{zero_pad}_{data_format}_{add_space}\"\n","    \n","    if batch_key in _precomputed_batches:\n","        print(\"Using precomputed batches\")\n","        batch_list, total = _precomputed_batches[batch_key]\n","    else:\n","        print(\"Creating new batches\")\n","        batch_list, total = prepare_addition_batches(\n","            config, encode, num_digit=num_digit, zero_pad=zero_pad,\n","            data_type=data_type, operator=operator, data_format=data_format, add_space=add_space, mode=mode\n","        )\n","\n","    # Evaluate using the batches\n","    return evaluate_addition_precomputed(\n","        config, model, ctx, decode, batch_list, total, verbose=verbose,\n","        num_digit=num_digit, zero_pad=zero_pad, data_format=data_format,\n","        add_space=add_space, operator=operator, verbose_correct=verbose_correct, analyze=analyze, mode=mode\n","    )\n","\n","def evaluate_multiple_files(config, model, ctx, encode, decode, test_file, iter_num, result_dir,\n","                          verbose=False, num_digit=3, zero_pad=False,\n","                          data_type='binary', operator='+', data_format='plain', add_space=False, analyze=False, mode: str = \"compute_gold\"):\n","    \"\"\"\n","    Evaluate model on multiple test files and store results.\n","    Args:\n","        test_files: List of test file paths\n","        iter_num: Current iteration number\n","        result_dir: Directory to store results\n","    Returns:\n","        dict: Dictionary containing accuracies for each test file\n","    \"\"\"\n","    \n","    # Get test file name without path and extension\n","    test_name = os.path.splitext(os.path.basename(test_file))[0]\n","    \n","    # Set the current test file as start\n","    config['start'] = f\"FILE:{test_file}\"\n","    \n","    # Run evaluation\n","    accuracy, metrics, correct, incorrect = evaluate_addition_batch(\n","        config, model, ctx, encode=encode, decode=decode,\n","        verbose=verbose, num_digit=num_digit, zero_pad=zero_pad,\n","        data_type=data_type, operator=operator,\n","        data_format=data_format, analyze=analyze, mode=mode\n","    )\n","    \n","    # Path for this test file's results\n","    results_file = os.path.join(result_dir, f'{test_name}_results.csv')\n","    \n","    # Combine correct and incorrect examples and sort by operands to maintain consistent order\n","    all_examples = correct + incorrect\n","    all_examples.sort(key=lambda x: x[0])  # Sort by operands\n","    \n","    # Create new DataFrame with operands and actual results\n","    new_df = pd.DataFrame({\n","        'operands': [ex[0] for ex in all_examples],\n","        'actual': [ex[1] for ex in all_examples],\n","        f'pred_iter_{iter_num}': [ex[3] for ex in all_examples]\n","    })\n","    \n","    # Read existing results if file exists and merge\n","    if os.path.exists(results_file):\n","        old_df = pd.read_csv(results_file)\n","        # # Merge based on operands, keeping all predictions\n","        # if 'operands' in old_df.columns:\n","        #     merged_df = pd.merge(old_df, new_df, on=['operands', 'actual'], how='outer')\n","        # else:\n","        #     merged_df = new_df\n","        # ── Normalize keys so they truly match ──\n","        for df in (old_df, new_df):\n","            # strip whitespace from the operands strings\n","            df['operands'] = df['operands'].str.strip()\n","            df['actual']   = df['actual'].str.strip()\n","\n","        merged_df = pd.merge(\n","            old_df, new_df,\n","            on=['operands', 'actual'],\n","            how='outer'\n","        )\n","    else:\n","        merged_df = new_df\n","    \n","    # Save results\n","    merged_df.to_csv(results_file, index=False)\n","    \n","    # Save accuracy separately in a summary file\n","    accuracy_file = os.path.join(result_dir, f'{test_name}_accuracy.csv')\n","    if os.path.exists(accuracy_file):\n","        acc_df = pd.read_csv(accuracy_file)\n","    else:\n","        acc_df = pd.DataFrame(columns=['iteration', 'accuracy'])\n","    \n","    # Add new accuracy\n","    new_row = pd.DataFrame({'iteration': [iter_num], 'accuracy': [accuracy]})\n","    acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","    acc_df.to_csv(accuracy_file, index=False)\n","    \n","    return test_name, accuracy, metrics, correct, incorrect"]}]},{"cell_type":"code","source":["!python train.py 4_operands_addition_scratchpad.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hGSNQOkyPFQF","outputId":"acebeb25-d932-41bf-a7ba-4fc6a5e3d883","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overriding config with ./configuration_files/4_operands_addition_scratchpad.txt:\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_0_to_999_scratchpad'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad'\n","data_format='scratchpad'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 64 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: num of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 50\n","\n","drop_leading_digit = False\n","\n","# whether need zero‐padding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 300000\n","lr_decay_iters = 300000 # make equal to max_iters usually (300000)\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory\n","out_dir = '/content/drive/MyDrive/addition/results/4_operands_0_to_999_scratchpad/out'\n","\n","# to edit: training data\n","train_data_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_scratchpad/train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_path = \"/content/drive/MyDrive/addition/data/4_operands_0_to_999_scratchpad/train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_scratchpad/val.txt'\n","\n","# to edit: test data; start is just the test file. It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","eval_addition = True\n","test_file_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_scratchpad/test.txt'\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval1 = False\n","early_eval_interval1 = 25\n","early_eval_border1 = 1000\n","\n","more_early_eval2 = False\n","early_eval_interval2 = 5\n","early_eval_border2 = 750\n","\n","# to edit: whether do mutual information measurement between diffrent pairs of digits (include input-output and output-output)\n","mi_measurement = False\n","\n","# (optional) data for additional statistical measurement\n","stats_measurement_data_file_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/4_operand_addition_stats_measurement_data_reversed.txt'\n","\n","early_mi_measure_border = 200000\n","early_mi_measure_interval = 5000\n","final_mi_measure_interval = 5000\n","Seeded everything: 1337\n","Using vocabulary size: 20\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.65M\n","/content/drive/MyDrive/addition/train.py:339: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_scratchpad/out/wandb/run-20250910_154903-lrilpmcc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m4_operands_0_to_999_scratchpad\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/lrilpmcc\u001b[0m\n","WARNING: results directory /content/drive/MyDrive/addition/results/4_operands_0_to_999_scratchpad/out/4_operands_0_to_999_scratchpad already exists, overwriting...\n","max_new_tokens: 50\n"]}]},{"cell_type":"markdown","source":["## 4 Operand Max"],"metadata":{"id":"dfLn10UAHrM2"}},{"cell_type":"code","source":["%cat configuration_files/4_operands_max.txt"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"PFYzFx4vK6Px","executionInfo":{"status":"ok","timestamp":1757793266964,"user_tz":300,"elapsed":960,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"4a40023b-e288-48b1-c44d-d4bc4644e1a5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 50\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_max_balanced_digit'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max'\n","data_format='max'\n","operator=''\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: num of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 10\n","\n","drop_leading_digit = False\n","\n","# whether need zero‐padding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually (300000)\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory\n","out_dir = '/content/drive/MyDrive/addition/results/4_operands_max_balanced_digit/out'\n","\n","# to edit: training data\n","train_data_path = '/content/drive/MyDrive/addition/data/4_operands_max_balanced_digit/train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_path = \"/content/drive/MyDrive/addition/data/4_operands_max_balanced_digit/train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_path = '/content/drive/MyDrive/addition/data/4_operands_max_balanced_digit/val.txt'\n","\n","# to edit: test data; start is just the test file. It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","eval_addition = True\n","test_file_path = '/content/drive/MyDrive/addition/data/4_operands_max_balanced_digit/test.txt'\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval1 = False\n","early_eval_interval1 = 25\n","early_eval_border1 = 1000\n","\n","more_early_eval2 = False\n","early_eval_interval2 = 5\n","early_eval_border2 = 750\n","\n","# to edit: whether do mutual information measurement between diffrent pairs of digits (include input-output and output-output)\n","mi_measurement = False\n","\n","# (optional) data for additional statistical measurement\n","stats_measurement_data_file_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/4_operand_addition_stats_measurement_data_reversed.txt'\n","\n","early_mi_measure_border = 200000\n","early_mi_measure_interval = 5000\n","final_mi_measure_interval = 5000"]}]},{"cell_type":"code","source":["%cat train.py"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"CEGW4g1dLgnQ","executionInfo":{"status":"ok","timestamp":1757768722508,"user_tz":300,"elapsed":2446,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"f616d8e7-6e50-417f-93f2-592b7961da56"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["import os\n","import pickle\n","import requests\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","import copy\n","import time\n","\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import yaml\n","from torch.utils.data import Dataset, DataLoader\n","import wandb\n","import torch.nn.functional as F\n","import math\n","\n","from model import GPTConfig, GPT\n","from main_utilities import *\n","from evaluation import *\n","from statistical_measurements import *\n","\n","import re\n","\n","def create_meta_for_addition(data):\n","    \"\"\"Create metadata for addition data.\"\"\"\n","    # Define the vocabulary for addition problems\n","    # This includes digits, operators, equals sign, and newline\n","    chars = sorted(list(set(data)))\n","\n","    # ensure special pad/eos token exists\n","    if '$' not in chars:\n","        chars.append('$')\n","        chars = sorted(chars)\n","        \n","    vocab_size = len(chars)\n","    # Create encoder and decoder dictionaries\n","    stoi = {ch: i for i, ch in enumerate(chars)}\n","    itos = {i: ch for i, ch in enumerate(chars)}\n","    \n","    meta = {\n","        'vocab_size': vocab_size,\n","        'vocab': chars,\n","        'stoi': stoi,\n","        'itos': itos\n","    }\n","    return meta\n","\n","def encode_addition(text, meta):\n","    \"\"\"Encode text to tensor using the metadata.\"\"\"\n","    return torch.tensor([meta['stoi'][c] for c in text], dtype=torch.long)\n","\n","def decode_addition(tensor, meta):\n","    \"\"\"Decode tensor to text using the metadata.\"\"\"\n","    if isinstance(tensor, torch.Tensor):\n","        return ''.join([meta['itos'][i.item()] for i in tensor])\n","    else:\n","        return ''.join([meta['itos'][i] for i in tensor])\n","    \n","def pad_sequence(x: torch.Tensor, length: int, pad_value: int):\n","    if x.size(0) < length:\n","        padding = torch.full((length - x.size(0),), pad_value, dtype=torch.long)\n","        return torch.cat([x, padding], dim=0)\n","    else:\n","        return x\n","\n","class AdditionDataset(Dataset):\n","    def __init__(self, file_path, meta):\n","        self.meta = meta\n","        # Read the text file\n","        with open(file_path, 'r') as f:\n","            self.lines = f.readlines()\n","        # Remove any empty lines and strip whitespace\n","        self.lines = [line.strip() for line in self.lines if line.strip()]\n","        self.block_size = block_size  # from your config\n","        \n","    def __len__(self):\n","        return len(self.lines)\n","    \n","    def __getitem__(self, idx):\n","        line = self.lines[idx]\n","        # Convert the line to tensor using our encoder\n","        raw = encode_addition(line, self.meta)\n","        x = pad_sequence(raw[:-1], self.block_size, pad_value=meta['stoi']['$'])  # all but last char\n","        y = pad_sequence(raw[1:], self.block_size, pad_value=-1)   # all but first char\n","        return x, y\n","\n","# I/O\n","\n","out_dir = '/drive/MyDrive/addition/plain_no_pad/out'\n","resume_dir = None\n","resume_iter = False # if True, resume from saved iter_num, otherwise resume from iter_num 0\n","eval_interval = 2000\n","log_interval = 1\n","eval_iters = 200\n","eval_only = False # if True, script exits right after the first eval\n","always_save_checkpoint = True # if True, always save a checkpoint after each eval\n","init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n","\n","# wandb logging\n","wandb_entity = 'ssdd'\n","wandb_log = False # disabled by default\n","wandb_project = 'owt'\n","wandb_run_name = 'gpt2' # 'run' + str(time.time())\n","exp_name = 'default_exp_name'\n","\n","# data\n","dataset = 'bal'\n","gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n","test_batch_size = 128\n","batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n","block_size = 1024\n","train_data_path = 'train.bin'\n","val_data_path = 'val.bin'\n","multi_digit = False\n","num_digit = 3\n","max_new_tokens = 5\n","binary = False\n","\n","# using two data - data1 = text / data2 = addition\n","train_both = False # use seperate text/add data for train/val (get_batch uses this to sample from two differernt datasets)\n","data_ratio = 0.2 # ratio of data_path2 compared with data_path1\n","train_data_path2 = 'train_addition.bin' # only used when train_both = True\n","val_data_path2 = 'val_addition.bin'\n","\n","# evaluation\n","eval_text = False # if True get perplexity using eval_text_data_path\n","eval_text_data_path = None # directory to text data (.bin file) - ex. 'data/shakespeare_add_ar_mixed/val_text.bin'\n","eval_addition = False # if True compute test accuracy of \"a+b=\"\n","test_file_path = None\n","eval_other = False # use this to evaluate other operations (ex. train on operator '-' but evaluate on other_operator '+')\n","other_operator = '+'\n","eval_addition_train = False\n","train_data_test_path = None\n","zero_pad = False\n","algo_reason = False\n","add_space = False\n","\n","# model\n","n_layer = 6\n","n_head = 6\n","n_embd = 768\n","dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n","bias = False # do we use bias inside LayerNorm and Linear layers?\n","ckpt_path_name = 'ckpt.pt'\n","save_final = True\n","\n","# adamw optimizer\n","learning_rate = 6e-4 # max learning rate\n","max_iters = 600000 # total number of training iterations\n","weight_decay = 1e-1\n","beta1 = 0.9\n","beta2 = 0.95\n","grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n","# learning rate decay settings\n","decay_lr = True # whether to decay the learning rate\n","warmup_iters = 2000 # how many steps to warm up for\n","lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n","min_lr = None # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n","\n","# DDP settings\n","backend = 'nccl' # 'nccl', 'gloo', etc.\n","# system\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n","dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n","compile = True # use PyTorch 2.0 to compile the model to be faster\n","use_flash = True\n","data_type = 'binary' # 'binary' by default, can be 'text'\n","operator = '+' # can be '+', '-', '*', 'sin', 'sqrt'\n","data_shuffle = True\n","data_format = 'plain' # 'plain' or 'reverse' or 'algo_reasoning'\n","vocabulary = 'all_ascii_chars' # can be 'all_ascii_chars' or 'numbers_only' or 'custom_input_data'\n","meta_path_specified = True # use saved meta_file (False if data_type='text')\n","eps = 0\n","tokenizer = 'char' # by default, use char level tokenizer. but for pretrained models, use openai tokenizer eg: 'gpt2'\n","\n","simple=False\n","random_A=False\n","random_C=False\n","\n","use_lora = False # use lora (from minLoRA)\n","print_interval = 2  # if we're using gpt-2 model, I want to see it prompted on text\n","\n","mode = \"compute_gold\"  # Mode for evaluation: \"compute_gold\" or \"read_gold_as_str\"\n","\n","more_early_eval1 = False # if True, do early, more frequent eval on train and val data\n","early_eval_interval1 = 25\n","early_eval_border1 = 1000\n","\n","more_early_eval2 = False # if True, do even earlier, even more frequent eval on train and val data\n","early_eval_interval2 = 5\n","early_eval_border2 = 500\n","\n","stats_measurement_data_file_path = \"\"\n","\n","drop_leading_digit = False\n","\n","config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str, type(None)))]\n","exec(open('configurator.py').read()) # overrides from command line or config file\n","config = {k: globals()[k] for k in config_keys} # will be useful for logging\n","\n","# additional statistical measurements\n","mi_measurement = False # whether to do mutual information measurement\n","early_mi_measure_border = 20000 # border for early mutual information measurement\n","early_mi_measure_interval = 1000 # interval for early mutual information measurement\n","final_mi_measure_interval = 5000 # interval for final mutual information measurement\n","\n","mi_measure_iters = set(\n","    list(range(0,  early_mi_measure_border, early_mi_measure_interval)) +    # every 20 steps before 200\n","    # list(range(100000, 100000, 20)) +   # every 50 steps from 200 up to 1500\n","    list(range(early_mi_measure_border, max_iters+1, final_mi_measure_interval))  # every 100 steps thereafter\n",")\n","\n","# function to set seed for all random number generators\n","def set_seed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    # to make sure GPU runs are deterministic even if they are slower set this to True\n","    torch.backends.cudnn.deterministic = False\n","    # warning: this causes the code to vary across runs\n","    torch.backends.cudnn.benchmark = True\n","    print(\"Seeded everything: {}\".format(seed))\n","\n","if min_lr == None:\n","    min_lr = learning_rate/10\n","master_process = True\n","seed_offset = 0\n","if master_process:\n","  os.makedirs(out_dir, exist_ok=True)\n","torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","torch.backends.cudnn.benchmark = True # cudnn auto-tuner\n","torch.backends.cudnn.deterministic = False # cudnn auto-tuner\n","# this is probably overkill but seed everything again\n","set_seed(1337 + seed_offset)\n","\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","# note: float16 data type will automatically use a GradScaler\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# Read the data files\n","with open(train_data_path, 'r') as f:\n","    train_data = f.read()\n","with open(val_data_path, 'r') as f:\n","    val_data = f.read()\n","\n","# Create metadata from the combined data\n","all_data = train_data + val_data\n","meta = create_meta_for_addition(train_data)\n","meta_vocab_size = meta['vocab_size']\n","print(f\"Using vocabulary size: {meta_vocab_size}\")\n","\n","config['eos_id'] = meta['stoi']['$']\n","\n","if mi_measurement:\n","    with open(stats_measurement_data_file_path, 'r', encoding='utf-8') as f:\n","        lines = [line.rstrip() for line in f]\n","\n","    if drop_leading_digit:\n","            S = num_digit\n","    else:\n","        S = num_digit + 1\n","    # a simple way to parse test strings\n","    padded_lines = [] # add 0 padding, remove $; an example padded_lines[6] is '932+084+230+349=5951'\n","    for i in range(len(lines)):\n","        numbers = re.split(r'[+=]', lines[i])\n","        numbers[-1] = numbers[-1][:-1]\n","        for k, number in enumerate(numbers[:-1]):\n","            numbers[k] = '0' * (3-len(number)) + number\n","        numbers[-1] = numbers[-1] + '0' * (S-len(numbers[-1]))\n","        padded_lines.append(\"+\".join(numbers[:-1]) + \"=\" + numbers[-1])\n","\n","    stats_measurement_data = torch.cat([encode_addition(padded_lines[i], meta).unsqueeze(0) for i in range(len(padded_lines))], dim=0)\n","\n","# # get 16 different datasets (including the base dataset) by randomizing input/output integers of the base dataset\n","# stats_measurement_dataset_list = gen_randomized_datasets(\n","#     stats_measurement_data,\n","#     meta,\n","#     digits_per_num=num_digit,\n","#     base_seed=2005\n","# )\n","\n","# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n","iter_num = 0\n","best_val_loss = 1e9\n","best_perplexity = 1e9 # on text data\n","best_accuracy = -1 # on addition data\n","\n","\n","model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n","                  bias=bias, vocab_size=None, dropout=dropout, use_flash=use_flash) # start with model_args from command line\n","if init_from == 'scratch':\n","    # init a new model from scratch\n","    print(\"Initializing a new model from scratch\")\n","    # determine the vocab size we'll use for from-scratch training\n","    if meta_vocab_size is None:\n","        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n","    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    model.to(device)\n","elif init_from == 'resume':\n","    if resume_dir:\n","        print(f\"Resuming training from {resume_dir}\")\n","        checkpoint = torch.load(resume_dir, map_location=device)\n","    else:\n","        print(f\"Resuming training from {out_dir}\")\n","        # resume training from a checkpoint.\n","        ckpt_path = os.path.join(out_dir, ckpt_path_name)\n","        checkpoint = torch.load(ckpt_path, map_location=device)\n","    checkpoint_model_args = checkpoint['model_args']\n","    # force these config attributes to be equal otherwise we can't even resume training\n","    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n","    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n","        model_args[k] = checkpoint_model_args[k]\n","    # create the model\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    model.to(device)\n","    state_dict = checkpoint['model']\n","    # fix the keys of the state dictionary :(\n","    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)\n","    iter_num = checkpoint['iter_num'] if resume_iter else 0\n","    max_iters += iter_num\n","    best_val_loss = checkpoint['best_val_loss']\n","    if 'best_perplexity' in checkpoint.keys():\n","        best_perplexity = checkpoint['best_perplexity']\n","    if 'best_accuracy' in checkpoint.keys():\n","        best_accuracy = checkpoint['best_accuracy']\n","\n","scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","\n","optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n","\n","# compile the model\n","if compile:\n","    print(\"compiling the model... (takes a ~minute)\")\n","    unoptimized_model = model\n","    model = torch.compile(model) # requires PyTorch 2.0\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        # Get an iterator from the DataLoader\n","        dataloader = train_loader if split == 'train' else val_loader\n","        dataloader_iter = iter(dataloader)\n","\n","        for k in range(eval_iters):\n","            try:\n","                X, Y = next(dataloader_iter)\n","\n","            except StopIteration:\n","                # If we run out of batches, create a new iterator\n","                dataloader_iter = iter(dataloader)\n","                X, Y = next(dataloader_iter)\n","\n","            with ctx:\n","                X, Y = X.to(device), Y.to(device)\n","                logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","\n","def get_lr_for_iter(iter_num):\n","    \"\"\"Calculate learning rate based on iteration number using cosine decay with warmup.\"\"\"\n","    if iter_num < warmup_iters:\n","        return learning_rate * (iter_num + 1) / warmup_iters\n","    \n","    if iter_num >= lr_decay_iters:\n","        return min_lr\n","    \n","    decay_ratio = (iter_num - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n","    return min_lr + coeff * (learning_rate - min_lr)\n","\n","# logging\n","if wandb_log and master_process:\n","    import wandb\n","    wandb.init(project=wandb_project, name=wandb_run_name, config=config, dir = out_dir)\n","\n","\n","\n","\n","train_dataset = AdditionDataset(train_data_path, meta)\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    pin_memory=(device_type=='cuda')\n",")\n","\n","val_dataset = AdditionDataset(val_data_path, meta)\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    pin_memory=(device_type=='cuda')\n",")\n","\n","# encode, decode = get_encode_decode(meta_path, tokenizer=tokenizer)\n","\n","# Initialize result_dict with basic metrics\n","result_dict = {\n","    'iter': [],\n","    'train_loss': [],\n","    'val_loss': [],\n","    'test_acc': [],\n","    'train_acc': []\n","}\n","\n","# Initialize test accuracy keys for all test files\n","result_dict[f'test_acc'] = []\n","\n","result_dir = get_results_dir(config)\n","config['result_dir'] = result_dir\n","with open(os.path.join(result_dir, \"config.yaml\"), \"w\") as yaml_file:\n","    yaml.dump(config, yaml_file, default_flow_style=False)\n","\n","\n","# # build a dict of open file handles, one per dataset\n","# csv_writers = {}\n","# for dataset in stats_measurement_dataset_list:\n","#     name = dataset['name']\n","#     path = os.path.join(result_dir, f\"{name}_stats.csv\")\n","#     f = open(path, 'w', newline='')\n","#     writer = csv.DictWriter(f, fieldnames=[\n","#         'iter',\n","#         'ave_correct_probs',\n","#         'ave_correct_preds',\n","#         'ave_diff_probs_L1',\n","#         'ave_diff_probs_L2',\n","#         'ave_diff_probs_kl',\n","#         'ave_diff_logits_L1',\n","#         'ave_diff_logits_L2',\n","#         'ave_diff_preds',\n","#     ])\n","#     writer.writeheader()\n","#     csv_writers[name] = writer\n","\n","\n","# Initialize additional metrics for statistical measurements\n","stats_oo = [] # output-output mutual information\n","stats_io = [] # input-output mutual information\n","\n","\n","import time\n","t0 = time.time()\n","local_iter_num = 0 # number of iterations in the lifetime of this process\n","raw_model = model\n","running_mfu = -1.0\n","iter_num = 0\n","\n","max_iters = config.get('max_iters', 10000)\n"," # number of epochs to warm up learning rate\n","\n","# Initialize tracking variables\n","iter_num = 0\n","best_val_loss = 1e9\n","best_accuracy = -1\n","running_mfu = -1.0\n","\n","# Create infinite data loader\n","def get_infinite_dataloader(dataloader):\n","    while True:\n","        for batch in dataloader:\n","            yield batch\n","\n","train_loader_iter = get_infinite_dataloader(train_loader)\n","if 'max_new_tokens' in config.keys():\n","    print(f\"max_new_tokens: {config['max_new_tokens']}\")\n","else:\n","    print(f\"max_new_tokens used: {num_digit+2}\")\n","\n","# Training loop - iteration based\n","while iter_num < max_iters:\n","    model.train()\n","    \n","    # Get learning rate for current iteration\n","    if decay_lr:\n","        lr = get_lr_for_iter(iter_num)\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","    \n","    # Get next batch\n","    X, Y = next(train_loader_iter)\n","    X, Y = X.to(device), Y.to(device)\n","    \n","    # Forward pass\n","    with ctx:\n","        logits, loss = model(X, Y)\n","    \n","    # Backward pass\n","    scaler.scale(loss).backward()\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    scaler.step(optimizer)\n","    scaler.update()\n","    optimizer.zero_grad(set_to_none=True)\n","    \n","    # Do additional statistical measurements\n","    if mi_measurement:\n","        if iter_num in mi_measure_iters:\n","            model.eval()\n","            \n","            with torch.no_grad():\n","                # eval_res = eval_model(model, meta, stats_measurement_dataset_list, digits_per_num=num_digit, batch_size=test_batch_size)\n","                mi_stats = calc_model_dataset_mi(\n","                    model = model,\n","                    metadata = meta,\n","                    data = stats_measurement_data,\n","                    digits_per_num = num_digit,\n","                    batch_size = test_batch_size,\n","                    drop_leading_digit = drop_leading_digit\n","                )\n","\n","            # for name, stats in eval_res.items():\n","            #     if name == \"model_embeddings\":\n","            #         continue\n","            #     if name == 'base':\n","            #         row = {\n","            #             'iter': iter_num,\n","            #             'ave_correct_probs': stats['ave_correct_probs'],\n","            #             'ave_correct_preds': stats['ave_correct_preds'],\n","            #         }\n","            #     else:\n","            #         row = {\n","            #             'iter': iter_num,\n","            #             'ave_correct_probs': stats['ave_correct_probs'],\n","            #             'ave_correct_preds': stats['ave_correct_preds'],\n","            #             'ave_diff_probs_L1': stats['ave_diff_probs_L1'],\n","            #             'ave_diff_probs_L2': stats['ave_diff_probs_L2'],\n","            #             'ave_diff_probs_kl': stats['ave_diff_probs_kl'],\n","            #             'ave_diff_logits_L1': stats['ave_diff_logits_L1'],\n","            #             'ave_diff_logits_L2': stats['ave_diff_logits_L2'],\n","            #             'ave_diff_preds': stats['ave_diff_preds'],\n","            #         }\n","            #     # Write to the CSV file for this dataset\n","            #     csv_writers[name].writerow(row)\n","\n","            \n","            # Calculate output-output mutual information\n","            mi_mat = mi_stats['output-output']['mutual_info']\n","            nmi_mat = mi_stats['output-output']['normalized_mutual_info']\n","            for i in range(mi_mat.shape[0]):\n","                for j in range(i, mi_mat.shape[1]):\n","                    stats_oo.append({\n","                        'iter': iter_num,\n","                        'i': i,\n","                        'j': j,\n","                        'mi': mi_mat[i, j].item(),\n","                        'nmi': nmi_mat[i, j].item()\n","                    })\n","\n","            # also calculate input-output mutual information\n","            mi_mat_io = mi_stats['input-output']['mutual_info']\n","            nmi_mat_io = mi_stats['input-output']['normalized_mutual_info']\n","            for i in range(mi_mat_io.shape[0]):\n","                for j in range(mi_mat_io.shape[1]):\n","                    stats_io.append({\n","                        'iter': iter_num,\n","                        'i': i,\n","                        'j': j,\n","                        'mi': mi_mat_io[i, j].item(),\n","                        'nmi': nmi_mat_io[i, j].item()\n","                    })\n","\n","            # **NOW write out the two MI CSVs immediately:**\n","            stats_oo_df = pd.DataFrame(stats_oo)\n","            stats_oo_df.to_csv(os.path.join(result_dir, 'output_output_mi.csv'), index=False)\n","\n","            stats_io_df = pd.DataFrame(stats_io)\n","            stats_io_df.to_csv(os.path.join(result_dir, 'input_output_mi.csv'), index=False)\n","\n","            model.train()\n","        \n","    # Evaluation\n","    if iter_num % eval_interval == 0 or (more_early_eval1 and iter_num <= early_eval_border1 and iter_num % early_eval_interval1 == 0) or (more_early_eval2 and iter_num <= early_eval_border2 and iter_num % early_eval_interval2 == 0):\n","        losses = estimate_loss()\n","        print(f\"iter {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        \n","        # Initialize wandb_dict for this iteration\n","        wandb_dict = {\n","            \"iter\": iter_num,\n","            \"train/loss\": losses['train'],\n","            \"val/loss\": losses['val'],\n","            \"lr\": lr,\n","        }\n","\n","        if losses['val'] < best_val_loss:\n","            best_val_loss = losses['val']\n","        \n","        # Regular test evaluation\n","        test_accuracy = None\n","        if eval_addition:\n","            test_name, test_accuracy, _ , correct, incorrect = evaluate_multiple_files(\n","                config, model, ctx,\n","                encode=lambda x: encode_addition(x, meta),\n","                decode=lambda x: decode_addition(x, meta),\n","                test_file=test_file_path,\n","                iter_num=iter_num,\n","                result_dir=result_dir,\n","                verbose=False,\n","                num_digit=num_digit,\n","                zero_pad=zero_pad,\n","                data_type=data_type,\n","                operator=operator,\n","                data_format=data_format,\n","                analyze=True,\n","                mode=mode\n","            )\n","\n","            # Log results\n","            print(\"\\nTest Results:\")\n","            print(f\"{test_name}: {test_accuracy:.2f}%\")\n","\n","            print()\n","            \n","            # Add test accuracy to wandb_dict\n","            wandb_dict[\"test/accuracy\"] = test_accuracy\n","            \n","            if test_accuracy > best_accuracy and iter_num % 5 * eval_interval == 0:\n","                best_accuracy = test_accuracy\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'best_accuracy': best_accuracy,\n","                    'config': config,\n","                    'meta': meta,\n","                }\n","                torch.save(checkpoint, os.path.join(out_dir, f'ckpt_iter_{iter_num}_acc.pt'))\n","        \n","        # Training data evaluation\n","        train_accuracy = None\n","        if eval_addition_train:\n","            config['start'] = f\"FILE:{train_data_test_path}\"\n","            train_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","                config, model, ctx, \n","                encode=lambda x: encode_addition(x, meta),\n","                decode=lambda x: decode_addition(x, meta), \n","                verbose=False, \n","                num_digit=num_digit, \n","                zero_pad=zero_pad,\n","                data_type=data_type, \n","                operator=operator, \n","                data_format=data_format,\n","                mode=mode\n","            )\n","            \n","            # Add train accuracy to wandb_dict\n","            wandb_dict[\"train/accuracy\"] = train_accuracy\n","        \n","        # Update and save basic metrics\n","        result_dict['iter'].append(iter_num)\n","        result_dict['train_loss'].append(losses['train'].item())\n","        result_dict['val_loss'].append(losses['val'].item())\n","        result_dict['test_acc'].append(test_accuracy)\n","        result_dict['train_acc'].append(train_accuracy)\n","        \n","        # Save results to CSV after each evaluation\n","        result_df = pd.DataFrame(result_dict)\n","        result_df.to_csv(os.path.join(result_dir, 'training_metrics.csv'), index=False)\n","        \n","        # Single wandb log per iteration with all metrics\n","        if wandb_log:\n","            wandb.log(wandb_dict)\n","    \n","    iter_num += 1\n","\n","# Save final checkpoint\n","checkpoint = {\n","    'model': raw_model.state_dict(),\n","    'optimizer': optimizer.state_dict(),\n","    'model_args': model_args,\n","    'iter_num': iter_num,\n","    'best_val_loss': best_val_loss,\n","    'best_accuracy': best_accuracy,\n","    'config': config,\n","    'meta': meta,\n","}\n","torch.save(checkpoint, os.path.join(out_dir, f'ckpt_final.pt'))\n","\n","\n","losses = estimate_loss()\n","\n","if eval_addition:\n","    config['start'] = f\"FILE:{test_file_path}\"\n","    test_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","        config, model, ctx, \n","        encode=lambda x: encode_addition(x, meta),\n","        decode=lambda x: decode_addition(x, meta), \n","        verbose=False, \n","        num_digit=num_digit, \n","        zero_pad=zero_pad,\n","        data_type=data_type, \n","        operator=operator, \n","        data_format=data_format, \n","        analyze=True,\n","        mode=mode\n","    )\n","    import csv\n","    # Save correct examples\n","    correct_path = os.path.join(result_dir, 'correct_examples.csv')\n","    with open(correct_path, 'w', newline='') as csvfile:\n","        fieldnames = ['operands', 'result', 'outcome', 'c_hat2']\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","        writer.writeheader()\n","        for i, nums in enumerate(correct):\n","            operands, result, outcome, c_hat2 = nums\n","            writer.writerow({'operands': operands, 'result': result, 'outcome': outcome, 'c_hat2': c_hat2})\n","    \n","    # Save incorrect examples\n","    incorrect_path = os.path.join(result_dir, 'incorrect_examples.csv')\n","    with open(incorrect_path, 'w', newline='') as csvfile:\n","        fieldnames = ['operands', 'result', 'outcome', 'c_hat2']\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","        writer.writeheader()\n","        for i, nums in enumerate(incorrect):\n","            operands, result, outcome, c_hat2 = nums\n","            writer.writerow({'operands': operands, 'result': result, 'outcome': outcome, 'c_hat2': c_hat2})\n","\n","if eval_addition_train:\n","    config['start'] = f\"FILE:{train_data_test_path}\"\n","    train_accuracy, _ , correct, incorrect = evaluate_addition_batch(\n","        config, model, ctx, \n","        encode=lambda x: encode_addition(x, meta),\n","        decode=lambda x: decode_addition(x, meta), \n","        verbose=False, \n","        num_digit=num_digit, \n","        zero_pad=zero_pad,\n","        data_type=data_type, \n","        operator=operator, \n","        data_format=data_format,\n","        mode=mode\n","    )\n","    \n","    \n","test_name, accuracy, metrics, correct, incorrect = evaluate_multiple_files(\n","    config, model, ctx,\n","    encode=lambda x: encode_addition(x, meta),\n","    decode=lambda x: decode_addition(x, meta),\n","    test_file=test_file_path,\n","    iter_num='final',\n","    result_dir=result_dir,\n","    verbose=False,\n","    num_digit=num_digit,\n","    zero_pad=zero_pad,\n","    data_type=data_type,\n","    operator=operator,\n","    data_format=data_format,\n","    analyze=True,\n","    mode=mode\n",")\n","\n","print(\"\\nFinal Test Results:\")\n","print(f\"{test_name}: {accuracy:.2f}%\")\n","print()\n","\n","\n","# Final wandb logging\n","if wandb_log:\n","    final_dict = {\n","        \"iter\": iter_num,\n","        \"train/loss\": losses['train'],\n","        \"val/loss\": losses['val'],\n","        \"lr\": lr,\n","        \"test/accuracy\": test_accuracy if eval_addition else None,\n","        \"train/accuracy\": train_accuracy if eval_addition_train else None,\n","    }\n","    final_dict[f\"final_test/accuracy\"] = accuracy\n","    wandb.log(final_dict)\n","\n","# Save final DataFrame\n","result_df = pd.DataFrame(result_dict)\n","result_df.to_csv(os.path.join(result_dir, 'training_metrics.csv'), index=False)\n"]}]},{"cell_type":"code","source":["%cat model.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"SSG2giBcf0u9","executionInfo":{"status":"ok","timestamp":1757790855500,"user_tz":300,"elapsed":338,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"ced2ce13-db0e-4a27-f77b-10b409535090"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","import math\n","import inspect\n","from dataclasses import dataclass\n","\n","def new_gelu(x):\n","    \"\"\"\n","    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n","    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n","    \"\"\"\n","    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n","\n","class LayerNorm(nn.Module):\n","    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n","\n","    def __init__(self, ndim, bias):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(ndim))\n","        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n","\n","    def forward(self, input):\n","        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n","    \n","class CasualSelfAttention(nn.Module):\n","  def __init__(self,config):\n","    super().__init__()\n","    assert config.n_embd % config.n_head == 0\n","    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n","    #output_projection\n","    self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n","    #regularization\n","    self.attn_dropout = nn.Dropout(config.dropout)\n","    self.resid_dropout = nn.Dropout(config.dropout)\n","    self.n_head = config.n_head\n","    self.n_embd = config.n_embd\n","    self.dropout = config.dropout\n","    self.use_flash = config.use_flash\n","    self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and self.use_flash\n","    if not self.flash:\n","            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n","            # causal mask to ensure that attention is only applied to the left in the input sequence\n","            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n","                                        .view(1, 1, config.block_size, config.block_size))\n","    else:\n","      print(\"Using Flash attention\")\n","  def forward(self,x):\n","    B, T, C = x.size() # batch_size, sequence length, embedding dimension\n","    q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n","    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n","    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n","\n","    if self.flash:\n","      y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask = None, dropout_p = self.dropout if self.training else 0, is_causal = True)\n","    else:\n","      #manual implementation of attention\n","      att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n","      att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n","      att = F.softmax(att, dim=-1)\n","      att = self.attn_dropout(att)\n","      y = att @ v\n","    y = y.transpose(1, 2).contiguous().view(B, T, C)\n","    y = self.resid_dropout(self.c_proj(y))\n","\n","    return y\n","  \n","class MLP(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n","        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","    def forward(self, x):\n","        x = self.c_fc(x)\n","        x = new_gelu(x)\n","        x = self.c_proj(x)\n","        x = self.dropout(x)\n","        return x\n","    \n","class Block(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n","        self.attn = CasualSelfAttention(config)\n","        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n","        self.mlp = MLP(config)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln_1(x))\n","        x = x + self.mlp(self.ln_2(x))\n","        return x\n","    \n","@dataclass\n","class GPTConfig:\n","    block_size: int = 512\n","    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n","    n_layer: int = 6\n","    n_head: int = 6\n","    n_embd: int = 768\n","    dropout: float = 0.0\n","    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n","    use_flash: bool = True # use Flash Attention CUDA kernels for fast attention\n","\n","\n","class GPT(nn.Module):\n","  def __init__(self,config, pad_id):\n","    super().__init__()\n","    assert config.vocab_size is not None\n","    assert config.block_size is not None\n","    self.config = config\n","    self.pad_id = pad_id\n","\n","    self.transformer = nn.ModuleDict(dict(\n","        wte = nn.Embedding(config.vocab_size, config.n_embd),\n","        wpe = nn.Embedding(config.block_size, config.n_embd),\n","        drop = nn.Dropout(config.dropout),\n","        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n","        ln_f = LayerNorm(config.n_embd, bias=config.bias),\n","    ))\n","    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False)\n","    self.transformer.wte.weight = self.lm_head.weight # Tie weights\n","\n","    # init all weights\n","    self.apply(self._init_weights)\n","\n","    for pn, p in self.named_parameters():\n","      if pn.endswith('c_proj.weight'):\n","        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n","    print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,) )\n","\n","  def _init_weights(self, module):\n","      if isinstance(module, nn.Linear):\n","        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","        if module.bias is not None:\n","          torch.nn.init.zeros_(module.bias)\n","      elif isinstance(module, nn.Embedding):\n","        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","  def get_num_params(self, non_embedding=True):\n","    n_params = sum(p.numel() for p in self.parameters())\n","    if non_embedding:\n","      n_params -= self.transformer.wte.weight.numel()\n","    return n_params\n","\n","  def forward(self, idx, targets=None):\n","    device = idx.device\n","    b, t = idx.size()\n","    assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n","    pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n","\n","    tok_emd = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n","    pos_emd = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n","    x = self.transformer.drop(tok_emd + pos_emd)\n","    for block in self.transformer.h:\n","      x = block(x)\n","    x = self.transformer.ln_f(x)\n","\n","    if targets is not None:\n","      logits = self.lm_head(x)\n","      logits = logits.view(-1, logits.size(-1))   # (B*L, V)\n","      targets = targets.view(-1)                  # (B*L,)\n","      loss = F.cross_entropy(logits, targets, ignore_index=self.pad_id)\n","    else:\n","      logits = self.lm_head(x[:,[-1],:])\n","      loss = None\n","    return logits, loss\n","\n","\n","\n","  def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n","    decay = set()\n","    no_decay = set()\n","    whitelist_weight_modules = (torch.nn.Linear, )\n","    blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n","    for mn, m in self.named_modules():\n","      for pn, p in m.named_parameters():\n","        fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n","        # random note\n","\n","        if pn.endswith('bias'):\n","          no_decay.add(fpn)\n","        elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n","          decay.add(fpn)\n","        elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n","          no_decay.add(fpn)\n","\n","    decay.remove('lm_head.weight')\n","\n","    param_dict = {pn: p for pn, p in self.named_parameters()}\n","    inter_params = decay & no_decay\n","    union_params = decay | no_decay\n","    assert len(inter_params) == 0, f\"parameters {str(inter_params)} made it into both decay/no_decay sets!\"\n","    assert len(param_dict.keys() - union_params) == 0, f\"parameters {str(param_dict.keys() - union_params)} were not separated into either decay/no_decay set!\"\n","\n","    optim_groups = [\n","        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n","        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n","    ]\n","      # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n","    use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n","\n","    print(f\"using fused:{use_fused}\")\n","    extra_args = dict(fused=True) if use_fused else dict()\n","    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n","    return optimizer\n","\n","  @torch.no_grad()\n","  def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n","    for _ in range(max_new_tokens):\n","      idx_cond = idx if idx.size(1) <=self.config.block_size else idx[:, -self.config.block_size:]\n","      logits, _ = self(idx_cond)\n","\n","      logits = logits[:,-1,:] / temperature\n","\n","      if top_k is not None:\n","        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n","        logits[logits < v[:,[-1]]] = -float('Inf')\n","      probs = F.softmax(logits, dim=-1)\n","      idx_next = torch.multinomial(probs, num_samples=1)\n","      idx = torch.cat((idx, idx_next), dim=1)\n","    return idx"]}]},{"cell_type":"code","source":["!python train.py 4_operands_max.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"X2hV4DOEHtrh","executionInfo":{"status":"ok","timestamp":1757796874058,"user_tz":300,"elapsed":631531,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"61ad6c26-1627-4dd3-90e9-7e3566013d02"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=7608\n","Skipping y_hat=7608\n","Skipping y_hat=7608\n","Skipping y_hat=7608\n","Skipping y_hat=7608\n","Skipping y_hat=9836\n","Skipping y_hat=9836\n","Skipping y_hat=9836\n","Skipping y_hat=9836\n","Skipping y_hat=9836\n","Skipping y_hat=9836\n","Skipping y_hat=9836\n","Skipping y_hat=6726\n","Skipping y_hat=6726\n","Skipping y_hat=6726\n","Skipping y_hat=6726\n","Skipping y_hat=6726\n","Skipping y_hat=6726\n","Skipping y_hat=6726\n","Skipping y_hat=8243\n","Skipping y_hat=8243\n","Skipping y_hat=8243\n","Skipping y_hat=8243\n","Skipping y_hat=8243\n","Skipping y_hat=8243\n","Skipping y_hat=8243\n","Skipping y_hat=9038\n","Skipping y_hat=9038\n","Skipping y_hat=9038\n","Skipping y_hat=9038\n","Skipping y_hat=9038\n","Skipping y_hat=9038\n","Skipping y_hat=9038\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=7521\n","Skipping y_hat=5097\n","Skipping y_hat=5097\n","Skipping y_hat=5097\n","Skipping y_hat=5097\n","Skipping y_hat=5097\n","Skipping y_hat=5097\n","Skipping y_hat=5097\n","Skipping y_hat=6136\n","Skipping y_hat=6136\n","Skipping y_hat=6136\n","Skipping y_hat=6136\n","Skipping y_hat=6136\n","Skipping y_hat=6136\n","Skipping y_hat=6136\n","Skipping y_hat=6293\n","Skipping y_hat=6293\n","Skipping y_hat=6293\n","Skipping y_hat=6293\n","Skipping y_hat=6293\n","Skipping y_hat=6293\n","Skipping y_hat=6293\n","Skipping y_hat=7783\n","Skipping y_hat=7783\n","Skipping y_hat=7783\n","Skipping y_hat=7783\n","Skipping y_hat=7783\n","Skipping y_hat=7783\n","Skipping y_hat=7783\n","Skipping y_hat=7410\n","Skipping y_hat=7410\n","Skipping y_hat=7410\n","Skipping y_hat=7410\n","Skipping y_hat=7410\n","Skipping y_hat=7410\n","Skipping y_hat=7410\n","Skipping y_hat=4082\n","Skipping y_hat=4082\n","Skipping y_hat=4082\n","Skipping y_hat=4082\n","Skipping y_hat=4082\n","Skipping y_hat=4082\n","Skipping y_hat=4082\n","Skipping y_hat=9200\n","Skipping y_hat=9200\n","Skipping y_hat=9200\n","Skipping y_hat=9200\n","Skipping y_hat=9200\n","Skipping y_hat=9200\n","Skipping y_hat=9200\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=2483\n","Skipping y_hat=5175\n","Skipping y_hat=5175\n","Skipping y_hat=5175\n","Skipping y_hat=5175\n","Skipping y_hat=5175\n","Skipping y_hat=5175\n","Skipping y_hat=5175\n","Skipping y_hat=8131\n","Skipping y_hat=8131\n","Skipping y_hat=8131\n","Skipping y_hat=8131\n","Skipping y_hat=8131\n","Skipping y_hat=8131\n","Skipping y_hat=8131\n","Skipping y_hat=7311\n","Skipping y_hat=7311\n","Skipping y_hat=7311\n","Skipping y_hat=7311\n","Skipping y_hat=7311\n","Skipping y_hat=7311\n","Skipping y_hat=7311\n","Skipping y_hat=5712\n","Skipping y_hat=5712\n","Skipping y_hat=5712\n","Skipping y_hat=5712\n","Skipping y_hat=5712\n","Skipping y_hat=5712\n","Skipping y_hat=5712\n","Skipping y_hat=6184\n","Skipping y_hat=6184\n","Skipping y_hat=6184\n","Skipping y_hat=6184\n","Skipping y_hat=6184\n","Skipping y_hat=6184\n","Skipping y_hat=6184\n","Skipping y_hat=4949\n","Skipping y_hat=4949\n","Skipping y_hat=4949\n","Skipping y_hat=4949\n","Skipping y_hat=4949\n","Skipping y_hat=4949\n","Skipping y_hat=4949\n","Skipping y_hat=4389\n","Skipping y_hat=4389\n","Skipping y_hat=4389\n","Skipping y_hat=4389\n","Skipping y_hat=4389\n","Skipping y_hat=4389\n","Skipping y_hat=4389\n","Skipping y_hat=6702\n","Skipping y_hat=6702\n","Skipping y_hat=6702\n","Skipping y_hat=6702\n","Skipping y_hat=6702\n","Skipping y_hat=6702\n","Skipping y_hat=6702\n","Skipping y_hat=9686\n","Skipping y_hat=9686\n","Skipping y_hat=9686\n","Skipping y_hat=9686\n","Skipping y_hat=9686\n","Skipping y_hat=9686\n","Skipping y_hat=9686\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=1096\n","Skipping y_hat=4018\n","Skipping y_hat=4018\n","Skipping y_hat=4018\n","Skipping y_hat=4018\n","Skipping y_hat=4018\n","Skipping y_hat=4018\n","Skipping y_hat=4018\n","Skipping y_hat=3240\n","Skipping y_hat=3240\n","Skipping y_hat=3240\n","Skipping y_hat=3240\n","Skipping y_hat=3240\n","Skipping y_hat=3240\n","Skipping y_hat=3240\n","Skipping y_hat=5483\n","Skipping y_hat=5483\n","Skipping y_hat=5483\n","Skipping y_hat=5483\n","Skipping y_hat=5483\n","Skipping y_hat=5483\n","Skipping y_hat=5483\n","Skipping y_hat=8070\n","Skipping y_hat=8070\n","Skipping y_hat=8070\n","Skipping y_hat=8070\n","Skipping y_hat=8070\n","Skipping y_hat=8070\n","Skipping y_hat=8070\n","Skipping y_hat=7726\n","Skipping y_hat=7726\n","Skipping y_hat=7726\n","Skipping y_hat=7726\n","Skipping y_hat=7726\n","Skipping y_hat=7726\n","Skipping y_hat=7726\n","Skipping y_hat=5509\n","Skipping y_hat=5509\n","Skipping y_hat=5509\n","Skipping y_hat=5509\n","Skipping y_hat=5509\n","Skipping y_hat=5509\n","Skipping y_hat=5509\n","Skipping y_hat=6046\n","Skipping y_hat=6046\n","Skipping y_hat=6046\n","Skipping y_hat=6046\n","Skipping y_hat=6046\n","Skipping y_hat=6046\n","Skipping y_hat=6046\n","Skipping y_hat=7830\n","Skipping y_hat=7830\n","Skipping y_hat=7830\n","Skipping y_hat=7830\n","Skipping y_hat=7830\n","Skipping y_hat=7830\n","Skipping y_hat=7830\n","Skipping y_hat=4185\n","Skipping y_hat=4185\n","Skipping y_hat=4185\n","Skipping y_hat=4185\n","Skipping y_hat=4185\n","Skipping y_hat=4185\n","Skipping y_hat=4185\n","Skipping y_hat=7651\n","Skipping y_hat=7651\n","Skipping y_hat=7651\n","Skipping y_hat=7651\n","Skipping y_hat=7651\n","Skipping y_hat=7651\n","Skipping y_hat=7651\n","Skipping y_hat=4373\n","Skipping y_hat=4373\n","Skipping y_hat=4373\n","Skipping y_hat=4373\n","Skipping y_hat=4373\n","Skipping y_hat=4373\n","Skipping y_hat=4373\n","Skipping y_hat=4934\n","Skipping y_hat=4934\n","Skipping y_hat=4934\n","Skipping y_hat=4934\n","Skipping y_hat=4934\n","Skipping y_hat=4934\n","Skipping y_hat=4934\n","Skipping y_hat=5997\n","Skipping y_hat=5997\n","Skipping y_hat=5997\n","Skipping y_hat=5997\n","Skipping y_hat=5997\n","Skipping y_hat=5997\n","Skipping y_hat=5997\n","Skipping y_hat=5839\n","Skipping y_hat=5839\n","Skipping y_hat=5839\n","Skipping y_hat=5839\n","Skipping y_hat=5839\n","Skipping y_hat=5839\n","Skipping y_hat=5839\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=9911\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=2493\n","Skipping y_hat=5766\n","Skipping y_hat=5766\n","Skipping y_hat=5766\n","Skipping y_hat=5766\n","Skipping y_hat=5766\n","Skipping y_hat=5766\n","Skipping y_hat=5766\n","Skipping y_hat=9539\n","Skipping y_hat=9539\n","Skipping y_hat=9539\n","Skipping y_hat=9539\n","Skipping y_hat=9539\n","Skipping y_hat=9539\n","Skipping y_hat=9539\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=6654\n","Skipping y_hat=6654\n","Skipping y_hat=6654\n","Skipping y_hat=6654\n","Skipping y_hat=6654\n","Skipping y_hat=6654\n","Skipping y_hat=6654\n","Skipping y_hat=7342\n","Skipping y_hat=7342\n","Skipping y_hat=7342\n","Skipping y_hat=7342\n","Skipping y_hat=7342\n","Skipping y_hat=7342\n","Skipping y_hat=7342\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n","Skipping y_hat=9551\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=8886\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=3747\n","Skipping y_hat=3747\n","Skipping y_hat=3747\n","Skipping y_hat=3747\n","Skipping y_hat=3747\n","Skipping y_hat=3747\n","Skipping y_hat=3747\n","Skipping y_hat=5941\n","Skipping y_hat=5941\n","Skipping y_hat=5941\n","Skipping y_hat=5941\n","Skipping y_hat=5941\n","Skipping y_hat=5941\n","Skipping y_hat=5941\n","Skipping y_hat=7686\n","Skipping y_hat=7686\n","Skipping y_hat=7686\n","Skipping y_hat=7686\n","Skipping y_hat=7686\n","Skipping y_hat=7686\n","Skipping y_hat=7686\n","Skipping y_hat=8636\n","Skipping y_hat=8636\n","Skipping y_hat=8636\n","Skipping y_hat=8636\n","Skipping y_hat=8636\n","Skipping y_hat=8636\n","Skipping y_hat=8636\n","Skipping y_hat=3736\n","Skipping y_hat=3736\n","Skipping y_hat=3736\n","Skipping y_hat=3736\n","Skipping y_hat=3736\n","Skipping y_hat=3736\n","Skipping y_hat=3736\n","Skipping y_hat=9252\n","Skipping y_hat=9252\n","Skipping y_hat=9252\n","Skipping y_hat=9252\n","Skipping y_hat=9252\n","Skipping y_hat=9252\n","Skipping y_hat=9252\n","Skipping y_hat=8015\n","Skipping y_hat=8015\n","Skipping y_hat=8015\n","Skipping y_hat=8015\n","Skipping y_hat=8015\n","Skipping y_hat=8015\n","Skipping y_hat=8015\n","Skipping y_hat=7997\n","Skipping y_hat=7997\n","Skipping y_hat=7997\n","Skipping y_hat=7997\n","Skipping y_hat=7997\n","Skipping y_hat=7997\n","Skipping y_hat=7997\n","Skipping y_hat=8876\n","Skipping y_hat=8876\n","Skipping y_hat=8876\n","Skipping y_hat=8876\n","Skipping y_hat=8876\n","Skipping y_hat=8876\n","Skipping y_hat=8876\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=9229\n","Skipping y_hat=9229\n","Skipping y_hat=9229\n","Skipping y_hat=9229\n","Skipping y_hat=9229\n","Skipping y_hat=9229\n","Skipping y_hat=9229\n","Skipping y_hat=9173\n","Skipping y_hat=9173\n","Skipping y_hat=9173\n","Skipping y_hat=9173\n","Skipping y_hat=9173\n","Skipping y_hat=9173\n","Skipping y_hat=9173\n","Skipping y_hat=8456\n","Skipping y_hat=8456\n","Skipping y_hat=8456\n","Skipping y_hat=8456\n","Skipping y_hat=8456\n","Skipping y_hat=8456\n","Skipping y_hat=8456\n","Skipping y_hat=7798\n","Skipping y_hat=7798\n","Skipping y_hat=7798\n","Skipping y_hat=7798\n","Skipping y_hat=7798\n","Skipping y_hat=7798\n","Skipping y_hat=7798\n","Skipping y_hat=5457\n","Skipping y_hat=5457\n","Skipping y_hat=5457\n","Skipping y_hat=5457\n","Skipping y_hat=5457\n","Skipping y_hat=5457\n","Skipping y_hat=5457\n","Skipping y_hat=4844\n","Skipping y_hat=4844\n","Skipping y_hat=4844\n","Skipping y_hat=4844\n","Skipping y_hat=4844\n","Skipping y_hat=4844\n","Skipping y_hat=4844\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=1118\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=7807\n","Skipping y_hat=7807\n","Skipping y_hat=7807\n","Skipping y_hat=7807\n","Skipping y_hat=7807\n","Skipping y_hat=7807\n","Skipping y_hat=7807\n","Skipping y_hat=8979\n","Skipping y_hat=8979\n","Skipping y_hat=8979\n","Skipping y_hat=8979\n","Skipping y_hat=8979\n","Skipping y_hat=8979\n","Skipping y_hat=8979\n","Skipping y_hat=8188\n","Skipping y_hat=8188\n","Skipping y_hat=8188\n","Skipping y_hat=8188\n","Skipping y_hat=8188\n","Skipping y_hat=8188\n","Skipping y_hat=8188\n","Skipping y_hat=9475\n","Skipping y_hat=9475\n","Skipping y_hat=9475\n","Skipping y_hat=9475\n","Skipping y_hat=9475\n","Skipping y_hat=9475\n","Skipping y_hat=9475\n","Skipping y_hat=8865\n","Skipping y_hat=8865\n","Skipping y_hat=8865\n","Skipping y_hat=8865\n","Skipping y_hat=8865\n","Skipping y_hat=8865\n","Skipping y_hat=8865\n","Skipping y_hat=5321\n","Skipping y_hat=5321\n","Skipping y_hat=5321\n","Skipping y_hat=5321\n","Skipping y_hat=5321\n","Skipping y_hat=5321\n","Skipping y_hat=5321\n","Skipping y_hat=9776\n","Skipping y_hat=9776\n","Skipping y_hat=9776\n","Skipping y_hat=9776\n","Skipping y_hat=9776\n","Skipping y_hat=9776\n","Skipping y_hat=9776\n","Skipping y_hat=2998\n","Skipping y_hat=2998\n","Skipping y_hat=2998\n","Skipping y_hat=2998\n","Skipping y_hat=2998\n","Skipping y_hat=2998\n","Skipping y_hat=2998\n","Skipping y_hat=7596\n","Skipping y_hat=7596\n","Skipping y_hat=7596\n","Skipping y_hat=7596\n","Skipping y_hat=7596\n","Skipping y_hat=7596\n","Skipping y_hat=7596\n","Skipping y_hat=6065\n","Skipping y_hat=6065\n","Skipping y_hat=6065\n","Skipping y_hat=6065\n","Skipping y_hat=6065\n","Skipping y_hat=6065\n","Skipping y_hat=6065\n","Skipping y_hat=9306\n","Skipping y_hat=9306\n","Skipping y_hat=9306\n","Skipping y_hat=9306\n","Skipping y_hat=9306\n","Skipping y_hat=9306\n","Skipping y_hat=9306\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=1613\n","Skipping y_hat=6854\n","Skipping y_hat=6854\n","Skipping y_hat=6854\n","Skipping y_hat=6854\n","Skipping y_hat=6854\n","Skipping y_hat=6854\n","Skipping y_hat=6854\n","Skipping y_hat=9864\n","Skipping y_hat=9864\n","Skipping y_hat=9864\n","Skipping y_hat=9864\n","Skipping y_hat=9864\n","Skipping y_hat=9864\n","Skipping y_hat=9864\n","Skipping y_hat=3745\n","Skipping y_hat=3745\n","Skipping y_hat=3745\n","Skipping y_hat=3745\n","Skipping y_hat=3745\n","Skipping y_hat=3745\n","Skipping y_hat=3745\n","Skipping y_hat=9736\n","Skipping y_hat=9736\n","Skipping y_hat=9736\n","Skipping y_hat=9736\n","Skipping y_hat=9736\n","Skipping y_hat=9736\n","Skipping y_hat=9736\n","Skipping y_hat=6076\n","Skipping y_hat=6076\n","Skipping y_hat=6076\n","Skipping y_hat=6076\n","Skipping y_hat=6076\n","Skipping y_hat=6076\n","Skipping y_hat=6076\n","Skipping y_hat=8899\n","Skipping y_hat=8899\n","Skipping y_hat=8899\n","Skipping y_hat=8899\n","Skipping y_hat=8899\n","Skipping y_hat=8899\n","Skipping y_hat=8899\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6181\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=6831\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=4034\n","Skipping y_hat=4034\n","Skipping y_hat=4034\n","Skipping y_hat=4034\n","Skipping y_hat=4034\n","Skipping y_hat=4034\n","Skipping y_hat=4034\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9092\n","Skipping y_hat=9677\n","Skipping y_hat=9677\n","Skipping y_hat=9677\n","Skipping y_hat=9677\n","Skipping y_hat=9677\n","Skipping y_hat=9677\n","Skipping y_hat=9677\n","Skipping y_hat=9247\n","Skipping y_hat=9247\n","Skipping y_hat=9247\n","Skipping y_hat=9247\n","Skipping y_hat=9247\n","Skipping y_hat=9247\n","Skipping y_hat=9247\n","Skipping y_hat=7796\n","Skipping y_hat=7796\n","Skipping y_hat=7796\n","Skipping y_hat=7796\n","Skipping y_hat=7796\n","Skipping y_hat=7796\n","Skipping y_hat=7796\n","Skipping y_hat=9787\n","Skipping y_hat=9787\n","Skipping y_hat=9787\n","Skipping y_hat=9787\n","Skipping y_hat=9787\n","Skipping y_hat=9787\n","Skipping y_hat=9787\n","Skipping y_hat=2199\n","Skipping y_hat=2199\n","Skipping y_hat=2199\n","Skipping y_hat=2199\n","Skipping y_hat=2199\n","Skipping y_hat=2199\n","Skipping y_hat=2199\n","Skipping y_hat=4102\n","Skipping y_hat=4102\n","Skipping y_hat=4102\n","Skipping y_hat=4102\n","Skipping y_hat=4102\n","Skipping y_hat=4102\n","Skipping y_hat=4102\n","Skipping y_hat=8511\n","Skipping y_hat=8511\n","Skipping y_hat=8511\n","Skipping y_hat=8511\n","Skipping y_hat=8511\n","Skipping y_hat=8511\n","Skipping y_hat=8511\n"," 91% 78/86 [00:24<00:02,  3.54it/s]Skipping y_hat=2589\n","Skipping y_hat=2589\n","Skipping y_hat=2589\n","Skipping y_hat=2589\n","Skipping y_hat=2589\n","Skipping y_hat=2589\n","Skipping y_hat=2589\n","Skipping y_hat=8226\n","Skipping y_hat=8226\n","Skipping y_hat=8226\n","Skipping y_hat=8226\n","Skipping y_hat=8226\n","Skipping y_hat=8226\n","Skipping y_hat=8226\n","Skipping y_hat=8030\n","Skipping y_hat=8030\n","Skipping y_hat=8030\n","Skipping y_hat=8030\n","Skipping y_hat=8030\n","Skipping y_hat=8030\n","Skipping y_hat=8030\n","Skipping y_hat=4709\n","Skipping y_hat=4709\n","Skipping y_hat=4709\n","Skipping y_hat=4709\n","Skipping y_hat=4709\n","Skipping y_hat=4709\n","Skipping y_hat=4709\n","Skipping y_hat=7399\n","Skipping y_hat=7399\n","Skipping y_hat=7399\n","Skipping y_hat=7399\n","Skipping y_hat=7399\n","Skipping y_hat=7399\n","Skipping y_hat=7399\n","Skipping y_hat=5859\n","Skipping y_hat=5859\n","Skipping y_hat=5859\n","Skipping y_hat=5859\n","Skipping y_hat=5859\n","Skipping y_hat=5859\n","Skipping y_hat=5859\n","Skipping y_hat=9705\n","Skipping y_hat=9705\n","Skipping y_hat=9705\n","Skipping y_hat=9705\n","Skipping y_hat=9705\n","Skipping y_hat=9705\n","Skipping y_hat=9705\n","Skipping y_hat=4978\n","Skipping y_hat=4978\n","Skipping y_hat=4978\n","Skipping y_hat=4978\n","Skipping y_hat=4978\n","Skipping y_hat=4978\n","Skipping y_hat=4978\n","Skipping y_hat=9933\n","Skipping y_hat=9933\n","Skipping y_hat=9933\n","Skipping y_hat=9933\n","Skipping y_hat=9933\n","Skipping y_hat=9933\n","Skipping y_hat=9933\n","Skipping y_hat=6757\n","Skipping y_hat=6757\n","Skipping y_hat=6757\n","Skipping y_hat=6757\n","Skipping y_hat=6757\n","Skipping y_hat=6757\n","Skipping y_hat=6757\n","Skipping y_hat=7732\n","Skipping y_hat=7732\n","Skipping y_hat=7732\n","Skipping y_hat=7732\n","Skipping y_hat=7732\n","Skipping y_hat=7732\n","Skipping y_hat=7732\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=5962\n","Skipping y_hat=8996\n","Skipping y_hat=8996\n","Skipping y_hat=8996\n","Skipping y_hat=8996\n","Skipping y_hat=8996\n","Skipping y_hat=8996\n","Skipping y_hat=8996\n","Skipping y_hat=7993\n","Skipping y_hat=7993\n","Skipping y_hat=7993\n","Skipping y_hat=7993\n","Skipping y_hat=7993\n","Skipping y_hat=7993\n","Skipping y_hat=7993\n","Skipping y_hat=8674\n","Skipping y_hat=8674\n","Skipping y_hat=8674\n","Skipping y_hat=8674\n","Skipping y_hat=8674\n","Skipping y_hat=8674\n","Skipping y_hat=8674\n","Skipping y_hat=9125\n","Skipping y_hat=9125\n","Skipping y_hat=9125\n","Skipping y_hat=9125\n","Skipping y_hat=9125\n","Skipping y_hat=9125\n","Skipping y_hat=9125\n","Skipping y_hat=2769\n","Skipping y_hat=2769\n","Skipping y_hat=2769\n","Skipping y_hat=2769\n","Skipping y_hat=2769\n","Skipping y_hat=2769\n","Skipping y_hat=2769\n","Skipping y_hat=9772\n","Skipping y_hat=9772\n","Skipping y_hat=9772\n","Skipping y_hat=9772\n","Skipping y_hat=9772\n","Skipping y_hat=9772\n","Skipping y_hat=9772\n","Skipping y_hat=9030\n","Skipping y_hat=9030\n","Skipping y_hat=9030\n","Skipping y_hat=9030\n","Skipping y_hat=9030\n","Skipping y_hat=9030\n","Skipping y_hat=9030\n","Skipping y_hat=8342\n","Skipping y_hat=8342\n","Skipping y_hat=8342\n","Skipping y_hat=8342\n","Skipping y_hat=8342\n","Skipping y_hat=8342\n","Skipping y_hat=8342\n","Skipping y_hat=9219\n","Skipping y_hat=9219\n","Skipping y_hat=9219\n","Skipping y_hat=9219\n","Skipping y_hat=9219\n","Skipping y_hat=9219\n","Skipping y_hat=9219\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=9581\n","Skipping y_hat=7199\n","Skipping y_hat=7199\n","Skipping y_hat=7199\n","Skipping y_hat=7199\n","Skipping y_hat=7199\n","Skipping y_hat=7199\n","Skipping y_hat=7199\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=2970\n","Skipping y_hat=2228\n","Skipping y_hat=2228\n","Skipping y_hat=2228\n","Skipping y_hat=2228\n","Skipping y_hat=2228\n","Skipping y_hat=2228\n","Skipping y_hat=2228\n","Skipping y_hat=9273\n","Skipping y_hat=9273\n","Skipping y_hat=9273\n","Skipping y_hat=9273\n","Skipping y_hat=9273\n","Skipping y_hat=9273\n","Skipping y_hat=9273\n","Skipping y_hat=6767\n","Skipping y_hat=6767\n","Skipping y_hat=6767\n","Skipping y_hat=6767\n","Skipping y_hat=6767\n","Skipping y_hat=6767\n","Skipping y_hat=6767\n","Skipping y_hat=8508\n","Skipping y_hat=8508\n","Skipping y_hat=8508\n","Skipping y_hat=8508\n","Skipping y_hat=8508\n","Skipping y_hat=8508\n","Skipping y_hat=8508\n","Skipping y_hat=9546\n","Skipping y_hat=9546\n","Skipping y_hat=9546\n","Skipping y_hat=9546\n","Skipping y_hat=9546\n","Skipping y_hat=9546\n","Skipping y_hat=9546\n","Skipping y_hat=6020\n","Skipping y_hat=6020\n","Skipping y_hat=6020\n","Skipping y_hat=6020\n","Skipping y_hat=6020\n","Skipping y_hat=6020\n","Skipping y_hat=6020\n","Skipping y_hat=7415\n","Skipping y_hat=7415\n","Skipping y_hat=7415\n","Skipping y_hat=7415\n","Skipping y_hat=7415\n","Skipping y_hat=7415\n","Skipping y_hat=7415\n","Skipping y_hat=3011\n","Skipping y_hat=3011\n","Skipping y_hat=3011\n","Skipping y_hat=3011\n","Skipping y_hat=3011\n","Skipping y_hat=3011\n","Skipping y_hat=3011\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=6884\n","Skipping y_hat=9442\n","Skipping y_hat=9442\n","Skipping y_hat=9442\n","Skipping y_hat=9442\n","Skipping y_hat=9442\n","Skipping y_hat=9442\n","Skipping y_hat=9442\n","Skipping y_hat=4453\n","Skipping y_hat=4453\n","Skipping y_hat=4453\n","Skipping y_hat=4453\n","Skipping y_hat=4453\n","Skipping y_hat=4453\n","Skipping y_hat=4453\n","Skipping y_hat=7685\n","Skipping y_hat=7685\n","Skipping y_hat=7685\n","Skipping y_hat=7685\n","Skipping y_hat=7685\n","Skipping y_hat=7685\n","Skipping y_hat=7685\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=1190\n","Skipping y_hat=8524\n","Skipping y_hat=8524\n","Skipping y_hat=8524\n","Skipping y_hat=8524\n","Skipping y_hat=8524\n","Skipping y_hat=8524\n","Skipping y_hat=8524\n","Skipping y_hat=8352\n","Skipping y_hat=8352\n","Skipping y_hat=8352\n","Skipping y_hat=8352\n","Skipping y_hat=8352\n","Skipping y_hat=8352\n","Skipping y_hat=8352\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=1862\n","Skipping y_hat=6538\n","Skipping y_hat=6538\n","Skipping y_hat=6538\n","Skipping y_hat=6538\n","Skipping y_hat=6538\n","Skipping y_hat=6538\n","Skipping y_hat=6538\n","Skipping y_hat=7224\n","Skipping y_hat=7224\n","Skipping y_hat=7224\n","Skipping y_hat=7224\n","Skipping y_hat=7224\n","Skipping y_hat=7224\n","Skipping y_hat=7224\n","Skipping y_hat=8877\n","Skipping y_hat=8877\n","Skipping y_hat=8877\n","Skipping y_hat=8877\n","Skipping y_hat=8877\n","Skipping y_hat=8877\n","Skipping y_hat=8877\n","Skipping y_hat=5225\n","Skipping y_hat=5225\n","Skipping y_hat=5225\n","Skipping y_hat=5225\n","Skipping y_hat=5225\n","Skipping y_hat=5225\n","Skipping y_hat=5225\n","Skipping y_hat=6097\n","Skipping y_hat=6097\n","Skipping y_hat=6097\n","Skipping y_hat=6097\n","Skipping y_hat=6097\n","Skipping y_hat=6097\n","Skipping y_hat=6097\n","Skipping y_hat=5216\n","Skipping y_hat=5216\n","Skipping y_hat=5216\n","Skipping y_hat=5216\n","Skipping y_hat=5216\n","Skipping y_hat=5216\n","Skipping y_hat=5216\n","Skipping y_hat=4626\n","Skipping y_hat=4626\n","Skipping y_hat=4626\n","Skipping y_hat=4626\n","Skipping y_hat=4626\n","Skipping y_hat=4626\n","Skipping y_hat=4626\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=8540\n","Skipping y_hat=7988\n","Skipping y_hat=7988\n","Skipping y_hat=7988\n","Skipping y_hat=7988\n","Skipping y_hat=7988\n","Skipping y_hat=7988\n","Skipping y_hat=7988\n","Skipping y_hat=8896\n","Skipping y_hat=8896\n","Skipping y_hat=8896\n","Skipping y_hat=8896\n","Skipping y_hat=8896\n","Skipping y_hat=8896\n","Skipping y_hat=8896\n","Skipping y_hat=8624\n","Skipping y_hat=8624\n","Skipping y_hat=8624\n","Skipping y_hat=8624\n","Skipping y_hat=8624\n","Skipping y_hat=8624\n","Skipping y_hat=8624\n","Skipping y_hat=9218\n","Skipping y_hat=9218\n","Skipping y_hat=9218\n","Skipping y_hat=9218\n","Skipping y_hat=9218\n","Skipping y_hat=9218\n","Skipping y_hat=9218\n","Skipping y_hat=2372\n","Skipping y_hat=2372\n","Skipping y_hat=2372\n","Skipping y_hat=2372\n","Skipping y_hat=2372\n","Skipping y_hat=2372\n","Skipping y_hat=2372\n","Skipping y_hat=9890\n","Skipping y_hat=9890\n","Skipping y_hat=9890\n","Skipping y_hat=9890\n","Skipping y_hat=9890\n","Skipping y_hat=9890\n","Skipping y_hat=9890\n","Skipping y_hat=9928\n","Skipping y_hat=9928\n","Skipping y_hat=9928\n","Skipping y_hat=9928\n","Skipping y_hat=9928\n","Skipping y_hat=9928\n","Skipping y_hat=9928\n","Skipping y_hat=9242\n","Skipping y_hat=9242\n","Skipping y_hat=9242\n","Skipping y_hat=9242\n","Skipping y_hat=9242\n","Skipping y_hat=9242\n","Skipping y_hat=9242\n","Skipping y_hat=3663\n","Skipping y_hat=3663\n","Skipping y_hat=3663\n","Skipping y_hat=3663\n","Skipping y_hat=3663\n","Skipping y_hat=3663\n","Skipping y_hat=3663\n","Skipping y_hat=7833\n","Skipping y_hat=7833\n","Skipping y_hat=7833\n","Skipping y_hat=7833\n","Skipping y_hat=7833\n","Skipping y_hat=7833\n","Skipping y_hat=7833\n","Skipping y_hat=6132\n","Skipping y_hat=6132\n","Skipping y_hat=6132\n","Skipping y_hat=6132\n","Skipping y_hat=6132\n","Skipping y_hat=6132\n","Skipping y_hat=6132\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=9290\n","Skipping y_hat=6629\n","Skipping y_hat=6629\n","Skipping y_hat=6629\n","Skipping y_hat=6629\n","Skipping y_hat=6629\n","Skipping y_hat=6629\n","Skipping y_hat=6629\n","Skipping y_hat=5385\n","Skipping y_hat=5385\n","Skipping y_hat=5385\n","Skipping y_hat=5385\n","Skipping y_hat=5385\n","Skipping y_hat=5385\n","Skipping y_hat=5385\n","Skipping y_hat=9920\n","Skipping y_hat=9920\n","Skipping y_hat=9920\n","Skipping y_hat=9920\n","Skipping y_hat=9920\n","Skipping y_hat=9920\n","Skipping y_hat=9920\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=6462\n","Skipping y_hat=9506\n","Skipping y_hat=9506\n","Skipping y_hat=9506\n","Skipping y_hat=9506\n","Skipping y_hat=9506\n","Skipping y_hat=9506\n","Skipping y_hat=9506\n","Skipping y_hat=7264\n","Skipping y_hat=7264\n","Skipping y_hat=7264\n","Skipping y_hat=7264\n","Skipping y_hat=7264\n","Skipping y_hat=7264\n","Skipping y_hat=7264\n","Skipping y_hat=3887\n","Skipping y_hat=3887\n","Skipping y_hat=3887\n","Skipping y_hat=3887\n","Skipping y_hat=3887\n","Skipping y_hat=3887\n","Skipping y_hat=3887\n","Skipping y_hat=3554\n","Skipping y_hat=3554\n","Skipping y_hat=3554\n","Skipping y_hat=3554\n","Skipping y_hat=3554\n","Skipping y_hat=3554\n","Skipping y_hat=3554\n","Skipping y_hat=5862\n","Skipping y_hat=5862\n","Skipping y_hat=5862\n","Skipping y_hat=5862\n","Skipping y_hat=5862\n","Skipping y_hat=5862\n","Skipping y_hat=5862\n","Skipping y_hat=8814\n","Skipping y_hat=8814\n","Skipping y_hat=8814\n","Skipping y_hat=8814\n","Skipping y_hat=8814\n","Skipping y_hat=8814\n","Skipping y_hat=8814\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=9251\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=4970\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=6890\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=3959\n","Skipping y_hat=3959\n","Skipping y_hat=3959\n","Skipping y_hat=3959\n","Skipping y_hat=3959\n","Skipping y_hat=3959\n","Skipping y_hat=3959\n","Skipping y_hat=7108\n","Skipping y_hat=7108\n","Skipping y_hat=7108\n","Skipping y_hat=7108\n","Skipping y_hat=7108\n","Skipping y_hat=7108\n","Skipping y_hat=7108\n","Skipping y_hat=7614\n","Skipping y_hat=7614\n","Skipping y_hat=7614\n","Skipping y_hat=7614\n","Skipping y_hat=7614\n","Skipping y_hat=7614\n","Skipping y_hat=7614\n","Skipping y_hat=6898\n","Skipping y_hat=6898\n","Skipping y_hat=6898\n","Skipping y_hat=6898\n","Skipping y_hat=6898\n","Skipping y_hat=6898\n","Skipping y_hat=6898\n","Skipping y_hat=9744\n","Skipping y_hat=9744\n","Skipping y_hat=9744\n","Skipping y_hat=9744\n","Skipping y_hat=9744\n","Skipping y_hat=9744\n","Skipping y_hat=9744\n","Skipping y_hat=6314\n","Skipping y_hat=6314\n","Skipping y_hat=6314\n","Skipping y_hat=6314\n","Skipping y_hat=6314\n","Skipping y_hat=6314\n","Skipping y_hat=6314\n","Skipping y_hat=8676\n","Skipping y_hat=8676\n","Skipping y_hat=8676\n","Skipping y_hat=8676\n","Skipping y_hat=8676\n","Skipping y_hat=8676\n","Skipping y_hat=8676\n","Skipping y_hat=8192\n","Skipping y_hat=8192\n","Skipping y_hat=8192\n","Skipping y_hat=8192\n","Skipping y_hat=8192\n","Skipping y_hat=8192\n","Skipping y_hat=8192\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=6080\n","Skipping y_hat=3234\n","Skipping y_hat=3234\n","Skipping y_hat=3234\n","Skipping y_hat=3234\n","Skipping y_hat=3234\n","Skipping y_hat=3234\n","Skipping y_hat=3234\n","Skipping y_hat=7957\n","Skipping y_hat=7957\n","Skipping y_hat=7957\n","Skipping y_hat=7957\n","Skipping y_hat=7957\n","Skipping y_hat=7957\n","Skipping y_hat=7957\n","Skipping y_hat=6117\n","Skipping y_hat=6117\n","Skipping y_hat=6117\n","Skipping y_hat=6117\n","Skipping y_hat=6117\n","Skipping y_hat=6117\n","Skipping y_hat=6117\n","Skipping y_hat=8298\n","Skipping y_hat=8298\n","Skipping y_hat=8298\n","Skipping y_hat=8298\n","Skipping y_hat=8298\n","Skipping y_hat=8298\n","Skipping y_hat=8298\n","Skipping y_hat=9367\n","Skipping y_hat=9367\n","Skipping y_hat=9367\n","Skipping y_hat=9367\n","Skipping y_hat=9367\n","Skipping y_hat=9367\n","Skipping y_hat=9367\n","Skipping y_hat=9444\n","Skipping y_hat=9444\n","Skipping y_hat=9444\n","Skipping y_hat=9444\n","Skipping y_hat=9444\n","Skipping y_hat=9444\n","Skipping y_hat=9444\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=6291\n","Skipping y_hat=3565\n","Skipping y_hat=3565\n","Skipping y_hat=3565\n","Skipping y_hat=3565\n","Skipping y_hat=3565\n","Skipping y_hat=3565\n","Skipping y_hat=3565\n","Skipping y_hat=9436\n","Skipping y_hat=9436\n","Skipping y_hat=9436\n","Skipping y_hat=9436\n","Skipping y_hat=9436\n","Skipping y_hat=9436\n","Skipping y_hat=9436\n","Skipping y_hat=8224\n","Skipping y_hat=8224\n","Skipping y_hat=8224\n","Skipping y_hat=8224\n","Skipping y_hat=8224\n","Skipping y_hat=8224\n","Skipping y_hat=8224\n","Skipping y_hat=8031\n","Skipping y_hat=8031\n","Skipping y_hat=8031\n","Skipping y_hat=8031\n","Skipping y_hat=8031\n","Skipping y_hat=8031\n","Skipping y_hat=8031\n","Skipping y_hat=9806\n","Skipping y_hat=9806\n","Skipping y_hat=9806\n","Skipping y_hat=9806\n","Skipping y_hat=9806\n","Skipping y_hat=9806\n","Skipping y_hat=9806\n","Skipping y_hat=8727\n","Skipping y_hat=8727\n","Skipping y_hat=8727\n","Skipping y_hat=8727\n","Skipping y_hat=8727\n","Skipping y_hat=8727\n","Skipping y_hat=8727\n","Skipping y_hat=2783\n","Skipping y_hat=2783\n","Skipping y_hat=2783\n","Skipping y_hat=2783\n","Skipping y_hat=2783\n","Skipping y_hat=2783\n","Skipping y_hat=2783\n","Skipping y_hat=7393\n","Skipping y_hat=7393\n","Skipping y_hat=7393\n","Skipping y_hat=7393\n","Skipping y_hat=7393\n","Skipping y_hat=7393\n","Skipping y_hat=7393\n","Skipping y_hat=7541\n","Skipping y_hat=7541\n","Skipping y_hat=7541\n","Skipping y_hat=7541\n","Skipping y_hat=7541\n","Skipping y_hat=7541\n","Skipping y_hat=7541\n","Skipping y_hat=5895\n","Skipping y_hat=5895\n","Skipping y_hat=5895\n","Skipping y_hat=5895\n","Skipping y_hat=5895\n","Skipping y_hat=5895\n","Skipping y_hat=5895\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=3051\n","Skipping y_hat=8435\n","Skipping y_hat=8435\n","Skipping y_hat=8435\n","Skipping y_hat=8435\n","Skipping y_hat=8435\n","Skipping y_hat=8435\n","Skipping y_hat=8435\n","Skipping y_hat=9729\n","Skipping y_hat=9729\n","Skipping y_hat=9729\n","Skipping y_hat=9729\n","Skipping y_hat=9729\n","Skipping y_hat=9729\n","Skipping y_hat=9729\n","Skipping y_hat=8387\n","Skipping y_hat=8387\n","Skipping y_hat=8387\n","Skipping y_hat=8387\n","Skipping y_hat=8387\n","Skipping y_hat=8387\n","Skipping y_hat=8387\n","Skipping y_hat=4343\n","Skipping y_hat=4343\n","Skipping y_hat=4343\n","Skipping y_hat=4343\n","Skipping y_hat=4343\n","Skipping y_hat=4343\n","Skipping y_hat=4343\n","Skipping y_hat=9935\n","Skipping y_hat=9935\n","Skipping y_hat=9935\n","Skipping y_hat=9935\n","Skipping y_hat=9935\n","Skipping y_hat=9935\n","Skipping y_hat=9935\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=5690\n","Skipping y_hat=3279\n","Skipping y_hat=3279\n","Skipping y_hat=3279\n","Skipping y_hat=3279\n","Skipping y_hat=3279\n","Skipping y_hat=3279\n","Skipping y_hat=3279\n","Skipping y_hat=4758\n","Skipping y_hat=4758\n","Skipping y_hat=4758\n","Skipping y_hat=4758\n","Skipping y_hat=4758\n","Skipping y_hat=4758\n","Skipping y_hat=4758\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=4911\n","Skipping y_hat=9572\n","Skipping y_hat=9572\n","Skipping y_hat=9572\n","Skipping y_hat=9572\n","Skipping y_hat=9572\n","Skipping y_hat=9572\n","Skipping y_hat=9572\n","Skipping y_hat=9406\n","Skipping y_hat=9406\n","Skipping y_hat=9406\n","Skipping y_hat=9406\n","Skipping y_hat=9406\n","Skipping y_hat=9406\n","Skipping y_hat=9406\n","Skipping y_hat=7903\n","Skipping y_hat=7903\n","Skipping y_hat=7903\n","Skipping y_hat=7903\n","Skipping y_hat=7903\n","Skipping y_hat=7903\n","Skipping y_hat=7903\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=2750\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=9433\n","Skipping y_hat=9894\n","Skipping y_hat=9894\n","Skipping y_hat=9894\n","Skipping y_hat=9894\n","Skipping y_hat=9894\n","Skipping y_hat=9894\n","Skipping y_hat=9894\n","Skipping y_hat=4455\n","Skipping y_hat=4455\n","Skipping y_hat=4455\n","Skipping y_hat=4455\n","Skipping y_hat=4455\n","Skipping y_hat=4455\n","Skipping y_hat=4455\n","Skipping y_hat=5913\n","Skipping y_hat=5913\n","Skipping y_hat=5913\n","Skipping y_hat=5913\n","Skipping y_hat=5913\n","Skipping y_hat=5913\n","Skipping y_hat=5913\n","Skipping y_hat=7104\n","Skipping y_hat=7104\n","Skipping y_hat=7104\n","Skipping y_hat=7104\n","Skipping y_hat=7104\n","Skipping y_hat=7104\n","Skipping y_hat=7104\n","Skipping y_hat=2654\n","Skipping y_hat=2654\n","Skipping y_hat=2654\n","Skipping y_hat=2654\n","Skipping y_hat=2654\n","Skipping y_hat=2654\n","Skipping y_hat=2654\n","Skipping y_hat=8724\n","Skipping y_hat=8724\n","Skipping y_hat=8724\n","Skipping y_hat=8724\n","Skipping y_hat=8724\n","Skipping y_hat=8724\n","Skipping y_hat=8724\n","Skipping y_hat=6430\n","Skipping y_hat=6430\n","Skipping y_hat=6430\n","Skipping y_hat=6430\n","Skipping y_hat=6430\n","Skipping y_hat=6430\n","Skipping y_hat=6430\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=3737\n","Skipping y_hat=3737\n","Skipping y_hat=3737\n","Skipping y_hat=3737\n","Skipping y_hat=3737\n","Skipping y_hat=3737\n","Skipping y_hat=3737\n","Skipping y_hat=8509\n","Skipping y_hat=8509\n","Skipping y_hat=8509\n","Skipping y_hat=8509\n","Skipping y_hat=8509\n","Skipping y_hat=8509\n","Skipping y_hat=8509\n","Skipping y_hat=3296\n","Skipping y_hat=3296\n","Skipping y_hat=3296\n","Skipping y_hat=3296\n","Skipping y_hat=3296\n","Skipping y_hat=3296\n","Skipping y_hat=3296\n","Skipping y_hat=6309\n","Skipping y_hat=6309\n","Skipping y_hat=6309\n","Skipping y_hat=6309\n","Skipping y_hat=6309\n","Skipping y_hat=6309\n","Skipping y_hat=6309\n"," 92% 79/86 [00:25<00:02,  2.95it/s]Skipping y_hat=8126\n","Skipping y_hat=8126\n","Skipping y_hat=8126\n","Skipping y_hat=8126\n","Skipping y_hat=8126\n","Skipping y_hat=8126\n","Skipping y_hat=8126\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=6667\n","Skipping y_hat=6667\n","Skipping y_hat=6667\n","Skipping y_hat=6667\n","Skipping y_hat=6667\n","Skipping y_hat=6667\n","Skipping y_hat=6667\n","Skipping y_hat=8476\n","Skipping y_hat=8476\n","Skipping y_hat=8476\n","Skipping y_hat=8476\n","Skipping y_hat=8476\n","Skipping y_hat=8476\n","Skipping y_hat=8476\n","Skipping y_hat=5583\n","Skipping y_hat=5583\n","Skipping y_hat=5583\n","Skipping y_hat=5583\n","Skipping y_hat=5583\n","Skipping y_hat=5583\n","Skipping y_hat=5583\n","Skipping y_hat=7576\n","Skipping y_hat=7576\n","Skipping y_hat=7576\n","Skipping y_hat=7576\n","Skipping y_hat=7576\n","Skipping y_hat=7576\n","Skipping y_hat=7576\n","Skipping y_hat=2329\n","Skipping y_hat=2329\n","Skipping y_hat=2329\n","Skipping y_hat=2329\n","Skipping y_hat=2329\n","Skipping y_hat=2329\n","Skipping y_hat=2329\n","Skipping y_hat=5882\n","Skipping y_hat=5882\n","Skipping y_hat=5882\n","Skipping y_hat=5882\n","Skipping y_hat=5882\n","Skipping y_hat=5882\n","Skipping y_hat=5882\n","Skipping y_hat=9478\n","Skipping y_hat=9478\n","Skipping y_hat=9478\n","Skipping y_hat=9478\n","Skipping y_hat=9478\n","Skipping y_hat=9478\n","Skipping y_hat=9478\n","Skipping y_hat=8374\n","Skipping y_hat=8374\n","Skipping y_hat=8374\n","Skipping y_hat=8374\n","Skipping y_hat=8374\n","Skipping y_hat=8374\n","Skipping y_hat=8374\n","Skipping y_hat=9829\n","Skipping y_hat=9829\n","Skipping y_hat=9829\n","Skipping y_hat=9829\n","Skipping y_hat=9829\n","Skipping y_hat=9829\n","Skipping y_hat=9829\n","Skipping y_hat=9199\n","Skipping y_hat=9199\n","Skipping y_hat=9199\n","Skipping y_hat=9199\n","Skipping y_hat=9199\n","Skipping y_hat=9199\n","Skipping y_hat=9199\n","Skipping y_hat=2196\n","Skipping y_hat=2196\n","Skipping y_hat=2196\n","Skipping y_hat=2196\n","Skipping y_hat=2196\n","Skipping y_hat=2196\n","Skipping y_hat=2196\n","Skipping y_hat=5584\n","Skipping y_hat=5584\n","Skipping y_hat=5584\n","Skipping y_hat=5584\n","Skipping y_hat=5584\n","Skipping y_hat=5584\n","Skipping y_hat=5584\n","Skipping y_hat=6819\n","Skipping y_hat=6819\n","Skipping y_hat=6819\n","Skipping y_hat=6819\n","Skipping y_hat=6819\n","Skipping y_hat=6819\n","Skipping y_hat=6819\n","Skipping y_hat=4410\n","Skipping y_hat=4410\n","Skipping y_hat=4410\n","Skipping y_hat=4410\n","Skipping y_hat=4410\n","Skipping y_hat=4410\n","Skipping y_hat=4410\n","Skipping y_hat=3212\n","Skipping y_hat=3212\n","Skipping y_hat=3212\n","Skipping y_hat=3212\n","Skipping y_hat=3212\n","Skipping y_hat=3212\n","Skipping y_hat=3212\n","Skipping y_hat=9983\n","Skipping y_hat=9983\n","Skipping y_hat=9983\n","Skipping y_hat=9983\n","Skipping y_hat=9983\n","Skipping y_hat=9983\n","Skipping y_hat=9983\n","Skipping y_hat=7518\n","Skipping y_hat=7518\n","Skipping y_hat=7518\n","Skipping y_hat=7518\n","Skipping y_hat=7518\n","Skipping y_hat=7518\n","Skipping y_hat=7518\n","Skipping y_hat=3372\n","Skipping y_hat=3372\n","Skipping y_hat=3372\n","Skipping y_hat=3372\n","Skipping y_hat=3372\n","Skipping y_hat=3372\n","Skipping y_hat=3372\n","Skipping y_hat=8234\n","Skipping y_hat=8234\n","Skipping y_hat=8234\n","Skipping y_hat=8234\n","Skipping y_hat=8234\n","Skipping y_hat=8234\n","Skipping y_hat=8234\n","Skipping y_hat=7916\n","Skipping y_hat=7916\n","Skipping y_hat=7916\n","Skipping y_hat=7916\n","Skipping y_hat=7916\n","Skipping y_hat=7916\n","Skipping y_hat=7916\n","Skipping y_hat=3308\n","Skipping y_hat=3308\n","Skipping y_hat=3308\n","Skipping y_hat=3308\n","Skipping y_hat=3308\n","Skipping y_hat=3308\n","Skipping y_hat=3308\n","Skipping y_hat=9322\n","Skipping y_hat=9322\n","Skipping y_hat=9322\n","Skipping y_hat=9322\n","Skipping y_hat=9322\n","Skipping y_hat=9322\n","Skipping y_hat=9322\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=2940\n","Skipping y_hat=5563\n","Skipping y_hat=5563\n","Skipping y_hat=5563\n","Skipping y_hat=5563\n","Skipping y_hat=5563\n","Skipping y_hat=5563\n","Skipping y_hat=5563\n","Skipping y_hat=8550\n","Skipping y_hat=8550\n","Skipping y_hat=8550\n","Skipping y_hat=8550\n","Skipping y_hat=8550\n","Skipping y_hat=8550\n","Skipping y_hat=8550\n","Skipping y_hat=9246\n","Skipping y_hat=9246\n","Skipping y_hat=9246\n","Skipping y_hat=9246\n","Skipping y_hat=9246\n","Skipping y_hat=9246\n","Skipping y_hat=9246\n","Skipping y_hat=7525\n","Skipping y_hat=7525\n","Skipping y_hat=7525\n","Skipping y_hat=7525\n","Skipping y_hat=7525\n","Skipping y_hat=7525\n","Skipping y_hat=7525\n","Skipping y_hat=7208\n","Skipping y_hat=7208\n","Skipping y_hat=7208\n","Skipping y_hat=7208\n","Skipping y_hat=7208\n","Skipping y_hat=7208\n","Skipping y_hat=7208\n","Skipping y_hat=9216\n","Skipping y_hat=9216\n","Skipping y_hat=9216\n","Skipping y_hat=9216\n","Skipping y_hat=9216\n","Skipping y_hat=9216\n","Skipping y_hat=9216\n","Skipping y_hat=5909\n","Skipping y_hat=5909\n","Skipping y_hat=5909\n","Skipping y_hat=5909\n","Skipping y_hat=5909\n","Skipping y_hat=5909\n","Skipping y_hat=5909\n","Skipping y_hat=2448\n","Skipping y_hat=2448\n","Skipping y_hat=2448\n","Skipping y_hat=2448\n","Skipping y_hat=2448\n","Skipping y_hat=2448\n","Skipping y_hat=2448\n","Skipping y_hat=4046\n","Skipping y_hat=4046\n","Skipping y_hat=4046\n","Skipping y_hat=4046\n","Skipping y_hat=4046\n","Skipping y_hat=4046\n","Skipping y_hat=4046\n","Skipping y_hat=8612\n","Skipping y_hat=8612\n","Skipping y_hat=8612\n","Skipping y_hat=8612\n","Skipping y_hat=8612\n","Skipping y_hat=8612\n","Skipping y_hat=8612\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=6381\n","Skipping y_hat=6381\n","Skipping y_hat=6381\n","Skipping y_hat=6381\n","Skipping y_hat=6381\n","Skipping y_hat=6381\n","Skipping y_hat=6381\n","Skipping y_hat=9493\n","Skipping y_hat=9493\n","Skipping y_hat=9493\n","Skipping y_hat=9493\n","Skipping y_hat=9493\n","Skipping y_hat=9493\n","Skipping y_hat=9493\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=4648\n","Skipping y_hat=4648\n","Skipping y_hat=4648\n","Skipping y_hat=4648\n","Skipping y_hat=4648\n","Skipping y_hat=4648\n","Skipping y_hat=4648\n","Skipping y_hat=5240\n","Skipping y_hat=5240\n","Skipping y_hat=5240\n","Skipping y_hat=5240\n","Skipping y_hat=5240\n","Skipping y_hat=5240\n","Skipping y_hat=5240\n","Skipping y_hat=1826\n","Skipping y_hat=1826\n","Skipping y_hat=1826\n","Skipping y_hat=1826\n","Skipping y_hat=1826\n","Skipping y_hat=1826\n","Skipping y_hat=1826\n","Skipping y_hat=7267\n","Skipping y_hat=7267\n","Skipping y_hat=7267\n","Skipping y_hat=7267\n","Skipping y_hat=7267\n","Skipping y_hat=7267\n","Skipping y_hat=7267\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=9381\n","Skipping y_hat=5989\n","Skipping y_hat=5989\n","Skipping y_hat=5989\n","Skipping y_hat=5989\n","Skipping y_hat=5989\n","Skipping y_hat=5989\n","Skipping y_hat=5989\n","Skipping y_hat=6408\n","Skipping y_hat=6408\n","Skipping y_hat=6408\n","Skipping y_hat=6408\n","Skipping y_hat=6408\n","Skipping y_hat=6408\n","Skipping y_hat=6408\n","Skipping y_hat=4686\n","Skipping y_hat=4686\n","Skipping y_hat=4686\n","Skipping y_hat=4686\n","Skipping y_hat=4686\n","Skipping y_hat=4686\n","Skipping y_hat=4686\n","Skipping y_hat=7452\n","Skipping y_hat=7452\n","Skipping y_hat=7452\n","Skipping y_hat=7452\n","Skipping y_hat=7452\n","Skipping y_hat=7452\n","Skipping y_hat=7452\n","Skipping y_hat=8129\n","Skipping y_hat=8129\n","Skipping y_hat=8129\n","Skipping y_hat=8129\n","Skipping y_hat=8129\n","Skipping y_hat=8129\n","Skipping y_hat=8129\n","Skipping y_hat=7927\n","Skipping y_hat=7927\n","Skipping y_hat=7927\n","Skipping y_hat=7927\n","Skipping y_hat=7927\n","Skipping y_hat=7927\n","Skipping y_hat=7927\n","Skipping y_hat=7391\n","Skipping y_hat=7391\n","Skipping y_hat=7391\n","Skipping y_hat=7391\n","Skipping y_hat=7391\n","Skipping y_hat=7391\n","Skipping y_hat=7391\n","Skipping y_hat=6867\n","Skipping y_hat=6867\n","Skipping y_hat=6867\n","Skipping y_hat=6867\n","Skipping y_hat=6867\n","Skipping y_hat=6867\n","Skipping y_hat=6867\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=7553\n","Skipping y_hat=4341\n","Skipping y_hat=4341\n","Skipping y_hat=4341\n","Skipping y_hat=4341\n","Skipping y_hat=4341\n","Skipping y_hat=4341\n","Skipping y_hat=4341\n","Skipping y_hat=8245\n","Skipping y_hat=8245\n","Skipping y_hat=8245\n","Skipping y_hat=8245\n","Skipping y_hat=8245\n","Skipping y_hat=8245\n","Skipping y_hat=8245\n","Skipping y_hat=3634\n","Skipping y_hat=3634\n","Skipping y_hat=3634\n","Skipping y_hat=3634\n","Skipping y_hat=3634\n","Skipping y_hat=3634\n","Skipping y_hat=3634\n","Skipping y_hat=3118\n","Skipping y_hat=3118\n","Skipping y_hat=3118\n","Skipping y_hat=3118\n","Skipping y_hat=3118\n","Skipping y_hat=3118\n","Skipping y_hat=3118\n","Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=5811\n","Skipping y_hat=7197\n","Skipping y_hat=7197\n","Skipping y_hat=7197\n","Skipping y_hat=7197\n","Skipping y_hat=7197\n","Skipping y_hat=7197\n","Skipping y_hat=7197\n","Skipping y_hat=7294\n","Skipping y_hat=7294\n","Skipping y_hat=7294\n","Skipping y_hat=7294\n","Skipping y_hat=7294\n","Skipping y_hat=7294\n","Skipping y_hat=7294\n","Skipping y_hat=7566\n","Skipping y_hat=7566\n","Skipping y_hat=7566\n","Skipping y_hat=7566\n","Skipping y_hat=7566\n","Skipping y_hat=7566\n","Skipping y_hat=7566\n","Skipping y_hat=5723\n","Skipping y_hat=5723\n","Skipping y_hat=5723\n","Skipping y_hat=5723\n","Skipping y_hat=5723\n","Skipping y_hat=5723\n","Skipping y_hat=5723\n","Skipping y_hat=7717\n","Skipping y_hat=7717\n","Skipping y_hat=7717\n","Skipping y_hat=7717\n","Skipping y_hat=7717\n","Skipping y_hat=7717\n","Skipping y_hat=7717\n","Skipping y_hat=4727\n","Skipping y_hat=4727\n","Skipping y_hat=4727\n","Skipping y_hat=4727\n","Skipping y_hat=4727\n","Skipping y_hat=4727\n","Skipping y_hat=4727\n","Skipping y_hat=9756\n","Skipping y_hat=9756\n","Skipping y_hat=9756\n","Skipping y_hat=9756\n","Skipping y_hat=9756\n","Skipping y_hat=9756\n","Skipping y_hat=9756\n","Skipping y_hat=7538\n","Skipping y_hat=7538\n","Skipping y_hat=7538\n","Skipping y_hat=7538\n","Skipping y_hat=7538\n","Skipping y_hat=7538\n","Skipping y_hat=7538\n","Skipping y_hat=4053\n","Skipping y_hat=4053\n","Skipping y_hat=4053\n","Skipping y_hat=4053\n","Skipping y_hat=4053\n","Skipping y_hat=4053\n","Skipping y_hat=4053\n","Skipping y_hat=4115\n","Skipping y_hat=4115\n","Skipping y_hat=4115\n","Skipping y_hat=4115\n","Skipping y_hat=4115\n","Skipping y_hat=4115\n","Skipping y_hat=4115\n","Skipping y_hat=4477\n","Skipping y_hat=4477\n","Skipping y_hat=4477\n","Skipping y_hat=4477\n","Skipping y_hat=4477\n","Skipping y_hat=4477\n","Skipping y_hat=4477\n","Skipping y_hat=8503\n","Skipping y_hat=8503\n","Skipping y_hat=8503\n","Skipping y_hat=8503\n","Skipping y_hat=8503\n","Skipping y_hat=8503\n","Skipping y_hat=8503\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=1451\n","Skipping y_hat=6225\n","Skipping y_hat=6225\n","Skipping y_hat=6225\n","Skipping y_hat=6225\n","Skipping y_hat=6225\n","Skipping y_hat=6225\n","Skipping y_hat=6225\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=8649\n","Skipping y_hat=8649\n","Skipping y_hat=8649\n","Skipping y_hat=8649\n","Skipping y_hat=8649\n","Skipping y_hat=8649\n","Skipping y_hat=8649\n","Skipping y_hat=6474\n","Skipping y_hat=6474\n","Skipping y_hat=6474\n","Skipping y_hat=6474\n","Skipping y_hat=6474\n","Skipping y_hat=6474\n","Skipping y_hat=6474\n","Skipping y_hat=8238\n","Skipping y_hat=8238\n","Skipping y_hat=8238\n","Skipping y_hat=8238\n","Skipping y_hat=8238\n","Skipping y_hat=8238\n","Skipping y_hat=8238\n","Skipping y_hat=9325\n","Skipping y_hat=9325\n","Skipping y_hat=9325\n","Skipping y_hat=9325\n","Skipping y_hat=9325\n","Skipping y_hat=9325\n","Skipping y_hat=9325\n","Skipping y_hat=7578\n","Skipping y_hat=7578\n","Skipping y_hat=7578\n","Skipping y_hat=7578\n","Skipping y_hat=7578\n","Skipping y_hat=7578\n","Skipping y_hat=7578\n","Skipping y_hat=8742\n","Skipping y_hat=8742\n","Skipping y_hat=8742\n","Skipping y_hat=8742\n","Skipping y_hat=8742\n","Skipping y_hat=8742\n","Skipping y_hat=8742\n","Skipping y_hat=6249\n","Skipping y_hat=6249\n","Skipping y_hat=6249\n","Skipping y_hat=6249\n","Skipping y_hat=6249\n","Skipping y_hat=6249\n","Skipping y_hat=6249\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=2710\n","Skipping y_hat=8583\n","Skipping y_hat=8583\n","Skipping y_hat=8583\n","Skipping y_hat=8583\n","Skipping y_hat=8583\n","Skipping y_hat=8583\n","Skipping y_hat=8583\n","Skipping y_hat=9789\n","Skipping y_hat=9789\n","Skipping y_hat=9789\n","Skipping y_hat=9789\n","Skipping y_hat=9789\n","Skipping y_hat=9789\n","Skipping y_hat=9789\n","Skipping y_hat=9128\n","Skipping y_hat=9128\n","Skipping y_hat=9128\n","Skipping y_hat=9128\n","Skipping y_hat=9128\n","Skipping y_hat=9128\n","Skipping y_hat=9128\n","Skipping y_hat=4147\n","Skipping y_hat=4147\n","Skipping y_hat=4147\n","Skipping y_hat=4147\n","Skipping y_hat=4147\n","Skipping y_hat=4147\n","Skipping y_hat=4147\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=3163\n","Skipping y_hat=9863\n","Skipping y_hat=9863\n","Skipping y_hat=9863\n","Skipping y_hat=9863\n","Skipping y_hat=9863\n","Skipping y_hat=9863\n","Skipping y_hat=9863\n","Skipping y_hat=8710\n","Skipping y_hat=8710\n","Skipping y_hat=8710\n","Skipping y_hat=8710\n","Skipping y_hat=8710\n","Skipping y_hat=8710\n","Skipping y_hat=8710\n","Skipping y_hat=7966\n","Skipping y_hat=7966\n","Skipping y_hat=7966\n","Skipping y_hat=7966\n","Skipping y_hat=7966\n","Skipping y_hat=7966\n","Skipping y_hat=7966\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=9259\n","Skipping y_hat=8426\n","Skipping y_hat=8426\n","Skipping y_hat=8426\n","Skipping y_hat=8426\n","Skipping y_hat=8426\n","Skipping y_hat=8426\n","Skipping y_hat=8426\n","Skipping y_hat=6277\n","Skipping y_hat=6277\n","Skipping y_hat=6277\n","Skipping y_hat=6277\n","Skipping y_hat=6277\n","Skipping y_hat=6277\n","Skipping y_hat=6277\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=1438\n","Skipping y_hat=7427\n","Skipping y_hat=7427\n","Skipping y_hat=7427\n","Skipping y_hat=7427\n","Skipping y_hat=7427\n","Skipping y_hat=7427\n","Skipping y_hat=7427\n","Skipping y_hat=9507\n","Skipping y_hat=9507\n","Skipping y_hat=9507\n","Skipping y_hat=9507\n","Skipping y_hat=9507\n","Skipping y_hat=9507\n","Skipping y_hat=9507\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=7121\n","Skipping y_hat=8554\n","Skipping y_hat=8554\n","Skipping y_hat=8554\n","Skipping y_hat=8554\n","Skipping y_hat=8554\n","Skipping y_hat=8554\n","Skipping y_hat=8554\n","Skipping y_hat=8737\n","Skipping y_hat=8737\n","Skipping y_hat=8737\n","Skipping y_hat=8737\n","Skipping y_hat=8737\n","Skipping y_hat=8737\n","Skipping y_hat=8737\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=4570\n","Skipping y_hat=7776\n","Skipping y_hat=7776\n","Skipping y_hat=7776\n","Skipping y_hat=7776\n","Skipping y_hat=7776\n","Skipping y_hat=7776\n","Skipping y_hat=7776\n","Skipping y_hat=9734\n","Skipping y_hat=9734\n","Skipping y_hat=9734\n","Skipping y_hat=9734\n","Skipping y_hat=9734\n","Skipping y_hat=9734\n","Skipping y_hat=9734\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=6427\n","Skipping y_hat=6427\n","Skipping y_hat=6427\n","Skipping y_hat=6427\n","Skipping y_hat=6427\n","Skipping y_hat=6427\n","Skipping y_hat=6427\n","Skipping y_hat=3694\n","Skipping y_hat=3694\n","Skipping y_hat=3694\n","Skipping y_hat=3694\n","Skipping y_hat=3694\n","Skipping y_hat=3694\n","Skipping y_hat=3694\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=6072\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=4091\n","Skipping y_hat=3328\n","Skipping y_hat=3328\n","Skipping y_hat=3328\n","Skipping y_hat=3328\n","Skipping y_hat=3328\n","Skipping y_hat=3328\n","Skipping y_hat=3328\n","Skipping y_hat=7607\n","Skipping y_hat=7607\n","Skipping y_hat=7607\n","Skipping y_hat=7607\n","Skipping y_hat=7607\n","Skipping y_hat=7607\n","Skipping y_hat=7607\n","Skipping y_hat=9302\n","Skipping y_hat=9302\n","Skipping y_hat=9302\n","Skipping y_hat=9302\n","Skipping y_hat=9302\n","Skipping y_hat=9302\n","Skipping y_hat=9302\n","Skipping y_hat=5974\n","Skipping y_hat=5974\n","Skipping y_hat=5974\n","Skipping y_hat=5974\n","Skipping y_hat=5974\n","Skipping y_hat=5974\n","Skipping y_hat=5974\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=9682\n","Skipping y_hat=7335\n","Skipping y_hat=7335\n","Skipping y_hat=7335\n","Skipping y_hat=7335\n","Skipping y_hat=7335\n","Skipping y_hat=7335\n","Skipping y_hat=7335\n","Skipping y_hat=3504\n","Skipping y_hat=3504\n","Skipping y_hat=3504\n","Skipping y_hat=3504\n","Skipping y_hat=3504\n","Skipping y_hat=3504\n","Skipping y_hat=3504\n","Skipping y_hat=4916\n","Skipping y_hat=4916\n","Skipping y_hat=4916\n","Skipping y_hat=4916\n","Skipping y_hat=4916\n","Skipping y_hat=4916\n","Skipping y_hat=4916\n","Skipping y_hat=1594\n","Skipping y_hat=1594\n","Skipping y_hat=1594\n","Skipping y_hat=1594\n","Skipping y_hat=1594\n","Skipping y_hat=1594\n","Skipping y_hat=1594\n","Skipping y_hat=4149\n","Skipping y_hat=4149\n","Skipping y_hat=4149\n","Skipping y_hat=4149\n","Skipping y_hat=4149\n","Skipping y_hat=4149\n","Skipping y_hat=4149\n","Skipping y_hat=9959\n","Skipping y_hat=9959\n","Skipping y_hat=9959\n","Skipping y_hat=9959\n","Skipping y_hat=9959\n","Skipping y_hat=9959\n","Skipping y_hat=9959\n","Skipping y_hat=6037\n","Skipping y_hat=6037\n","Skipping y_hat=6037\n","Skipping y_hat=6037\n","Skipping y_hat=6037\n","Skipping y_hat=6037\n","Skipping y_hat=6037\n","Skipping y_hat=5062\n","Skipping y_hat=5062\n","Skipping y_hat=5062\n","Skipping y_hat=5062\n","Skipping y_hat=5062\n","Skipping y_hat=5062\n","Skipping y_hat=5062\n","Skipping y_hat=4398\n","Skipping y_hat=4398\n","Skipping y_hat=4398\n","Skipping y_hat=4398\n","Skipping y_hat=4398\n","Skipping y_hat=4398\n","Skipping y_hat=4398\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=8181\n","Skipping y_hat=4299\n","Skipping y_hat=4299\n","Skipping y_hat=4299\n","Skipping y_hat=4299\n","Skipping y_hat=4299\n","Skipping y_hat=4299\n","Skipping y_hat=4299\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=8860\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=3702\n","Skipping y_hat=4615\n","Skipping y_hat=4615\n","Skipping y_hat=4615\n","Skipping y_hat=4615\n","Skipping y_hat=4615\n","Skipping y_hat=4615\n","Skipping y_hat=4615\n","Skipping y_hat=8388\n","Skipping y_hat=8388\n","Skipping y_hat=8388\n","Skipping y_hat=8388\n","Skipping y_hat=8388\n","Skipping y_hat=8388\n","Skipping y_hat=8388\n","Skipping y_hat=4655\n","Skipping y_hat=4655\n","Skipping y_hat=4655\n","Skipping y_hat=4655\n","Skipping y_hat=4655\n","Skipping y_hat=4655\n","Skipping y_hat=4655\n","Skipping y_hat=7426\n","Skipping y_hat=7426\n","Skipping y_hat=7426\n","Skipping y_hat=7426\n","Skipping y_hat=7426\n","Skipping y_hat=7426\n","Skipping y_hat=7426\n"," 93% 80/86 [00:25<00:01,  3.10it/s]Skipping y_hat=8043\n","Skipping y_hat=8043\n","Skipping y_hat=8043\n","Skipping y_hat=8043\n","Skipping y_hat=8043\n","Skipping y_hat=8043\n","Skipping y_hat=8043\n","Skipping y_hat=1951\n","Skipping y_hat=1951\n","Skipping y_hat=1951\n","Skipping y_hat=1951\n","Skipping y_hat=1951\n","Skipping y_hat=1951\n","Skipping y_hat=1951\n","Skipping y_hat=7235\n","Skipping y_hat=7235\n","Skipping y_hat=7235\n","Skipping y_hat=7235\n","Skipping y_hat=7235\n","Skipping y_hat=7235\n","Skipping y_hat=7235\n","Skipping y_hat=8855\n","Skipping y_hat=8855\n","Skipping y_hat=8855\n","Skipping y_hat=8855\n","Skipping y_hat=8855\n","Skipping y_hat=8855\n","Skipping y_hat=8855\n","Skipping y_hat=9255\n","Skipping y_hat=9255\n","Skipping y_hat=9255\n","Skipping y_hat=9255\n","Skipping y_hat=9255\n","Skipping y_hat=9255\n","Skipping y_hat=9255\n","Skipping y_hat=5904\n","Skipping y_hat=5904\n","Skipping y_hat=5904\n","Skipping y_hat=5904\n","Skipping y_hat=5904\n","Skipping y_hat=5904\n","Skipping y_hat=5904\n","Skipping y_hat=6409\n","Skipping y_hat=6409\n","Skipping y_hat=6409\n","Skipping y_hat=6409\n","Skipping y_hat=6409\n","Skipping y_hat=6409\n","Skipping y_hat=6409\n","Skipping y_hat=9471\n","Skipping y_hat=9471\n","Skipping y_hat=9471\n","Skipping y_hat=9471\n","Skipping y_hat=9471\n","Skipping y_hat=9471\n","Skipping y_hat=9471\n","Skipping y_hat=8244\n","Skipping y_hat=8244\n","Skipping y_hat=8244\n","Skipping y_hat=8244\n","Skipping y_hat=8244\n","Skipping y_hat=8244\n","Skipping y_hat=8244\n","Skipping y_hat=2305\n","Skipping y_hat=2305\n","Skipping y_hat=2305\n","Skipping y_hat=2305\n","Skipping y_hat=2305\n","Skipping y_hat=2305\n","Skipping y_hat=2305\n","Skipping y_hat=7843\n","Skipping y_hat=7843\n","Skipping y_hat=7843\n","Skipping y_hat=7843\n","Skipping y_hat=7843\n","Skipping y_hat=7843\n","Skipping y_hat=7843\n","Skipping y_hat=7728\n","Skipping y_hat=7728\n","Skipping y_hat=7728\n","Skipping y_hat=7728\n","Skipping y_hat=7728\n","Skipping y_hat=7728\n","Skipping y_hat=7728\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=8693\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=2270\n","Skipping y_hat=9912\n","Skipping y_hat=9912\n","Skipping y_hat=9912\n","Skipping y_hat=9912\n","Skipping y_hat=9912\n","Skipping y_hat=9912\n","Skipping y_hat=9912\n","Skipping y_hat=6563\n","Skipping y_hat=6563\n","Skipping y_hat=6563\n","Skipping y_hat=6563\n","Skipping y_hat=6563\n","Skipping y_hat=6563\n","Skipping y_hat=6563\n","Skipping y_hat=8927\n","Skipping y_hat=8927\n","Skipping y_hat=8927\n","Skipping y_hat=8927\n","Skipping y_hat=8927\n","Skipping y_hat=8927\n","Skipping y_hat=8927\n","Skipping y_hat=8184\n","Skipping y_hat=8184\n","Skipping y_hat=8184\n","Skipping y_hat=8184\n","Skipping y_hat=8184\n","Skipping y_hat=8184\n","Skipping y_hat=8184\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=2513\n","Skipping y_hat=7163\n","Skipping y_hat=7163\n","Skipping y_hat=7163\n","Skipping y_hat=7163\n","Skipping y_hat=7163\n","Skipping y_hat=7163\n","Skipping y_hat=7163\n","Skipping y_hat=8863\n","Skipping y_hat=8863\n","Skipping y_hat=8863\n","Skipping y_hat=8863\n","Skipping y_hat=8863\n","Skipping y_hat=8863\n","Skipping y_hat=8863\n","Skipping y_hat=7127\n","Skipping y_hat=7127\n","Skipping y_hat=7127\n","Skipping y_hat=7127\n","Skipping y_hat=7127\n","Skipping y_hat=7127\n","Skipping y_hat=7127\n","Skipping y_hat=7395\n","Skipping y_hat=7395\n","Skipping y_hat=7395\n","Skipping y_hat=7395\n","Skipping y_hat=7395\n","Skipping y_hat=7395\n","Skipping y_hat=7395\n","Skipping y_hat=2655\n","Skipping y_hat=2655\n","Skipping y_hat=2655\n","Skipping y_hat=2655\n","Skipping y_hat=2655\n","Skipping y_hat=2655\n","Skipping y_hat=2655\n","Skipping y_hat=5012\n","Skipping y_hat=5012\n","Skipping y_hat=5012\n","Skipping y_hat=5012\n","Skipping y_hat=5012\n","Skipping y_hat=5012\n","Skipping y_hat=5012\n","Skipping y_hat=8606\n","Skipping y_hat=8606\n","Skipping y_hat=8606\n","Skipping y_hat=8606\n","Skipping y_hat=8606\n","Skipping y_hat=8606\n","Skipping y_hat=8606\n","Skipping y_hat=5978\n","Skipping y_hat=5978\n","Skipping y_hat=5978\n","Skipping y_hat=5978\n","Skipping y_hat=5978\n","Skipping y_hat=5978\n","Skipping y_hat=5978\n","Skipping y_hat=9297\n","Skipping y_hat=9297\n","Skipping y_hat=9297\n","Skipping y_hat=9297\n","Skipping y_hat=9297\n","Skipping y_hat=9297\n","Skipping y_hat=9297\n","Skipping y_hat=7909\n","Skipping y_hat=7909\n","Skipping y_hat=7909\n","Skipping y_hat=7909\n","Skipping y_hat=7909\n","Skipping y_hat=7909\n","Skipping y_hat=7909\n","Skipping y_hat=9151\n","Skipping y_hat=9151\n","Skipping y_hat=9151\n","Skipping y_hat=9151\n","Skipping y_hat=9151\n","Skipping y_hat=9151\n","Skipping y_hat=9151\n","Skipping y_hat=8097\n","Skipping y_hat=8097\n","Skipping y_hat=8097\n","Skipping y_hat=8097\n","Skipping y_hat=8097\n","Skipping y_hat=8097\n","Skipping y_hat=8097\n","Skipping y_hat=7287\n","Skipping y_hat=7287\n","Skipping y_hat=7287\n","Skipping y_hat=7287\n","Skipping y_hat=7287\n","Skipping y_hat=7287\n","Skipping y_hat=7287\n","Skipping y_hat=7657\n","Skipping y_hat=7657\n","Skipping y_hat=7657\n","Skipping y_hat=7657\n","Skipping y_hat=7657\n","Skipping y_hat=7657\n","Skipping y_hat=7657\n","Skipping y_hat=5322\n","Skipping y_hat=5322\n","Skipping y_hat=5322\n","Skipping y_hat=5322\n","Skipping y_hat=5322\n","Skipping y_hat=5322\n","Skipping y_hat=5322\n","Skipping y_hat=5260\n","Skipping y_hat=5260\n","Skipping y_hat=5260\n","Skipping y_hat=5260\n","Skipping y_hat=5260\n","Skipping y_hat=5260\n","Skipping y_hat=5260\n","Skipping y_hat=6617\n","Skipping y_hat=6617\n","Skipping y_hat=6617\n","Skipping y_hat=6617\n","Skipping y_hat=6617\n","Skipping y_hat=6617\n","Skipping y_hat=6617\n","Skipping y_hat=6565\n","Skipping y_hat=6565\n","Skipping y_hat=6565\n","Skipping y_hat=6565\n","Skipping y_hat=6565\n","Skipping y_hat=6565\n","Skipping y_hat=6565\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=4112\n","Skipping y_hat=1885\n","Skipping y_hat=1885\n","Skipping y_hat=1885\n","Skipping y_hat=1885\n","Skipping y_hat=1885\n","Skipping y_hat=1885\n","Skipping y_hat=1885\n","Skipping y_hat=3290\n","Skipping y_hat=3290\n","Skipping y_hat=3290\n","Skipping y_hat=3290\n","Skipping y_hat=3290\n","Skipping y_hat=3290\n","Skipping y_hat=3290\n","Skipping y_hat=9574\n","Skipping y_hat=9574\n","Skipping y_hat=9574\n","Skipping y_hat=9574\n","Skipping y_hat=9574\n","Skipping y_hat=9574\n","Skipping y_hat=9574\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=7941\n","Skipping y_hat=5689\n","Skipping y_hat=5689\n","Skipping y_hat=5689\n","Skipping y_hat=5689\n","Skipping y_hat=5689\n","Skipping y_hat=5689\n","Skipping y_hat=5689\n","Skipping y_hat=6610\n","Skipping y_hat=6610\n","Skipping y_hat=6610\n","Skipping y_hat=6610\n","Skipping y_hat=6610\n","Skipping y_hat=6610\n","Skipping y_hat=6610\n","Skipping y_hat=9451\n","Skipping y_hat=9451\n","Skipping y_hat=9451\n","Skipping y_hat=9451\n","Skipping y_hat=9451\n","Skipping y_hat=9451\n","Skipping y_hat=9451\n","Skipping y_hat=9548\n","Skipping y_hat=9548\n","Skipping y_hat=9548\n","Skipping y_hat=9548\n","Skipping y_hat=9548\n","Skipping y_hat=9548\n","Skipping y_hat=9548\n","Skipping y_hat=9775\n","Skipping y_hat=9775\n","Skipping y_hat=9775\n","Skipping y_hat=9775\n","Skipping y_hat=9775\n","Skipping y_hat=9775\n","Skipping y_hat=9775\n","Skipping y_hat=3994\n","Skipping y_hat=3994\n","Skipping y_hat=3994\n","Skipping y_hat=3994\n","Skipping y_hat=3994\n","Skipping y_hat=3994\n","Skipping y_hat=3994\n","Skipping y_hat=8254\n","Skipping y_hat=8254\n","Skipping y_hat=8254\n","Skipping y_hat=8254\n","Skipping y_hat=8254\n","Skipping y_hat=8254\n","Skipping y_hat=8254\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=9226\n","Skipping y_hat=6393\n","Skipping y_hat=6393\n","Skipping y_hat=6393\n","Skipping y_hat=6393\n","Skipping y_hat=6393\n","Skipping y_hat=6393\n","Skipping y_hat=6393\n","Skipping y_hat=5236\n","Skipping y_hat=5236\n","Skipping y_hat=5236\n","Skipping y_hat=5236\n","Skipping y_hat=5236\n","Skipping y_hat=5236\n","Skipping y_hat=5236\n","Skipping y_hat=5119\n","Skipping y_hat=5119\n","Skipping y_hat=5119\n","Skipping y_hat=5119\n","Skipping y_hat=5119\n","Skipping y_hat=5119\n","Skipping y_hat=5119\n","Skipping y_hat=5793\n","Skipping y_hat=5793\n","Skipping y_hat=5793\n","Skipping y_hat=5793\n","Skipping y_hat=5793\n","Skipping y_hat=5793\n","Skipping y_hat=5793\n","Skipping y_hat=7413\n","Skipping y_hat=7413\n","Skipping y_hat=7413\n","Skipping y_hat=7413\n","Skipping y_hat=7413\n","Skipping y_hat=7413\n","Skipping y_hat=7413\n","Skipping y_hat=7540\n","Skipping y_hat=7540\n","Skipping y_hat=7540\n","Skipping y_hat=7540\n","Skipping y_hat=7540\n","Skipping y_hat=7540\n","Skipping y_hat=7540\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=9842\n","Skipping y_hat=6804\n","Skipping y_hat=6804\n","Skipping y_hat=6804\n","Skipping y_hat=6804\n","Skipping y_hat=6804\n","Skipping y_hat=6804\n","Skipping y_hat=6804\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=7480\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=3721\n","Skipping y_hat=3686\n","Skipping y_hat=3686\n","Skipping y_hat=3686\n","Skipping y_hat=3686\n","Skipping y_hat=3686\n","Skipping y_hat=3686\n","Skipping y_hat=3686\n","Skipping y_hat=5973\n","Skipping y_hat=5973\n","Skipping y_hat=5973\n","Skipping y_hat=5973\n","Skipping y_hat=5973\n","Skipping y_hat=5973\n","Skipping y_hat=5973\n","Skipping y_hat=9818\n","Skipping y_hat=9818\n","Skipping y_hat=9818\n","Skipping y_hat=9818\n","Skipping y_hat=9818\n","Skipping y_hat=9818\n","Skipping y_hat=9818\n","Skipping y_hat=9632\n","Skipping y_hat=9632\n","Skipping y_hat=9632\n","Skipping y_hat=9632\n","Skipping y_hat=9632\n","Skipping y_hat=9632\n","Skipping y_hat=9632\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=8111\n","Skipping y_hat=7429\n","Skipping y_hat=7429\n","Skipping y_hat=7429\n","Skipping y_hat=7429\n","Skipping y_hat=7429\n","Skipping y_hat=7429\n","Skipping y_hat=7429\n","Skipping y_hat=6748\n","Skipping y_hat=6748\n","Skipping y_hat=6748\n","Skipping y_hat=6748\n","Skipping y_hat=6748\n","Skipping y_hat=6748\n","Skipping y_hat=6748\n","Skipping y_hat=6875\n","Skipping y_hat=6875\n","Skipping y_hat=6875\n","Skipping y_hat=6875\n","Skipping y_hat=6875\n","Skipping y_hat=6875\n","Skipping y_hat=6875\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=5900\n","Skipping y_hat=5900\n","Skipping y_hat=5900\n","Skipping y_hat=5900\n","Skipping y_hat=5900\n","Skipping y_hat=5900\n","Skipping y_hat=5900\n","Skipping y_hat=8954\n","Skipping y_hat=8954\n","Skipping y_hat=8954\n","Skipping y_hat=8954\n","Skipping y_hat=8954\n","Skipping y_hat=8954\n","Skipping y_hat=8954\n","Skipping y_hat=7577\n","Skipping y_hat=7577\n","Skipping y_hat=7577\n","Skipping y_hat=7577\n","Skipping y_hat=7577\n","Skipping y_hat=7577\n","Skipping y_hat=7577\n","Skipping y_hat=5113\n","Skipping y_hat=5113\n","Skipping y_hat=5113\n","Skipping y_hat=5113\n","Skipping y_hat=5113\n","Skipping y_hat=5113\n","Skipping y_hat=5113\n","Skipping y_hat=8506\n","Skipping y_hat=8506\n","Skipping y_hat=8506\n","Skipping y_hat=8506\n","Skipping y_hat=8506\n","Skipping y_hat=8506\n","Skipping y_hat=8506\n","Skipping y_hat=4047\n","Skipping y_hat=4047\n","Skipping y_hat=4047\n","Skipping y_hat=4047\n","Skipping y_hat=4047\n","Skipping y_hat=4047\n","Skipping y_hat=4047\n","Skipping y_hat=7616\n","Skipping y_hat=7616\n","Skipping y_hat=7616\n","Skipping y_hat=7616\n","Skipping y_hat=7616\n","Skipping y_hat=7616\n","Skipping y_hat=7616\n","Skipping y_hat=3699\n","Skipping y_hat=3699\n","Skipping y_hat=3699\n","Skipping y_hat=3699\n","Skipping y_hat=3699\n","Skipping y_hat=3699\n","Skipping y_hat=3699\n","Skipping y_hat=5399\n","Skipping y_hat=5399\n","Skipping y_hat=5399\n","Skipping y_hat=5399\n","Skipping y_hat=5399\n","Skipping y_hat=5399\n","Skipping y_hat=5399\n","Skipping y_hat=5623\n","Skipping y_hat=5623\n","Skipping y_hat=5623\n","Skipping y_hat=5623\n","Skipping y_hat=5623\n","Skipping y_hat=5623\n","Skipping y_hat=5623\n","Skipping y_hat=6524\n","Skipping y_hat=6524\n","Skipping y_hat=6524\n","Skipping y_hat=6524\n","Skipping y_hat=6524\n","Skipping y_hat=6524\n","Skipping y_hat=6524\n","Skipping y_hat=5098\n","Skipping y_hat=5098\n","Skipping y_hat=5098\n","Skipping y_hat=5098\n","Skipping y_hat=5098\n","Skipping y_hat=5098\n","Skipping y_hat=5098\n","Skipping y_hat=7746\n","Skipping y_hat=7746\n","Skipping y_hat=7746\n","Skipping y_hat=7746\n","Skipping y_hat=7746\n","Skipping y_hat=7746\n","Skipping y_hat=7746\n","Skipping y_hat=8235\n","Skipping y_hat=8235\n","Skipping y_hat=8235\n","Skipping y_hat=8235\n","Skipping y_hat=8235\n","Skipping y_hat=8235\n","Skipping y_hat=8235\n","Skipping y_hat=6597\n","Skipping y_hat=6597\n","Skipping y_hat=6597\n","Skipping y_hat=6597\n","Skipping y_hat=6597\n","Skipping y_hat=6597\n","Skipping y_hat=6597\n","Skipping y_hat=8335\n","Skipping y_hat=8335\n","Skipping y_hat=8335\n","Skipping y_hat=8335\n","Skipping y_hat=8335\n","Skipping y_hat=8335\n","Skipping y_hat=8335\n","Skipping y_hat=8828\n","Skipping y_hat=8828\n","Skipping y_hat=8828\n","Skipping y_hat=8828\n","Skipping y_hat=8828\n","Skipping y_hat=8828\n","Skipping y_hat=8828\n","Skipping y_hat=6815\n","Skipping y_hat=6815\n","Skipping y_hat=6815\n","Skipping y_hat=6815\n","Skipping y_hat=6815\n","Skipping y_hat=6815\n","Skipping y_hat=6815\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=6935\n","Skipping y_hat=8330\n","Skipping y_hat=8330\n","Skipping y_hat=8330\n","Skipping y_hat=8330\n","Skipping y_hat=8330\n","Skipping y_hat=8330\n","Skipping y_hat=8330\n","Skipping y_hat=4704\n","Skipping y_hat=4704\n","Skipping y_hat=4704\n","Skipping y_hat=4704\n","Skipping y_hat=4704\n","Skipping y_hat=4704\n","Skipping y_hat=4704\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=9170\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=8354\n","Skipping y_hat=8354\n","Skipping y_hat=8354\n","Skipping y_hat=8354\n","Skipping y_hat=8354\n","Skipping y_hat=8354\n","Skipping y_hat=8354\n","Skipping y_hat=5539\n","Skipping y_hat=5539\n","Skipping y_hat=5539\n","Skipping y_hat=5539\n","Skipping y_hat=5539\n","Skipping y_hat=5539\n","Skipping y_hat=5539\n","Skipping y_hat=8494\n","Skipping y_hat=8494\n","Skipping y_hat=8494\n","Skipping y_hat=8494\n","Skipping y_hat=8494\n","Skipping y_hat=8494\n","Skipping y_hat=8494\n","Skipping y_hat=5833\n","Skipping y_hat=5833\n","Skipping y_hat=5833\n","Skipping y_hat=5833\n","Skipping y_hat=5833\n","Skipping y_hat=5833\n","Skipping y_hat=5833\n","Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=8980\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=6171\n","Skipping y_hat=6388\n","Skipping y_hat=6388\n","Skipping y_hat=6388\n","Skipping y_hat=6388\n","Skipping y_hat=6388\n","Skipping y_hat=6388\n","Skipping y_hat=6388\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=9691\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=8962\n","Skipping y_hat=6904\n","Skipping y_hat=6904\n","Skipping y_hat=6904\n","Skipping y_hat=6904\n","Skipping y_hat=6904\n","Skipping y_hat=6904\n","Skipping y_hat=6904\n","Skipping y_hat=8363\n","Skipping y_hat=8363\n","Skipping y_hat=8363\n","Skipping y_hat=8363\n","Skipping y_hat=8363\n","Skipping y_hat=8363\n","Skipping y_hat=8363\n","Skipping y_hat=5555\n","Skipping y_hat=5555\n","Skipping y_hat=5555\n","Skipping y_hat=5555\n","Skipping y_hat=5555\n","Skipping y_hat=5555\n","Skipping y_hat=5555\n","Skipping y_hat=7225\n","Skipping y_hat=7225\n","Skipping y_hat=7225\n","Skipping y_hat=7225\n","Skipping y_hat=7225\n","Skipping y_hat=7225\n","Skipping y_hat=7225\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=6170\n","Skipping y_hat=5496\n","Skipping y_hat=5496\n","Skipping y_hat=5496\n","Skipping y_hat=5496\n","Skipping y_hat=5496\n","Skipping y_hat=5496\n","Skipping y_hat=5496\n","Skipping y_hat=6836\n","Skipping y_hat=6836\n","Skipping y_hat=6836\n","Skipping y_hat=6836\n","Skipping y_hat=6836\n","Skipping y_hat=6836\n","Skipping y_hat=6836\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=8257\n","Skipping y_hat=9700\n","Skipping y_hat=9700\n","Skipping y_hat=9700\n","Skipping y_hat=9700\n","Skipping y_hat=9700\n","Skipping y_hat=9700\n","Skipping y_hat=9700\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=1777\n","Skipping y_hat=8090\n","Skipping y_hat=8090\n","Skipping y_hat=8090\n","Skipping y_hat=8090\n","Skipping y_hat=8090\n","Skipping y_hat=8090\n","Skipping y_hat=8090\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=9492\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=5721\n","Skipping y_hat=9820\n","Skipping y_hat=9820\n","Skipping y_hat=9820\n","Skipping y_hat=9820\n","Skipping y_hat=9820\n","Skipping y_hat=9820\n","Skipping y_hat=9820\n","Skipping y_hat=6605\n","Skipping y_hat=6605\n","Skipping y_hat=6605\n","Skipping y_hat=6605\n","Skipping y_hat=6605\n","Skipping y_hat=6605\n","Skipping y_hat=6605\n","Skipping y_hat=9042\n","Skipping y_hat=9042\n","Skipping y_hat=9042\n","Skipping y_hat=9042\n","Skipping y_hat=9042\n","Skipping y_hat=9042\n","Skipping y_hat=9042\n","Skipping y_hat=9896\n","Skipping y_hat=9896\n","Skipping y_hat=9896\n","Skipping y_hat=9896\n","Skipping y_hat=9896\n","Skipping y_hat=9896\n","Skipping y_hat=9896\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=5691\n","Skipping y_hat=6929\n","Skipping y_hat=6929\n","Skipping y_hat=6929\n","Skipping y_hat=6929\n","Skipping y_hat=6929\n","Skipping y_hat=6929\n","Skipping y_hat=6929\n","Skipping y_hat=7505\n","Skipping y_hat=7505\n","Skipping y_hat=7505\n","Skipping y_hat=7505\n","Skipping y_hat=7505\n","Skipping y_hat=7505\n","Skipping y_hat=7505\n","Skipping y_hat=8305\n","Skipping y_hat=8305\n","Skipping y_hat=8305\n","Skipping y_hat=8305\n","Skipping y_hat=8305\n","Skipping y_hat=8305\n","Skipping y_hat=8305\n","Skipping y_hat=8400\n","Skipping y_hat=8400\n","Skipping y_hat=8400\n","Skipping y_hat=8400\n","Skipping y_hat=8400\n","Skipping y_hat=8400\n","Skipping y_hat=8400\n","Skipping y_hat=5938\n","Skipping y_hat=5938\n","Skipping y_hat=5938\n","Skipping y_hat=5938\n","Skipping y_hat=5938\n","Skipping y_hat=5938\n","Skipping y_hat=5938\n","Skipping y_hat=8442\n","Skipping y_hat=8442\n","Skipping y_hat=8442\n","Skipping y_hat=8442\n","Skipping y_hat=8442\n","Skipping y_hat=8442\n","Skipping y_hat=8442\n","Skipping y_hat=3486\n","Skipping y_hat=3486\n","Skipping y_hat=3486\n","Skipping y_hat=3486\n","Skipping y_hat=3486\n","Skipping y_hat=3486\n","Skipping y_hat=3486\n","Skipping y_hat=9648\n","Skipping y_hat=9648\n","Skipping y_hat=9648\n","Skipping y_hat=9648\n","Skipping y_hat=9648\n","Skipping y_hat=9648\n","Skipping y_hat=9648\n"," 94% 81/86 [00:25<00:01,  3.27it/s]Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=1705\n","Skipping y_hat=8523\n","Skipping y_hat=8523\n","Skipping y_hat=8523\n","Skipping y_hat=8523\n","Skipping y_hat=8523\n","Skipping y_hat=8523\n","Skipping y_hat=8523\n","Skipping y_hat=4425\n","Skipping y_hat=4425\n","Skipping y_hat=4425\n","Skipping y_hat=4425\n","Skipping y_hat=4425\n","Skipping y_hat=4425\n","Skipping y_hat=4425\n","Skipping y_hat=2669\n","Skipping y_hat=2669\n","Skipping y_hat=2669\n","Skipping y_hat=2669\n","Skipping y_hat=2669\n","Skipping y_hat=2669\n","Skipping y_hat=2669\n","Skipping y_hat=8595\n","Skipping y_hat=8595\n","Skipping y_hat=8595\n","Skipping y_hat=8595\n","Skipping y_hat=8595\n","Skipping y_hat=8595\n","Skipping y_hat=8595\n","Skipping y_hat=8183\n","Skipping y_hat=8183\n","Skipping y_hat=8183\n","Skipping y_hat=8183\n","Skipping y_hat=8183\n","Skipping y_hat=8183\n","Skipping y_hat=8183\n","Skipping y_hat=6549\n","Skipping y_hat=6549\n","Skipping y_hat=6549\n","Skipping y_hat=6549\n","Skipping y_hat=6549\n","Skipping y_hat=6549\n","Skipping y_hat=6549\n","Skipping y_hat=8153\n","Skipping y_hat=8153\n","Skipping y_hat=8153\n","Skipping y_hat=8153\n","Skipping y_hat=8153\n","Skipping y_hat=8153\n","Skipping y_hat=8153\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=78\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=10\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=45\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=30\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=26\n","Skipping y_hat=26\n","Skipping y_hat=26\n","Skipping y_hat=26\n","Skipping y_hat=26\n","Skipping y_hat=26\n","Skipping y_hat=26\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=52\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=19\n","Skipping y_hat=19\n","Skipping y_hat=19\n","Skipping y_hat=19\n","Skipping y_hat=19\n","Skipping y_hat=19\n","Skipping y_hat=19\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=31\n","Skipping y_hat=31\n","Skipping y_hat=31\n","Skipping y_hat=31\n","Skipping y_hat=31\n","Skipping y_hat=31\n","Skipping y_hat=31\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=62\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=33\n","Skipping y_hat=77\n","Skipping y_hat=77\n","Skipping y_hat=77\n","Skipping y_hat=77\n","Skipping y_hat=77\n","Skipping y_hat=77\n","Skipping y_hat=77\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=61\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=76\n","Skipping y_hat=76\n","Skipping y_hat=76\n","Skipping y_hat=76\n","Skipping y_hat=76\n","Skipping y_hat=76\n","Skipping y_hat=76\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=99\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=55\n","Skipping y_hat=89\n","Skipping y_hat=89\n","Skipping y_hat=89\n","Skipping y_hat=89\n","Skipping y_hat=89\n","Skipping y_hat=89\n","Skipping y_hat=89\n","Skipping y_hat=81\n","Skipping y_hat=81\n","Skipping y_hat=81\n","Skipping y_hat=81\n","Skipping y_hat=81\n","Skipping y_hat=81\n","Skipping y_hat=81\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=66\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=69\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=83\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=93\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=75\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=36\n","Skipping y_hat=36\n","Skipping y_hat=36\n","Skipping y_hat=36\n","Skipping y_hat=36\n","Skipping y_hat=36\n","Skipping y_hat=36\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=70\n","Skipping y_hat=74\n","Skipping y_hat=74\n","Skipping y_hat=74\n","Skipping y_hat=74\n","Skipping y_hat=74\n","Skipping y_hat=74\n","Skipping y_hat=74\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=11\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=60\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=17\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=28\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=24\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=22\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=94\n","Skipping y_hat=94\n","Skipping y_hat=94\n","Skipping y_hat=94\n","Skipping y_hat=94\n","Skipping y_hat=94\n","Skipping y_hat=94\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=48\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=56\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=64\n","Skipping y_hat=50\n","Skipping y_hat=50\n","Skipping y_hat=50\n","Skipping y_hat=50\n","Skipping y_hat=50\n","Skipping y_hat=50\n","Skipping y_hat=50\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=20\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=40\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=15\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=21\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=80\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=63\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=58\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=53\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=67\n","Skipping y_hat=29\n","Skipping y_hat=29\n","Skipping y_hat=29\n","Skipping y_hat=29\n","Skipping y_hat=29\n","Skipping y_hat=29\n","Skipping y_hat=29\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=72\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=91\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=97\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=49\n","Skipping y_hat=39\n","Skipping y_hat=39\n","Skipping y_hat=39\n","Skipping y_hat=39\n","Skipping y_hat=39\n","Skipping y_hat=39\n","Skipping y_hat=39\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=96\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n","Skipping y_hat=86\n"," 97% 83/86 [00:26<00:00,  3.38it/s]Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=95\n","Skipping y_hat=43\n","Skipping y_hat=43\n","Skipping y_hat=43\n","Skipping y_hat=43\n","Skipping y_hat=43\n","Skipping y_hat=43\n","Skipping y_hat=43\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=41\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=54\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=44\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=59\n","Skipping y_hat=14\n","Skipping y_hat=14\n","Skipping y_hat=14\n","Skipping y_hat=14\n","Skipping y_hat=14\n","Skipping y_hat=14\n","Skipping y_hat=14\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=84\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=34\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=35\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=25\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=73\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=38\n","Skipping y_hat=37\n","Skipping y_hat=37\n","Skipping y_hat=37\n","Skipping y_hat=37\n","Skipping y_hat=37\n","Skipping y_hat=37\n","Skipping y_hat=37\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=88\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=57\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=13\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=51\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=12\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=47\n","Skipping y_hat=32\n","Skipping y_hat=32\n","Skipping y_hat=32\n","Skipping y_hat=32\n","Skipping y_hat=32\n","Skipping y_hat=32\n","Skipping y_hat=32\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n","Skipping y_hat=46\n"," 98% 84/86 [00:26<00:00,  4.01it/s]Skipping y_hat=8274\n","Skipping y_hat=8274\n","Skipping y_hat=8274\n","Skipping y_hat=8274\n","Skipping y_hat=8274\n","Skipping y_hat=8274\n","Skipping y_hat=8274\n","Skipping y_hat=9886\n","Skipping y_hat=9886\n","Skipping y_hat=9886\n","Skipping y_hat=9886\n","Skipping y_hat=9886\n","Skipping y_hat=9886\n","Skipping y_hat=9886\n","Skipping y_hat=6491\n","Skipping y_hat=6491\n","Skipping y_hat=6491\n","Skipping y_hat=6491\n","Skipping y_hat=6491\n","Skipping y_hat=6491\n","Skipping y_hat=6491\n","Skipping y_hat=4522\n","Skipping y_hat=4522\n","Skipping y_hat=4522\n","Skipping y_hat=4522\n","Skipping y_hat=4522\n","Skipping y_hat=4522\n","Skipping y_hat=4522\n","Skipping y_hat=5177\n","Skipping y_hat=5177\n","Skipping y_hat=5177\n","Skipping y_hat=5177\n","Skipping y_hat=5177\n","Skipping y_hat=5177\n","Skipping y_hat=5177\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=8410\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=5521\n","Skipping y_hat=8444\n","Skipping y_hat=8444\n","Skipping y_hat=8444\n","Skipping y_hat=8444\n","Skipping y_hat=8444\n","Skipping y_hat=8444\n","Skipping y_hat=8444\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=8361\n","Skipping y_hat=8103\n","Skipping y_hat=8103\n","Skipping y_hat=8103\n","Skipping y_hat=8103\n","Skipping y_hat=8103\n","Skipping y_hat=8103\n","Skipping y_hat=8103\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9725\n","Skipping y_hat=9883\n","Skipping y_hat=9883\n","Skipping y_hat=9883\n","Skipping y_hat=9883\n","Skipping y_hat=9883\n","Skipping y_hat=9883\n","Skipping y_hat=9883\n","Skipping y_hat=8977\n","Skipping y_hat=8977\n","Skipping y_hat=8977\n","Skipping y_hat=8977\n","Skipping y_hat=8977\n","Skipping y_hat=8977\n","Skipping y_hat=8977\n","Skipping y_hat=9748\n","Skipping y_hat=9748\n","Skipping y_hat=9748\n","Skipping y_hat=9748\n","Skipping y_hat=9748\n","Skipping y_hat=9748\n","Skipping y_hat=9748\n","Skipping y_hat=6154\n","Skipping y_hat=6154\n","Skipping y_hat=6154\n","Skipping y_hat=6154\n","Skipping y_hat=6154\n","Skipping y_hat=6154\n","Skipping y_hat=6154\n","Skipping y_hat=9477\n","Skipping y_hat=9477\n","Skipping y_hat=9477\n","Skipping y_hat=9477\n","Skipping y_hat=9477\n","Skipping y_hat=9477\n","Skipping y_hat=9477\n","Skipping y_hat=8078\n","Skipping y_hat=8078\n","Skipping y_hat=8078\n","Skipping y_hat=8078\n","Skipping y_hat=8078\n","Skipping y_hat=8078\n","Skipping y_hat=8078\n","Skipping y_hat=9026\n","Skipping y_hat=9026\n","Skipping y_hat=9026\n","Skipping y_hat=9026\n","Skipping y_hat=9026\n","Skipping y_hat=9026\n","Skipping y_hat=9026\n","Skipping y_hat=9532\n","Skipping y_hat=9532\n","Skipping y_hat=9532\n","Skipping y_hat=9532\n","Skipping y_hat=9532\n","Skipping y_hat=9532\n","Skipping y_hat=9532\n","Skipping y_hat=9800\n","Skipping y_hat=9800\n","Skipping y_hat=9800\n","Skipping y_hat=9800\n","Skipping y_hat=9800\n","Skipping y_hat=9800\n","Skipping y_hat=9800\n","Skipping y_hat=9391\n","Skipping y_hat=9391\n","Skipping y_hat=9391\n","Skipping y_hat=9391\n","Skipping y_hat=9391\n","Skipping y_hat=9391\n","Skipping y_hat=9391\n","Skipping y_hat=9635\n","Skipping y_hat=9635\n","Skipping y_hat=9635\n","Skipping y_hat=9635\n","Skipping y_hat=9635\n","Skipping y_hat=9635\n","Skipping y_hat=9635\n","Skipping y_hat=7495\n","Skipping y_hat=7495\n","Skipping y_hat=7495\n","Skipping y_hat=7495\n","Skipping y_hat=7495\n","Skipping y_hat=7495\n","Skipping y_hat=7495\n","Skipping y_hat=5506\n","Skipping y_hat=5506\n","Skipping y_hat=5506\n","Skipping y_hat=5506\n","Skipping y_hat=5506\n","Skipping y_hat=5506\n","Skipping y_hat=5506\n","Skipping y_hat=9414\n","Skipping y_hat=9414\n","Skipping y_hat=9414\n","Skipping y_hat=9414\n","Skipping y_hat=9414\n","Skipping y_hat=9414\n","Skipping y_hat=9414\n","Skipping y_hat=8520\n","Skipping y_hat=8520\n","Skipping y_hat=8520\n","Skipping y_hat=8520\n","Skipping y_hat=8520\n","Skipping y_hat=8520\n","Skipping y_hat=8520\n","Skipping y_hat=7352\n","Skipping y_hat=7352\n","Skipping y_hat=7352\n","Skipping y_hat=7352\n","Skipping y_hat=7352\n","Skipping y_hat=7352\n","Skipping y_hat=7352\n","Skipping y_hat=8578\n","Skipping y_hat=8578\n","Skipping y_hat=8578\n","Skipping y_hat=8578\n","Skipping y_hat=8578\n","Skipping y_hat=8578\n","Skipping y_hat=8578\n","Skipping y_hat=7113\n","Skipping y_hat=7113\n","Skipping y_hat=7113\n","Skipping y_hat=7113\n","Skipping y_hat=7113\n","Skipping y_hat=7113\n","Skipping y_hat=7113\n","Skipping y_hat=9163\n","Skipping y_hat=9163\n","Skipping y_hat=9163\n","Skipping y_hat=9163\n","Skipping y_hat=9163\n","Skipping y_hat=9163\n","Skipping y_hat=9163\n","Skipping y_hat=6817\n","Skipping y_hat=6817\n","Skipping y_hat=6817\n","Skipping y_hat=6817\n","Skipping y_hat=6817\n","Skipping y_hat=6817\n","Skipping y_hat=6817\n","Skipping y_hat=5612\n","Skipping y_hat=5612\n","Skipping y_hat=5612\n","Skipping y_hat=5612\n","Skipping y_hat=5612\n","Skipping y_hat=5612\n","Skipping y_hat=5612\n","Skipping y_hat=6902\n","Skipping y_hat=6902\n","Skipping y_hat=6902\n","Skipping y_hat=6902\n","Skipping y_hat=6902\n","Skipping y_hat=6902\n","Skipping y_hat=6902\n","Skipping y_hat=5759\n","Skipping y_hat=5759\n","Skipping y_hat=5759\n","Skipping y_hat=5759\n","Skipping y_hat=5759\n","Skipping y_hat=5759\n","Skipping y_hat=5759\n","Skipping y_hat=9472\n","Skipping y_hat=9472\n","Skipping y_hat=9472\n","Skipping y_hat=9472\n","Skipping y_hat=9472\n","Skipping y_hat=9472\n","Skipping y_hat=9472\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=7761\n","Skipping y_hat=8703\n","Skipping y_hat=8703\n","Skipping y_hat=8703\n","Skipping y_hat=8703\n","Skipping y_hat=8703\n","Skipping y_hat=8703\n","Skipping y_hat=8703\n","Skipping y_hat=8551\n","Skipping y_hat=8551\n","Skipping y_hat=8551\n","Skipping y_hat=8551\n","Skipping y_hat=8551\n","Skipping y_hat=8551\n","Skipping y_hat=8551\n","Skipping y_hat=8433\n","Skipping y_hat=8433\n","Skipping y_hat=8433\n","Skipping y_hat=8433\n","Skipping y_hat=8433\n","Skipping y_hat=8433\n","Skipping y_hat=8433\n"," 99% 85/86 [00:26<00:00,  4.70it/s]Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=4\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=5\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=8\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=9\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=7\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","Skipping y_hat=6\n","100% 86/86 [00:26<00:00,  3.21it/s]\n","accuracy of 10000 examples: 9994/10000 (99.94%)\n","/content/drive/MyDrive/addition/evaluation.py:343: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  old_df = pd.read_csv(results_file)\n","\n","Final Test Results:\n","test: 99.94%\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m4_operands_max_balanced_digit\u001b[0m at: \u001b[34mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/mejdgs8g\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mresults/4_operands_max_balanced_digit/out/wandb/run-20250913_195440-mejdgs8g/logs\u001b[0m\n"]}]},{"cell_type":"markdown","source":["## 4 Operand Sorting"],"metadata":{"id":"7UHO3zBc_jY3"}},{"cell_type":"code","source":["%cat configuration_files/4_operands_sorting.txt"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"97LE0OY_BKTY","executionInfo":{"status":"ok","timestamp":1757940735625,"user_tz":300,"elapsed":1016,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"}},"outputId":"f0d49cbd-af8d-41c2-a54c-807cb68852e3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_sorting_balanced_digit_symbol'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='sorting'\n","operator=''\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 64 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: num of digits in each operand\n","num_digit = 4\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 16\n","\n","drop_leading_digit = False\n","\n","# whether need zero‐padding\n","zero_pad = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 100000\n","lr_decay_iters = 100000 # make equal to max_iters usually (300000)\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory\n","out_dir = '/content/drive/MyDrive/addition/results/4_operands_sorting_balanced_digit_symbol/out'\n","\n","# to edit: training data\n","train_data_path = '/content/drive/MyDrive/addition/data/4_operands_sorting_balanced_digit_symbol/train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = False\n","train_data_test_path = \"/content/drive/MyDrive/addition/data/4_operands_sorting_balanced_digit/train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_path = '/content/drive/MyDrive/addition/data/4_operands_sorting_balanced_digit_symbol/val.txt'\n","\n","# to edit: test data; start is just the test file. It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","eval_addition = True\n","test_file_path = '/content/drive/MyDrive/addition/data/4_operands_sorting_balanced_digit_symbol/test.txt'\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: whether to do more frequenly evaluations in early stages\n","more_early_eval1 = False\n","early_eval_interval1 = 25\n","early_eval_border1 = 1000\n","\n","more_early_eval2 = False\n","early_eval_interval2 = 5\n","early_eval_border2 = 750\n","\n","# to edit: whether do mutual information measurement between diffrent pairs of digits (include input-output and output-output)\n","mi_measurement = False\n","\n","# (optional) data for additional statistical measurement\n","stats_measurement_data_file_path = '/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_wo_leading_digit/4_operand_addition_stats_measurement_data_reversed.txt'\n","\n","early_mi_measure_border = 200000\n","early_mi_measure_interval = 5000\n","final_mi_measure_interval = 5000"]}]},{"cell_type":"code","source":["!python train.py 4_operands_sorting.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"elF2BaDD_izl","outputId":"f7636cdd-343b-448e-fea7-697e24bad2b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Skipping y_hat=W,X,V\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=Y,X,W\n","Skipping y_hat=Y,X,W\n","Skipping y_hat=Y,X,W\n","Skipping y_hat=Y,X,W\n","Skipping y_hat=Y,X,W\n","Skipping y_hat=Y,X,W\n","Skipping y_hat=Y,X,W\n","Skipping y_hat=K,L,J\n","Skipping y_hat=K,L,J\n","Skipping y_hat=K,L,J\n","Skipping y_hat=K,L,J\n","Skipping y_hat=K,L,J\n","Skipping y_hat=K,L,J\n","Skipping y_hat=K,L,J\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=Y,Z,X,A\n","Skipping y_hat=Y,Z,X,A\n","Skipping y_hat=Y,Z,X,A\n","Skipping y_hat=Y,Z,X,A\n","Skipping y_hat=Y,Z,X,A\n","Skipping y_hat=Y,Z,X,A\n","Skipping y_hat=Y,Z,X,A\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=B,A,C\n","Skipping y_hat=B,A,C\n","Skipping y_hat=B,A,C\n","Skipping y_hat=B,A,C\n","Skipping y_hat=B,A,C\n","Skipping y_hat=B,A,C\n","Skipping y_hat=B,A,C\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=J,I,H\n","Skipping y_hat=J,I,H\n","Skipping y_hat=J,I,H\n","Skipping y_hat=J,I,H\n","Skipping y_hat=J,I,H\n","Skipping y_hat=J,I,H\n","Skipping y_hat=J,I,H\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=V,U,W\n","Skipping y_hat=V,U,W\n","Skipping y_hat=V,U,W\n","Skipping y_hat=V,U,W\n","Skipping y_hat=V,U,W\n","Skipping y_hat=V,U,W\n","Skipping y_hat=V,U,W\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=C,E,D\n","Skipping y_hat=C,E,D\n","Skipping y_hat=C,E,D\n","Skipping y_hat=C,E,D\n","Skipping y_hat=C,E,D\n","Skipping y_hat=C,E,D\n","Skipping y_hat=C,E,D\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=F,E,D\n","Skipping y_hat=F,E,D\n","Skipping y_hat=F,E,D\n","Skipping y_hat=F,E,D\n","Skipping y_hat=F,E,D\n","Skipping y_hat=F,E,D\n","Skipping y_hat=F,E,D\n","Skipping y_hat=P,R,Q\n","Skipping y_hat=P,R,Q\n","Skipping y_hat=P,R,Q\n","Skipping y_hat=P,R,Q\n","Skipping y_hat=P,R,Q\n","Skipping y_hat=P,R,Q\n","Skipping y_hat=P,R,Q\n","Skipping y_hat=Y,W,X\n","Skipping y_hat=Y,W,X\n","Skipping y_hat=Y,W,X\n","Skipping y_hat=Y,W,X\n","Skipping y_hat=Y,W,X\n","Skipping y_hat=Y,W,X\n","Skipping y_hat=Y,W,X\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=N,O,M\n","Skipping y_hat=N,O,M\n","Skipping y_hat=N,O,M\n","Skipping y_hat=N,O,M\n","Skipping y_hat=N,O,M\n","Skipping y_hat=N,O,M\n","Skipping y_hat=N,O,M\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=P,R,S,Q\n","Skipping y_hat=P,R,S,Q\n","Skipping y_hat=P,R,S,Q\n","Skipping y_hat=P,R,S,Q\n","Skipping y_hat=P,R,S,Q\n","Skipping y_hat=P,R,S,Q\n","Skipping y_hat=P,R,S,Q\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,G,J,H\n","Skipping y_hat=I,G,J,H\n","Skipping y_hat=I,G,J,H\n","Skipping y_hat=I,G,J,H\n","Skipping y_hat=I,G,J,H\n","Skipping y_hat=I,G,J,H\n","Skipping y_hat=I,G,J,H\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=S,R,Q\n","Skipping y_hat=S,R,Q\n","Skipping y_hat=S,R,Q\n","Skipping y_hat=S,R,Q\n","Skipping y_hat=S,R,Q\n","Skipping y_hat=S,R,Q\n","Skipping y_hat=S,R,Q\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=Q,O,P\n","Skipping y_hat=S,U,T\n","Skipping y_hat=S,U,T\n","Skipping y_hat=S,U,T\n","Skipping y_hat=S,U,T\n","Skipping y_hat=S,U,T\n","Skipping y_hat=S,U,T\n","Skipping y_hat=S,U,T\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n","Skipping y_hat=V,W,X\n","Skipping y_hat=V,W,X\n","Skipping y_hat=V,W,X\n","Skipping y_hat=V,W,X\n","Skipping y_hat=V,W,X\n","Skipping y_hat=V,W,X\n","Skipping y_hat=V,W,X\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=Z,X,Y\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=V,T,U\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=A,Z,Y\n","Skipping y_hat=A,Z,Y\n","Skipping y_hat=A,Z,Y\n","Skipping y_hat=A,Z,Y\n","Skipping y_hat=A,Z,Y\n","Skipping y_hat=A,Z,Y\n","Skipping y_hat=A,Z,Y\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=D,C,B\n","Skipping y_hat=W,X,Z,Y\n","Skipping y_hat=W,X,Z,Y\n","Skipping y_hat=W,X,Z,Y\n","Skipping y_hat=W,X,Z,Y\n","Skipping y_hat=W,X,Z,Y\n","Skipping y_hat=W,X,Z,Y\n","Skipping y_hat=W,X,Z,Y\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=D,E,C\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=J,L,K\n","Skipping y_hat=J,L,K\n","Skipping y_hat=J,L,K\n","Skipping y_hat=J,L,K\n","Skipping y_hat=J,L,K\n","Skipping y_hat=J,L,K\n","Skipping y_hat=J,L,K\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=N,L,M\n","Skipping y_hat=N,L,M\n","Skipping y_hat=N,L,M\n","Skipping y_hat=N,L,M\n","Skipping y_hat=N,L,M\n","Skipping y_hat=N,L,M\n","Skipping y_hat=N,L,M\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=Y,Z,A\n","Skipping y_hat=P,O,N\n","Skipping y_hat=P,O,N\n","Skipping y_hat=P,O,N\n","Skipping y_hat=P,O,N\n","Skipping y_hat=P,O,N\n","Skipping y_hat=P,O,N\n","Skipping y_hat=P,O,N\n","Skipping y_hat=K,L,M\n","Skipping y_hat=K,L,M\n","Skipping y_hat=K,L,M\n","Skipping y_hat=K,L,M\n","Skipping y_hat=K,L,M\n","Skipping y_hat=K,L,M\n","Skipping y_hat=K,L,M\n","Skipping y_hat=S,T,U\n","Skipping y_hat=S,T,U\n","Skipping y_hat=S,T,U\n","Skipping y_hat=S,T,U\n","Skipping y_hat=S,T,U\n","Skipping y_hat=S,T,U\n","Skipping y_hat=S,T,U\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n"," 69% 27/39 [00:07<00:03,  3.11it/s]Skipping y_hat=A,B,C\n","Skipping y_hat=A,B,C\n","Skipping y_hat=A,B,C\n","Skipping y_hat=A,B,C\n","Skipping y_hat=A,B,C\n","Skipping y_hat=A,B,C\n","Skipping y_hat=A,B,C\n","Skipping y_hat=A,C,D,B\n","Skipping y_hat=A,C,D,B\n","Skipping y_hat=A,C,D,B\n","Skipping y_hat=A,C,D,B\n","Skipping y_hat=A,C,D,B\n","Skipping y_hat=A,C,D,B\n","Skipping y_hat=A,C,D,B\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=K,I,J\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=B,A,Z\n","Skipping y_hat=B,A,Z\n","Skipping y_hat=B,A,Z\n","Skipping y_hat=B,A,Z\n","Skipping y_hat=B,A,Z\n","Skipping y_hat=B,A,Z\n","Skipping y_hat=B,A,Z\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=M,K,L\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=O,P,Q\n","Skipping y_hat=O,P,Q\n","Skipping y_hat=O,P,Q\n","Skipping y_hat=O,P,Q\n","Skipping y_hat=O,P,Q\n","Skipping y_hat=O,P,Q\n","Skipping y_hat=O,P,Q\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=Q,R,P\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=U,W,V\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n","Skipping y_hat=N,M,O\n"," 72% 28/39 [00:07<00:02,  3.69it/s]Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=Z,A,Y\n","Skipping y_hat=H,I,J\n","Skipping y_hat=H,I,J\n","Skipping y_hat=H,I,J\n","Skipping y_hat=H,I,J\n","Skipping y_hat=H,I,J\n","Skipping y_hat=H,I,J\n","Skipping y_hat=H,I,J\n","Skipping y_hat=F,G,H,E\n","Skipping y_hat=F,G,H,E\n","Skipping y_hat=F,G,H,E\n","Skipping y_hat=F,G,H,E\n","Skipping y_hat=F,G,H,E\n","Skipping y_hat=F,G,H,E\n","Skipping y_hat=F,G,H,E\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=G,F,H\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=D,B,C\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=R,S,T,U\n","Skipping y_hat=R,S,T,U\n","Skipping y_hat=R,S,T,U\n","Skipping y_hat=R,S,T,U\n","Skipping y_hat=R,S,T,U\n","Skipping y_hat=R,S,T,U\n","Skipping y_hat=R,S,T,U\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=W,U,V\n","Skipping y_hat=W,U,V\n","Skipping y_hat=W,U,V\n","Skipping y_hat=W,U,V\n","Skipping y_hat=W,U,V\n","Skipping y_hat=W,U,V\n","Skipping y_hat=W,U,V\n","Skipping y_hat=G,H,F\n","Skipping y_hat=G,H,F\n","Skipping y_hat=G,H,F\n","Skipping y_hat=G,H,F\n","Skipping y_hat=G,H,F\n","Skipping y_hat=G,H,F\n","Skipping y_hat=G,H,F\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=J,K,I\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,F,G\n","Skipping y_hat=H,I,G\n","Skipping y_hat=H,I,G\n","Skipping y_hat=H,I,G\n","Skipping y_hat=H,I,G\n","Skipping y_hat=H,I,G\n","Skipping y_hat=H,I,G\n","Skipping y_hat=H,I,G\n","Skipping y_hat=A,Y,Z\n","Skipping y_hat=A,Y,Z\n","Skipping y_hat=A,Y,Z\n","Skipping y_hat=A,Y,Z\n","Skipping y_hat=A,Y,Z\n","Skipping y_hat=A,Y,Z\n","Skipping y_hat=A,Y,Z\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=D,E,F,C\n","Skipping y_hat=D,E,F,C\n","Skipping y_hat=D,E,F,C\n","Skipping y_hat=D,E,F,C\n","Skipping y_hat=D,E,F,C\n","Skipping y_hat=D,E,F,C\n","Skipping y_hat=D,E,F,C\n","Skipping y_hat=Z,W,Y,X\n","Skipping y_hat=Z,W,Y,X\n","Skipping y_hat=Z,W,Y,X\n","Skipping y_hat=Z,W,Y,X\n","Skipping y_hat=Z,W,Y,X\n","Skipping y_hat=Z,W,Y,X\n","Skipping y_hat=Z,W,Y,X\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=F,H,G\n","Skipping y_hat=F,H,G\n","Skipping y_hat=F,H,G\n","Skipping y_hat=F,H,G\n","Skipping y_hat=F,H,G\n","Skipping y_hat=F,H,G\n","Skipping y_hat=F,H,G\n","Skipping y_hat=C,B,D\n","Skipping y_hat=C,B,D\n","Skipping y_hat=C,B,D\n","Skipping y_hat=C,B,D\n","Skipping y_hat=C,B,D\n","Skipping y_hat=C,B,D\n","Skipping y_hat=C,B,D\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=H,J,I,K\n","Skipping y_hat=H,J,I,K\n","Skipping y_hat=H,J,I,K\n","Skipping y_hat=H,J,I,K\n","Skipping y_hat=H,J,I,K\n","Skipping y_hat=H,J,I,K\n","Skipping y_hat=H,J,I,K\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=K,J,H,I\n","Skipping y_hat=K,J,H,I\n","Skipping y_hat=K,J,H,I\n","Skipping y_hat=K,J,H,I\n","Skipping y_hat=K,J,H,I\n","Skipping y_hat=K,J,H,I\n","Skipping y_hat=K,J,H,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=V,X,W\n","Skipping y_hat=V,X,W\n","Skipping y_hat=V,X,W\n","Skipping y_hat=V,X,W\n","Skipping y_hat=V,X,W\n","Skipping y_hat=V,X,W\n","Skipping y_hat=V,X,W\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=F,E,G\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=I,J,H\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=O,Q,P\n","Skipping y_hat=Z,W,X,Y\n","Skipping y_hat=Z,W,X,Y\n","Skipping y_hat=Z,W,X,Y\n","Skipping y_hat=Z,W,X,Y\n","Skipping y_hat=Z,W,X,Y\n","Skipping y_hat=Z,W,X,Y\n","Skipping y_hat=Z,W,X,Y\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=J,K,L\n","Skipping y_hat=J,K,L\n","Skipping y_hat=J,K,L\n","Skipping y_hat=J,K,L\n","Skipping y_hat=J,K,L\n","Skipping y_hat=J,K,L\n","Skipping y_hat=J,K,L\n","Skipping y_hat=O,M,N\n","Skipping y_hat=O,M,N\n","Skipping y_hat=O,M,N\n","Skipping y_hat=O,M,N\n","Skipping y_hat=O,M,N\n","Skipping y_hat=O,M,N\n","Skipping y_hat=O,M,N\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=E,C,D\n","Skipping y_hat=L,J,K\n","Skipping y_hat=L,J,K\n","Skipping y_hat=L,J,K\n","Skipping y_hat=L,J,K\n","Skipping y_hat=L,J,K\n","Skipping y_hat=L,J,K\n","Skipping y_hat=L,J,K\n","Skipping y_hat=D,C,B,A\n","Skipping y_hat=D,C,B,A\n","Skipping y_hat=D,C,B,A\n","Skipping y_hat=D,C,B,A\n","Skipping y_hat=D,C,B,A\n","Skipping y_hat=D,C,B,A\n","Skipping y_hat=D,C,B,A\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=U,T,V\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=S,Q,R\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=H,J,I\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=H,G,E,F\n","Skipping y_hat=H,G,E,F\n","Skipping y_hat=H,G,E,F\n","Skipping y_hat=H,G,E,F\n","Skipping y_hat=H,G,E,F\n","Skipping y_hat=H,G,E,F\n","Skipping y_hat=H,G,E,F\n","Skipping y_hat=F,G,I,H\n","Skipping y_hat=F,G,I,H\n","Skipping y_hat=F,G,I,H\n","Skipping y_hat=F,G,I,H\n","Skipping y_hat=F,G,I,H\n","Skipping y_hat=F,G,I,H\n","Skipping y_hat=F,G,I,H\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=A,Z,B\n","Skipping y_hat=A,Z,B\n","Skipping y_hat=A,Z,B\n","Skipping y_hat=A,Z,B\n","Skipping y_hat=A,Z,B\n","Skipping y_hat=A,Z,B\n","Skipping y_hat=A,Z,B\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=W,X,V,U\n","Skipping y_hat=W,X,V,U\n","Skipping y_hat=W,X,V,U\n","Skipping y_hat=W,X,V,U\n","Skipping y_hat=W,X,V,U\n","Skipping y_hat=W,X,V,U\n","Skipping y_hat=W,X,V,U\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=C,D,E\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=X,U,W,V\n","Skipping y_hat=X,U,W,V\n","Skipping y_hat=X,U,W,V\n","Skipping y_hat=X,U,W,V\n","Skipping y_hat=X,U,W,V\n","Skipping y_hat=X,U,W,V\n","Skipping y_hat=X,U,W,V\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=Q,R,S\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=P,Q,R\n","Skipping y_hat=F,I,H,G\n","Skipping y_hat=F,I,H,G\n","Skipping y_hat=F,I,H,G\n","Skipping y_hat=F,I,H,G\n","Skipping y_hat=F,I,H,G\n","Skipping y_hat=F,I,H,G\n","Skipping y_hat=F,I,H,G\n","Skipping y_hat=R,S,Q\n","Skipping y_hat=R,S,Q\n","Skipping y_hat=R,S,Q\n","Skipping y_hat=R,S,Q\n","Skipping y_hat=R,S,Q\n","Skipping y_hat=R,S,Q\n","Skipping y_hat=R,S,Q\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=A,Y,Z,B\n","Skipping y_hat=A,Y,Z,B\n","Skipping y_hat=A,Y,Z,B\n","Skipping y_hat=A,Y,Z,B\n","Skipping y_hat=A,Y,Z,B\n","Skipping y_hat=A,Y,Z,B\n","Skipping y_hat=A,Y,Z,B\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=R,S,T\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=T,U,V\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=R,Q,S\n","Skipping y_hat=R,Q,S\n","Skipping y_hat=R,Q,S\n","Skipping y_hat=R,Q,S\n","Skipping y_hat=R,Q,S\n","Skipping y_hat=R,Q,S\n","Skipping y_hat=R,Q,S\n","Skipping y_hat=W,X,V\n","Skipping y_hat=W,X,V\n","Skipping y_hat=W,X,V\n","Skipping y_hat=W,X,V\n","Skipping y_hat=W,X,V\n","Skipping y_hat=W,X,V\n","Skipping y_hat=W,X,V\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,V,W\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=X,W,V\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=M,N,L\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=V,U,T\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=G,E,F\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=H,G,I\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=D,C,E\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=T,U,S\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=T,U,V,W\n","Skipping y_hat=T,U,V,W\n","Skipping y_hat=T,U,V,W\n","Skipping y_hat=T,U,V,W\n","Skipping y_hat=T,U,V,W\n","Skipping y_hat=T,U,V,W\n","Skipping y_hat=T,U,V,W\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=N,M,P,O\n","Skipping y_hat=N,M,P,O\n","Skipping y_hat=N,M,P,O\n","Skipping y_hat=N,M,P,O\n","Skipping y_hat=N,M,P,O\n","Skipping y_hat=N,M,P,O\n","Skipping y_hat=N,M,P,O\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=Y,A,Z\n","Skipping y_hat=Y,A,Z\n","Skipping y_hat=Y,A,Z\n","Skipping y_hat=Y,A,Z\n","Skipping y_hat=Y,A,Z\n","Skipping y_hat=Y,A,Z\n","Skipping y_hat=Y,A,Z\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=B,Z,A\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=F,D,E\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=S,R,T\n","Skipping y_hat=S,R,T\n","Skipping y_hat=S,R,T\n","Skipping y_hat=S,R,T\n","Skipping y_hat=S,R,T\n","Skipping y_hat=S,R,T\n","Skipping y_hat=S,R,T\n","Skipping y_hat=S,T,R\n","Skipping y_hat=S,T,R\n","Skipping y_hat=S,T,R\n","Skipping y_hat=S,T,R\n","Skipping y_hat=S,T,R\n","Skipping y_hat=S,T,R\n","Skipping y_hat=S,T,R\n","Skipping y_hat=M,P,N,O\n","Skipping y_hat=M,P,N,O\n","Skipping y_hat=M,P,N,O\n","Skipping y_hat=M,P,N,O\n","Skipping y_hat=M,P,N,O\n","Skipping y_hat=M,P,N,O\n","Skipping y_hat=M,P,N,O\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=W,V,X\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=L,K,M\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=M,L,K\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=O,N,M\n","Skipping y_hat=T,S,R\n","Skipping y_hat=T,S,R\n","Skipping y_hat=T,S,R\n","Skipping y_hat=T,S,R\n","Skipping y_hat=T,S,R\n","Skipping y_hat=T,S,R\n","Skipping y_hat=T,S,R\n","Skipping y_hat=H,G,F\n","Skipping y_hat=H,G,F\n","Skipping y_hat=H,G,F\n","Skipping y_hat=H,G,F\n","Skipping y_hat=H,G,F\n","Skipping y_hat=H,G,F\n","Skipping y_hat=H,G,F\n","Skipping y_hat=G,F,E\n","Skipping y_hat=G,F,E\n","Skipping y_hat=G,F,E\n","Skipping y_hat=G,F,E\n","Skipping y_hat=G,F,E\n","Skipping y_hat=G,F,E\n","Skipping y_hat=G,F,E\n","Skipping y_hat=Z,A,B\n","Skipping y_hat=Z,A,B\n","Skipping y_hat=Z,A,B\n","Skipping y_hat=Z,A,B\n","Skipping y_hat=Z,A,B\n","Skipping y_hat=Z,A,B\n","Skipping y_hat=Z,A,B\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=R,Q,O,P\n","Skipping y_hat=W,X,Y\n","Skipping y_hat=W,X,Y\n","Skipping y_hat=W,X,Y\n","Skipping y_hat=W,X,Y\n","Skipping y_hat=W,X,Y\n","Skipping y_hat=W,X,Y\n","Skipping y_hat=W,X,Y\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=B,C,D,E\n","Skipping y_hat=B,C,D,E\n","Skipping y_hat=B,C,D,E\n","Skipping y_hat=B,C,D,E\n","Skipping y_hat=B,C,D,E\n","Skipping y_hat=B,C,D,E\n","Skipping y_hat=B,C,D,E\n","Skipping y_hat=E,D,C\n","Skipping y_hat=E,D,C\n","Skipping y_hat=E,D,C\n","Skipping y_hat=E,D,C\n","Skipping y_hat=E,D,C\n","Skipping y_hat=E,D,C\n","Skipping y_hat=E,D,C\n","Skipping y_hat=K,J,L\n","Skipping y_hat=K,J,L\n","Skipping y_hat=K,J,L\n","Skipping y_hat=K,J,L\n","Skipping y_hat=K,J,L\n","Skipping y_hat=K,J,L\n","Skipping y_hat=K,J,L\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=U,T,S\n","Skipping y_hat=C,A,B\n","Skipping y_hat=C,A,B\n","Skipping y_hat=C,A,B\n","Skipping y_hat=C,A,B\n","Skipping y_hat=C,A,B\n","Skipping y_hat=C,A,B\n","Skipping y_hat=C,A,B\n","Skipping y_hat=L,J,I,K\n","Skipping y_hat=L,J,I,K\n","Skipping y_hat=L,J,I,K\n","Skipping y_hat=L,J,I,K\n","Skipping y_hat=L,J,I,K\n","Skipping y_hat=L,J,I,K\n","Skipping y_hat=L,J,I,K\n","Skipping y_hat=V,W,U\n","Skipping y_hat=V,W,U\n","Skipping y_hat=V,W,U\n","Skipping y_hat=V,W,U\n","Skipping y_hat=V,W,U\n","Skipping y_hat=V,W,U\n","Skipping y_hat=V,W,U\n","Skipping y_hat=C,B,A\n","Skipping y_hat=C,B,A\n","Skipping y_hat=C,B,A\n","Skipping y_hat=C,B,A\n","Skipping y_hat=C,B,A\n","Skipping y_hat=C,B,A\n","Skipping y_hat=C,B,A\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=K,J,I\n","Skipping y_hat=K,J,I\n","Skipping y_hat=K,J,I\n","Skipping y_hat=K,J,I\n","Skipping y_hat=K,J,I\n","Skipping y_hat=K,J,I\n","Skipping y_hat=K,J,I\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=D,E,F\n","Skipping y_hat=M,O,N\n","Skipping y_hat=M,O,N\n","Skipping y_hat=M,O,N\n","Skipping y_hat=M,O,N\n","Skipping y_hat=M,O,N\n","Skipping y_hat=M,O,N\n","Skipping y_hat=M,O,N\n"," 74% 29/39 [00:08<00:02,  3.50it/s]Skipping y_hat=X,Y,W\n","Skipping y_hat=X,Y,W\n","Skipping y_hat=X,Y,W\n","Skipping y_hat=X,Y,W\n","Skipping y_hat=X,Y,W\n","Skipping y_hat=X,Y,W\n","Skipping y_hat=X,Y,W\n","Skipping y_hat=K,M,L\n","Skipping y_hat=K,M,L\n","Skipping y_hat=K,M,L\n","Skipping y_hat=K,M,L\n","Skipping y_hat=K,M,L\n","Skipping y_hat=K,M,L\n","Skipping y_hat=K,M,L\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=P,Q,O\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=A,C,B\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=I,J,K\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n","Skipping y_hat=T,V,U\n"," 77% 30/39 [00:08<00:02,  4.23it/s]Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=T,R,S,Q\n","Skipping y_hat=T,R,S,Q\n","Skipping y_hat=T,R,S,Q\n","Skipping y_hat=T,R,S,Q\n","Skipping y_hat=T,R,S,Q\n","Skipping y_hat=T,R,S,Q\n","Skipping y_hat=T,R,S,Q\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=W,U,V,T\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=B,Z,C,A\n","Skipping y_hat=B,Z,C,A\n","Skipping y_hat=B,Z,C,A\n","Skipping y_hat=B,Z,C,A\n","Skipping y_hat=B,Z,C,A\n","Skipping y_hat=B,Z,C,A\n","Skipping y_hat=B,Z,C,A\n","Skipping y_hat=X,Y,Z,W\n","Skipping y_hat=X,Y,Z,W\n","Skipping y_hat=X,Y,Z,W\n","Skipping y_hat=X,Y,Z,W\n","Skipping y_hat=X,Y,Z,W\n","Skipping y_hat=X,Y,Z,W\n","Skipping y_hat=X,Y,Z,W\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=B,A,C,D\n","Skipping y_hat=B,A,C,D\n","Skipping y_hat=B,A,C,D\n","Skipping y_hat=B,A,C,D\n","Skipping y_hat=B,A,C,D\n","Skipping y_hat=B,A,C,D\n","Skipping y_hat=B,A,C,D\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=K,L,I,J\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=J,L,M,K\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=S,U,T,R\n","Skipping y_hat=S,U,T,R\n","Skipping y_hat=S,U,T,R\n","Skipping y_hat=S,U,T,R\n","Skipping y_hat=S,U,T,R\n","Skipping y_hat=S,U,T,R\n","Skipping y_hat=S,U,T,R\n","Skipping y_hat=O,P,Q,N\n","Skipping y_hat=O,P,Q,N\n","Skipping y_hat=O,P,Q,N\n","Skipping y_hat=O,P,Q,N\n","Skipping y_hat=O,P,Q,N\n","Skipping y_hat=O,P,Q,N\n","Skipping y_hat=O,P,Q,N\n","Skipping y_hat=I,H,K,J\n","Skipping y_hat=I,H,K,J\n","Skipping y_hat=I,H,K,J\n","Skipping y_hat=I,H,K,J\n","Skipping y_hat=I,H,K,J\n","Skipping y_hat=I,H,K,J\n","Skipping y_hat=I,H,K,J\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=O,P,N,Q\n","Skipping y_hat=O,P,N,Q\n","Skipping y_hat=O,P,N,Q\n","Skipping y_hat=O,P,N,Q\n","Skipping y_hat=O,P,N,Q\n","Skipping y_hat=O,P,N,Q\n","Skipping y_hat=O,P,N,Q\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=N,M,L,K\n","Skipping y_hat=N,M,L,K\n","Skipping y_hat=N,M,L,K\n","Skipping y_hat=N,M,L,K\n","Skipping y_hat=N,M,L,K\n","Skipping y_hat=N,M,L,K\n","Skipping y_hat=N,M,L,K\n","Skipping y_hat=Q,T,R,S\n","Skipping y_hat=Q,T,R,S\n","Skipping y_hat=Q,T,R,S\n","Skipping y_hat=Q,T,R,S\n","Skipping y_hat=Q,T,R,S\n","Skipping y_hat=Q,T,R,S\n","Skipping y_hat=Q,T,R,S\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=E,F,G,H\n","Skipping y_hat=E,F,G,H\n","Skipping y_hat=E,F,G,H\n","Skipping y_hat=E,F,G,H\n","Skipping y_hat=E,F,G,H\n","Skipping y_hat=E,F,G,H\n","Skipping y_hat=E,F,G,H\n","Skipping y_hat=Y,Z,B,A\n","Skipping y_hat=Y,Z,B,A\n","Skipping y_hat=Y,Z,B,A\n","Skipping y_hat=Y,Z,B,A\n","Skipping y_hat=Y,Z,B,A\n","Skipping y_hat=Y,Z,B,A\n","Skipping y_hat=Y,Z,B,A\n","Skipping y_hat=I,J,K,H\n","Skipping y_hat=I,J,K,H\n","Skipping y_hat=I,J,K,H\n","Skipping y_hat=I,J,K,H\n","Skipping y_hat=I,J,K,H\n","Skipping y_hat=I,J,K,H\n","Skipping y_hat=I,J,K,H\n","Skipping y_hat=G,D,E,F\n","Skipping y_hat=G,D,E,F\n","Skipping y_hat=G,D,E,F\n","Skipping y_hat=G,D,E,F\n","Skipping y_hat=G,D,E,F\n","Skipping y_hat=G,D,E,F\n","Skipping y_hat=G,D,E,F\n","Skipping y_hat=S,Q,P,R\n","Skipping y_hat=S,Q,P,R\n","Skipping y_hat=S,Q,P,R\n","Skipping y_hat=S,Q,P,R\n","Skipping y_hat=S,Q,P,R\n","Skipping y_hat=S,Q,P,R\n","Skipping y_hat=S,Q,P,R\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=L,M,K,N\n","Skipping y_hat=L,M,K,N\n","Skipping y_hat=L,M,K,N\n","Skipping y_hat=L,M,K,N\n","Skipping y_hat=L,M,K,N\n","Skipping y_hat=L,M,K,N\n","Skipping y_hat=L,M,K,N\n","Skipping y_hat=O,P,Q,R\n","Skipping y_hat=O,P,Q,R\n","Skipping y_hat=O,P,Q,R\n","Skipping y_hat=O,P,Q,R\n","Skipping y_hat=O,P,Q,R\n","Skipping y_hat=O,P,Q,R\n","Skipping y_hat=O,P,Q,R\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=J,L,K,I\n","Skipping y_hat=J,L,K,I\n","Skipping y_hat=J,L,K,I\n","Skipping y_hat=J,L,K,I\n","Skipping y_hat=J,L,K,I\n","Skipping y_hat=J,L,K,I\n","Skipping y_hat=J,L,K,I\n","Skipping y_hat=A,Y,X,Z\n","Skipping y_hat=A,Y,X,Z\n","Skipping y_hat=A,Y,X,Z\n","Skipping y_hat=A,Y,X,Z\n","Skipping y_hat=A,Y,X,Z\n","Skipping y_hat=A,Y,X,Z\n","Skipping y_hat=A,Y,X,Z\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=C,A,Z,B\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=E,C,F,D\n","Skipping y_hat=E,C,F,D\n","Skipping y_hat=E,C,F,D\n","Skipping y_hat=E,C,F,D\n","Skipping y_hat=E,C,F,D\n","Skipping y_hat=E,C,F,D\n","Skipping y_hat=E,C,F,D\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=G,H,F,I\n","Skipping y_hat=G,H,F,I\n","Skipping y_hat=G,H,F,I\n","Skipping y_hat=G,H,F,I\n","Skipping y_hat=G,H,F,I\n","Skipping y_hat=G,H,F,I\n","Skipping y_hat=G,H,F,I\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=V,W,X,Y\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=H,F,G,E\n","Skipping y_hat=H,F,G,E\n","Skipping y_hat=H,F,G,E\n","Skipping y_hat=H,F,G,E\n","Skipping y_hat=H,F,G,E\n","Skipping y_hat=H,F,G,E\n","Skipping y_hat=H,F,G,E\n","Skipping y_hat=R,P,Q,S\n","Skipping y_hat=R,P,Q,S\n","Skipping y_hat=R,P,Q,S\n","Skipping y_hat=R,P,Q,S\n","Skipping y_hat=R,P,Q,S\n","Skipping y_hat=R,P,Q,S\n","Skipping y_hat=R,P,Q,S\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=U,V,W,X\n","Skipping y_hat=U,V,W,X\n","Skipping y_hat=U,V,W,X\n","Skipping y_hat=U,V,W,X\n","Skipping y_hat=U,V,W,X\n","Skipping y_hat=U,V,W,X\n","Skipping y_hat=U,V,W,X\n","Skipping y_hat=S,T,R,U\n","Skipping y_hat=S,T,R,U\n","Skipping y_hat=S,T,R,U\n","Skipping y_hat=S,T,R,U\n","Skipping y_hat=S,T,R,U\n","Skipping y_hat=S,T,R,U\n","Skipping y_hat=S,T,R,U\n","Skipping y_hat=Y,X,W,Z\n","Skipping y_hat=Y,X,W,Z\n","Skipping y_hat=Y,X,W,Z\n","Skipping y_hat=Y,X,W,Z\n","Skipping y_hat=Y,X,W,Z\n","Skipping y_hat=Y,X,W,Z\n","Skipping y_hat=Y,X,W,Z\n","Skipping y_hat=K,M,L,N\n","Skipping y_hat=K,M,L,N\n","Skipping y_hat=K,M,L,N\n","Skipping y_hat=K,M,L,N\n","Skipping y_hat=K,M,L,N\n","Skipping y_hat=K,M,L,N\n","Skipping y_hat=K,M,L,N\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=P,N,Q,O\n","Skipping y_hat=P,N,Q,O\n","Skipping y_hat=P,N,Q,O\n","Skipping y_hat=P,N,Q,O\n","Skipping y_hat=P,N,Q,O\n","Skipping y_hat=P,N,Q,O\n","Skipping y_hat=P,N,Q,O\n","Skipping y_hat=W,X,Y,Z\n","Skipping y_hat=W,X,Y,Z\n","Skipping y_hat=W,X,Y,Z\n","Skipping y_hat=W,X,Y,Z\n","Skipping y_hat=W,X,Y,Z\n","Skipping y_hat=W,X,Y,Z\n","Skipping y_hat=W,X,Y,Z\n","Skipping y_hat=E,B,C,D\n","Skipping y_hat=E,B,C,D\n","Skipping y_hat=E,B,C,D\n","Skipping y_hat=E,B,C,D\n","Skipping y_hat=E,B,C,D\n","Skipping y_hat=E,B,C,D\n","Skipping y_hat=E,B,C,D\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=N,O,P,M\n","Skipping y_hat=N,O,P,M\n","Skipping y_hat=N,O,P,M\n","Skipping y_hat=N,O,P,M\n","Skipping y_hat=N,O,P,M\n","Skipping y_hat=N,O,P,M\n","Skipping y_hat=N,O,P,M\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=K,J,I,L\n","Skipping y_hat=K,J,I,L\n","Skipping y_hat=K,J,I,L\n","Skipping y_hat=K,J,I,L\n","Skipping y_hat=K,J,I,L\n","Skipping y_hat=K,J,I,L\n","Skipping y_hat=K,J,I,L\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=K,L,M,N\n","Skipping y_hat=D,C,E,B\n","Skipping y_hat=D,C,E,B\n","Skipping y_hat=D,C,E,B\n","Skipping y_hat=D,C,E,B\n","Skipping y_hat=D,C,E,B\n","Skipping y_hat=D,C,E,B\n","Skipping y_hat=D,C,E,B\n","Skipping y_hat=T,S,R,Q\n","Skipping y_hat=T,S,R,Q\n","Skipping y_hat=T,S,R,Q\n","Skipping y_hat=T,S,R,Q\n","Skipping y_hat=T,S,R,Q\n","Skipping y_hat=T,S,R,Q\n","Skipping y_hat=T,S,R,Q\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=I,L,K,J\n","Skipping y_hat=I,L,K,J\n","Skipping y_hat=I,L,K,J\n","Skipping y_hat=I,L,K,J\n","Skipping y_hat=I,L,K,J\n","Skipping y_hat=I,L,K,J\n","Skipping y_hat=I,L,K,J\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=G,F,H,E\n","Skipping y_hat=M,N,O,L\n","Skipping y_hat=M,N,O,L\n","Skipping y_hat=M,N,O,L\n","Skipping y_hat=M,N,O,L\n","Skipping y_hat=M,N,O,L\n","Skipping y_hat=M,N,O,L\n","Skipping y_hat=M,N,O,L\n","Skipping y_hat=V,W,U,X\n","Skipping y_hat=V,W,U,X\n","Skipping y_hat=V,W,U,X\n","Skipping y_hat=V,W,U,X\n","Skipping y_hat=V,W,U,X\n","Skipping y_hat=V,W,U,X\n","Skipping y_hat=V,W,U,X\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=W,X,V,Y\n","Skipping y_hat=W,X,V,Y\n","Skipping y_hat=W,X,V,Y\n","Skipping y_hat=W,X,V,Y\n","Skipping y_hat=W,X,V,Y\n","Skipping y_hat=W,X,V,Y\n","Skipping y_hat=W,X,V,Y\n","Skipping y_hat=F,D,G,E\n","Skipping y_hat=F,D,G,E\n","Skipping y_hat=F,D,G,E\n","Skipping y_hat=F,D,G,E\n","Skipping y_hat=F,D,G,E\n","Skipping y_hat=F,D,G,E\n","Skipping y_hat=F,D,G,E\n","Skipping y_hat=T,Q,S,R\n","Skipping y_hat=T,Q,S,R\n","Skipping y_hat=T,Q,S,R\n","Skipping y_hat=T,Q,S,R\n","Skipping y_hat=T,Q,S,R\n","Skipping y_hat=T,Q,S,R\n","Skipping y_hat=T,Q,S,R\n","Skipping y_hat=K,H,J,I\n","Skipping y_hat=K,H,J,I\n","Skipping y_hat=K,H,J,I\n","Skipping y_hat=K,H,J,I\n","Skipping y_hat=K,H,J,I\n","Skipping y_hat=K,H,J,I\n","Skipping y_hat=K,H,J,I\n","Skipping y_hat=P,Q,O,N\n","Skipping y_hat=P,Q,O,N\n","Skipping y_hat=P,Q,O,N\n","Skipping y_hat=P,Q,O,N\n","Skipping y_hat=P,Q,O,N\n","Skipping y_hat=P,Q,O,N\n","Skipping y_hat=P,Q,O,N\n","Skipping y_hat=S,T,V,U\n","Skipping y_hat=S,T,V,U\n","Skipping y_hat=S,T,V,U\n","Skipping y_hat=S,T,V,U\n","Skipping y_hat=S,T,V,U\n","Skipping y_hat=S,T,V,U\n","Skipping y_hat=S,T,V,U\n","Skipping y_hat=M,K,J,L\n","Skipping y_hat=M,K,J,L\n","Skipping y_hat=M,K,J,L\n","Skipping y_hat=M,K,J,L\n","Skipping y_hat=M,K,J,L\n","Skipping y_hat=M,K,J,L\n","Skipping y_hat=M,K,J,L\n","Skipping y_hat=D,E,F,G\n","Skipping y_hat=D,E,F,G\n","Skipping y_hat=D,E,F,G\n","Skipping y_hat=D,E,F,G\n","Skipping y_hat=D,E,F,G\n","Skipping y_hat=D,E,F,G\n","Skipping y_hat=D,E,F,G\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=X,Y,W,Z\n","Skipping y_hat=M,N,O,P\n","Skipping y_hat=M,N,O,P\n","Skipping y_hat=M,N,O,P\n","Skipping y_hat=M,N,O,P\n","Skipping y_hat=M,N,O,P\n","Skipping y_hat=M,N,O,P\n","Skipping y_hat=M,N,O,P\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=X,W,Y,Z\n","Skipping y_hat=X,W,Y,Z\n","Skipping y_hat=X,W,Y,Z\n","Skipping y_hat=X,W,Y,Z\n","Skipping y_hat=X,W,Y,Z\n","Skipping y_hat=X,W,Y,Z\n","Skipping y_hat=X,W,Y,Z\n","Skipping y_hat=U,X,V,W\n","Skipping y_hat=U,X,V,W\n","Skipping y_hat=U,X,V,W\n","Skipping y_hat=U,X,V,W\n","Skipping y_hat=U,X,V,W\n","Skipping y_hat=U,X,V,W\n","Skipping y_hat=U,X,V,W\n","Skipping y_hat=C,A,B,D\n","Skipping y_hat=C,A,B,D\n","Skipping y_hat=C,A,B,D\n","Skipping y_hat=C,A,B,D\n","Skipping y_hat=C,A,B,D\n","Skipping y_hat=C,A,B,D\n","Skipping y_hat=C,A,B,D\n","Skipping y_hat=Z,B,C,A\n","Skipping y_hat=Z,B,C,A\n","Skipping y_hat=Z,B,C,A\n","Skipping y_hat=Z,B,C,A\n","Skipping y_hat=Z,B,C,A\n","Skipping y_hat=Z,B,C,A\n","Skipping y_hat=Z,B,C,A\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=D,B,E,C\n","Skipping y_hat=E,F,D,G\n","Skipping y_hat=E,F,D,G\n","Skipping y_hat=E,F,D,G\n","Skipping y_hat=E,F,D,G\n","Skipping y_hat=E,F,D,G\n","Skipping y_hat=E,F,D,G\n","Skipping y_hat=E,F,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=F,E,D,G\n","Skipping y_hat=I,J,L,K\n","Skipping y_hat=I,J,L,K\n","Skipping y_hat=I,J,L,K\n","Skipping y_hat=I,J,L,K\n","Skipping y_hat=I,J,L,K\n","Skipping y_hat=I,J,L,K\n","Skipping y_hat=I,J,L,K\n","Skipping y_hat=D,C,F,E\n","Skipping y_hat=D,C,F,E\n","Skipping y_hat=D,C,F,E\n","Skipping y_hat=D,C,F,E\n","Skipping y_hat=D,C,F,E\n","Skipping y_hat=D,C,F,E\n","Skipping y_hat=D,C,F,E\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,I,G,H\n","Skipping y_hat=J,I,G,H\n","Skipping y_hat=J,I,G,H\n","Skipping y_hat=J,I,G,H\n","Skipping y_hat=J,I,G,H\n","Skipping y_hat=J,I,G,H\n","Skipping y_hat=J,I,G,H\n","Skipping y_hat=Y,A,Z,B\n","Skipping y_hat=Y,A,Z,B\n","Skipping y_hat=Y,A,Z,B\n","Skipping y_hat=Y,A,Z,B\n","Skipping y_hat=Y,A,Z,B\n","Skipping y_hat=Y,A,Z,B\n","Skipping y_hat=Y,A,Z,B\n","Skipping y_hat=C,D,B,A\n","Skipping y_hat=C,D,B,A\n","Skipping y_hat=C,D,B,A\n","Skipping y_hat=C,D,B,A\n","Skipping y_hat=C,D,B,A\n","Skipping y_hat=C,D,B,A\n","Skipping y_hat=C,D,B,A\n","Skipping y_hat=E,F,G,D\n","Skipping y_hat=E,F,G,D\n","Skipping y_hat=E,F,G,D\n","Skipping y_hat=E,F,G,D\n","Skipping y_hat=E,F,G,D\n","Skipping y_hat=E,F,G,D\n","Skipping y_hat=E,F,G,D\n","Skipping y_hat=Y,Z,A,B\n","Skipping y_hat=Y,Z,A,B\n","Skipping y_hat=Y,Z,A,B\n","Skipping y_hat=Y,Z,A,B\n","Skipping y_hat=Y,Z,A,B\n","Skipping y_hat=Y,Z,A,B\n","Skipping y_hat=Y,Z,A,B\n","Skipping y_hat=G,H,I,J\n","Skipping y_hat=G,H,I,J\n","Skipping y_hat=G,H,I,J\n","Skipping y_hat=G,H,I,J\n","Skipping y_hat=G,H,I,J\n","Skipping y_hat=G,H,I,J\n","Skipping y_hat=G,H,I,J\n"," 79% 31/39 [00:08<00:01,  4.22it/s]Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=Q,P,O\n","Skipping y_hat=Q,P,O\n","Skipping y_hat=Q,P,O\n","Skipping y_hat=Q,P,O\n","Skipping y_hat=Q,P,O\n","Skipping y_hat=Q,P,O\n","Skipping y_hat=Q,P,O\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=Q,S,R\n","Skipping y_hat=Q,S,R\n","Skipping y_hat=Q,S,R\n","Skipping y_hat=Q,S,R\n","Skipping y_hat=Q,S,R\n","Skipping y_hat=Q,S,R\n","Skipping y_hat=Q,S,R\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=T,R,S\n","Skipping y_hat=T,R,S\n","Skipping y_hat=T,R,S\n","Skipping y_hat=T,R,S\n","Skipping y_hat=T,R,S\n","Skipping y_hat=T,R,S\n","Skipping y_hat=T,R,S\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=Z,Y,A\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=X,Y,Z\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=N,P,O\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=J,I,K\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=O,P,N\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=F,G,H\n","Skipping y_hat=G,H,I\n","Skipping y_hat=G,H,I\n","Skipping y_hat=G,H,I\n","Skipping y_hat=G,H,I\n","Skipping y_hat=G,H,I\n","Skipping y_hat=G,H,I\n","Skipping y_hat=G,H,I\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=N,O,P\n","Skipping y_hat=N,O,P\n","Skipping y_hat=N,O,P\n","Skipping y_hat=N,O,P\n","Skipping y_hat=N,O,P\n","Skipping y_hat=N,O,P\n","Skipping y_hat=N,O,P\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=P,O,Q\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=U,V,W\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=M,N,O\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=Y,Z,X\n","Skipping y_hat=Y,Z,X\n","Skipping y_hat=Y,Z,X\n","Skipping y_hat=Y,Z,X\n","Skipping y_hat=Y,Z,X\n","Skipping y_hat=Y,Z,X\n","Skipping y_hat=Y,Z,X\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=Z,Y\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=D,F,E\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=J,H,I\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=E,F,G\n","Skipping y_hat=E,F,G\n","Skipping y_hat=E,F,G\n","Skipping y_hat=E,F,G\n","Skipping y_hat=E,F,G\n","Skipping y_hat=E,F,G\n","Skipping y_hat=E,F,G\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=W,V,U\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=Z,Y,X\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=T,S\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=F,G\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=P,N,O\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=M,L,N\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,C,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=A,B,Z\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=J,I\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=N,M,L\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,J\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=I,H,G\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=L,M,N\n","Skipping y_hat=L,M,N\n","Skipping y_hat=L,M,N\n","Skipping y_hat=L,M,N\n","Skipping y_hat=L,M,N\n","Skipping y_hat=L,M,N\n","Skipping y_hat=L,M,N\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=I,K,J\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=C,D,B\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=I,J\n","Skipping y_hat=I,J\n","Skipping y_hat=I,J\n","Skipping y_hat=I,J\n","Skipping y_hat=I,J\n","Skipping y_hat=I,J\n","Skipping y_hat=I,J\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=F,G,E\n","Skipping y_hat=F,G,E\n","Skipping y_hat=F,G,E\n","Skipping y_hat=F,G,E\n","Skipping y_hat=F,G,E\n","Skipping y_hat=F,G,E\n","Skipping y_hat=F,G,E\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n"," 82% 32/39 [00:08<00:02,  2.95it/s]Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=T,S,U\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n"," 85% 33/39 [00:09<00:01,  3.71it/s]Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=V,U\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=G,F\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=M,L\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=W,X\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=H,G\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=U,T\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=F,E\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=O,N\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=H,I\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=N,M\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=O,P\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=R,Q\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=Y,Z\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=B,A\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=K,J\n","Skipping y_hat=P,O\n","Skipping y_hat=P,O\n","Skipping y_hat=P,O\n","Skipping y_hat=P,O\n","Skipping y_hat=P,O\n","Skipping y_hat=P,O\n","Skipping y_hat=P,O\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=U,V\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=R,S\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=Y,X\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=X,W\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=N,O\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n"," 87% 34/39 [00:09<00:01,  3.92it/s]Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=H,I,J,K\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=U,R,S,T\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=G,E,F,D\n","Skipping y_hat=G,E,F,D\n","Skipping y_hat=G,E,F,D\n","Skipping y_hat=G,E,F,D\n","Skipping y_hat=G,E,F,D\n","Skipping y_hat=G,E,F,D\n","Skipping y_hat=G,E,F,D\n","Skipping y_hat=R,S,T,Q\n","Skipping y_hat=R,S,T,Q\n","Skipping y_hat=R,S,T,Q\n","Skipping y_hat=R,S,T,Q\n","Skipping y_hat=R,S,T,Q\n","Skipping y_hat=R,S,T,Q\n","Skipping y_hat=R,S,T,Q\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=J,I,H,K\n","Skipping y_hat=J,I,H,K\n","Skipping y_hat=J,I,H,K\n","Skipping y_hat=J,I,H,K\n","Skipping y_hat=J,I,H,K\n","Skipping y_hat=J,I,H,K\n","Skipping y_hat=J,I,H,K\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=D,B,C,A\n","Skipping y_hat=D,B,C,A\n","Skipping y_hat=D,B,C,A\n","Skipping y_hat=D,B,C,A\n","Skipping y_hat=D,B,C,A\n","Skipping y_hat=D,B,C,A\n","Skipping y_hat=D,B,C,A\n","Skipping y_hat=C,D,F,E\n","Skipping y_hat=C,D,F,E\n","Skipping y_hat=C,D,F,E\n","Skipping y_hat=C,D,F,E\n","Skipping y_hat=C,D,F,E\n","Skipping y_hat=C,D,F,E\n","Skipping y_hat=C,D,F,E\n","Skipping y_hat=C,E,B,D\n","Skipping y_hat=C,E,B,D\n","Skipping y_hat=C,E,B,D\n","Skipping y_hat=C,E,B,D\n","Skipping y_hat=C,E,B,D\n","Skipping y_hat=C,E,B,D\n","Skipping y_hat=C,E,B,D\n","Skipping y_hat=R,P,S,Q\n","Skipping y_hat=R,P,S,Q\n","Skipping y_hat=R,P,S,Q\n","Skipping y_hat=R,P,S,Q\n","Skipping y_hat=R,P,S,Q\n","Skipping y_hat=R,P,S,Q\n","Skipping y_hat=R,P,S,Q\n","Skipping y_hat=I,F,H,G\n","Skipping y_hat=I,F,H,G\n","Skipping y_hat=I,F,H,G\n","Skipping y_hat=I,F,H,G\n","Skipping y_hat=I,F,H,G\n","Skipping y_hat=I,F,H,G\n","Skipping y_hat=I,F,H,G\n","Skipping y_hat=G,D,F,E\n","Skipping y_hat=G,D,F,E\n","Skipping y_hat=G,D,F,E\n","Skipping y_hat=G,D,F,E\n","Skipping y_hat=G,D,F,E\n","Skipping y_hat=G,D,F,E\n","Skipping y_hat=G,D,F,E\n","Skipping y_hat=J,I,H,G\n","Skipping y_hat=J,I,H,G\n","Skipping y_hat=J,I,H,G\n","Skipping y_hat=J,I,H,G\n","Skipping y_hat=J,I,H,G\n","Skipping y_hat=J,I,H,G\n","Skipping y_hat=J,I,H,G\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=V,X,Y,W\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=I,H,G,J\n","Skipping y_hat=I,H,G,J\n","Skipping y_hat=I,H,G,J\n","Skipping y_hat=I,H,G,J\n","Skipping y_hat=I,H,G,J\n","Skipping y_hat=I,H,G,J\n","Skipping y_hat=I,H,G,J\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=Q,O,R,P\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=R,S,Q,P\n","Skipping y_hat=R,S,Q,P\n","Skipping y_hat=R,S,Q,P\n","Skipping y_hat=R,S,Q,P\n","Skipping y_hat=R,S,Q,P\n","Skipping y_hat=R,S,Q,P\n","Skipping y_hat=R,S,Q,P\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=B,D,C,A\n","Skipping y_hat=G,J,H,I\n","Skipping y_hat=G,J,H,I\n","Skipping y_hat=G,J,H,I\n","Skipping y_hat=G,J,H,I\n","Skipping y_hat=G,J,H,I\n","Skipping y_hat=G,J,H,I\n","Skipping y_hat=G,J,H,I\n","Skipping y_hat=M,O,N,L\n","Skipping y_hat=M,O,N,L\n","Skipping y_hat=M,O,N,L\n","Skipping y_hat=M,O,N,L\n","Skipping y_hat=M,O,N,L\n","Skipping y_hat=M,O,N,L\n","Skipping y_hat=M,O,N,L\n","Skipping y_hat=M,P,O,N\n","Skipping y_hat=M,P,O,N\n","Skipping y_hat=M,P,O,N\n","Skipping y_hat=M,P,O,N\n","Skipping y_hat=M,P,O,N\n","Skipping y_hat=M,P,O,N\n","Skipping y_hat=M,P,O,N\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=J,H,I,G\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,D,C,E\n","Skipping y_hat=F,D,C,E\n","Skipping y_hat=F,D,C,E\n","Skipping y_hat=F,D,C,E\n","Skipping y_hat=F,D,C,E\n","Skipping y_hat=F,D,C,E\n","Skipping y_hat=F,D,C,E\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=Y,W,V,X\n","Skipping y_hat=C,A,B,Z\n","Skipping y_hat=C,A,B,Z\n","Skipping y_hat=C,A,B,Z\n","Skipping y_hat=C,A,B,Z\n","Skipping y_hat=C,A,B,Z\n","Skipping y_hat=C,A,B,Z\n","Skipping y_hat=C,A,B,Z\n","Skipping y_hat=W,V,X,U\n","Skipping y_hat=W,V,X,U\n","Skipping y_hat=W,V,X,U\n","Skipping y_hat=W,V,X,U\n","Skipping y_hat=W,V,X,U\n","Skipping y_hat=W,V,X,U\n","Skipping y_hat=W,V,X,U\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=N,O,P,Q\n","Skipping y_hat=U,S,T,R\n","Skipping y_hat=U,S,T,R\n","Skipping y_hat=U,S,T,R\n","Skipping y_hat=U,S,T,R\n","Skipping y_hat=U,S,T,R\n","Skipping y_hat=U,S,T,R\n","Skipping y_hat=U,S,T,R\n","Skipping y_hat=N,P,M,O\n","Skipping y_hat=N,P,M,O\n","Skipping y_hat=N,P,M,O\n","Skipping y_hat=N,P,M,O\n","Skipping y_hat=N,P,M,O\n","Skipping y_hat=N,P,M,O\n","Skipping y_hat=N,P,M,O\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=K,I,J,L\n","Skipping y_hat=N,O,M,L\n","Skipping y_hat=N,O,M,L\n","Skipping y_hat=N,O,M,L\n","Skipping y_hat=N,O,M,L\n","Skipping y_hat=N,O,M,L\n","Skipping y_hat=N,O,M,L\n","Skipping y_hat=N,O,M,L\n","Skipping y_hat=B,A,D,C\n","Skipping y_hat=B,A,D,C\n","Skipping y_hat=B,A,D,C\n","Skipping y_hat=B,A,D,C\n","Skipping y_hat=B,A,D,C\n","Skipping y_hat=B,A,D,C\n","Skipping y_hat=B,A,D,C\n","Skipping y_hat=M,O,N,P\n","Skipping y_hat=M,O,N,P\n","Skipping y_hat=M,O,N,P\n","Skipping y_hat=M,O,N,P\n","Skipping y_hat=M,O,N,P\n","Skipping y_hat=M,O,N,P\n","Skipping y_hat=M,O,N,P\n","Skipping y_hat=U,T,R,S\n","Skipping y_hat=U,T,R,S\n","Skipping y_hat=U,T,R,S\n","Skipping y_hat=U,T,R,S\n","Skipping y_hat=U,T,R,S\n","Skipping y_hat=U,T,R,S\n","Skipping y_hat=U,T,R,S\n","Skipping y_hat=U,T,W,V\n","Skipping y_hat=U,T,W,V\n","Skipping y_hat=U,T,W,V\n","Skipping y_hat=U,T,W,V\n","Skipping y_hat=U,T,W,V\n","Skipping y_hat=U,T,W,V\n","Skipping y_hat=U,T,W,V\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=G,E,H,F\n","Skipping y_hat=G,E,H,F\n","Skipping y_hat=G,E,H,F\n","Skipping y_hat=G,E,H,F\n","Skipping y_hat=G,E,H,F\n","Skipping y_hat=G,E,H,F\n","Skipping y_hat=G,E,H,F\n","Skipping y_hat=P,N,M,O\n","Skipping y_hat=P,N,M,O\n","Skipping y_hat=P,N,M,O\n","Skipping y_hat=P,N,M,O\n","Skipping y_hat=P,N,M,O\n","Skipping y_hat=P,N,M,O\n","Skipping y_hat=P,N,M,O\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=P,Q,O,R\n","Skipping y_hat=P,Q,O,R\n","Skipping y_hat=P,Q,O,R\n","Skipping y_hat=P,Q,O,R\n","Skipping y_hat=P,Q,O,R\n","Skipping y_hat=P,Q,O,R\n","Skipping y_hat=P,Q,O,R\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=V,Y,W,X\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=F,E,G,D\n","Skipping y_hat=Z,Y,A,X\n","Skipping y_hat=Z,Y,A,X\n","Skipping y_hat=Z,Y,A,X\n","Skipping y_hat=Z,Y,A,X\n","Skipping y_hat=Z,Y,A,X\n","Skipping y_hat=Z,Y,A,X\n","Skipping y_hat=Z,Y,A,X\n","Skipping y_hat=P,N,O,Q\n","Skipping y_hat=P,N,O,Q\n","Skipping y_hat=P,N,O,Q\n","Skipping y_hat=P,N,O,Q\n","Skipping y_hat=P,N,O,Q\n","Skipping y_hat=P,N,O,Q\n","Skipping y_hat=P,N,O,Q\n","Skipping y_hat=A,Y,Z,X\n","Skipping y_hat=A,Y,Z,X\n","Skipping y_hat=A,Y,Z,X\n","Skipping y_hat=A,Y,Z,X\n","Skipping y_hat=A,Y,Z,X\n","Skipping y_hat=A,Y,Z,X\n","Skipping y_hat=A,Y,Z,X\n","Skipping y_hat=K,L,N,M\n","Skipping y_hat=K,L,N,M\n","Skipping y_hat=K,L,N,M\n","Skipping y_hat=K,L,N,M\n","Skipping y_hat=K,L,N,M\n","Skipping y_hat=K,L,N,M\n","Skipping y_hat=K,L,N,M\n"," 90% 35/39 [00:09<00:00,  4.36it/s]Skipping y_hat=J,I,K,L\n","Skipping y_hat=J,I,K,L\n","Skipping y_hat=J,I,K,L\n","Skipping y_hat=J,I,K,L\n","Skipping y_hat=J,I,K,L\n","Skipping y_hat=J,I,K,L\n","Skipping y_hat=J,I,K,L\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=O,Q,P,N\n","Skipping y_hat=C,B,A,D\n","Skipping y_hat=C,B,A,D\n","Skipping y_hat=C,B,A,D\n","Skipping y_hat=C,B,A,D\n","Skipping y_hat=C,B,A,D\n","Skipping y_hat=C,B,A,D\n","Skipping y_hat=C,B,A,D\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=T,S,U,R\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=A,B,D,C\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=O,M,N,P\n","Skipping y_hat=W,U,V,X\n","Skipping y_hat=W,U,V,X\n","Skipping y_hat=W,U,V,X\n","Skipping y_hat=W,U,V,X\n","Skipping y_hat=W,U,V,X\n","Skipping y_hat=W,U,V,X\n","Skipping y_hat=W,U,V,X\n","Skipping y_hat=B,D,E,C\n","Skipping y_hat=B,D,E,C\n","Skipping y_hat=B,D,E,C\n","Skipping y_hat=B,D,E,C\n","Skipping y_hat=B,D,E,C\n","Skipping y_hat=B,D,E,C\n","Skipping y_hat=B,D,E,C\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=W,V,X,Y\n","Skipping y_hat=V,X,W,U\n","Skipping y_hat=V,X,W,U\n","Skipping y_hat=V,X,W,U\n","Skipping y_hat=V,X,W,U\n","Skipping y_hat=V,X,W,U\n","Skipping y_hat=V,X,W,U\n","Skipping y_hat=V,X,W,U\n","Skipping y_hat=U,W,V,T\n","Skipping y_hat=U,W,V,T\n","Skipping y_hat=U,W,V,T\n","Skipping y_hat=U,W,V,T\n","Skipping y_hat=U,W,V,T\n","Skipping y_hat=U,W,V,T\n","Skipping y_hat=U,W,V,T\n","Skipping y_hat=G,F,I,H\n","Skipping y_hat=G,F,I,H\n","Skipping y_hat=G,F,I,H\n","Skipping y_hat=G,F,I,H\n","Skipping y_hat=G,F,I,H\n","Skipping y_hat=G,F,I,H\n","Skipping y_hat=G,F,I,H\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=J,L,K,M\n","Skipping y_hat=W,X,Y,V\n","Skipping y_hat=W,X,Y,V\n","Skipping y_hat=W,X,Y,V\n","Skipping y_hat=W,X,Y,V\n","Skipping y_hat=W,X,Y,V\n","Skipping y_hat=W,X,Y,V\n","Skipping y_hat=W,X,Y,V\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=A,B,C,D\n","Skipping y_hat=N,M,O,P\n","Skipping y_hat=N,M,O,P\n","Skipping y_hat=N,M,O,P\n","Skipping y_hat=N,M,O,P\n","Skipping y_hat=N,M,O,P\n","Skipping y_hat=N,M,O,P\n","Skipping y_hat=N,M,O,P\n","Skipping y_hat=M,N,L,K\n","Skipping y_hat=M,N,L,K\n","Skipping y_hat=M,N,L,K\n","Skipping y_hat=M,N,L,K\n","Skipping y_hat=M,N,L,K\n","Skipping y_hat=M,N,L,K\n","Skipping y_hat=M,N,L,K\n","Skipping y_hat=S,T,U,R\n","Skipping y_hat=S,T,U,R\n","Skipping y_hat=S,T,U,R\n","Skipping y_hat=S,T,U,R\n","Skipping y_hat=S,T,U,R\n","Skipping y_hat=S,T,U,R\n","Skipping y_hat=S,T,U,R\n"," 92% 36/39 [00:09<00:00,  5.16it/s]Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=E,D\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=A,Z\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=Q,P\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=C,B\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=M,N\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=V,W\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=W,V\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=D,C\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=E,F\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=J,K\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=Q,R\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=K,L\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=G,H\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=L,K\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=Z,A\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=D,E\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=B,C\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=C,D\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=X,Y\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=P,Q\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=A,B\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=I,H\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=S,T\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=T,U\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=L,M\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n","Skipping y_hat=S,R\n"," 95% 37/39 [00:09<00:00,  5.74it/s]Skipping y_hat=U,T,S,V\n","Skipping y_hat=U,T,S,V\n","Skipping y_hat=U,T,S,V\n","Skipping y_hat=U,T,S,V\n","Skipping y_hat=U,T,S,V\n","Skipping y_hat=U,T,S,V\n","Skipping y_hat=U,T,S,V\n","Skipping y_hat=W,U,X,V\n","Skipping y_hat=W,U,X,V\n","Skipping y_hat=W,U,X,V\n","Skipping y_hat=W,U,X,V\n","Skipping y_hat=W,U,X,V\n","Skipping y_hat=W,U,X,V\n","Skipping y_hat=W,U,X,V\n","Skipping y_hat=G,E,F,H\n","Skipping y_hat=G,E,F,H\n","Skipping y_hat=G,E,F,H\n","Skipping y_hat=G,E,F,H\n","Skipping y_hat=G,E,F,H\n","Skipping y_hat=G,E,F,H\n","Skipping y_hat=G,E,F,H\n","Skipping y_hat=D,E,B,C\n","Skipping y_hat=D,E,B,C\n","Skipping y_hat=D,E,B,C\n","Skipping y_hat=D,E,B,C\n","Skipping y_hat=D,E,B,C\n","Skipping y_hat=D,E,B,C\n","Skipping y_hat=D,E,B,C\n","Skipping y_hat=F,C,E,D\n","Skipping y_hat=F,C,E,D\n","Skipping y_hat=F,C,E,D\n","Skipping y_hat=F,C,E,D\n","Skipping y_hat=F,C,E,D\n","Skipping y_hat=F,C,E,D\n","Skipping y_hat=F,C,E,D\n","Skipping y_hat=R,P,Q,O\n","Skipping y_hat=R,P,Q,O\n","Skipping y_hat=R,P,Q,O\n","Skipping y_hat=R,P,Q,O\n","Skipping y_hat=R,P,Q,O\n","Skipping y_hat=R,P,Q,O\n","Skipping y_hat=R,P,Q,O\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=Z,Y,A,B\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=U,T,V,W\n","Skipping y_hat=U,T,V,W\n","Skipping y_hat=U,T,V,W\n","Skipping y_hat=U,T,V,W\n","Skipping y_hat=U,T,V,W\n","Skipping y_hat=U,T,V,W\n","Skipping y_hat=U,T,V,W\n","Skipping y_hat=L,O,N,M\n","Skipping y_hat=L,O,N,M\n","Skipping y_hat=L,O,N,M\n","Skipping y_hat=L,O,N,M\n","Skipping y_hat=L,O,N,M\n","Skipping y_hat=L,O,N,M\n","Skipping y_hat=L,O,N,M\n","Skipping y_hat=C,A,D,B\n","Skipping y_hat=C,A,D,B\n","Skipping y_hat=C,A,D,B\n","Skipping y_hat=C,A,D,B\n","Skipping y_hat=C,A,D,B\n","Skipping y_hat=C,A,D,B\n","Skipping y_hat=C,A,D,B\n","Skipping y_hat=Q,O,P,R\n","Skipping y_hat=Q,O,P,R\n","Skipping y_hat=Q,O,P,R\n","Skipping y_hat=Q,O,P,R\n","Skipping y_hat=Q,O,P,R\n","Skipping y_hat=Q,O,P,R\n","Skipping y_hat=Q,O,P,R\n","Skipping y_hat=F,E,C,D\n","Skipping y_hat=F,E,C,D\n","Skipping y_hat=F,E,C,D\n","Skipping y_hat=F,E,C,D\n","Skipping y_hat=F,E,C,D\n","Skipping y_hat=F,E,C,D\n","Skipping y_hat=F,E,C,D\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=P,M,N,O\n","Skipping y_hat=Y,W,X,Z\n","Skipping y_hat=Y,W,X,Z\n","Skipping y_hat=Y,W,X,Z\n","Skipping y_hat=Y,W,X,Z\n","Skipping y_hat=Y,W,X,Z\n","Skipping y_hat=Y,W,X,Z\n","Skipping y_hat=Y,W,X,Z\n","Skipping y_hat=I,G,H,F\n","Skipping y_hat=I,G,H,F\n","Skipping y_hat=I,G,H,F\n","Skipping y_hat=I,G,H,F\n","Skipping y_hat=I,G,H,F\n","Skipping y_hat=I,G,H,F\n","Skipping y_hat=I,G,H,F\n","Skipping y_hat=I,J,H,K\n","Skipping y_hat=I,J,H,K\n","Skipping y_hat=I,J,H,K\n","Skipping y_hat=I,J,H,K\n","Skipping y_hat=I,J,H,K\n","Skipping y_hat=I,J,H,K\n","Skipping y_hat=I,J,H,K\n","Skipping y_hat=U,S,T,V\n","Skipping y_hat=U,S,T,V\n","Skipping y_hat=U,S,T,V\n","Skipping y_hat=U,S,T,V\n","Skipping y_hat=U,S,T,V\n","Skipping y_hat=U,S,T,V\n","Skipping y_hat=U,S,T,V\n","Skipping y_hat=J,K,H,I\n","Skipping y_hat=J,K,H,I\n","Skipping y_hat=J,K,H,I\n","Skipping y_hat=J,K,H,I\n","Skipping y_hat=J,K,H,I\n","Skipping y_hat=J,K,H,I\n","Skipping y_hat=J,K,H,I\n","Skipping y_hat=X,Z,Y,A\n","Skipping y_hat=X,Z,Y,A\n","Skipping y_hat=X,Z,Y,A\n","Skipping y_hat=X,Z,Y,A\n","Skipping y_hat=X,Z,Y,A\n","Skipping y_hat=X,Z,Y,A\n","Skipping y_hat=X,Z,Y,A\n","Skipping y_hat=K,J,L,I\n","Skipping y_hat=K,J,L,I\n","Skipping y_hat=K,J,L,I\n","Skipping y_hat=K,J,L,I\n","Skipping y_hat=K,J,L,I\n","Skipping y_hat=K,J,L,I\n","Skipping y_hat=K,J,L,I\n","Skipping y_hat=S,P,R,Q\n","Skipping y_hat=S,P,R,Q\n","Skipping y_hat=S,P,R,Q\n","Skipping y_hat=S,P,R,Q\n","Skipping y_hat=S,P,R,Q\n","Skipping y_hat=S,P,R,Q\n","Skipping y_hat=S,P,R,Q\n","Skipping y_hat=K,L,J,M\n","Skipping y_hat=K,L,J,M\n","Skipping y_hat=K,L,J,M\n","Skipping y_hat=K,L,J,M\n","Skipping y_hat=K,L,J,M\n","Skipping y_hat=K,L,J,M\n","Skipping y_hat=K,L,J,M\n","Skipping y_hat=E,G,D,F\n","Skipping y_hat=E,G,D,F\n","Skipping y_hat=E,G,D,F\n","Skipping y_hat=E,G,D,F\n","Skipping y_hat=E,G,D,F\n","Skipping y_hat=E,G,D,F\n","Skipping y_hat=E,G,D,F\n","Skipping y_hat=Y,W,X,V\n","Skipping y_hat=Y,W,X,V\n","Skipping y_hat=Y,W,X,V\n","Skipping y_hat=Y,W,X,V\n","Skipping y_hat=Y,W,X,V\n","Skipping y_hat=Y,W,X,V\n","Skipping y_hat=Y,W,X,V\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=Y,V,W,X\n","Skipping y_hat=U,X,W,V\n","Skipping y_hat=U,X,W,V\n","Skipping y_hat=U,X,W,V\n","Skipping y_hat=U,X,W,V\n","Skipping y_hat=U,X,W,V\n","Skipping y_hat=U,X,W,V\n","Skipping y_hat=U,X,W,V\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Y,Z,W,X\n","Skipping y_hat=Z,B,A,Y\n","Skipping y_hat=Z,B,A,Y\n","Skipping y_hat=Z,B,A,Y\n","Skipping y_hat=Z,B,A,Y\n","Skipping y_hat=Z,B,A,Y\n","Skipping y_hat=Z,B,A,Y\n","Skipping y_hat=Z,B,A,Y\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=F,E,G,H\n","Skipping y_hat=T,U,S,V\n","Skipping y_hat=T,U,S,V\n","Skipping y_hat=T,U,S,V\n","Skipping y_hat=T,U,S,V\n","Skipping y_hat=T,U,S,V\n","Skipping y_hat=T,U,S,V\n","Skipping y_hat=T,U,S,V\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n","Skipping y_hat=U,V,T,W\n"," 97% 38/39 [00:09<00:00,  6.25it/s]Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=M,N,L,O\n","Skipping y_hat=Z,Y,X,A\n","Skipping y_hat=Z,Y,X,A\n","Skipping y_hat=Z,Y,X,A\n","Skipping y_hat=Z,Y,X,A\n","Skipping y_hat=Z,Y,X,A\n","Skipping y_hat=Z,Y,X,A\n","Skipping y_hat=Z,Y,X,A\n","100% 39/39 [00:09<00:00,  3.95it/s]\n","accuracy of 3000 examples: 2685/3000 (89.5%)\n","/content/drive/MyDrive/addition/evaluation.py:263: RuntimeWarning: Mean of empty slice\n","  metric_type: np.nanmean(error_dict[f'{metric_type}'])\n","\n","Test Results:\n","test: 89.50%\n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
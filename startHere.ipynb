{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":15733,"status":"ok","timestamp":1769386660267,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"},"user_tz":360},"id":"aftd1RezTYnF","outputId":"085eda99-6d1a-4296-b4aa-74557bf78a66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/addition\n","configuration_files\t       __pycache__\n","configurator.py\t\t       README.md\n","data\t\t\t       result_analysis.ipynb\n","data_generate.py\t       result_analysis.py\n","data_generation_script\t       result_analysis_script\n","eval_ckpt.py\t\t       results\n","evaluation_ckpt.py\t       startHere100M.ipynb\n","evaluation_comparison.py       startHere1B.ipynb\n","evaluation.py\t\t       startHere20M.ipynb\n","extra_result_analysis_scripts  startHere.ipynb\n","legacy_code\t\t       statistical_measurements.py\n","main_utilities.py\t       test.txt\n","meta_all_ascii_chars.pkl       train.py\n","model.py\t\t       train.txt\n","model_rope.py\t\t       train_work_perfect_per_example_batch.py\n","model_t5bias.py\t\t       val.txt\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change directory to your code\n","%cd /content/drive/MyDrive/addition\n","%pwd   # verify you‚Äôre in the right place\n","!ls    # should show train.py, 4_operands_addition.txt, etc."]},{"cell_type":"markdown","metadata":{"id":"lbEXozJBCm_X"},"source":["## Generate Training Data (Choose one task)"]},{"cell_type":"markdown","metadata":{"id":"fUKKZHtTI1n3"},"source":["### Addition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbKQwH__Bq4E"},"outputs":[],"source":["!python data_generate.py --task addition --num_operands 4 --experiment_name 4_operands_0_to_999_uniform_test --train_size 1000000 --test_size 10000 --val_size 10000 --train_eval True --sample-size 10000 --generate_reverse True"]},{"cell_type":"markdown","source":["#### Ablation in Addition (e.g. randomize thousands-place of the output)"],"metadata":{"id":"h95zEWGChhca"}},{"cell_type":"code","source":["!python data_generate.py --task addition --randomize thousands --num_operands 4 --experiment_name 4_operands_0_to_999_output_randomize_thousands --train_size 1000000 --test_size 10000 --val_size 10000 --train_eval True --sample-size 10000 --generate_reverse True\n"],"metadata":{"id":"_JKK2-u_hhAC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DBmQXdsiI6YE"},"source":["### Multiplication"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hr3n0w_gIf7i"},"outputs":[],"source":["!python data_generate.py --task multiplication --experiment_name 40_digit_times_1_digit --train_size 1000000 --test_size 10000 --val_size 10000 \\\n","--a_max_digits 40 --b_max_digits 1 --train_eval True --sample-size 10000 --generate_reverse True"]},{"cell_type":"markdown","source":["### Comparison (Balanced data)"],"metadata":{"id":"H8G1HEBCl161"}},{"cell_type":"code","source":["!python data_generate.py --task comparison --experiment_name comparison_bal_test --train_eval True --sample-size 5000"],"metadata":{"id":"eY6q7tsEl--q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fB5aQJxiI8_h"},"source":["### Sorting (Doubly balanced data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Q7blcAsIVHV"},"outputs":[],"source":["!python data_generate.py --task sorting --experiment_name 4_operands_sorting_doubly_balanced_test --train_eval True --sample-size 5000"]},{"cell_type":"markdown","metadata":{"id":"8i_CDutAI_fC"},"source":["## Let's Start Training!"]},{"cell_type":"markdown","metadata":{"id":"Et7rULRph_Ne"},"source":["#### The .txt file is the configuration file"]},{"cell_type":"markdown","metadata":{"id":"dZJI2octhC3o"},"source":["## 2 Operands"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"QK9_9lzwn3VX","outputId":"e972e230-4aff-494f-c8f5-ef9bf09fd181"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading config file: /content/drive/MyDrive/addition/configuration_files/2_operands_addition_reversed.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 50\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '2_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '2_operands_0_to_999_uniform_reverse'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/2_operands_0_to_999_uniform/reverse_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/2_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","Seeded everything: 1337\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test_reverse.txt\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.63M\n","/content/drive/MyDrive/addition/train_work_perfect_per_example_batch.py:443: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/2_operands_0_to_999_uniform/reverse_out/wandb/run-20260105_053059-ddn9h2et\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2_operands_0_to_999_uniform_reverse\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/2_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/2_op_addition/runs/ddn9h2et\u001b[0m\n","WARNING: results directory results/2_operands_0_to_999_uniform/reverse_out/2_operands_0_to_999_uniform_reverse already exists, overwriting...\n","max_new_tokens: 5\n","iter 0: train loss 2.6638, val loss 2.6732\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","100% 25/25 [00:01<00:00, 20.35it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 0/3000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","100% 25/25 [00:00<00:00, 36.67it/s]\n","W0105 05:31:22.357000 3853 torch/_inductor/utils.py:1558] [0/2] Not enough SMs to use max_autotune_gemm mode\n","iter 50: train loss 1.6669, val loss 1.6490\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 1/3000  (0.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.47it/s]\n","iter 100: train loss 1.6440, val loss 1.6485\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 2/3000  (0.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.43it/s]\n","iter 150: train loss 1.6420, val loss 1.6568\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 3/3000  (0.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.18it/s]\n","iter 200: train loss 1.6414, val loss 1.6471\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 34.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 1/3000  (0.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.16it/s]\n","iter 250: train loss 1.6503, val loss 1.6498\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 2/3000  (0.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.70it/s]\n","iter 300: train loss 1.6422, val loss 1.6432\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 2/3000  (0.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.64it/s]\n","iter 350: train loss 1.6413, val loss 1.6499\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 2/3000  (0.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.27it/s]\n","iter 400: train loss 1.6338, val loss 1.6469\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 4/3000  (0.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.85it/s]\n","iter 450: train loss 1.6446, val loss 1.6427\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 2/3000  (0.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 34.50it/s]\n","iter 500: train loss 1.6302, val loss 1.6265\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 3/3000  (0.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.88it/s]\n","iter 550: train loss 1.6171, val loss 1.6155\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 7/3000  (0.23%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.65it/s]\n","iter 600: train loss 1.6042, val loss 1.6046\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 13/3000  (0.43%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.69it/s]\n","iter 650: train loss 1.5553, val loss 1.5566\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 33.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 14/3000  (0.47%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.62it/s]\n","iter 700: train loss 1.5062, val loss 1.5196\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 34.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 20/3000  (0.67%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.23it/s]\n","iter 750: train loss 1.4904, val loss 1.4974\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 30/3000  (1.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.29it/s]\n","iter 800: train loss 1.4726, val loss 1.4775\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 33.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 27/3000  (0.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.32it/s]\n","iter 850: train loss 1.4509, val loss 1.4610\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 26/3000  (0.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 31.58it/s]\n","iter 900: train loss 1.4346, val loss 1.4418\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 45/3000  (1.50%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.76it/s]\n","iter 950: train loss 1.4450, val loss 1.4527\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 30/3000  (1.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.12it/s]\n","iter 1000: train loss 1.4315, val loss 1.4384\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 45/3000  (1.50%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.51it/s]\n","iter 1050: train loss 1.4232, val loss 1.4362\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 39/3000  (1.30%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.76it/s]\n","iter 1100: train loss 1.3581, val loss 1.3671\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 77/3000  (2.57%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.48it/s]\n","iter 1150: train loss 1.2901, val loss 1.3091\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 161/3000  (5.37%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.11it/s]\n","iter 1200: train loss 1.2726, val loss 1.2862\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 228/3000  (7.60%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 33.08it/s]\n","iter 1250: train loss 1.2602, val loss 1.2680\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 254/3000  (8.47%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.06it/s]\n","iter 1300: train loss 1.2668, val loss 1.2743\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 33.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 264/3000  (8.80%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.39it/s]\n","iter 1350: train loss 1.2563, val loss 1.2730\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 34.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 298/3000  (9.93%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.43it/s]\n","iter 1400: train loss 1.2503, val loss 1.2649\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 33.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 266/3000  (8.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.99it/s]\n","iter 1450: train loss 1.2502, val loss 1.2525\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 284/3000  (9.47%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 33.55it/s]\n","iter 1500: train loss 1.2343, val loss 1.2471\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 336/3000  (11.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 34.33it/s]\n","iter 1550: train loss 1.2340, val loss 1.2429\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 435/3000  (14.50%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.65it/s]\n","iter 1600: train loss 1.2254, val loss 1.2407\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 32.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 390/3000  (13.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.23it/s]\n","iter 1650: train loss 1.2220, val loss 1.2279\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 487/3000  (16.23%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 32.77it/s]\n","iter 1700: train loss 1.2283, val loss 1.2373\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 447/3000  (14.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.71it/s]\n","iter 1750: train loss 1.2224, val loss 1.2301\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 455/3000  (15.17%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.33it/s]\n","iter 1800: train loss 1.2203, val loss 1.2213\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 472/3000  (15.73%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.33it/s]\n","iter 1850: train loss 1.2204, val loss 1.2291\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 472/3000  (15.73%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.00it/s]\n","iter 1900: train loss 1.2274, val loss 1.2254\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 516/3000  (17.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.38it/s]\n","iter 1950: train loss 1.2163, val loss 1.2351\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 497/3000  (16.57%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.29it/s]\n","iter 2000: train loss 1.2250, val loss 1.2254\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 500/3000  (16.67%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.99it/s]\n","iter 2050: train loss 1.2193, val loss 1.2315\n","Using precomputed batches\n"," 80% 20/25 [00:00<00:00, 36.10it/s]"]}],"source":["!python train_work_perfect_per_example_batch.py 2_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"qTJ1568DCuHp"},"outputs":[],"source":["!python train.py 2_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1359876,"status":"ok","timestamp":1767589405355,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":360},"id":"3C_q2HCzg9rf","outputId":"18f7aaf1-0a36-4cde-f410-48aefbe6fb1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/2_operands_addition_plain.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 50\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '2_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '2_operands_0_to_999_uniform_plain'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='plain'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_plain.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/2_operands_0_to_999_uniform/plain_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/2_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test.txt'\n","main_test_name = \"test\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","all the unique characters: \\n, $, +, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, <pad>, =\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.63M\n","/content/drive/MyDrive/addition/train.py:583: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/2_operands_0_to_999_uniform/plain_out/wandb/run-20260105_044125-ciscy9ie\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2_operands_0_to_999_uniform_plain\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/2_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/2_op_addition/runs/ciscy9ie\u001b[0m\n","WARNING: results directory results/2_operands_0_to_999_uniform/plain_out/2_operands_0_to_999_uniform_plain already exists, overwriting...\n","max_new_tokens: 5\n","W0105 04:41:51.825000 3624 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","iter 0: train loss 2.6788, val loss 2.6888\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","100% 25/25 [00:01<00:00, 19.55it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test.txt, 3000 examples: 0/3000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/train_eval.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","100% 25/25 [00:00<00:00, 36.00it/s]\n","iter 50: train loss 1.6241, val loss 1.6345\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.90it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 0/3000  (0.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.48it/s]\n","iter 100: train loss 1.5488, val loss 1.5573\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.89it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 11/3000  (0.37%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.78it/s]\n","iter 150: train loss 1.5024, val loss 1.5086\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.83it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 14/3000  (0.47%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.28it/s]\n","iter 200: train loss 1.4671, val loss 1.4652\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.62it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 28/3000  (0.93%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.22it/s]\n","iter 250: train loss 1.3683, val loss 1.3738\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.01it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 99/3000  (3.30%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.20it/s]\n","iter 300: train loss 1.3504, val loss 1.3446\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.01it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 120/3000  (4.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.04it/s]\n","iter 350: train loss 1.3223, val loss 1.3290\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.33it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 155/3000  (5.17%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.96it/s]\n","iter 400: train loss 1.3226, val loss 1.3170\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.08it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 182/3000  (6.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.86it/s]\n","iter 450: train loss 1.3065, val loss 1.3244\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.15it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 162/3000  (5.40%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 33.98it/s]\n","iter 500: train loss 1.3120, val loss 1.3110\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.56it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 187/3000  (6.23%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.30it/s]\n","iter 550: train loss 1.2937, val loss 1.3114\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.78it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 176/3000  (5.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.86it/s]\n","iter 600: train loss 1.2880, val loss 1.3008\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.85it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 179/3000  (5.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.59it/s]\n","iter 650: train loss 1.2949, val loss 1.3001\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.50it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 184/3000  (6.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.37it/s]\n","iter 700: train loss 1.2839, val loss 1.2993\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.25it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 209/3000  (6.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.10it/s]\n","iter 750: train loss 1.2831, val loss 1.2866\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.58it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 234/3000  (7.80%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.05it/s]\n","iter 800: train loss 1.2813, val loss 1.2846\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.75it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 223/3000  (7.43%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.31it/s]\n","iter 850: train loss 1.2755, val loss 1.2862\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.11it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 240/3000  (8.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.64it/s]\n","iter 900: train loss 1.2651, val loss 1.2753\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.27it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 232/3000  (7.73%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.93it/s]\n","iter 950: train loss 1.2640, val loss 1.2812\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.94it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 237/3000  (7.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.14it/s]\n","iter 1000: train loss 1.2641, val loss 1.2771\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.69it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 255/3000  (8.50%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.42it/s]\n","iter 1050: train loss 1.2082, val loss 1.2129\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 38.19it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 643/3000  (21.43%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.74it/s]\n","iter 1100: train loss 1.1560, val loss 1.1588\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.42it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 1135/3000  (37.83%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.22it/s]\n","iter 1150: train loss 1.1340, val loss 1.1448\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.83it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 1459/3000  (48.63%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.26it/s]\n","iter 1200: train loss 1.1124, val loss 1.1136\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.24it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 1860/3000  (62.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.07it/s]\n","iter 1250: train loss 1.0996, val loss 1.1015\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.00it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2172/3000  (72.40%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.51it/s]\n","iter 1300: train loss 1.0746, val loss 1.0845\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.05it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2336/3000  (77.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.03it/s]\n","iter 1350: train loss 1.0685, val loss 1.0757\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.47it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2404/3000  (80.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.92it/s]\n","iter 1400: train loss 1.0556, val loss 1.0648\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.24it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2546/3000  (84.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.23it/s]\n","iter 1450: train loss 1.0461, val loss 1.0600\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.40it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2575/3000  (85.83%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.47it/s]\n","iter 1500: train loss 1.0429, val loss 1.0567\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.57it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2663/3000  (88.77%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.79it/s]\n","iter 1550: train loss 1.0372, val loss 1.0538\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.79it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2697/3000  (89.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.66it/s]\n","iter 1600: train loss 1.0399, val loss 1.0530\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.45it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2699/3000  (89.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.14it/s]\n","iter 1650: train loss 1.0523, val loss 1.0538\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.26it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2708/3000  (90.27%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.76it/s]\n","iter 1700: train loss 1.0422, val loss 1.0524\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.58it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2764/3000  (92.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.01it/s]\n","iter 1750: train loss 1.0381, val loss 1.0516\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.91it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2758/3000  (91.93%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.58it/s]\n","iter 1800: train loss 1.0321, val loss 1.0503\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.27it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2748/3000  (91.60%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.35it/s]\n","iter 1850: train loss 1.0370, val loss 1.0528\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.01it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2775/3000  (92.50%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 34.99it/s]\n","iter 1900: train loss 1.0347, val loss 1.0482\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.05it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2815/3000  (93.83%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.91it/s]\n","iter 1950: train loss 1.0226, val loss 1.0535\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.07it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2802/3000  (93.40%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.25it/s]\n","iter 2000: train loss 1.0207, val loss 1.0487\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.90it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2799/3000  (93.30%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.79it/s]\n","iter 2050: train loss 1.0312, val loss 1.0502\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.30it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2819/3000  (93.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.77it/s]\n","iter 2100: train loss 1.0271, val loss 1.0495\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.09it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2829/3000  (94.30%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.52it/s]\n","iter 2150: train loss 1.0272, val loss 1.0531\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.28it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2820/3000  (94.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.57it/s]\n","iter 2200: train loss 1.0141, val loss 1.0523\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.12it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2822/3000  (94.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.92it/s]\n","iter 2250: train loss 1.0218, val loss 1.0549\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.11it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2831/3000  (94.37%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.00it/s]\n","iter 2300: train loss 1.0188, val loss 1.0517\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.23it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2841/3000  (94.70%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.90it/s]\n","iter 2350: train loss 1.0175, val loss 1.0503\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.71it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2856/3000  (95.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.94it/s]\n","iter 2400: train loss 1.0181, val loss 1.0512\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.39it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2851/3000  (95.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.70it/s]\n","iter 2450: train loss 1.0212, val loss 1.0541\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.27it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2836/3000  (94.53%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.99it/s]\n","iter 2500: train loss 1.0158, val loss 1.0552\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.12it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2864/3000  (95.47%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.57it/s]\n","iter 2550: train loss 1.0132, val loss 1.0546\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.33it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2883/3000  (96.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.21it/s]\n","iter 2600: train loss 1.0037, val loss 1.0554\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.47it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2879/3000  (95.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.31it/s]\n","iter 2650: train loss 1.0049, val loss 1.0569\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.58it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2877/3000  (95.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.73it/s]\n","iter 2700: train loss 1.0097, val loss 1.0552\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.43it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2868/3000  (95.60%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.57it/s]\n","iter 2750: train loss 1.0069, val loss 1.0582\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.58it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2873/3000  (95.77%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.23it/s]\n","iter 2800: train loss 1.0063, val loss 1.0597\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.09it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2884/3000  (96.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.13it/s]\n","iter 2850: train loss 1.0010, val loss 1.0645\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.90it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2869/3000  (95.63%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.03it/s]\n","iter 2900: train loss 0.9983, val loss 1.0639\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.43it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2882/3000  (96.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.41it/s]\n","iter 2950: train loss 0.9904, val loss 1.0718\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.71it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2871/3000  (95.70%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.84it/s]\n","iter 3000: train loss 0.9933, val loss 1.0707\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.59it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2889/3000  (96.30%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.59it/s]\n","iter 3050: train loss 0.9824, val loss 1.0734\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.14it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2892/3000  (96.40%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.79it/s]\n","iter 3100: train loss 0.9856, val loss 1.0691\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.02it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2899/3000  (96.63%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.63it/s]\n","iter 3150: train loss 0.9810, val loss 1.0788\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.59it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2898/3000  (96.60%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.67it/s]\n","iter 3200: train loss 0.9722, val loss 1.0786\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.16it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2907/3000  (96.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.90it/s]\n","iter 3250: train loss 0.9719, val loss 1.0849\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.68it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2901/3000  (96.70%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.99it/s]\n","iter 3300: train loss 0.9676, val loss 1.0869\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 38.03it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2892/3000  (96.40%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.41it/s]\n","iter 3350: train loss 0.9700, val loss 1.0955\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.03it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2900/3000  (96.67%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.26it/s]\n","iter 3400: train loss 0.9650, val loss 1.0953\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.83it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2901/3000  (96.70%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.03it/s]\n","iter 3450: train loss 0.9559, val loss 1.1012\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.36it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2908/3000  (96.93%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.14it/s]\n","iter 3500: train loss 0.9530, val loss 1.1038\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.08it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2904/3000  (96.80%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.07it/s]\n","iter 3550: train loss 0.9448, val loss 1.1056\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.73it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2907/3000  (96.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.59it/s]\n","iter 3600: train loss 0.9474, val loss 1.1124\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.94it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2911/3000  (97.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.15it/s]\n","iter 3650: train loss 0.9332, val loss 1.1125\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.66it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2900/3000  (96.67%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.41it/s]\n","iter 3700: train loss 0.9305, val loss 1.1173\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.32it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2918/3000  (97.27%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.22it/s]\n","iter 3750: train loss 0.9337, val loss 1.1219\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.31it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2906/3000  (96.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.95it/s]\n","iter 3800: train loss 0.9165, val loss 1.1197\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.14it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2900/3000  (96.67%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.19it/s]\n","iter 3850: train loss 0.9174, val loss 1.1340\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.12it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2914/3000  (97.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.88it/s]\n","iter 3900: train loss 0.9225, val loss 1.1313\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.02it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2904/3000  (96.80%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.56it/s]\n","iter 3950: train loss 0.9075, val loss 1.1434\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.78it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2913/3000  (97.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.26it/s]\n","iter 4000: train loss 0.9050, val loss 1.1464\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.69it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2909/3000  (96.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.83it/s]\n","iter 4050: train loss 0.8924, val loss 1.1511\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.66it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2911/3000  (97.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.18it/s]\n","iter 4100: train loss 0.8894, val loss 1.1566\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.65it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2914/3000  (97.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.36it/s]\n","iter 4150: train loss 0.8821, val loss 1.1667\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.48it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2911/3000  (97.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.45it/s]\n","iter 4200: train loss 0.8727, val loss 1.1647\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.15it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2909/3000  (96.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.84it/s]\n","iter 4250: train loss 0.8729, val loss 1.1783\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.18it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2914/3000  (97.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.96it/s]\n","iter 4300: train loss 0.8745, val loss 1.1777\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.99it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2913/3000  (97.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.67it/s]\n","iter 4350: train loss 0.8666, val loss 1.1909\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.17it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2916/3000  (97.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.03it/s]\n","iter 4400: train loss 0.8627, val loss 1.1906\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.25it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2913/3000  (97.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.29it/s]\n","iter 4450: train loss 0.8598, val loss 1.1915\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.30it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2921/3000  (97.37%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.06it/s]\n","iter 4500: train loss 0.8506, val loss 1.1999\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.59it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2921/3000  (97.37%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.79it/s]\n","iter 4550: train loss 0.8510, val loss 1.2034\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.60it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2920/3000  (97.33%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.65it/s]\n","iter 4600: train loss 0.8357, val loss 1.2089\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.38it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2909/3000  (96.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.25it/s]\n","iter 4650: train loss 0.8468, val loss 1.2123\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.35it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2910/3000  (97.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.45it/s]\n","iter 4700: train loss 0.8436, val loss 1.2159\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 38.03it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2916/3000  (97.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.03it/s]\n","iter 4750: train loss 0.8316, val loss 1.2310\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.20it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2910/3000  (97.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.03it/s]\n","iter 4800: train loss 0.8259, val loss 1.2367\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 35.97it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2913/3000  (97.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.01it/s]\n","iter 4850: train loss 0.8171, val loss 1.2394\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.84it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2910/3000  (97.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.31it/s]\n","iter 4900: train loss 0.8148, val loss 1.2403\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.31it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2918/3000  (97.27%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.70it/s]\n","iter 4950: train loss 0.8184, val loss 1.2429\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.36it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2916/3000  (97.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.01it/s]\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 37.04it/s]\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.69it/s]\n","Using precomputed batches\n","100% 25/25 [00:00<00:00, 36.96it/s]\n","\n","Final Test Results:\n","test.txt, 3000 examples: 2912/3000  (97.07%)\n","\n","Running result analysis on results/2_operands_0_to_999_uniform/plain_out/2_operands_0_to_999_uniform_plain_1/test_results.csv\n","Saved digit error plot to results/2_operands_0_to_999_uniform/plain_out/2_operands_0_to_999_uniform_plain_1/digitwise_errors.png\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33m2_operands_0_to_999_uniform_plain\u001b[0m at: \u001b[34m\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mresults/2_operands_0_to_999_uniform/plain_out/wandb/run-20260105_044125-ciscy9ie/logs\u001b[0m\n"]}],"source":["!python train.py 2_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"JLXLrm_chIgo"},"source":["## 4 Operands"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"8S_LDDfvhGLM","outputId":"90566ef0-8b67-41fa-8d8d-9856f2fa7ba1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/addition/statistical_measurements.py:255: SyntaxWarning: invalid escape sequence '\\m'\n","  conditional distribution :math:`p(y \\mid x_i)`.\n","Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/4_operands_addition_reversed.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 2000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_0_to_999_uniform_reverse'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000 # for four operands addition, recommend >= 200000\n","lr_decay_iters = 800000 # make equal to max_iters\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/reverse_out_complete_MI_1M_lines'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: Can be 'units', 'tens', 'hundreds', 'thousands', or None\n","randomize = None\n","\n","# to edit: whether do mutual information tacking (addition supported only)\n","mi_measurement = True\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","all the unique characters: \\n, $, +, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, <pad>, =\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.63M\n","/content/drive/MyDrive/addition/train.py:556: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/reverse_out_complete_MI_1M_lines/wandb/run-20260112_165050-gk1mvdeb\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m4_operands_0_to_999_uniform_reverse\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/gk1mvdeb\u001b[0m\n","max_new_tokens: 5\n","W0112 16:56:18.909000 5418 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","iter 0: train loss 2.7106, val loss 2.7093\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:02<00:00, 27.73it/s]\n","/content/drive/MyDrive/addition/evaluation.py:567: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02<00:00, 34.74it/s]\n","iter 2000: train loss 1.6502, val loss 1.6611\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.85it/s]\n","iter 4000: train loss 1.5647, val loss 1.5651\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 75/10000  (0.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.91it/s]\n","iter 6000: train loss 1.5582, val loss 1.5639\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 36.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 85/10000  (0.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.06it/s]\n","iter 8000: train loss 1.5429, val loss 1.5459\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.85it/s]\n"]}],"source":["!python train.py 4_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0RjN0ldhMGz"},"outputs":[],"source":["!python train.py 4_operands_addition_plain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lbSWGEKuPrVZ","collapsed":true,"outputId":"77ad2281-d6fc-414c-c58b-ed37f08fb2d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/4_operands_addition_randomize_hundreds_plain.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 2000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_0_to_999_uniform_randomize_hundreds_plain'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='plain'\n","operator='+'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000 # for four operands addition, recommend >= 200000\n","lr_decay_iters = 800000 # make equal to max_iters\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_output_randomize_hundreds/plain_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_output_randomize_hundreds/'\n","\n","# to edit: training data\n","train_data_name = 'train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test.txt'\n","main_test_name = \"test\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: Can be 'units', 'tens', 'hundreds', 'thousands', or None\n","randomize = 'hundreds'\n","\n","# to edit: whether do mutual information tacking (addition supported only)\n","mi_measurement = True\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","all the unique characters: \\n, $, +, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, <pad>, =\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_randomize_hundreds/test.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.63M\n","/content/drive/MyDrive/addition/train.py:556: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_output_randomize_hundreds/plain_out/wandb/run-20260120_033810-sn95ot6n\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m4_operands_0_to_999_uniform_randomize_hundreds_plain\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/sn95ot6n\u001b[0m\n","max_new_tokens: 5\n","W0120 03:43:27.063000 3362 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","iter 0: train loss 2.6772, val loss 2.6859\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_randomize_hundreds/test.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_randomize_hundreds/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02<00:00, 28.20it/s]\n","/content/drive/MyDrive/addition/evaluation.py:567: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test.txt, 10000 examples: 5/10000  (0.05%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_randomize_hundreds/train_eval.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_output_randomize_hundreds/train_eval.txt\n","/content/drive/MyDrive/addition/evaluation.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 82 batches\n","100% 82/82 [00:02<00:00, 35.81it/s]\n","iter 2000: train loss 1.6642, val loss 1.6631\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.51it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 91/10000  (0.91%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.76it/s]\n","iter 4000: train loss 1.6613, val loss 1.6599\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.85it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 115/10000  (1.15%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.30it/s]\n","iter 6000: train loss 1.6600, val loss 1.6593\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.30it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 69/10000  (0.69%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.53it/s]\n","iter 8000: train loss 1.6634, val loss 1.6594\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.28it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 89/10000  (0.89%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.36it/s]\n","iter 10000: train loss 1.6631, val loss 1.6581\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.52it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 99/10000  (0.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.83it/s]\n","iter 12000: train loss 1.6609, val loss 1.6586\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.17it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 111/10000  (1.11%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.74it/s]\n","iter 14000: train loss 1.6633, val loss 1.6573\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.67it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 92/10000  (0.92%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.29it/s]\n","iter 16000: train loss 1.6532, val loss 1.6580\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.41it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 97/10000  (0.97%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.61it/s]\n","iter 18000: train loss 1.6632, val loss 1.6586\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.11it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 98/10000  (0.98%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.38it/s]\n","iter 20000: train loss 1.6574, val loss 1.6580\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.34it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 94/10000  (0.94%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.22it/s]\n","iter 22000: train loss 1.6626, val loss 1.6577\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.74it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 91/10000  (0.91%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.48it/s]\n","iter 24000: train loss 1.6608, val loss 1.6567\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.09it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 99/10000  (0.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.75it/s]\n","iter 26000: train loss 1.6605, val loss 1.6578\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.73it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.62it/s]\n","iter 28000: train loss 1.6618, val loss 1.6577\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.10it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.75it/s]\n","iter 30000: train loss 1.6587, val loss 1.6571\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.05it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 82/10000  (0.82%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.46it/s]\n","iter 32000: train loss 1.6616, val loss 1.6564\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.16it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 99/10000  (0.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.97it/s]\n","iter 34000: train loss 1.6562, val loss 1.6568\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.24it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.47it/s]\n","iter 36000: train loss 1.6613, val loss 1.6585\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.72it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 88/10000  (0.88%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.92it/s]\n","iter 38000: train loss 1.6569, val loss 1.6573\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.67it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.34it/s]\n","iter 40000: train loss 1.6604, val loss 1.6565\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.38it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.41it/s]\n","iter 42000: train loss 1.6590, val loss 1.6571\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.73it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 78/10000  (0.78%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.85it/s]\n","iter 44000: train loss 1.6629, val loss 1.6552\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.42it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 88/10000  (0.88%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.80it/s]\n","iter 46000: train loss 1.6612, val loss 1.6568\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.59it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 81/10000  (0.81%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.28it/s]\n","iter 48000: train loss 1.6546, val loss 1.6558\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.18it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 83/10000  (0.83%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.76it/s]\n","iter 50000: train loss 1.6586, val loss 1.6551\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.56it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 97/10000  (0.97%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.97it/s]\n","iter 52000: train loss 1.6612, val loss 1.6554\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.67it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 101/10000  (1.01%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.08it/s]\n","iter 54000: train loss 1.6598, val loss 1.6551\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.68it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 88/10000  (0.88%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.12it/s]\n","iter 56000: train loss 1.6576, val loss 1.6555\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.21it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 115/10000  (1.15%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.38it/s]\n","iter 58000: train loss 1.6555, val loss 1.6545\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.58it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 100/10000  (1.00%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.60it/s]\n","iter 60000: train loss 1.6631, val loss 1.6540\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.11it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 102/10000  (1.02%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.09it/s]\n","iter 62000: train loss 1.6559, val loss 1.6547\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.59it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 93/10000  (0.93%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.41it/s]\n","iter 64000: train loss 1.6557, val loss 1.6534\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.71it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 97/10000  (0.97%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.80it/s]\n","iter 66000: train loss 1.6611, val loss 1.6534\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.42it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 90/10000  (0.90%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.38it/s]\n","iter 68000: train loss 1.6513, val loss 1.6545\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.13it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 109/10000  (1.09%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.38it/s]\n","iter 70000: train loss 1.6570, val loss 1.6530\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.30it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 131/10000  (1.31%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.64it/s]\n","iter 72000: train loss 1.6572, val loss 1.6512\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.10it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 129/10000  (1.29%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.43it/s]\n","iter 74000: train loss 1.6490, val loss 1.6510\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.54it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 134/10000  (1.34%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.37it/s]\n","iter 76000: train loss 1.6464, val loss 1.6461\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 109/10000  (1.09%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.97it/s]\n","iter 78000: train loss 1.6400, val loss 1.6423\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.17it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 164/10000  (1.64%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.45it/s]\n","iter 80000: train loss 1.6448, val loss 1.6402\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.34it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 152/10000  (1.52%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.69it/s]\n","iter 82000: train loss 1.6360, val loss 1.6365\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.37it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 171/10000  (1.71%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.07it/s]\n","iter 84000: train loss 1.6310, val loss 1.6297\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.13it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 189/10000  (1.89%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.03it/s]\n","iter 86000: train loss 1.6251, val loss 1.6194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 255/10000  (2.55%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.44it/s]\n","iter 88000: train loss 1.6124, val loss 1.6118\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.42it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 277/10000  (2.77%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.92it/s]\n","iter 90000: train loss 1.6147, val loss 1.6091\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.66it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 296/10000  (2.96%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.02it/s]\n","iter 92000: train loss 1.6006, val loss 1.5971\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.71it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 323/10000  (3.23%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.38it/s]\n","iter 94000: train loss 1.6055, val loss 1.5977\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 337/10000  (3.37%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.27it/s]\n","iter 96000: train loss 1.5983, val loss 1.5980\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.69it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 353/10000  (3.53%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.80it/s]\n","iter 98000: train loss 1.6023, val loss 1.5995\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.64it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 374/10000  (3.74%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.09it/s]\n","iter 100000: train loss 1.5976, val loss 1.5954\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.65it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 409/10000  (4.09%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.41it/s]\n","iter 102000: train loss 1.5873, val loss 1.5867\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.73it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 422/10000  (4.22%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.02it/s]\n","iter 104000: train loss 1.5910, val loss 1.5915\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.76it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 452/10000  (4.52%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.24it/s]\n","iter 106000: train loss 1.5936, val loss 1.5836\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.98it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 469/10000  (4.69%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.38it/s]\n","iter 108000: train loss 1.5811, val loss 1.5835\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.20it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 498/10000  (4.98%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.31it/s]\n","iter 110000: train loss 1.5756, val loss 1.5785\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.86it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 500/10000  (5.00%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.19it/s]\n","iter 112000: train loss 1.5792, val loss 1.5786\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.68it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 502/10000  (5.02%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.82it/s]\n","iter 114000: train loss 1.5816, val loss 1.5794\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.86it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 491/10000  (4.91%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.10it/s]\n","iter 116000: train loss 1.5873, val loss 1.5852\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.09it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 468/10000  (4.68%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.54it/s]\n","iter 118000: train loss 1.5786, val loss 1.5817\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.70it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 487/10000  (4.87%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.29it/s]\n","iter 120000: train loss 1.5844, val loss 1.5815\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.76it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 511/10000  (5.11%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.17it/s]\n","iter 122000: train loss 1.5792, val loss 1.5814\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.31it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 489/10000  (4.89%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.23it/s]\n","iter 124000: train loss 1.5725, val loss 1.5742\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.01it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 566/10000  (5.66%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.53it/s]\n","iter 126000: train loss 1.5355, val loss 1.5337\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.45it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 1306/10000  (13.06%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 33.94it/s]\n","iter 128000: train loss 1.5317, val loss 1.5299\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.21it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 1438/10000  (14.38%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.27it/s]\n","iter 130000: train loss 1.5295, val loss 1.5207\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.12it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 1639/10000  (16.39%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.78it/s]\n","iter 132000: train loss 1.5256, val loss 1.5211\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.17it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 1599/10000  (15.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.20it/s]\n","iter 134000: train loss 1.5201, val loss 1.5179\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.05it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 1746/10000  (17.46%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.17it/s]\n","iter 136000: train loss 1.5158, val loss 1.5113\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.71it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 1907/10000  (19.07%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.40it/s]\n","iter 138000: train loss 1.5081, val loss 1.5073\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.08it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2078/10000  (20.78%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.17it/s]\n","iter 140000: train loss 1.5212, val loss 1.5177\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.60it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 1837/10000  (18.37%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.32it/s]\n","iter 142000: train loss 1.5065, val loss 1.5030\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.11it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2313/10000  (23.13%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.17it/s]\n","iter 144000: train loss 1.5100, val loss 1.5033\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.84it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2349/10000  (23.49%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 33.87it/s]\n","iter 146000: train loss 1.5084, val loss 1.5025\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.07it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2338/10000  (23.38%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.06it/s]\n","iter 148000: train loss 1.5092, val loss 1.5073\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.66it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2171/10000  (21.71%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.43it/s]\n","iter 150000: train loss 1.5018, val loss 1.4913\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.12it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2826/10000  (28.26%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.38it/s]\n","iter 152000: train loss 1.4964, val loss 1.4941\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.22it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2795/10000  (27.95%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.59it/s]\n","iter 154000: train loss 1.4974, val loss 1.5034\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.50it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2440/10000  (24.40%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.64it/s]\n","iter 156000: train loss 1.4974, val loss 1.4944\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.98it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2775/10000  (27.75%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.24it/s]\n","iter 158000: train loss 1.4916, val loss 1.4905\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.11it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2884/10000  (28.84%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.16it/s]\n","iter 160000: train loss 1.4898, val loss 1.4901\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.84it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3033/10000  (30.33%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.84it/s]\n","iter 162000: train loss 1.4955, val loss 1.4911\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.45it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3055/10000  (30.55%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.99it/s]\n","iter 164000: train loss 1.4918, val loss 1.4935\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.35it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2826/10000  (28.26%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.57it/s]\n","iter 166000: train loss 1.4944, val loss 1.4967\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.55it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3064/10000  (30.64%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.26it/s]\n","iter 168000: train loss 1.4832, val loss 1.4815\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.35it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3580/10000  (35.80%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.41it/s]\n","iter 170000: train loss 1.4959, val loss 1.4922\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.92it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3070/10000  (30.70%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.78it/s]\n","iter 172000: train loss 1.4878, val loss 1.4889\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.44it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3346/10000  (33.46%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.43it/s]\n","iter 174000: train loss 1.4848, val loss 1.4772\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.58it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3795/10000  (37.95%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.26it/s]\n","iter 176000: train loss 1.4819, val loss 1.4785\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.34it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3768/10000  (37.68%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.56it/s]\n","iter 178000: train loss 1.4886, val loss 1.4831\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.57it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3364/10000  (33.64%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.84it/s]\n","iter 180000: train loss 1.4828, val loss 1.4833\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.41it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3392/10000  (33.92%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.47it/s]\n","iter 182000: train loss 1.4900, val loss 1.4850\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.89it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3732/10000  (37.32%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.94it/s]\n","iter 184000: train loss 1.4860, val loss 1.4797\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.45it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3768/10000  (37.68%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.70it/s]\n","iter 186000: train loss 1.4811, val loss 1.4850\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.66it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3349/10000  (33.49%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.82it/s]\n","iter 188000: train loss 1.4869, val loss 1.4827\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.96it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3604/10000  (36.04%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 33.91it/s]\n","iter 190000: train loss 1.4908, val loss 1.4797\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.24it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3758/10000  (37.58%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.35it/s]\n","iter 192000: train loss 1.4738, val loss 1.4780\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.72it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3812/10000  (38.12%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.63it/s]\n","iter 194000: train loss 1.4735, val loss 1.4772\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.34it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3934/10000  (39.34%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.67it/s]\n","iter 196000: train loss 1.4899, val loss 1.4857\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.22it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3595/10000  (35.95%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.74it/s]\n","iter 198000: train loss 1.4846, val loss 1.4786\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.85it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3953/10000  (39.53%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.23it/s]\n","iter 200000: train loss 1.4856, val loss 1.4815\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.59it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3819/10000  (38.19%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.05it/s]\n","iter 202000: train loss 1.4861, val loss 1.4853\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.62it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3756/10000  (37.56%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.37it/s]\n","iter 204000: train loss 1.4749, val loss 1.4755\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.38it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3937/10000  (39.37%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.23it/s]\n","iter 206000: train loss 1.4720, val loss 1.4724\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.05it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 4389/10000  (43.89%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.18it/s]\n","iter 208000: train loss 1.4720, val loss 1.4719\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.21it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 4247/10000  (42.47%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.61it/s]\n","iter 210000: train loss 1.4369, val loss 1.4326\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.06it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 8490/10000  (84.90%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.48it/s]\n","iter 212000: train loss 1.4298, val loss 1.4296\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9064/10000  (90.64%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.45it/s]\n","iter 214000: train loss 1.4290, val loss 1.4225\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.28it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9624/10000  (96.24%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.05it/s]\n","iter 216000: train loss 1.4202, val loss 1.4210\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.08it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9734/10000  (97.34%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.89it/s]\n","iter 218000: train loss 1.4233, val loss 1.4210\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.18it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9758/10000  (97.58%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.23it/s]\n","iter 220000: train loss 1.4208, val loss 1.4205\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.10it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9859/10000  (98.59%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.60it/s]\n","iter 222000: train loss 1.4265, val loss 1.4211\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.53it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9763/10000  (97.63%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.01it/s]\n","iter 224000: train loss 1.4238, val loss 1.4198\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.06it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9831/10000  (98.31%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.38it/s]\n","iter 226000: train loss 1.4231, val loss 1.4198\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.53it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9869/10000  (98.69%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.45it/s]\n","iter 228000: train loss 1.4212, val loss 1.4197\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.65it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9885/10000  (98.85%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.70it/s]\n","iter 230000: train loss 1.4223, val loss 1.4201\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.71it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9891/10000  (98.91%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.17it/s]\n","iter 232000: train loss 1.4250, val loss 1.4209\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.79it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9825/10000  (98.25%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.32it/s]\n","iter 234000: train loss 1.4190, val loss 1.4195\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.48it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9899/10000  (98.99%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.99it/s]\n","iter 236000: train loss 1.4174, val loss 1.4198\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.42it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9906/10000  (99.06%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.55it/s]\n","iter 238000: train loss 1.4257, val loss 1.4206\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.89it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9830/10000  (98.30%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.20it/s]\n","iter 240000: train loss 1.4225, val loss 1.4194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.29it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9895/10000  (98.95%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.01it/s]\n","iter 242000: train loss 1.4218, val loss 1.4204\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.69it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9804/10000  (98.04%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.64it/s]\n","iter 244000: train loss 1.4220, val loss 1.4201\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.83it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9903/10000  (99.03%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.76it/s]\n","iter 246000: train loss 1.4274, val loss 1.4208\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 33.92it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9883/10000  (98.83%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.52it/s]\n","iter 248000: train loss 1.4215, val loss 1.4195\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.22it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9910/10000  (99.10%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.20it/s]\n","iter 250000: train loss 1.4195, val loss 1.4200\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.36it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9932/10000  (99.32%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.56it/s]\n","iter 252000: train loss 1.4164, val loss 1.4191\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.11it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9923/10000  (99.23%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.95it/s]\n","iter 254000: train loss 1.4200, val loss 1.4205\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.62it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9893/10000  (98.93%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.73it/s]\n","iter 256000: train loss 1.4204, val loss 1.4195\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.11it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9900/10000  (99.00%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.58it/s]\n","iter 258000: train loss 1.4202, val loss 1.4197\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.50it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9913/10000  (99.13%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.65it/s]\n","iter 260000: train loss 1.4263, val loss 1.4204\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.53it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9888/10000  (98.88%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.99it/s]\n","iter 262000: train loss 1.4187, val loss 1.4210\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.38it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9912/10000  (99.12%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.84it/s]\n","iter 264000: train loss 1.4223, val loss 1.4198\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.38it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9913/10000  (99.13%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.80it/s]\n","iter 266000: train loss 1.4246, val loss 1.4197\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.72it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9908/10000  (99.08%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.58it/s]\n","iter 268000: train loss 1.4228, val loss 1.4200\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.67it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9906/10000  (99.06%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.84it/s]\n","iter 270000: train loss 1.4217, val loss 1.4192\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.75it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9933/10000  (99.33%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.93it/s]\n","iter 272000: train loss 1.4215, val loss 1.4202\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.19it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9928/10000  (99.28%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.46it/s]\n","iter 274000: train loss 1.4202, val loss 1.4204\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.34it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9888/10000  (98.88%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.02it/s]\n","iter 276000: train loss 1.4210, val loss 1.4196\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.95it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9930/10000  (99.30%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.59it/s]\n","iter 278000: train loss 1.4203, val loss 1.4195\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.77it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9939/10000  (99.39%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.14it/s]\n","iter 280000: train loss 1.4182, val loss 1.4195\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.79it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9909/10000  (99.09%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.31it/s]\n","iter 282000: train loss 1.4249, val loss 1.4197\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.13it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9948/10000  (99.48%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.29it/s]\n","iter 284000: train loss 1.4170, val loss 1.4194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.17it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9923/10000  (99.23%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.40it/s]\n","iter 286000: train loss 1.4211, val loss 1.4195\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.39it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9907/10000  (99.07%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.07it/s]\n","iter 288000: train loss 1.4207, val loss 1.4199\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.34it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9876/10000  (98.76%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.57it/s]\n","iter 290000: train loss 1.4253, val loss 1.4194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.26it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9931/10000  (99.31%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.01it/s]\n","iter 292000: train loss 1.4236, val loss 1.4197\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.48it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9902/10000  (99.02%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.93it/s]\n","iter 294000: train loss 1.4222, val loss 1.4193\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.59it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9890/10000  (98.90%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.56it/s]\n","iter 296000: train loss 1.4229, val loss 1.4192\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.36it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9929/10000  (99.29%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.89it/s]\n","iter 298000: train loss 1.4196, val loss 1.4198\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.51it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9951/10000  (99.51%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.59it/s]\n","iter 300000: train loss 1.4259, val loss 1.4212\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.93it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9872/10000  (98.72%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.23it/s]\n","iter 302000: train loss 1.4247, val loss 1.4194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.79it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9921/10000  (99.21%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.11it/s]\n","iter 304000: train loss 1.4281, val loss 1.4194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9942/10000  (99.42%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.32it/s]\n","iter 306000: train loss 1.4226, val loss 1.4200\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.55it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9933/10000  (99.33%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.12it/s]\n","iter 308000: train loss 1.4169, val loss 1.4191\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.56it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9950/10000  (99.50%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.06it/s]\n","iter 310000: train loss 1.4213, val loss 1.4201\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.24it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9923/10000  (99.23%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.29it/s]\n","iter 312000: train loss 1.4208, val loss 1.4194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9895/10000  (98.95%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.65it/s]\n","iter 314000: train loss 1.4203, val loss 1.4198\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.47it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9925/10000  (99.25%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.92it/s]\n","iter 316000: train loss 1.4202, val loss 1.4199\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.15it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9942/10000  (99.42%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.63it/s]\n","iter 318000: train loss 1.4240, val loss 1.4196\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.96it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9933/10000  (99.33%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.36it/s]\n","iter 320000: train loss 1.4162, val loss 1.4196\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.21it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9937/10000  (99.37%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.20it/s]\n","iter 322000: train loss 1.4203, val loss 1.4188\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.50it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9940/10000  (99.40%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.34it/s]\n","iter 324000: train loss 1.4213, val loss 1.4195\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.94it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9917/10000  (99.17%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.63it/s]\n","iter 326000: train loss 1.4203, val loss 1.4194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.49it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9943/10000  (99.43%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.46it/s]\n","iter 328000: train loss 1.4214, val loss 1.4202\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.41it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9929/10000  (99.29%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.12it/s]\n","iter 330000: train loss 1.4222, val loss 1.4193\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.33it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9945/10000  (99.45%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.66it/s]\n","iter 332000: train loss 1.4218, val loss 1.4192\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.07it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9935/10000  (99.35%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.07it/s]\n","iter 334000: train loss 1.4178, val loss 1.4192\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.99it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9892/10000  (98.92%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.27it/s]\n","iter 336000: train loss 1.4227, val loss 1.4193\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.19it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9955/10000  (99.55%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.57it/s]\n","iter 338000: train loss 1.4219, val loss 1.4188\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.11it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9952/10000  (99.52%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.32it/s]\n","iter 340000: train loss 1.4260, val loss 1.4193\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.69it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9928/10000  (99.28%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.55it/s]\n","iter 342000: train loss 1.4209, val loss 1.4198\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.36it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9944/10000  (99.44%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.08it/s]\n","iter 344000: train loss 1.4262, val loss 1.4194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.06it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9939/10000  (99.39%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.44it/s]\n","iter 346000: train loss 1.4209, val loss 1.4195\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.17it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9936/10000  (99.36%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.22it/s]\n","iter 348000: train loss 1.4246, val loss 1.4189\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.39it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9961/10000  (99.61%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.71it/s]\n","iter 350000: train loss 1.4236, val loss 1.4191\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.97it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9945/10000  (99.45%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.01it/s]\n","iter 352000: train loss 1.4185, val loss 1.4192\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.69it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9953/10000  (99.53%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.67it/s]\n","iter 354000: train loss 1.4208, val loss 1.4193\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9941/10000  (99.41%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.39it/s]\n","iter 356000: train loss 1.4246, val loss 1.4192\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.11it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9934/10000  (99.34%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.88it/s]\n","iter 358000: train loss 1.4218, val loss 1.4195\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.60it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9943/10000  (99.43%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.46it/s]\n","iter 360000: train loss 1.4224, val loss 1.4189\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.47it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9944/10000  (99.44%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.98it/s]\n","iter 362000: train loss 1.4197, val loss 1.4190\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.46it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9941/10000  (99.41%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.18it/s]\n","iter 364000: train loss 1.4169, val loss 1.4197\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.08it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9925/10000  (99.25%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.29it/s]\n","iter 366000: train loss 1.4215, val loss 1.4188\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.56it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9944/10000  (99.44%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.01it/s]\n","iter 368000: train loss 1.4258, val loss 1.4194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.95it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9950/10000  (99.50%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.99it/s]\n","iter 370000: train loss 1.4183, val loss 1.4192\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.60it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9943/10000  (99.43%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.12it/s]\n","iter 372000: train loss 1.4202, val loss 1.4193\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.65it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9943/10000  (99.43%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.18it/s]\n","iter 374000: train loss 1.4243, val loss 1.4193\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.57it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9928/10000  (99.28%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.86it/s]\n","iter 376000: train loss 1.4230, val loss 1.4193\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.38it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9952/10000  (99.52%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.33it/s]\n","iter 378000: train loss 1.4197, val loss 1.4192\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.53it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9957/10000  (99.57%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.36it/s]\n","iter 380000: train loss 1.4174, val loss 1.4192\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.66it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9946/10000  (99.46%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 34.77it/s]\n","iter 382000: train loss 1.4229, val loss 1.4196\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.48it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9953/10000  (99.53%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.42it/s]\n","iter 384000: train loss 1.4232, val loss 1.4192\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.17it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9951/10000  (99.51%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.18it/s]\n","iter 386000: train loss 1.4239, val loss 1.4197\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.74it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9943/10000  (99.43%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.08it/s]\n","iter 388000: train loss 1.4220, val loss 1.4195\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.07it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9961/10000  (99.61%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.90it/s]\n","iter 390000: train loss 1.4251, val loss 1.4201\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.73it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9879/10000  (98.79%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.69it/s]\n","iter 392000: train loss 1.4273, val loss 1.4203\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.61it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9894/10000  (98.94%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.63it/s]\n","iter 394000: train loss 1.4215, val loss 1.4194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.83it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9951/10000  (99.51%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.64it/s]\n","iter 396000: train loss 1.4283, val loss 1.4194\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.19it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9951/10000  (99.51%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.69it/s]\n","iter 398000: train loss 1.4198, val loss 1.4191\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.40it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9961/10000  (99.61%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 35.85it/s]\n","iter 400000: train loss 1.4216, val loss 1.4191\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.74it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9955/10000  (99.55%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.90it/s]\n","iter 402000: train loss 1.4225, val loss 1.4197\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.65it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9963/10000  (99.63%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.70it/s]\n","iter 404000: train loss 1.4235, val loss 1.4192\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.87it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9956/10000  (99.56%)\n","\n","Using precomputed batches\n","100% 82/82 [00:02<00:00, 36.67it/s]\n"]}],"source":["!python train.py 4_operands_addition_perm_3412.txt"]},{"cell_type":"markdown","metadata":{"id":"QDxgYE9zhL02"},"source":["## Multiplication"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNcrd8bDha9j"},"outputs":[],"source":["!python train.py 2_operands_mul_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nn2GYDQhX7b"},"outputs":[],"source":["!python train.py 2_operands_mul_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"asYxDysPKOzI"},"source":["# Other Commands (May not be fully working)"]},{"cell_type":"markdown","metadata":{"id":"L0ttXoH9PGRV"},"source":["## 4 Operand Addition Scratchpad"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3F4qYRSCQZhE"},"outputs":[],"source":["%cat configuration_files/4_operands_addition_scratchpad.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ZEiUegj1UDm-"},"outputs":[],"source":["%cat evaluation.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"hGSNQOkyPFQF"},"outputs":[],"source":["!python train.py 4_operands_addition_scratchpad.txt"]},{"cell_type":"markdown","metadata":{"id":"dfLn10UAHrM2"},"source":["## 4 Operand Max"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"PFYzFx4vK6Px"},"outputs":[],"source":["%cat configuration_files/4_operands_max.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"CEGW4g1dLgnQ"},"outputs":[],"source":["%cat train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"SSG2giBcf0u9"},"outputs":[],"source":["%cat model.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"X2hV4DOEHtrh"},"outputs":[],"source":["!python train.py 4_operands_max.txt"]},{"cell_type":"markdown","metadata":{"id":"7UHO3zBc_jY3"},"source":["## 4 Operand Sorting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXiwk0EUyudt"},"outputs":[],"source":["%cat configuration_files/4_operands_sorting.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"elF2BaDD_izl","outputId":"8787b30f-51bc-486b-9f6c-c740642697fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/4_operands_sorting_doubly_bal.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 100\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'sorting_doubly_balanced_subskill_learning_order_dense_eval'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='sorting'\n","operator=''\n","batch_size = 512\n","block_size = 64 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: num of digits in each operand\n","num_digit = 4\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 32\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 10000\n","lr_decay_iters = 10000 # make equal to max_iters\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_sorting_doubly_balanced/subskill_learning_order_dense_eval'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_sorting_doubly_balanced/'\n","\n","# to edit: training data\n","train_data_name = 'train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# If it's a directory, \"main_test_name\" is the name of the test file (without extension) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test'\n","main_test_name = \"test\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: whether do mutual information tacking (addition supported only)\n","mi_measurement = False\n","more_early_eval1 = True \n","early_eval_interval1 = 10\n","early_eval_border1 = 800\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","all the unique characters: \\n, $, ,, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, <pad>, =\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_hundred.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_random.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_ten.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_thousand.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/test.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.65M\n","/content/drive/MyDrive/addition/train.py:556: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_sorting_doubly_balanced/subskill_learning_order_dense_eval/wandb/run-20260111_181457-vy5s6sow\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msorting_doubly_balanced_subskill_learning_order_dense_eval\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/vy5s6sow\u001b[0m\n","max_new_tokens: 32\n","W0111 18:15:10.663000 1824 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","iter 0: train loss 2.7141, val loss 2.7098\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_hundred.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_hundred.txt\n","/content/drive/MyDrive/addition/evaluation.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:10<00:00,  2.37it/s]\n","/content/drive/MyDrive/addition/evaluation.py:567: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_random.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_random.txt\n","/content/drive/MyDrive/addition/evaluation.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:10<00:00,  2.39it/s]\n","/content/drive/MyDrive/addition/evaluation.py:567: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_ten.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_ten.txt\n","/content/drive/MyDrive/addition/evaluation.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:09<00:00,  2.47it/s]\n","/content/drive/MyDrive/addition/evaluation.py:567: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_thousand.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_thousand.txt\n","/content/drive/MyDrive/addition/evaluation.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:09<00:00,  2.49it/s]\n","/content/drive/MyDrive/addition/evaluation.py:567: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/test.txt\n","Preparing batches for 5000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 41 batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","/content/drive/MyDrive/addition/evaluation.py:567: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test.txt, 5000 examples: 0/5000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/train_eval.txt\n","Preparing batches for 5000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/train_eval.txt\n","/content/drive/MyDrive/addition/evaluation.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 41 batches\n","100% 41/41 [00:14<00:00,  2.78it/s]\n","iter 10: train loss 2.0428, val loss 2.0281\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 0/5000  (0.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","iter 20: train loss 1.8702, val loss 1.8635\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 0/5000  (0.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 30: train loss 1.7247, val loss 1.7019\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 0/5000  (0.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 40: train loss 1.5848, val loss 1.5801\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 0/5000  (0.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 50: train loss 1.4863, val loss 1.4744\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 0/5000  (0.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 60: train loss 1.4279, val loss 1.4165\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 0/5000  (0.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","iter 70: train loss 1.3659, val loss 1.3404\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 0/5000  (0.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 80: train loss 1.3139, val loss 1.2666\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 1/5000  (0.02%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 90: train loss 1.2237, val loss 1.2097\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 5/5000  (0.10%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.73it/s]\n","iter 100: train loss 1.1775, val loss 1.1771\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 7/5000  (0.14%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 110: train loss 1.1489, val loss 1.1401\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 12/5000  (0.24%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 120: train loss 1.1424, val loss 1.1148\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 14/5000  (0.28%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 130: train loss 1.0816, val loss 1.0825\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 17/5000  (0.34%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 140: train loss 1.0961, val loss 1.0531\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 30/5000  (0.60%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 150: train loss 1.0214, val loss 1.0236\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.72it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 51/5000  (1.02%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 160: train loss 1.0157, val loss 1.0038\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.72it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 107/5000  (2.14%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","iter 170: train loss 0.9465, val loss 0.9463\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 207/5000  (4.14%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.73it/s]\n","iter 180: train loss 0.8818, val loss 0.8741\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.41it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.72it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 736/5000  (14.72%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.73it/s]\n","iter 190: train loss 0.8495, val loss 0.8379\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 1252/5000  (25.04%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","iter 200: train loss 0.8454, val loss 0.8258\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 1525/5000  (30.50%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 210: train loss 0.8477, val loss 0.8158\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 2000/5000  (40.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 220: train loss 0.8384, val loss 0.8004\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 2354/5000  (47.08%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","iter 230: train loss 0.8003, val loss 0.7971\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 2700/5000  (54.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 240: train loss 0.8081, val loss 0.7976\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 2646/5000  (52.92%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 250: train loss 0.8030, val loss 0.7908\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3051/5000  (61.02%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 260: train loss 0.7933, val loss 0.7869\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3062/5000  (61.24%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 270: train loss 0.8062, val loss 0.7843\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3301/5000  (66.02%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 280: train loss 0.8057, val loss 0.7824\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3353/5000  (67.06%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 290: train loss 0.7807, val loss 0.7780\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3663/5000  (73.26%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.73it/s]\n","iter 300: train loss 0.8036, val loss 0.7821\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3496/5000  (69.92%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 310: train loss 0.7891, val loss 0.7801\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3567/5000  (71.34%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 320: train loss 0.7757, val loss 0.7783\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3682/5000  (73.64%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","iter 330: train loss 0.7944, val loss 0.7793\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3650/5000  (73.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 340: train loss 0.7974, val loss 0.7802\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3651/5000  (73.02%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 350: train loss 0.7807, val loss 0.7756\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3823/5000  (76.46%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 360: train loss 0.7816, val loss 0.7769\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3785/5000  (75.70%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 370: train loss 0.7886, val loss 0.7732\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3971/5000  (79.42%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 380: train loss 0.7801, val loss 0.7732\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4011/5000  (80.22%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 390: train loss 0.7864, val loss 0.7740\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3926/5000  (78.52%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.76it/s]\n","iter 400: train loss 0.7856, val loss 0.7752\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:10<00:00,  2.33it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3819/5000  (76.38%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 410: train loss 0.7763, val loss 0.7731\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4050/5000  (81.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 420: train loss 0.8037, val loss 0.7738\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4016/5000  (80.32%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 430: train loss 0.7936, val loss 0.7728\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3966/5000  (79.32%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 440: train loss 0.7913, val loss 0.7722\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4092/5000  (81.84%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.72it/s]\n","iter 450: train loss 0.7904, val loss 0.7703\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4205/5000  (84.10%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 460: train loss 0.7819, val loss 0.7719\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4181/5000  (83.62%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 470: train loss 0.7903, val loss 0.7706\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4256/5000  (85.12%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 480: train loss 0.7827, val loss 0.7702\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4062/5000  (81.24%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 490: train loss 0.7844, val loss 0.7705\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4122/5000  (82.44%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 500: train loss 0.7811, val loss 0.7694\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4211/5000  (84.22%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 510: train loss 0.7897, val loss 0.7676\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4212/5000  (84.24%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 520: train loss 0.7739, val loss 0.7688\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.72it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4195/5000  (83.90%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 530: train loss 0.7879, val loss 0.7706\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4263/5000  (85.26%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","iter 540: train loss 0.7627, val loss 0.7678\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4247/5000  (84.94%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 550: train loss 0.7977, val loss 0.7693\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4286/5000  (85.72%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 560: train loss 0.7763, val loss 0.7671\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4293/5000  (85.86%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 570: train loss 0.7899, val loss 0.7678\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4315/5000  (86.30%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 580: train loss 0.7806, val loss 0.7669\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4286/5000  (85.72%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 590: train loss 0.7763, val loss 0.7668\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4336/5000  (86.72%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 600: train loss 0.7800, val loss 0.7672\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4271/5000  (85.42%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","iter 610: train loss 0.7693, val loss 0.7665\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4416/5000  (88.32%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 620: train loss 0.7706, val loss 0.7654\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4421/5000  (88.42%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 630: train loss 0.7845, val loss 0.7657\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4398/5000  (87.96%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 640: train loss 0.7620, val loss 0.7667\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4409/5000  (88.18%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 650: train loss 0.7807, val loss 0.7666\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4445/5000  (88.90%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.73it/s]\n","iter 660: train loss 0.7759, val loss 0.7631\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4509/5000  (90.18%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 670: train loss 0.7797, val loss 0.7641\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4513/5000  (90.26%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 680: train loss 0.7768, val loss 0.7639\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4474/5000  (89.48%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 690: train loss 0.7654, val loss 0.7659\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4412/5000  (88.24%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 700: train loss 0.7751, val loss 0.7656\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4467/5000  (89.34%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 710: train loss 0.7768, val loss 0.7644\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4490/5000  (89.80%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 720: train loss 0.7724, val loss 0.7657\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4483/5000  (89.66%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 730: train loss 0.7700, val loss 0.7633\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4505/5000  (90.10%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 740: train loss 0.7885, val loss 0.7633\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4597/5000  (91.94%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 750: train loss 0.7804, val loss 0.7643\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4442/5000  (88.84%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 760: train loss 0.7740, val loss 0.7634\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4541/5000  (90.82%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 770: train loss 0.7594, val loss 0.7634\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4543/5000  (90.86%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 780: train loss 0.7739, val loss 0.7627\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4544/5000  (90.88%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.72it/s]\n","iter 790: train loss 0.7757, val loss 0.7656\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4509/5000  (90.18%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 800: train loss 0.7851, val loss 0.7633\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4558/5000  (91.16%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","iter 900: train loss 0.7851, val loss 0.7622\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4622/5000  (92.44%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","iter 1000: train loss 0.7691, val loss 0.7619\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.41it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.72it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4624/5000  (92.48%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.72it/s]\n","iter 1100: train loss 0.7787, val loss 0.7609\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4687/5000  (93.74%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 1200: train loss 0.7778, val loss 0.7601\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4702/5000  (94.04%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 1300: train loss 0.7703, val loss 0.7615\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.42it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4734/5000  (94.68%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.72it/s]\n","iter 1400: train loss 0.7757, val loss 0.7600\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4705/5000  (94.10%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 1500: train loss 0.7689, val loss 0.7603\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4717/5000  (94.34%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15<00:00,  2.73it/s]\n","iter 1600: train loss 0.7652, val loss 0.7610\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4720/5000  (94.40%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.75it/s]\n","iter 1700: train loss 0.7623, val loss 0.7607\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4760/5000  (95.20%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14<00:00,  2.74it/s]\n","iter 1800: train loss 0.7611, val loss 0.7612\n","Using precomputed batches\n","100% 24/24 [00:09<00:00,  2.43it/s]\n","Using precomputed batches\n"," 83% 20/24 [00:08<00:01,  2.37it/s]"]}],"source":["!python train.py 4_operands_sorting_doubly_bal.txt"]},{"cell_type":"markdown","metadata":{"id":"hdumgWyAWlt6"},"source":["## Slicing -- Addition"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"b-jVt7X7Llry"},"outputs":[],"source":["!python train.py slicing_addition_2_operand.txt --batch slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"pdPiOKoWKw0q","outputId":"b03642c5-8328-466c-96ba-2aef5d654f06"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: slicing\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/slicing_addition_4_operand_plain.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'slicing_4_operand_plain_test'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='plain'\n","operator='+'\n","batch_size = 256\n","block_size = 512\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000\n","lr_decay_iters = 800000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/slicing_plain_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test.txt'\n","main_test_name = \"test\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 97\n","Using vocabulary size: 97\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test.txt\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.82M\n","/content/drive/MyDrive/addition/train.py:509: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/slicing_plain_out/wandb/run-20251226_204127-omorak6d\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mslicing_4_operand_plain_test\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/omorak6d\u001b[0m\n","max_new_tokens: 5\n","iter 0: train loss 4.3121, val loss 4.3124\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:03<00:00, 26.54it/s]\n","/content/drive/MyDrive/addition/evaluation.py:401: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02<00:00, 33.68it/s]\n","iter 1000: train loss 1.5218, val loss 1.5212\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.95it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.87it/s]\n","iter 2000: train loss 1.4745, val loss 1.4765\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.55it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 54/10000  (0.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.18it/s]\n","iter 3000: train loss 1.4484, val loss 1.4496\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.39it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 73/10000  (0.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.35it/s]\n","iter 4000: train loss 1.4414, val loss 1.4445\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.67it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 55/10000  (0.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.81it/s]\n","iter 5000: train loss 1.4132, val loss 1.4144\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.87it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 179/10000  (1.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.32it/s]\n","iter 6000: train loss 1.4097, val loss 1.4116\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.28it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 230/10000  (2.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.96it/s]\n","iter 7000: train loss 1.4081, val loss 1.4095\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.71it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 238/10000  (2.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.04it/s]\n","iter 8000: train loss 1.3964, val loss 1.3960\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.56it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 298/10000  (2.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.16it/s]\n","iter 9000: train loss 1.3834, val loss 1.3856\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 453/10000  (4.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.46it/s]\n","iter 10000: train loss 1.3645, val loss 1.3648\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.37it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 675/10000  (6.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.75it/s]\n","iter 11000: train loss 1.3553, val loss 1.3549\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.15it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 797/10000  (7.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.44it/s]\n","iter 12000: train loss 1.3249, val loss 1.3230\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.47it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 1656/10000  (16.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.96it/s]\n","iter 13000: train loss 1.3163, val loss 1.3153\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.43it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2115/10000  (21.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.87it/s]\n","iter 14000: train loss 1.3157, val loss 1.3169\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.84it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2140/10000  (21.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.38it/s]\n","iter 15000: train loss 1.3090, val loss 1.3094\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.37it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2526/10000  (25.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.13it/s]\n","iter 16000: train loss 1.3011, val loss 1.3008\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.77it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2943/10000  (29.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.48it/s]\n","iter 17000: train loss 1.2950, val loss 1.2962\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.78it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3584/10000  (35.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.29it/s]\n","iter 18000: train loss 1.2575, val loss 1.2575\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.14it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 7937/10000  (79.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.28it/s]\n","iter 19000: train loss 1.2444, val loss 1.2459\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.95it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9627/10000  (96.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.76it/s]\n","iter 20000: train loss 1.2429, val loss 1.2438\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.94it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9821/10000  (98.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.15it/s]\n","iter 21000: train loss 1.2419, val loss 1.2417\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.73it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9968/10000  (99.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.24it/s]\n","iter 22000: train loss 1.2425, val loss 1.2420\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.08it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9940/10000  (99.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.44it/s]\n","iter 23000: train loss 1.2412, val loss 1.2416\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.87it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9983/10000  (99.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.75it/s]\n","iter 24000: train loss 1.2416, val loss 1.2420\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.59it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9988/10000  (99.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.64it/s]\n","iter 25000: train loss 1.2407, val loss 1.2404\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.23it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9993/10000  (99.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.71it/s]\n","iter 26000: train loss 1.2413, val loss 1.2421\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.96it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9980/10000  (99.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.93it/s]\n","iter 27000: train loss 1.2407, val loss 1.2419\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.59it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9987/10000  (99.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.76it/s]\n","iter 28000: train loss 1.2407, val loss 1.2415\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.78it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.56it/s]\n","iter 29000: train loss 1.2403, val loss 1.2420\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.09it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9976/10000  (99.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.80it/s]\n","iter 30000: train loss 1.2401, val loss 1.2435\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.21it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.80it/s]\n","iter 31000: train loss 1.2389, val loss 1.2418\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.11it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.74it/s]\n","iter 32000: train loss 1.2382, val loss 1.2421\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.71it/s]\n","iter 33000: train loss 1.2373, val loss 1.2441\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.56it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.28it/s]\n","iter 34000: train loss 1.2357, val loss 1.2449\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.87it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.24it/s]\n","iter 35000: train loss 1.2366, val loss 1.2447\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.80it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.60it/s]\n","iter 36000: train loss 1.2340, val loss 1.2471\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.06it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.25it/s]\n","iter 37000: train loss 1.2317, val loss 1.2495\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.21it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9986/10000  (99.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.45it/s]\n","iter 38000: train loss 1.2309, val loss 1.2504\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.07it/s]\n","iter 39000: train loss 1.2283, val loss 1.2515\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.09it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.17it/s]\n","iter 40000: train loss 1.2251, val loss 1.2539\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.62it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9993/10000  (99.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.05it/s]\n","iter 41000: train loss 1.2221, val loss 1.2556\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9991/10000  (99.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.35it/s]\n","iter 42000: train loss 1.2195, val loss 1.2587\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.95it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.87it/s]\n","iter 43000: train loss 1.2174, val loss 1.2566\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.12it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.90it/s]\n","iter 44000: train loss 1.2142, val loss 1.2593\n","Using precomputed batches\n"," 20% 16/80 [00:00<00:01, 34.81it/s]"]}],"source":["!python train.py slicing_addition_4_operand_plain.txt --batch slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"zqlR08suC-Na","outputId":"16bb6284-36be-42ce-8a55-2c626410f15b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: slicing\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/slicing_addition_4_operand_reverse.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'slicing_4_operand_reversed_debugging'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","batch_size = 256\n","block_size = 512\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000\n","lr_decay_iters = 800000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/slicing_4_operand_reversed_debugging'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","all the unique characters: \\n,  , $, +, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, =\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.82M\n","/content/drive/MyDrive/addition/train.py:590: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/slicing_4_operand_reversed_debugging/wandb/run-20260104_024133-b5ylcp94\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mslicing_4_operand_reversed_debugging\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/b5ylcp94\u001b[0m\n","WARNING: results directory results/4_operands_0_to_999_uniform/slicing_4_operand_reversed_debugging/slicing_4_operand_reversed_debugging already exists, overwriting...\n","max_new_tokens: 5\n","W0104 02:42:02.580000 2535 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","iter 0: train loss 2.6918, val loss 2.6956\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:02<00:00, 28.20it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02<00:00, 35.06it/s]\n","iter 1000: train loss 1.5407, val loss 1.5391\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.61it/s]\n","iter 2000: train loss 1.5019, val loss 1.5021\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 23/10000  (0.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.71it/s]\n","iter 3000: train loss 1.4639, val loss 1.4642\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 42/10000  (0.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.59it/s]\n","iter 4000: train loss 1.4496, val loss 1.4493\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 56/10000  (0.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.13it/s]\n","iter 5000: train loss 1.4425, val loss 1.4410\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 84/10000  (0.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.89it/s]\n","iter 6000: train loss 1.4404, val loss 1.4410\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 97/10000  (0.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.27it/s]\n","iter 7000: train loss 1.4410, val loss 1.4406\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.39it/s]\n","iter 8000: train loss 1.4411, val loss 1.4395\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 106/10000  (1.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.31it/s]\n","iter 9000: train loss 1.4420, val loss 1.4400\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 106/10000  (1.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.69it/s]\n","iter 10000: train loss 1.4426, val loss 1.4441\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 102/10000  (1.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.56it/s]\n","iter 11000: train loss 1.4460, val loss 1.4475\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 107/10000  (1.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.74it/s]\n","iter 12000: train loss 1.4541, val loss 1.4553\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 99/10000  (0.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.09it/s]\n","iter 13000: train loss 1.4515, val loss 1.4522\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 95/10000  (0.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.19it/s]\n","iter 14000: train loss 1.4918, val loss 1.4910\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 89/10000  (0.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.28it/s]\n","iter 15000: train loss 1.4902, val loss 1.4907\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 100/10000  (1.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.65it/s]\n","iter 16000: train loss 1.5102, val loss 1.5104\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 102/10000  (1.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.68it/s]\n","iter 17000: train loss 1.5173, val loss 1.5181\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 98/10000  (0.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.63it/s]\n","iter 18000: train loss 1.4816, val loss 1.4827\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 89/10000  (0.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.43it/s]\n","iter 19000: train loss 1.5019, val loss 1.5064\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 101/10000  (1.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.11it/s]\n","iter 20000: train loss 1.7082, val loss 1.7043\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 72/10000  (0.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.67it/s]\n","iter 21000: train loss 1.4708, val loss 1.4709\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.92it/s]\n","iter 22000: train loss 1.4568, val loss 1.4580\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 108/10000  (1.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.64it/s]\n","iter 23000: train loss 1.4635, val loss 1.4658\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 115/10000  (1.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.51it/s]\n","iter 24000: train loss 1.9357, val loss 1.9328\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 99/10000  (0.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.28it/s]\n","iter 25000: train loss 1.5152, val loss 1.5185\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 102/10000  (1.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.47it/s]\n","iter 26000: train loss 1.4771, val loss 1.4767\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 69/10000  (0.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.87it/s]\n","iter 27000: train loss 4.2519, val loss 4.2521\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 99/10000  (0.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.86it/s]\n","iter 28000: train loss 1.7972, val loss 1.8079\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.22it/s]\n","iter 29000: train loss 2.6684, val loss 2.6596\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 13/10000  (0.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.01it/s]\n","iter 30000: train loss 2.4105, val loss 2.3816\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.76it/s]\n","iter 31000: train loss 1.6203, val loss 1.6188\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.42it/s]\n","iter 32000: train loss 1.7031, val loss 1.6934\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.25it/s]\n","iter 33000: train loss 2.1830, val loss 2.1976\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.74it/s]\n","iter 34000: train loss 1.6095, val loss 1.6180\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 14/10000  (0.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.29it/s]\n","iter 35000: train loss 4.7672, val loss 4.7469\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.97it/s]\n","iter 36000: train loss 2.2512, val loss 2.2257\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 4/10000  (0.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.69it/s]\n","iter 37000: train loss 2.7318, val loss 2.7113\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 5/10000  (0.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.42it/s]\n","iter 38000: train loss 1.6980, val loss 1.7019\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 5/10000  (0.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.16it/s]\n","iter 39000: train loss 1.6002, val loss 1.5981\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 13/10000  (0.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.82it/s]\n","iter 40000: train loss 5.8642, val loss 5.7381\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.92it/s]\n","iter 41000: train loss 1.7379, val loss 1.7296\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 16/10000  (0.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.46it/s]\n","iter 42000: train loss 3.1965, val loss 3.2533\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 5/10000  (0.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.87it/s]\n","iter 43000: train loss 10.3939, val loss 10.4891\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.16it/s]\n","iter 44000: train loss 1.9121, val loss 1.8985\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.07it/s]\n","iter 45000: train loss 53.3369, val loss 53.2971\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.98it/s]\n","iter 46000: train loss 4.4228, val loss 4.3888\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 13/10000  (0.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.92it/s]\n","iter 47000: train loss 1.7394, val loss 1.7416\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 17/10000  (0.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.18it/s]\n","iter 48000: train loss 12.2844, val loss 11.9358\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.72it/s]\n","iter 49000: train loss 24.2238, val loss 24.3381\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.37it/s]\n","iter 50000: train loss 1.9491, val loss 1.9627\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 3/10000  (0.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.67it/s]\n","iter 51000: train loss 1.8058, val loss 1.7951\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.29it/s]\n","iter 52000: train loss 1.9174, val loss 1.9392\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1/10000  (0.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.74it/s]\n","iter 53000: train loss 18.1640, val loss 18.0815\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.52it/s]\n","iter 54000: train loss 1.9621, val loss 1.9287\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.05it/s]\n","iter 55000: train loss 2.0110, val loss 1.9744\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.05it/s]\n","iter 56000: train loss 11.5790, val loss 11.6237\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.71it/s]\n","iter 57000: train loss 9.4991, val loss 9.4997\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.13it/s]\n","iter 58000: train loss 2.2658, val loss 2.2790\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.02it/s]\n","iter 59000: train loss 2.1413, val loss 2.2014\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 14/10000  (0.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.24it/s]\n","iter 60000: train loss 9.5259, val loss 9.4311\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 2/10000  (0.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.90it/s]\n","iter 61000: train loss 4.9433, val loss 4.8994\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.83it/s]\n","iter 62000: train loss 2.6514, val loss 2.6215\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.93it/s]\n","iter 63000: train loss 15.8749, val loss 15.8809\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 15/10000  (0.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.13it/s]\n","iter 64000: train loss 1.8158, val loss 1.8357\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1/10000  (0.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.37it/s]\n","iter 65000: train loss 10.6230, val loss 10.6212\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 15/10000  (0.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.02it/s]\n","iter 66000: train loss 1.9804, val loss 1.9503\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.92it/s]\n","iter 67000: train loss 11.8050, val loss 11.9251\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 36.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 4/10000  (0.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.48it/s]\n","iter 68000: train loss 3.0503, val loss 2.9694\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 2/10000  (0.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.30it/s]\n","iter 69000: train loss 5.2822, val loss 5.1259\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.72it/s]\n","iter 70000: train loss 4.2575, val loss 4.2125\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.59it/s]\n","iter 71000: train loss 2.7984, val loss 2.8292\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.62it/s]\n","iter 72000: train loss 2.8375, val loss 3.0798\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.01it/s]\n","iter 73000: train loss 27.0033, val loss 26.7539\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.50it/s]\n","iter 74000: train loss 2.9947, val loss 3.1106\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 4/10000  (0.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.73it/s]\n","iter 75000: train loss 3.0464, val loss 3.0967\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.06it/s]\n","iter 76000: train loss 2.8617, val loss 2.8768\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.46it/s]\n","iter 77000: train loss 2.9212, val loss 2.8436\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.42it/s]\n","iter 78000: train loss 7.9902, val loss 8.1756\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 3/10000  (0.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.84it/s]\n","iter 79000: train loss 8.7554, val loss 8.9175\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 2/10000  (0.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.33it/s]\n","iter 80000: train loss 41.4864, val loss 42.1651\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.18it/s]\n","iter 81000: train loss 34.9251, val loss 35.2725\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.27it/s]\n","iter 82000: train loss 3.9256, val loss 4.1376\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.70it/s]\n","iter 83000: train loss 41.7062, val loss 41.2735\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.84it/s]\n","iter 84000: train loss 34.6754, val loss 34.9752\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.31it/s]\n","iter 85000: train loss 8.7134, val loss 8.2566\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.58it/s]\n","iter 86000: train loss 156.6946, val loss 156.6878\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.77it/s]\n","iter 87000: train loss 116.4044, val loss 115.1710\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 15/10000  (0.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.77it/s]\n","iter 88000: train loss 61.8833, val loss 59.9917\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 36.07it/s]\n","iter 89000: train loss 96.0434, val loss 96.1241\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.84it/s]\n","iter 90000: train loss 151.7155, val loss 153.1028\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 5/10000  (0.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.38it/s]\n","iter 91000: train loss 4.1221, val loss 4.4190\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 13/10000  (0.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.66it/s]\n","iter 92000: train loss 85.3958, val loss 84.7342\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.65it/s]\n","iter 93000: train loss 4.2477, val loss 3.9717\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 34.88it/s]\n","iter 94000: train loss 4.2102, val loss 4.3607\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.96it/s]\n","iter 95000: train loss 4.5333, val loss 4.6869\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.20it/s]\n","iter 96000: train loss 4.0889, val loss 4.0410\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.35it/s]\n","iter 97000: train loss 4.9853, val loss 5.1369\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 3/10000  (0.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.81it/s]\n","iter 98000: train loss 2.8816, val loss 2.5970\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.58it/s]\n","iter 99000: train loss 18.4323, val loss 20.1786\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 36.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.77it/s]\n","iter 100000: train loss 33.9621, val loss 34.0119\n","Using precomputed batches\n","100% 80/80 [00:02<00:00, 35.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02<00:00, 35.57it/s]\n"]}],"source":["!python train.py slicing_addition_4_operand_reverse.txt --batch slicing"]},{"cell_type":"markdown","metadata":{"id":"30Zi4RZFvtkw"},"source":["## Positional Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"F9dDD_DmvtPz","outputId":"e5dfdb78-6fcc-4888-a853-81d92bb15348"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","test_reverse.txt, 10000 examples: 7217/10000  (72.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.35it/s]\n","iter 974000: train loss 1.4246, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7311/10000  (73.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.99it/s]\n","iter 976000: train loss 1.4283, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7315/10000  (73.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.87it/s]\n","iter 978000: train loss 1.4204, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7257/10000  (72.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 980000: train loss 1.4227, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7337/10000  (73.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.58it/s]\n","iter 982000: train loss 1.4214, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7087/10000  (70.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.27it/s]\n","iter 984000: train loss 1.4245, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7191/10000  (71.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.82it/s]\n","iter 986000: train loss 1.4272, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7206/10000  (72.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.57it/s]\n","iter 988000: train loss 1.4250, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7479/10000  (74.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.63it/s]\n","iter 990000: train loss 1.4234, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7213/10000  (72.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.27it/s]\n","iter 992000: train loss 1.4256, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7454/10000  (74.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.13it/s]\n","iter 994000: train loss 1.4245, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7324/10000  (73.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.38it/s]\n","iter 996000: train loss 1.4216, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7386/10000  (73.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.85it/s]\n","iter 998000: train loss 1.4310, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7263/10000  (72.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.44it/s]\n","iter 1000000: train loss 1.4252, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7353/10000  (73.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1002000: train loss 1.4229, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.58it/s]\n","iter 1004000: train loss 1.4226, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7328/10000  (73.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.56it/s]\n","iter 1006000: train loss 1.4204, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7363/10000  (73.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.31it/s]\n","iter 1008000: train loss 1.4253, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7399/10000  (73.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.48it/s]\n","iter 1010000: train loss 1.4256, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7171/10000  (71.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.50it/s]\n","iter 1012000: train loss 1.4305, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7373/10000  (73.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.46it/s]\n","iter 1014000: train loss 1.4202, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7246/10000  (72.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.08it/s]\n","iter 1016000: train loss 1.4264, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7215/10000  (72.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.67it/s]\n","iter 1018000: train loss 1.4187, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7304/10000  (73.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1020000: train loss 1.4228, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7212/10000  (72.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.59it/s]\n","iter 1022000: train loss 1.4175, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7169/10000  (71.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.10it/s]\n","iter 1024000: train loss 1.4254, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7505/10000  (75.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.31it/s]\n","iter 1026000: train loss 1.4283, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7066/10000  (70.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.57it/s]\n","iter 1028000: train loss 1.4240, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7261/10000  (72.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.48it/s]\n","iter 1030000: train loss 1.4227, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7188/10000  (71.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.57it/s]\n","iter 1032000: train loss 1.4234, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7321/10000  (73.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.77it/s]\n","iter 1034000: train loss 1.4233, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7232/10000  (72.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.03it/s]\n","iter 1036000: train loss 1.4247, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7269/10000  (72.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.51it/s]\n","iter 1038000: train loss 1.4195, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7285/10000  (72.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.06it/s]\n","iter 1040000: train loss 1.4248, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7342/10000  (73.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.19it/s]\n","iter 1042000: train loss 1.4210, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.31it/s]\n","iter 1044000: train loss 1.4221, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.97it/s]\n","iter 1046000: train loss 1.4223, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7095/10000  (70.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.87it/s]\n","iter 1048000: train loss 1.4179, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7339/10000  (73.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.43it/s]\n","iter 1050000: train loss 1.4206, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7257/10000  (72.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.00it/s]\n","iter 1052000: train loss 1.4203, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7351/10000  (73.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1054000: train loss 1.4275, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7407/10000  (74.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.40it/s]\n","iter 1056000: train loss 1.4243, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7320/10000  (73.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.83it/s]\n","iter 1058000: train loss 1.4206, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7270/10000  (72.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.44it/s]\n","iter 1060000: train loss 1.4243, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7362/10000  (73.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.47it/s]\n","iter 1062000: train loss 1.4250, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7437/10000  (74.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.25it/s]\n","iter 1064000: train loss 1.4240, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.96it/s]\n","iter 1066000: train loss 1.4263, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7252/10000  (72.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.83it/s]\n","iter 1068000: train loss 1.4282, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7318/10000  (73.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.99it/s]\n","iter 1070000: train loss 1.4212, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7215/10000  (72.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.52it/s]\n","iter 1072000: train loss 1.4260, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7255/10000  (72.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.67it/s]\n","iter 1074000: train loss 1.4225, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7199/10000  (71.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.89it/s]\n","iter 1076000: train loss 1.4267, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7361/10000  (73.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.48it/s]\n","iter 1078000: train loss 1.4231, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7253/10000  (72.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.95it/s]\n","iter 1080000: train loss 1.4176, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7227/10000  (72.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.32it/s]\n","iter 1082000: train loss 1.4217, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7282/10000  (72.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.95it/s]\n","iter 1084000: train loss 1.4245, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7162/10000  (71.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.25it/s]\n","iter 1086000: train loss 1.4268, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7270/10000  (72.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.82it/s]\n","iter 1088000: train loss 1.4229, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7439/10000  (74.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.27it/s]\n","iter 1090000: train loss 1.4241, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7151/10000  (71.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.75it/s]\n","iter 1092000: train loss 1.4229, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7313/10000  (73.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.32it/s]\n","iter 1094000: train loss 1.4252, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7226/10000  (72.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.45it/s]\n","iter 1096000: train loss 1.4241, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7345/10000  (73.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.99it/s]\n","iter 1098000: train loss 1.4219, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7418/10000  (74.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.31it/s]\n","iter 1100000: train loss 1.4233, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7246/10000  (72.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.52it/s]\n","iter 1102000: train loss 1.4183, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7423/10000  (74.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.45it/s]\n","iter 1104000: train loss 1.4284, val loss 1.4289\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7072/10000  (70.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.68it/s]\n","iter 1106000: train loss 1.4221, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7339/10000  (73.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.39it/s]\n","iter 1108000: train loss 1.4277, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7352/10000  (73.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.54it/s]\n","iter 1110000: train loss 1.4260, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7170/10000  (71.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1112000: train loss 1.4212, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7409/10000  (74.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.82it/s]\n","iter 1114000: train loss 1.4301, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.86it/s]\n","iter 1116000: train loss 1.4288, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7359/10000  (73.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.66it/s]\n","iter 1118000: train loss 1.4256, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7444/10000  (74.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.41it/s]\n","iter 1120000: train loss 1.4237, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7267/10000  (72.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.49it/s]\n","iter 1122000: train loss 1.4238, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7216/10000  (72.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.80it/s]\n","iter 1124000: train loss 1.4274, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7258/10000  (72.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.23it/s]\n","iter 1126000: train loss 1.4237, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7340/10000  (73.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.16it/s]\n","iter 1128000: train loss 1.4228, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7345/10000  (73.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.62it/s]\n","iter 1130000: train loss 1.4188, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7236/10000  (72.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.95it/s]\n","iter 1132000: train loss 1.4288, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7266/10000  (72.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.18it/s]\n","iter 1134000: train loss 1.4221, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7261/10000  (72.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.58it/s]\n","iter 1136000: train loss 1.4282, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7192/10000  (71.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.74it/s]\n","iter 1138000: train loss 1.4234, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7447/10000  (74.47%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.34it/s]\n","iter 1140000: train loss 1.4279, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6982/10000  (69.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.84it/s]\n","iter 1142000: train loss 1.4224, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7225/10000  (72.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.48it/s]\n","iter 1144000: train loss 1.4207, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7345/10000  (73.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.75it/s]\n","iter 1146000: train loss 1.4254, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7223/10000  (72.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.07it/s]\n","iter 1148000: train loss 1.4288, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7523/10000  (75.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.96it/s]\n","iter 1150000: train loss 1.4196, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7345/10000  (73.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.18it/s]\n","iter 1152000: train loss 1.4229, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7452/10000  (74.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.52it/s]\n","iter 1154000: train loss 1.4217, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7259/10000  (72.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.80it/s]\n","iter 1156000: train loss 1.4250, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7512/10000  (75.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.54it/s]\n","iter 1158000: train loss 1.4235, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7356/10000  (73.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.16it/s]\n","iter 1160000: train loss 1.4253, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7334/10000  (73.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.75it/s]\n","iter 1162000: train loss 1.4223, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7159/10000  (71.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.98it/s]\n","iter 1164000: train loss 1.4223, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7221/10000  (72.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1166000: train loss 1.4268, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.67it/s]\n","iter 1168000: train loss 1.4245, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7301/10000  (73.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.89it/s]\n","iter 1170000: train loss 1.4186, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7400/10000  (74.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.61it/s]\n","iter 1172000: train loss 1.4229, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7203/10000  (72.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.56it/s]\n","iter 1174000: train loss 1.4221, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7159/10000  (71.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.63it/s]\n","iter 1176000: train loss 1.4211, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7383/10000  (73.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.74it/s]\n","iter 1178000: train loss 1.4218, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7206/10000  (72.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.51it/s]\n","iter 1180000: train loss 1.4261, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7356/10000  (73.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.70it/s]\n","iter 1182000: train loss 1.4215, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7290/10000  (72.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.68it/s]\n","iter 1184000: train loss 1.4213, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7422/10000  (74.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.37it/s]\n","iter 1186000: train loss 1.4253, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.69it/s]\n","iter 1188000: train loss 1.4263, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.91it/s]\n","iter 1190000: train loss 1.4200, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.08it/s]\n","iter 1192000: train loss 1.4235, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.05it/s]\n","iter 1194000: train loss 1.4231, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7171/10000  (71.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.17it/s]\n","iter 1196000: train loss 1.4252, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7339/10000  (73.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.17it/s]\n","iter 1198000: train loss 1.4248, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7127/10000  (71.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.88it/s]\n","iter 1200000: train loss 1.4258, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7196/10000  (71.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.75it/s]\n","iter 1202000: train loss 1.4230, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7248/10000  (72.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.03it/s]\n","iter 1204000: train loss 1.4211, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7257/10000  (72.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.68it/s]\n","iter 1206000: train loss 1.4263, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7199/10000  (71.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.96it/s]\n","iter 1208000: train loss 1.4225, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7273/10000  (72.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.48it/s]\n","iter 1210000: train loss 1.4238, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7368/10000  (73.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.44it/s]\n","iter 1212000: train loss 1.4230, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7391/10000  (73.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.76it/s]\n","iter 1214000: train loss 1.4264, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7363/10000  (73.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.71it/s]\n","iter 1216000: train loss 1.4173, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7545/10000  (75.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.74it/s]\n","iter 1218000: train loss 1.4216, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7473/10000  (74.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.73it/s]\n","iter 1220000: train loss 1.4218, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7212/10000  (72.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.74it/s]\n","iter 1222000: train loss 1.4284, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7433/10000  (74.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.06it/s]\n","iter 1224000: train loss 1.4231, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7360/10000  (73.60%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.80it/s]\n","iter 1226000: train loss 1.4236, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7473/10000  (74.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.30it/s]\n","iter 1228000: train loss 1.4255, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7336/10000  (73.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.39it/s]\n","iter 1230000: train loss 1.4258, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7386/10000  (73.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.77it/s]\n","iter 1232000: train loss 1.4249, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7454/10000  (74.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.51it/s]\n","iter 1234000: train loss 1.4205, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7162/10000  (71.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.12it/s]\n","iter 1236000: train loss 1.4203, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.71it/s]\n","iter 1238000: train loss 1.4294, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7227/10000  (72.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.90it/s]\n","iter 1240000: train loss 1.4243, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.77it/s]\n","iter 1242000: train loss 1.4229, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7318/10000  (73.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.22it/s]\n","iter 1244000: train loss 1.4233, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.15it/s]\n","iter 1246000: train loss 1.4283, val loss 1.4282\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7261/10000  (72.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.20it/s]\n","iter 1248000: train loss 1.4249, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.35it/s]\n","iter 1250000: train loss 1.4224, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7322/10000  (73.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.48it/s]\n","iter 1252000: train loss 1.4249, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.58it/s]\n","iter 1254000: train loss 1.4223, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.60it/s]\n","iter 1256000: train loss 1.4207, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7218/10000  (72.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.74it/s]\n","iter 1258000: train loss 1.4168, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7502/10000  (75.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.38it/s]\n","iter 1260000: train loss 1.4246, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7174/10000  (71.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.48it/s]\n","iter 1262000: train loss 1.4237, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7421/10000  (74.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.79it/s]\n","iter 1264000: train loss 1.4228, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7495/10000  (74.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.42it/s]\n","iter 1266000: train loss 1.4215, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7353/10000  (73.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.28it/s]\n","iter 1268000: train loss 1.4255, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7250/10000  (72.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.06it/s]\n","iter 1270000: train loss 1.4177, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.79it/s]\n","iter 1272000: train loss 1.4238, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7222/10000  (72.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.71it/s]\n","iter 1274000: train loss 1.4283, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7504/10000  (75.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.02it/s]\n","iter 1276000: train loss 1.4223, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7469/10000  (74.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.40it/s]\n","iter 1278000: train loss 1.4220, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7530/10000  (75.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.68it/s]\n","iter 1280000: train loss 1.4257, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7444/10000  (74.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.13it/s]\n","iter 1282000: train loss 1.4270, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7481/10000  (74.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.68it/s]\n","iter 1284000: train loss 1.4263, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7332/10000  (73.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.22it/s]\n","iter 1286000: train loss 1.4176, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7368/10000  (73.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.86it/s]\n","iter 1288000: train loss 1.4243, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7081/10000  (70.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.11it/s]\n","iter 1290000: train loss 1.4237, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7147/10000  (71.47%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.58it/s]\n","iter 1292000: train loss 1.4215, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7392/10000  (73.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.22it/s]\n","iter 1294000: train loss 1.4224, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7238/10000  (72.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.60it/s]\n","iter 1296000: train loss 1.4228, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7477/10000  (74.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.38it/s]\n","iter 1298000: train loss 1.4226, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7310/10000  (73.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.40it/s]\n","iter 1300000: train loss 1.4181, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7349/10000  (73.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1302000: train loss 1.4217, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7308/10000  (73.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.55it/s]\n","iter 1304000: train loss 1.4304, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.35it/s]\n","iter 1306000: train loss 1.4219, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7214/10000  (72.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.31it/s]\n","iter 1308000: train loss 1.4238, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7276/10000  (72.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.05it/s]\n","iter 1310000: train loss 1.4261, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7334/10000  (73.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.97it/s]\n","iter 1312000: train loss 1.4272, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7300/10000  (73.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.51it/s]\n","iter 1314000: train loss 1.4268, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7427/10000  (74.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.72it/s]\n","iter 1316000: train loss 1.4284, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7322/10000  (73.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.92it/s]\n","iter 1318000: train loss 1.4225, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7250/10000  (72.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.65it/s]\n","iter 1320000: train loss 1.4226, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7434/10000  (74.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.75it/s]\n","iter 1322000: train loss 1.4278, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7481/10000  (74.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.04it/s]\n","iter 1324000: train loss 1.4244, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7356/10000  (73.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.93it/s]\n","iter 1326000: train loss 1.4241, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7453/10000  (74.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1328000: train loss 1.4234, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7218/10000  (72.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.70it/s]\n","iter 1330000: train loss 1.4278, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7259/10000  (72.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.09it/s]\n","iter 1332000: train loss 1.4268, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7324/10000  (73.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.41it/s]\n","iter 1334000: train loss 1.4186, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7212/10000  (72.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.36it/s]\n","iter 1336000: train loss 1.4257, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7320/10000  (73.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.32it/s]\n","iter 1338000: train loss 1.4229, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7290/10000  (72.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.12it/s]\n","iter 1340000: train loss 1.4222, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7193/10000  (71.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.68it/s]\n","iter 1342000: train loss 1.4232, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7430/10000  (74.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.00it/s]\n","iter 1344000: train loss 1.4180, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7410/10000  (74.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.62it/s]\n","iter 1346000: train loss 1.4236, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7364/10000  (73.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.25it/s]\n","iter 1348000: train loss 1.4231, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7406/10000  (74.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.81it/s]\n","iter 1350000: train loss 1.4295, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7316/10000  (73.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.12it/s]\n","iter 1352000: train loss 1.4285, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7393/10000  (73.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.03it/s]\n","iter 1354000: train loss 1.4221, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7230/10000  (72.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.76it/s]\n","iter 1356000: train loss 1.4269, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.63it/s]\n","iter 1358000: train loss 1.4235, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7367/10000  (73.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.96it/s]\n","iter 1360000: train loss 1.4274, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.42it/s]\n","iter 1362000: train loss 1.4191, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7257/10000  (72.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.14it/s]\n","iter 1364000: train loss 1.4229, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7250/10000  (72.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.93it/s]\n","iter 1366000: train loss 1.4243, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7389/10000  (73.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.72it/s]\n","iter 1368000: train loss 1.4252, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7264/10000  (72.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.91it/s]\n","iter 1370000: train loss 1.4197, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7344/10000  (73.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.79it/s]\n","iter 1372000: train loss 1.4211, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7425/10000  (74.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.88it/s]\n","iter 1374000: train loss 1.4239, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7144/10000  (71.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.96it/s]\n","iter 1376000: train loss 1.4232, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7302/10000  (73.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.44it/s]\n","iter 1378000: train loss 1.4275, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.44it/s]\n","iter 1380000: train loss 1.4245, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7390/10000  (73.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.20it/s]\n","iter 1382000: train loss 1.4229, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7288/10000  (72.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.61it/s]\n","iter 1384000: train loss 1.4216, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7274/10000  (72.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.82it/s]\n","iter 1386000: train loss 1.4251, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.25it/s]\n","iter 1388000: train loss 1.4293, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7435/10000  (74.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.54it/s]\n","iter 1390000: train loss 1.4228, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7519/10000  (75.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.92it/s]\n","iter 1392000: train loss 1.4235, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7318/10000  (73.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.39it/s]\n","iter 1394000: train loss 1.4238, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7237/10000  (72.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.74it/s]\n","iter 1396000: train loss 1.4214, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7360/10000  (73.60%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.63it/s]\n","iter 1398000: train loss 1.4238, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7411/10000  (74.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.17it/s]\n","iter 1400000: train loss 1.4222, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7265/10000  (72.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.41it/s]\n","iter 1402000: train loss 1.4226, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7311/10000  (73.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.66it/s]\n","iter 1404000: train loss 1.4231, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7273/10000  (72.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.40it/s]\n","iter 1406000: train loss 1.4214, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7423/10000  (74.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.37it/s]\n","iter 1408000: train loss 1.4285, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.44it/s]\n","iter 1410000: train loss 1.4202, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7205/10000  (72.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.81it/s]\n","iter 1412000: train loss 1.4243, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7194/10000  (71.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.68it/s]\n","iter 1414000: train loss 1.4260, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7036/10000  (70.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.16it/s]\n","iter 1416000: train loss 1.4243, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7243/10000  (72.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.55it/s]\n","iter 1418000: train loss 1.4265, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.09it/s]\n","iter 1420000: train loss 1.4253, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7279/10000  (72.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.44it/s]\n","iter 1422000: train loss 1.4244, val loss 1.4282\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7283/10000  (72.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.18it/s]\n","iter 1424000: train loss 1.4200, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.04it/s]\n","iter 1426000: train loss 1.4239, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7397/10000  (73.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.77it/s]\n","iter 1428000: train loss 1.4262, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7283/10000  (72.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.72it/s]\n","iter 1430000: train loss 1.4212, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7228/10000  (72.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.10it/s]\n","iter 1432000: train loss 1.4242, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7235/10000  (72.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.89it/s]\n","iter 1434000: train loss 1.4315, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7258/10000  (72.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.64it/s]\n","iter 1436000: train loss 1.4199, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7275/10000  (72.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.59it/s]\n","iter 1438000: train loss 1.4268, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7391/10000  (73.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.10it/s]\n","iter 1440000: train loss 1.4243, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7301/10000  (73.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.96it/s]\n","iter 1442000: train loss 1.4224, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7276/10000  (72.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.44it/s]\n","iter 1444000: train loss 1.4249, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7476/10000  (74.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.43it/s]\n","iter 1446000: train loss 1.4288, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7190/10000  (71.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.50it/s]\n","iter 1448000: train loss 1.4297, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7150/10000  (71.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.32it/s]\n","iter 1450000: train loss 1.4211, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7042/10000  (70.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.49it/s]\n","iter 1452000: train loss 1.4253, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7362/10000  (73.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.63it/s]\n","iter 1454000: train loss 1.4273, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7301/10000  (73.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.35it/s]\n","iter 1456000: train loss 1.4224, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7291/10000  (72.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.12it/s]\n","iter 1458000: train loss 1.4203, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7405/10000  (74.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.63it/s]\n","iter 1460000: train loss 1.4242, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7066/10000  (70.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.41it/s]\n","iter 1462000: train loss 1.4249, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7390/10000  (73.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.49it/s]\n","iter 1464000: train loss 1.4250, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7324/10000  (73.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.57it/s]\n","iter 1466000: train loss 1.4212, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7439/10000  (74.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.96it/s]\n","iter 1468000: train loss 1.4243, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7355/10000  (73.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.94it/s]\n","iter 1470000: train loss 1.4235, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7354/10000  (73.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.61it/s]\n","iter 1472000: train loss 1.4198, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.88it/s]\n","iter 1474000: train loss 1.4240, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7293/10000  (72.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.07it/s]\n","iter 1476000: train loss 1.4222, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7414/10000  (74.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.65it/s]\n","iter 1478000: train loss 1.4235, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7236/10000  (72.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.87it/s]\n","iter 1480000: train loss 1.4213, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7308/10000  (73.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.14it/s]\n","iter 1482000: train loss 1.4263, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7316/10000  (73.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.16it/s]\n","iter 1484000: train loss 1.4254, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7237/10000  (72.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.89it/s]\n","iter 1486000: train loss 1.4279, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7417/10000  (74.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.41it/s]\n","iter 1488000: train loss 1.4247, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7383/10000  (73.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.62it/s]\n","iter 1490000: train loss 1.4240, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.74it/s]\n","iter 1492000: train loss 1.4270, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7275/10000  (72.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.58it/s]\n","iter 1494000: train loss 1.4278, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7236/10000  (72.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.68it/s]\n","iter 1496000: train loss 1.4268, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7206/10000  (72.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.41it/s]\n","iter 1498000: train loss 1.4246, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7235/10000  (72.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.66it/s]\n","iter 1500000: train loss 1.4243, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7329/10000  (73.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.04it/s]\n","iter 1502000: train loss 1.4183, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7408/10000  (74.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.73it/s]\n","iter 1504000: train loss 1.4203, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7213/10000  (72.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.84it/s]\n","iter 1506000: train loss 1.4274, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7266/10000  (72.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.89it/s]\n","iter 1508000: train loss 1.4280, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7267/10000  (72.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.86it/s]\n","iter 1510000: train loss 1.4261, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7397/10000  (73.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.12it/s]\n","iter 1512000: train loss 1.4215, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.74it/s]\n","iter 1514000: train loss 1.4267, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7379/10000  (73.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.61it/s]\n","iter 1516000: train loss 1.4216, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7273/10000  (72.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.74it/s]\n","iter 1518000: train loss 1.4267, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7404/10000  (74.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.74it/s]\n","iter 1520000: train loss 1.4209, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7352/10000  (73.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.08it/s]\n","iter 1522000: train loss 1.4227, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7258/10000  (72.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.72it/s]\n","iter 1524000: train loss 1.4236, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7366/10000  (73.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.16it/s]\n","iter 1526000: train loss 1.4266, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7194/10000  (71.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.47it/s]\n","iter 1528000: train loss 1.4237, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7382/10000  (73.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.85it/s]\n","iter 1530000: train loss 1.4246, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7229/10000  (72.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.83it/s]\n","iter 1532000: train loss 1.4264, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7450/10000  (74.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.97it/s]\n","iter 1534000: train loss 1.4217, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7252/10000  (72.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.64it/s]\n","iter 1536000: train loss 1.4297, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.52it/s]\n","iter 1538000: train loss 1.4266, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7551/10000  (75.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.14it/s]\n","iter 1540000: train loss 1.4203, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7436/10000  (74.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.15it/s]\n","iter 1542000: train loss 1.4226, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7461/10000  (74.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.91it/s]\n","iter 1544000: train loss 1.4198, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7130/10000  (71.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.60it/s]\n","iter 1546000: train loss 1.4218, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7389/10000  (73.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.00it/s]\n","iter 1548000: train loss 1.4246, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7400/10000  (74.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.53it/s]\n","iter 1550000: train loss 1.4220, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.08it/s]\n","iter 1552000: train loss 1.4218, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7268/10000  (72.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.82it/s]\n","iter 1554000: train loss 1.4271, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7159/10000  (71.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.44it/s]\n","iter 1556000: train loss 1.4249, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7467/10000  (74.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.95it/s]\n","iter 1558000: train loss 1.4272, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7185/10000  (71.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.75it/s]\n","iter 1560000: train loss 1.4250, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7169/10000  (71.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.85it/s]\n","iter 1562000: train loss 1.4255, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7206/10000  (72.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.37it/s]\n","iter 1564000: train loss 1.4227, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7429/10000  (74.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.33it/s]\n","iter 1566000: train loss 1.4204, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7146/10000  (71.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1568000: train loss 1.4240, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7601/10000  (76.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.75it/s]\n","iter 1570000: train loss 1.4221, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7338/10000  (73.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.77it/s]\n","iter 1572000: train loss 1.4242, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7259/10000  (72.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.28it/s]\n","iter 1574000: train loss 1.4213, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7359/10000  (73.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.66it/s]\n","iter 1576000: train loss 1.4220, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7320/10000  (73.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.49it/s]\n","iter 1578000: train loss 1.4213, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7182/10000  (71.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.01it/s]\n","iter 1580000: train loss 1.4239, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7293/10000  (72.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.83it/s]\n","iter 1582000: train loss 1.4303, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7263/10000  (72.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.69it/s]\n","iter 1584000: train loss 1.4247, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7383/10000  (73.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.19it/s]\n","iter 1586000: train loss 1.4266, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.33it/s]\n","iter 1588000: train loss 1.4194, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7329/10000  (73.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.29it/s]\n","iter 1590000: train loss 1.4266, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7486/10000  (74.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.97it/s]\n","iter 1592000: train loss 1.4200, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7339/10000  (73.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.37it/s]\n","iter 1594000: train loss 1.4223, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7466/10000  (74.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.66it/s]\n","iter 1596000: train loss 1.4213, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7369/10000  (73.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.09it/s]\n","iter 1598000: train loss 1.4248, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.07it/s]\n","iter 1600000: train loss 1.4276, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7395/10000  (73.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.59it/s]\n","iter 1602000: train loss 1.4243, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7252/10000  (72.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.44it/s]\n","iter 1604000: train loss 1.4257, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7279/10000  (72.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.56it/s]\n","iter 1606000: train loss 1.4200, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7414/10000  (74.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.47it/s]\n","iter 1608000: train loss 1.4246, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7214/10000  (72.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.16it/s]\n","iter 1610000: train loss 1.4212, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.30it/s]\n","iter 1612000: train loss 1.4229, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7246/10000  (72.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.25it/s]\n","iter 1614000: train loss 1.4240, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.91it/s]\n","iter 1616000: train loss 1.4244, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7425/10000  (74.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.05it/s]\n","iter 1618000: train loss 1.4256, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7409/10000  (74.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.08it/s]\n","iter 1620000: train loss 1.4229, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7355/10000  (73.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.26it/s]\n","iter 1622000: train loss 1.4265, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7284/10000  (72.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.29it/s]\n","iter 1624000: train loss 1.4249, val loss 1.4266\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7393/10000  (73.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.49it/s]\n","iter 1626000: train loss 1.4206, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7281/10000  (72.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.70it/s]\n","iter 1628000: train loss 1.4214, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7234/10000  (72.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.33it/s]\n","iter 1630000: train loss 1.4282, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7480/10000  (74.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.06it/s]\n","iter 1632000: train loss 1.4253, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7470/10000  (74.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.60it/s]\n","iter 1634000: train loss 1.4197, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7455/10000  (74.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.45it/s]\n","iter 1636000: train loss 1.4271, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7315/10000  (73.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.68it/s]\n","iter 1638000: train loss 1.4229, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7303/10000  (73.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.63it/s]\n","iter 1640000: train loss 1.4215, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.61it/s]\n","iter 1642000: train loss 1.4255, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7241/10000  (72.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.48it/s]\n","iter 1644000: train loss 1.4191, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7194/10000  (71.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.95it/s]\n","iter 1646000: train loss 1.4282, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7093/10000  (70.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.60it/s]\n","iter 1648000: train loss 1.4240, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7268/10000  (72.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.96it/s]\n","iter 1650000: train loss 1.4244, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7396/10000  (73.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.64it/s]\n","iter 1652000: train loss 1.4216, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7249/10000  (72.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.07it/s]\n","iter 1654000: train loss 1.4239, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7286/10000  (72.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.61it/s]\n","iter 1656000: train loss 1.4244, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7393/10000  (73.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.83it/s]\n","iter 1658000: train loss 1.4193, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7296/10000  (72.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.16it/s]\n","iter 1660000: train loss 1.4252, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7258/10000  (72.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1662000: train loss 1.4258, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7202/10000  (72.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.55it/s]\n","iter 1664000: train loss 1.4262, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7201/10000  (72.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.10it/s]\n","iter 1666000: train loss 1.4222, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7311/10000  (73.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.78it/s]\n","iter 1668000: train loss 1.4234, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7285/10000  (72.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.24it/s]\n","iter 1670000: train loss 1.4174, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7187/10000  (71.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.84it/s]\n","iter 1672000: train loss 1.4254, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7242/10000  (72.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.29it/s]\n","iter 1674000: train loss 1.4258, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7331/10000  (73.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.59it/s]\n","iter 1676000: train loss 1.4221, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7366/10000  (73.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.32it/s]\n","iter 1678000: train loss 1.4250, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7555/10000  (75.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.57it/s]\n","iter 1680000: train loss 1.4216, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7315/10000  (73.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.06it/s]\n","iter 1682000: train loss 1.4264, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7261/10000  (72.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1684000: train loss 1.4211, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7267/10000  (72.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.26it/s]\n","iter 1686000: train loss 1.4172, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7328/10000  (73.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.50it/s]\n","iter 1688000: train loss 1.4219, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1690000: train loss 1.4192, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7280/10000  (72.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.67it/s]\n","iter 1692000: train loss 1.4199, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7390/10000  (73.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.75it/s]\n","iter 1694000: train loss 1.4192, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7294/10000  (72.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1696000: train loss 1.4215, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7454/10000  (74.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.54it/s]\n","iter 1698000: train loss 1.4245, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7457/10000  (74.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.96it/s]\n","iter 1700000: train loss 1.4236, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.67it/s]\n","iter 1702000: train loss 1.4232, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7384/10000  (73.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.88it/s]\n","iter 1704000: train loss 1.4220, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7416/10000  (74.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.54it/s]\n","iter 1706000: train loss 1.4213, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.36it/s]\n","iter 1708000: train loss 1.4226, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7338/10000  (73.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.96it/s]\n","iter 1710000: train loss 1.4267, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7253/10000  (72.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.49it/s]\n","iter 1712000: train loss 1.4239, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7157/10000  (71.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.16it/s]\n","iter 1714000: train loss 1.4241, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7419/10000  (74.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.83it/s]\n","iter 1716000: train loss 1.4255, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7313/10000  (73.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.21it/s]\n","iter 1718000: train loss 1.4230, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7330/10000  (73.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.33it/s]\n","iter 1720000: train loss 1.4239, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7248/10000  (72.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.91it/s]\n","iter 1722000: train loss 1.4255, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7329/10000  (73.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.46it/s]\n","iter 1724000: train loss 1.4264, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7229/10000  (72.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.17it/s]\n","iter 1726000: train loss 1.4276, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.18it/s]\n","iter 1728000: train loss 1.4247, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7234/10000  (72.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.62it/s]\n","iter 1730000: train loss 1.4228, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7300/10000  (73.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.79it/s]\n","iter 1732000: train loss 1.4223, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7505/10000  (75.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.43it/s]\n","iter 1734000: train loss 1.4203, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7319/10000  (73.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.19it/s]\n","iter 1736000: train loss 1.4259, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7307/10000  (73.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.12it/s]\n","iter 1738000: train loss 1.4274, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.99it/s]\n","iter 1740000: train loss 1.4256, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7402/10000  (74.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.64it/s]\n","iter 1742000: train loss 1.4208, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7440/10000  (74.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.51it/s]\n","iter 1744000: train loss 1.4218, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7315/10000  (73.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.83it/s]\n","iter 1746000: train loss 1.4238, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.14it/s]\n","iter 1748000: train loss 1.4243, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7373/10000  (73.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.16it/s]\n","iter 1750000: train loss 1.4272, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7411/10000  (74.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.66it/s]\n","iter 1752000: train loss 1.4264, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7343/10000  (73.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.90it/s]\n","iter 1754000: train loss 1.4240, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7249/10000  (72.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 1756000: train loss 1.4281, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7437/10000  (74.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.63it/s]\n","iter 1758000: train loss 1.4220, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7428/10000  (74.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.51it/s]\n","iter 1760000: train loss 1.4228, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7392/10000  (73.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.72it/s]\n","iter 1762000: train loss 1.4233, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7218/10000  (72.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.96it/s]\n","iter 1764000: train loss 1.4246, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7186/10000  (71.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.60it/s]\n","iter 1766000: train loss 1.4251, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7319/10000  (73.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.05it/s]\n","iter 1768000: train loss 1.4268, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7235/10000  (72.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.34it/s]\n","iter 1770000: train loss 1.4223, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7164/10000  (71.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.82it/s]\n","iter 1772000: train loss 1.4239, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7372/10000  (73.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.39it/s]\n","iter 1774000: train loss 1.4272, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7388/10000  (73.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.59it/s]\n","iter 1776000: train loss 1.4291, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7145/10000  (71.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.07it/s]\n","iter 1778000: train loss 1.4284, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7353/10000  (73.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.76it/s]\n","iter 1780000: train loss 1.4269, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7424/10000  (74.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.95it/s]\n","iter 1782000: train loss 1.4205, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7264/10000  (72.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.70it/s]\n","iter 1784000: train loss 1.4232, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7297/10000  (72.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.77it/s]\n","iter 1786000: train loss 1.4234, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7477/10000  (74.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.54it/s]\n","iter 1788000: train loss 1.4318, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7153/10000  (71.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.33it/s]\n","iter 1790000: train loss 1.4278, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7353/10000  (73.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.33it/s]\n","iter 1792000: train loss 1.4221, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7435/10000  (74.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.44it/s]\n","iter 1794000: train loss 1.4216, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.70it/s]\n","iter 1796000: train loss 1.4274, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7164/10000  (71.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.80it/s]\n","iter 1798000: train loss 1.4213, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7365/10000  (73.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.03it/s]\n","iter 1800000: train loss 1.4192, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7365/10000  (73.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.30it/s]\n","iter 1802000: train loss 1.4237, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7290/10000  (72.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.16it/s]\n","iter 1804000: train loss 1.4268, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7269/10000  (72.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.72it/s]\n","iter 1806000: train loss 1.4215, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7188/10000  (71.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.37it/s]\n","iter 1808000: train loss 1.4306, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.54it/s]\n","iter 1810000: train loss 1.4235, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7220/10000  (72.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.67it/s]\n","iter 1812000: train loss 1.4274, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.51it/s]\n","iter 1814000: train loss 1.4226, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7500/10000  (75.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.24it/s]\n","iter 1816000: train loss 1.4239, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7320/10000  (73.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.27it/s]\n","iter 1818000: train loss 1.4246, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.39it/s]\n","iter 1820000: train loss 1.4206, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7330/10000  (73.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.32it/s]\n","iter 1822000: train loss 1.4238, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7241/10000  (72.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.42it/s]\n","iter 1824000: train loss 1.4265, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7275/10000  (72.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.67it/s]\n","iter 1826000: train loss 1.4227, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7403/10000  (74.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.44it/s]\n","iter 1828000: train loss 1.4208, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7348/10000  (73.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.41it/s]\n","iter 1830000: train loss 1.4273, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7283/10000  (72.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.56it/s]\n","iter 1832000: train loss 1.4211, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7269/10000  (72.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.28it/s]\n","iter 1834000: train loss 1.4198, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7357/10000  (73.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.88it/s]\n","iter 1836000: train loss 1.4204, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7358/10000  (73.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.53it/s]\n","iter 1838000: train loss 1.4211, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7258/10000  (72.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.48it/s]\n","iter 1840000: train loss 1.4258, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7371/10000  (73.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.79it/s]\n","iter 1842000: train loss 1.4259, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7371/10000  (73.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.37it/s]\n","iter 1844000: train loss 1.4203, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7398/10000  (73.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.21it/s]\n","iter 1846000: train loss 1.4208, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7353/10000  (73.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.60it/s]\n","iter 1848000: train loss 1.4275, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.36it/s]\n","iter 1850000: train loss 1.4201, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7436/10000  (74.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.46it/s]\n","iter 1852000: train loss 1.4247, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7435/10000  (74.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.34it/s]\n","iter 1854000: train loss 1.4237, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7318/10000  (73.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.56it/s]\n","iter 1856000: train loss 1.4235, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7479/10000  (74.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.65it/s]\n","iter 1858000: train loss 1.4212, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7388/10000  (73.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.50it/s]\n","iter 1860000: train loss 1.4218, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7485/10000  (74.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.76it/s]\n","iter 1862000: train loss 1.4194, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7347/10000  (73.47%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.97it/s]\n","iter 1864000: train loss 1.4266, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7379/10000  (73.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.56it/s]\n","iter 1866000: train loss 1.4225, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.55it/s]\n","iter 1868000: train loss 1.4260, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7396/10000  (73.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.48it/s]\n","iter 1870000: train loss 1.4249, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7222/10000  (72.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.52it/s]\n","iter 1872000: train loss 1.4313, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7397/10000  (73.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.57it/s]\n","iter 1874000: train loss 1.4241, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7348/10000  (73.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.51it/s]\n","iter 1876000: train loss 1.4255, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7216/10000  (72.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.12it/s]\n","iter 1878000: train loss 1.4290, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7339/10000  (73.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.26it/s]\n","iter 1880000: train loss 1.4263, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7302/10000  (73.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.94it/s]\n","iter 1882000: train loss 1.4244, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7150/10000  (71.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.22it/s]\n","iter 1884000: train loss 1.4196, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.93it/s]\n","iter 1886000: train loss 1.4232, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7287/10000  (72.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.49it/s]\n","iter 1888000: train loss 1.4243, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7438/10000  (74.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.47it/s]\n","iter 1890000: train loss 1.4241, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7261/10000  (72.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.51it/s]\n","iter 1892000: train loss 1.4196, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7115/10000  (71.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.44it/s]\n","iter 1894000: train loss 1.4239, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7416/10000  (74.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.73it/s]\n","iter 1896000: train loss 1.4213, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7240/10000  (72.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.83it/s]\n","iter 1898000: train loss 1.4258, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7418/10000  (74.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.88it/s]\n","iter 1900000: train loss 1.4185, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7405/10000  (74.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 20.85it/s]\n","iter 1902000: train loss 1.4164, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7431/10000  (74.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.11it/s]\n","iter 1904000: train loss 1.4261, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7229/10000  (72.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.30it/s]\n","iter 1906000: train loss 1.4242, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7452/10000  (74.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.38it/s]\n","iter 1908000: train loss 1.4232, val loss 1.4266\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7246/10000  (72.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.68it/s]\n","iter 1910000: train loss 1.4282, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7382/10000  (73.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.57it/s]\n","iter 1912000: train loss 1.4207, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7421/10000  (74.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.62it/s]\n","iter 1914000: train loss 1.4257, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7323/10000  (73.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.00it/s]\n","iter 1916000: train loss 1.4219, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7391/10000  (73.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.08it/s]\n","iter 1918000: train loss 1.4253, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7248/10000  (72.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.34it/s]\n","iter 1920000: train loss 1.4238, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7242/10000  (72.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.12it/s]\n","iter 1922000: train loss 1.4238, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.44it/s]\n","iter 1924000: train loss 1.4242, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.08it/s]\n","iter 1926000: train loss 1.4251, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7175/10000  (71.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.62it/s]\n","iter 1928000: train loss 1.4277, val loss 1.4282\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7272/10000  (72.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.73it/s]\n","iter 1930000: train loss 1.4271, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7369/10000  (73.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.05it/s]\n","iter 1932000: train loss 1.4237, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7279/10000  (72.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.03it/s]\n","iter 1934000: train loss 1.4257, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7275/10000  (72.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.48it/s]\n","iter 1936000: train loss 1.4219, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7196/10000  (71.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.14it/s]\n","iter 1938000: train loss 1.4250, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7203/10000  (72.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.86it/s]\n","iter 1940000: train loss 1.4199, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7265/10000  (72.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.65it/s]\n","iter 1942000: train loss 1.4278, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.71it/s]\n","iter 1944000: train loss 1.4197, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7365/10000  (73.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.20it/s]\n","iter 1946000: train loss 1.4218, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7160/10000  (71.60%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.73it/s]\n","iter 1948000: train loss 1.4166, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7269/10000  (72.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.45it/s]\n","iter 1950000: train loss 1.4270, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7426/10000  (74.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.06it/s]\n","iter 1952000: train loss 1.4249, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7526/10000  (75.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.72it/s]\n","iter 1954000: train loss 1.4247, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7397/10000  (73.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.35it/s]\n","iter 1956000: train loss 1.4215, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.42it/s]\n","iter 1958000: train loss 1.4278, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7079/10000  (70.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.81it/s]\n","iter 1960000: train loss 1.4210, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.53it/s]\n","iter 1962000: train loss 1.4198, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7303/10000  (73.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.59it/s]\n","iter 1964000: train loss 1.4314, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7393/10000  (73.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.38it/s]\n","iter 1966000: train loss 1.4254, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7420/10000  (74.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.72it/s]\n","iter 1968000: train loss 1.4201, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7226/10000  (72.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.62it/s]\n","iter 1970000: train loss 1.4183, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7526/10000  (75.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.26it/s]\n","iter 1972000: train loss 1.4208, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7332/10000  (73.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.54it/s]\n","iter 1974000: train loss 1.4226, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.70it/s]\n","iter 1976000: train loss 1.4276, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.32it/s]\n","iter 1978000: train loss 1.4253, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7218/10000  (72.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.30it/s]\n","iter 1980000: train loss 1.4242, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7299/10000  (72.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.42it/s]\n","iter 1982000: train loss 1.4238, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.15it/s]\n","iter 1984000: train loss 1.4229, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7298/10000  (72.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.89it/s]\n","iter 1986000: train loss 1.4285, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7303/10000  (73.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.84it/s]\n","iter 1988000: train loss 1.4255, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7221/10000  (72.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.88it/s]\n","iter 1990000: train loss 1.4240, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7374/10000  (73.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.41it/s]\n","iter 1992000: train loss 1.4229, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7202/10000  (72.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.57it/s]\n","iter 1994000: train loss 1.4284, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7232/10000  (72.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.53it/s]\n","iter 1996000: train loss 1.4261, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 20.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.60it/s]\n","iter 1998000: train loss 1.4220, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7336/10000  (73.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.07it/s]\n","iter 2000000: train loss 1.4254, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7423/10000  (74.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.90it/s]\n","iter 2002000: train loss 1.4262, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7375/10000  (73.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.53it/s]\n","iter 2004000: train loss 1.4249, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7345/10000  (73.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.73it/s]\n","iter 2006000: train loss 1.4246, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7201/10000  (72.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.96it/s]\n","iter 2008000: train loss 1.4295, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7476/10000  (74.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.97it/s]\n","iter 2010000: train loss 1.4195, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7306/10000  (73.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.99it/s]\n","iter 2012000: train loss 1.4246, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7410/10000  (74.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.86it/s]\n","iter 2014000: train loss 1.4223, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7373/10000  (73.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.90it/s]\n","iter 2016000: train loss 1.4200, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7310/10000  (73.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.08it/s]\n","iter 2018000: train loss 1.4218, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7245/10000  (72.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.57it/s]\n","iter 2020000: train loss 1.4248, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7108/10000  (71.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.91it/s]\n","iter 2022000: train loss 1.4251, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.34it/s]\n","iter 2024000: train loss 1.4232, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.84it/s]\n","iter 2026000: train loss 1.4250, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7344/10000  (73.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.74it/s]\n","iter 2028000: train loss 1.4233, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7266/10000  (72.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.05it/s]\n","iter 2030000: train loss 1.4276, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7431/10000  (74.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.90it/s]\n","iter 2032000: train loss 1.4238, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7203/10000  (72.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.43it/s]\n","iter 2034000: train loss 1.4248, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7190/10000  (71.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.04it/s]\n","iter 2036000: train loss 1.4275, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7243/10000  (72.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.23it/s]\n","iter 2038000: train loss 1.4266, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7334/10000  (73.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.75it/s]\n","iter 2040000: train loss 1.4243, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7227/10000  (72.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.39it/s]\n","iter 2042000: train loss 1.4249, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7329/10000  (73.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.86it/s]\n","iter 2044000: train loss 1.4191, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7239/10000  (72.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.91it/s]\n","iter 2046000: train loss 1.4223, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.19it/s]\n","iter 2048000: train loss 1.4322, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7421/10000  (74.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.94it/s]\n","iter 2050000: train loss 1.4239, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.76it/s]\n","iter 2052000: train loss 1.4231, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7204/10000  (72.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.94it/s]\n","iter 2054000: train loss 1.4190, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7299/10000  (72.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 21.65it/s]\n","iter 2056000: train loss 1.4258, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.94it/s]\n","iter 2058000: train loss 1.4236, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 22.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.62it/s]\n","iter 2060000: train loss 1.4224, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 21.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7324/10000  (73.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 22.05it/s]\n","iter 2062000: train loss 1.4238, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7332/10000  (73.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.79it/s]\n","iter 2064000: train loss 1.4201, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7433/10000  (74.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.41it/s]\n","iter 2066000: train loss 1.4208, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7190/10000  (71.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.53it/s]\n","iter 2068000: train loss 1.4205, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7220/10000  (72.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.39it/s]\n","iter 2070000: train loss 1.4274, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7455/10000  (74.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.93it/s]\n","iter 2072000: train loss 1.4330, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7366/10000  (73.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.99it/s]\n","iter 2074000: train loss 1.4271, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7084/10000  (70.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.54it/s]\n","iter 2076000: train loss 1.4217, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7210/10000  (72.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.03it/s]\n","iter 2078000: train loss 1.4280, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 23.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7222/10000  (72.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.56it/s]\n","iter 2080000: train loss 1.4182, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7249/10000  (72.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 23.69it/s]\n","iter 2082000: train loss 1.4297, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03<00:00, 24.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7349/10000  (73.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03<00:00, 24.60it/s]\n"]}],"source":["!python train.py 4_operands_addition_reversed.txt --PE RoPE"]},{"cell_type":"markdown","metadata":{"id":"ZxrvGK6Eoebe"},"source":["## Long Multiplication"]},{"cell_type":"markdown","metadata":{"id":"cPspwmWgoh5S"},"source":["### reversed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RH9iNUA1nF70"},"outputs":[],"source":["! python train.py 40_1_digits_mul_reversed.txt"]},{"cell_type":"markdown","metadata":{"id":"PxLUxyr_u8m-"},"source":["### Greedy Decoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FG_n3Y_YvDr2"},"outputs":[],"source":["!python train.py 2_operands_addition_reversed.txt --greedy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_ThupuUvIyH"},"outputs":[],"source":["!python train.py 2_operands_addition_plain.txt --greedy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UE5ttFeNvBfb"},"outputs":[],"source":["!python train.py 4_operands_addition_reversed.txt --greedy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4yS-i2zvMLT"},"outputs":[],"source":["!python train.py 4_operands_addition_plain.txt --greedy"]},{"cell_type":"markdown","source":["# Result Analysis"],"metadata":{"id":"S_xUM7XznZfi"}},{"cell_type":"markdown","source":["## Addition Task"],"metadata":{"id":"wpid4Duzum1v"}},{"cell_type":"markdown","source":["#### Digitwise Error Rates (4 operands addition)"],"metadata":{"id":"ZLm0jWwznbvH"}},{"cell_type":"code","source":["!python result_analysis_script/digitwise_error.py results/4_operands_0_to_999_uniform/reverse_out/4_operands_0_to_999_uniform_reverse/test_reverse_results.csv"],"metadata":{"id":"zuRo6Qlfneh7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Fit Normal"],"metadata":{"id":"KKGV9OLkxorj"}},{"cell_type":"code","source":["!python result_analysis_script/fit_normal.py \\\n","  --input results/4_operands_0_to_999_uniform/reverse_out_early_dense_eval/early_dense_eval_for_normal_distr_4_operands_0_to_999_uniform_reverse/test_reverse_results.csv \\\n","  --iter-start 1000 --iter-end 1800 --iter-step 200 \\\n","  --diff-min -800 --diff-max 800\n"],"metadata":{"id":"64BwhPVFxsUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python result_analysis_script/fit_normal.py \\\n","  --input results/4_operands_0_to_999_uniform/reverse_out/4_operands_0_to_999_uniform_reverse/test_reverse_results.csv \\\n","  --iter-start 8000 --iter-end 12000 --iter-step 2000 \\\n","  --diff-min -100 --diff-max 100\n"],"metadata":{"id":"MAo9jUv32OZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python result_analysis_script/fit_normal.py \\\n","  --input results/4_operands_0_to_999_uniform/reverse_out/4_operands_0_to_999_uniform_reverse/test_reverse_results.csv \\\n","  --iter-start 60000 --iter-end 64000 --iter-step 2000 \\\n","  --diff-min -20 --diff-max 20\n"],"metadata":{"id":"XDuXMuie2lX6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Mutual Information Plot"],"metadata":{"id":"KeRml6m6u6D9"}},{"cell_type":"code","source":["!python result_analysis_script/plot_mi_metrics.py \\\n","  results/4_operands_0_to_999_uniform/reverse_out_complete_MI_1M_lines/4_operands_0_to_999_uniform_reverse/mi_metrics.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6uRsOwxFu9ix","executionInfo":{"status":"ok","timestamp":1769404706723,"user_tz":360,"elapsed":7341,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"}},"outputId":"7b03b3d5-ebb6-44f9-fb2e-31b7facb98e9"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved first figure to /content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/reverse_out_complete_MI_1M_lines/4_operands_0_to_999_uniform_reverse/mi_conditioned_on_z_plot_1M.pdf\n","Saved second figure to /content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/reverse_out_complete_MI_1M_lines/4_operands_0_to_999_uniform_reverse/mi_conditioned_on_carries_plot_1M.pdf\n"]}]},{"cell_type":"markdown","source":["## Simple Multiplication Task"],"metadata":{"id":"xO41d50Fuuhb"}},{"cell_type":"markdown","source":["#### Digitwise Error (Simple multiplication, Colormap)"],"metadata":{"id":"_MSBBSKMpivg"}},{"cell_type":"code","source":["!python result_analysis_script/mul_digitwise_error_colormap.py results/40_digit_times_1_digit/reverse_out/40_digit_times_1_digit/test_reverse_results.csv --max_steps 3000"],"metadata":{"id":"Ubs8HAlept0H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Comparison Error Rate"],"metadata":{"id":"fTNrvoFH1Gns"}},{"cell_type":"markdown","source":["Contrast Pairs"],"metadata":{"id":"4DW4MdEw1elY"}},{"cell_type":"code","source":["!python result_analysis_script/comparison_error_rate.py \\\n","  results/comparison_bal/comparison_bal_1/thousands_diff_only_results.csv \\\n","  results/comparison_bal/comparison_bal_1/hundreds_diff_only_results.csv \\\n","  results/comparison_bal/comparison_bal_1/tens_diff_only_results.csv \\\n","  results/comparison_bal/comparison_bal_1/units_diff_only_results.csv \\\n","  --output_file_name contrast_pair_error_rate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lvPu4BRg1hzv","executionInfo":{"status":"ok","timestamp":1769390017898,"user_tz":360,"elapsed":10936,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"}},"outputId":"c9bb6e08-9a6b-4bda-ce24-d767709cba20"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved plot to results/comparison_bal/comparison_bal_1/contrast_pair_error_rate\n"]}]},{"cell_type":"markdown","source":["## Sorting Task"],"metadata":{"id":"zRHI0Sm3uyhG"}},{"cell_type":"markdown","source":["#### Sorting Subskill Accuracy 10% to 90% Range"],"metadata":{"id":"nvWbmBTe_eo2"}},{"cell_type":"code","source":["!python result_analysis_script/sorting_acc_10_90_range.py \\\n","  --csv \\\n","    results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction/4_operands_sorting_doubly_balanced_conflicting_same_correction/test_results.csv \\\n","    results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction/4_operands_sorting_doubly_balanced_conflicting_same_correction/digitwise_random_results.csv \\\n","    results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction/4_operands_sorting_doubly_balanced_conflicting_same_correction/digitwise_thousand_results.csv \\\n","    results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction/4_operands_sorting_doubly_balanced_conflicting_same_correction/digitwise_hundred_results.csv \\\n","    results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction/4_operands_sorting_doubly_balanced_conflicting_same_correction/digitwise_ten_results.csv \\\n","  --positions 1,2,3,4 \\\n","  --mode length first second third fourth\n"],"metadata":{"id":"4i7Dx_c2_mC2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Sorting Mixing Error"],"metadata":{"id":"7Pn0bJmxsooN"}},{"cell_type":"code","source":["!python result_analysis_script/mixing_error.py results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction_v2/4_operands_sorting_doubly_balanced_conflicting_same_correction_v2/1_3_same_2_4_agreeing_v2_results.csv\n"],"metadata":{"id":"SSM9s55usiis"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
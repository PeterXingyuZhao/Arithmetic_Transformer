{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":14185,"status":"ok","timestamp":1767995019898,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":360},"id":"aftd1RezTYnF","outputId":"1f55e61f-095a-4013-e44f-65c8438640e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/addition\n","configuration_files\t       model_t5bias.py\n","configurator.py\t\t       __pycache__\n","data\t\t\t       README.md\n","data_generate.py\t       result_analysis.ipynb\n","data_generation_script\t       result_analysis.py\n","eval_ckpt.py\t\t       results\n","evaluation_ckpt.py\t       startHere100M.ipynb\n","evaluation.py\t\t       startHere1B.ipynb\n","extra_result_analysis_scripts  startHere20M.ipynb\n","legacy_code\t\t       startHere.ipynb\n","main_utilities.py\t       statistical_measurements.py\n","meta_all_ascii_chars.pkl       train.py\n","model.py\t\t       train_work_perfect_per_example_batch.py\n","model_rope.py\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change directory to your code\n","%cd /content/drive/MyDrive/addition\n","%pwd   # verify you‚Äôre in the right place\n","!ls    # should show train.py, 4_operands_addition.txt, etc."]},{"cell_type":"markdown","metadata":{"id":"lbEXozJBCm_X"},"source":["# Quick Start Example\n","## Choose One Task and Start Training"]},{"cell_type":"markdown","metadata":{"id":"fUKKZHtTI1n3"},"source":["#### Generate Addition Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbKQwH__Bq4E"},"outputs":[],"source":["!python data_generate.py --task addition --num_operands 2 --experiment_name 2_operands_0_to_999_uniform --train_size 10000 --test_size 3000 --val_size 3000 --train_eval True --sample-size 3000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"DBmQXdsiI6YE"},"source":["#### Generate Multiplication Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hr3n0w_gIf7i"},"outputs":[],"source":["!python data_generate.py --task multiplication --num_operands 6 --experiment_name 0_to_999999_times_1_digit \\\n","        --train_size 10000 --test_size 3000 --val_size 3000 --train_eval True --sample-size 3000 --generate_reverse True"]},{"cell_type":"markdown","metadata":{"id":"fB5aQJxiI8_h"},"source":["#### Generate Sorting Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Q7blcAsIVHV"},"outputs":[],"source":["!python data_generate.py --task sorting --experiment_name 4_operands_sorting_balanced_digit \\\n","        --train_size 1000000 --test_size 10000 --val_size 10000 --train_eval True --sample-size 10000"]},{"cell_type":"markdown","metadata":{"id":"8i_CDutAI_fC"},"source":["## Let's Start Training!"]},{"cell_type":"markdown","metadata":{"id":"Et7rULRph_Ne"},"source":["#### The .txt file is the configuration file"]},{"cell_type":"markdown","metadata":{"id":"dZJI2octhC3o"},"source":["## 2 Operands"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"QK9_9lzwn3VX","outputId":"e972e230-4aff-494f-c8f5-ef9bf09fd181"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading config file: /content/drive/MyDrive/addition/configuration_files/2_operands_addition_reversed.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 50\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '2_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '2_operands_0_to_999_uniform_reverse'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/2_operands_0_to_999_uniform/reverse_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/2_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","Seeded everything: 1337\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test_reverse.txt\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.63M\n","/content/drive/MyDrive/addition/train_work_perfect_per_example_batch.py:443: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/2_operands_0_to_999_uniform/reverse_out/wandb/run-20260105_053059-ddn9h2et\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2_operands_0_to_999_uniform_reverse\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/2_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/2_op_addition/runs/ddn9h2et\u001b[0m\n","WARNING: results directory results/2_operands_0_to_999_uniform/reverse_out/2_operands_0_to_999_uniform_reverse already exists, overwriting...\n","max_new_tokens: 5\n","iter 0: train loss 2.6638, val loss 2.6732\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","100% 25/25 [00:01\u003c00:00, 20.35it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 0/3000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","100% 25/25 [00:00\u003c00:00, 36.67it/s]\n","W0105 05:31:22.357000 3853 torch/_inductor/utils.py:1558] [0/2] Not enough SMs to use max_autotune_gemm mode\n","iter 50: train loss 1.6669, val loss 1.6490\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 1/3000  (0.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.47it/s]\n","iter 100: train loss 1.6440, val loss 1.6485\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 2/3000  (0.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.43it/s]\n","iter 150: train loss 1.6420, val loss 1.6568\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 3/3000  (0.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.18it/s]\n","iter 200: train loss 1.6414, val loss 1.6471\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 34.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 1/3000  (0.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.16it/s]\n","iter 250: train loss 1.6503, val loss 1.6498\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 2/3000  (0.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.70it/s]\n","iter 300: train loss 1.6422, val loss 1.6432\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 2/3000  (0.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.64it/s]\n","iter 350: train loss 1.6413, val loss 1.6499\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 2/3000  (0.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.27it/s]\n","iter 400: train loss 1.6338, val loss 1.6469\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 4/3000  (0.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.85it/s]\n","iter 450: train loss 1.6446, val loss 1.6427\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 2/3000  (0.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 34.50it/s]\n","iter 500: train loss 1.6302, val loss 1.6265\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 3/3000  (0.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.88it/s]\n","iter 550: train loss 1.6171, val loss 1.6155\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 7/3000  (0.23%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.65it/s]\n","iter 600: train loss 1.6042, val loss 1.6046\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 13/3000  (0.43%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.69it/s]\n","iter 650: train loss 1.5553, val loss 1.5566\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 33.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 14/3000  (0.47%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.62it/s]\n","iter 700: train loss 1.5062, val loss 1.5196\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 34.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 20/3000  (0.67%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.23it/s]\n","iter 750: train loss 1.4904, val loss 1.4974\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 30/3000  (1.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.29it/s]\n","iter 800: train loss 1.4726, val loss 1.4775\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 33.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 27/3000  (0.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.32it/s]\n","iter 850: train loss 1.4509, val loss 1.4610\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 26/3000  (0.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 31.58it/s]\n","iter 900: train loss 1.4346, val loss 1.4418\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 45/3000  (1.50%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.76it/s]\n","iter 950: train loss 1.4450, val loss 1.4527\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 30/3000  (1.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.12it/s]\n","iter 1000: train loss 1.4315, val loss 1.4384\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 45/3000  (1.50%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.51it/s]\n","iter 1050: train loss 1.4232, val loss 1.4362\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 39/3000  (1.30%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.76it/s]\n","iter 1100: train loss 1.3581, val loss 1.3671\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 77/3000  (2.57%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.48it/s]\n","iter 1150: train loss 1.2901, val loss 1.3091\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 161/3000  (5.37%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.11it/s]\n","iter 1200: train loss 1.2726, val loss 1.2862\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 228/3000  (7.60%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 33.08it/s]\n","iter 1250: train loss 1.2602, val loss 1.2680\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 254/3000  (8.47%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.06it/s]\n","iter 1300: train loss 1.2668, val loss 1.2743\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 33.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 264/3000  (8.80%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.39it/s]\n","iter 1350: train loss 1.2563, val loss 1.2730\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 34.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 298/3000  (9.93%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.43it/s]\n","iter 1400: train loss 1.2503, val loss 1.2649\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 33.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 266/3000  (8.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.99it/s]\n","iter 1450: train loss 1.2502, val loss 1.2525\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 284/3000  (9.47%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 33.55it/s]\n","iter 1500: train loss 1.2343, val loss 1.2471\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 336/3000  (11.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 34.33it/s]\n","iter 1550: train loss 1.2340, val loss 1.2429\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 435/3000  (14.50%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.65it/s]\n","iter 1600: train loss 1.2254, val loss 1.2407\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 32.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 390/3000  (13.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.23it/s]\n","iter 1650: train loss 1.2220, val loss 1.2279\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 487/3000  (16.23%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 32.77it/s]\n","iter 1700: train loss 1.2283, val loss 1.2373\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 447/3000  (14.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.71it/s]\n","iter 1750: train loss 1.2224, val loss 1.2301\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 455/3000  (15.17%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.33it/s]\n","iter 1800: train loss 1.2203, val loss 1.2213\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 472/3000  (15.73%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.33it/s]\n","iter 1850: train loss 1.2204, val loss 1.2291\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 472/3000  (15.73%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.00it/s]\n","iter 1900: train loss 1.2274, val loss 1.2254\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 516/3000  (17.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.38it/s]\n","iter 1950: train loss 1.2163, val loss 1.2351\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 497/3000  (16.57%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.29it/s]\n","iter 2000: train loss 1.2250, val loss 1.2254\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 3000 examples: 500/3000  (16.67%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.99it/s]\n","iter 2050: train loss 1.2193, val loss 1.2315\n","Using precomputed batches\n"," 80% 20/25 [00:00\u003c00:00, 36.10it/s]"]}],"source":["!python train_work_perfect_per_example_batch.py 2_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"qTJ1568DCuHp"},"outputs":[],"source":["!python train.py 2_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1359876,"status":"ok","timestamp":1767589405355,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":360},"id":"3C_q2HCzg9rf","outputId":"18f7aaf1-0a36-4cde-f410-48aefbe6fb1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/2_operands_addition_plain.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 50\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '2_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '2_operands_0_to_999_uniform_plain'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='plain'\n","operator='+'\n","dataset = 'bal'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_plain.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 5000\n","lr_decay_iters = 5000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/2_operands_0_to_999_uniform/plain_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/2_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test.txt'\n","main_test_name = \"test\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","all the unique characters: \\n, $, +, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \u003cpad\u003e, =\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.63M\n","/content/drive/MyDrive/addition/train.py:583: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W\u0026B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W\u0026B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/2_operands_0_to_999_uniform/plain_out/wandb/run-20260105_044125-ciscy9ie\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2_operands_0_to_999_uniform_plain\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/2_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/2_op_addition/runs/ciscy9ie\u001b[0m\n","WARNING: results directory results/2_operands_0_to_999_uniform/plain_out/2_operands_0_to_999_uniform_plain already exists, overwriting...\n","max_new_tokens: 5\n","W0105 04:41:51.825000 3624 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","iter 0: train loss 2.6788, val loss 2.6888\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","100% 25/25 [00:01\u003c00:00, 19.55it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test.txt, 3000 examples: 0/3000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/train_eval.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/2_operands_0_to_999_uniform/train_eval.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 25 batches\n","100% 25/25 [00:00\u003c00:00, 36.00it/s]\n","iter 50: train loss 1.6241, val loss 1.6345\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.90it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 0/3000  (0.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.48it/s]\n","iter 100: train loss 1.5488, val loss 1.5573\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.89it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 11/3000  (0.37%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.78it/s]\n","iter 150: train loss 1.5024, val loss 1.5086\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.83it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 14/3000  (0.47%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.28it/s]\n","iter 200: train loss 1.4671, val loss 1.4652\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.62it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 28/3000  (0.93%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.22it/s]\n","iter 250: train loss 1.3683, val loss 1.3738\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.01it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 99/3000  (3.30%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.20it/s]\n","iter 300: train loss 1.3504, val loss 1.3446\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.01it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 120/3000  (4.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.04it/s]\n","iter 350: train loss 1.3223, val loss 1.3290\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.33it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 155/3000  (5.17%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.96it/s]\n","iter 400: train loss 1.3226, val loss 1.3170\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.08it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 182/3000  (6.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.86it/s]\n","iter 450: train loss 1.3065, val loss 1.3244\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.15it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 162/3000  (5.40%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 33.98it/s]\n","iter 500: train loss 1.3120, val loss 1.3110\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.56it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 187/3000  (6.23%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.30it/s]\n","iter 550: train loss 1.2937, val loss 1.3114\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.78it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 176/3000  (5.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.86it/s]\n","iter 600: train loss 1.2880, val loss 1.3008\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.85it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 179/3000  (5.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.59it/s]\n","iter 650: train loss 1.2949, val loss 1.3001\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.50it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 184/3000  (6.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.37it/s]\n","iter 700: train loss 1.2839, val loss 1.2993\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.25it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 209/3000  (6.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.10it/s]\n","iter 750: train loss 1.2831, val loss 1.2866\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.58it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 234/3000  (7.80%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.05it/s]\n","iter 800: train loss 1.2813, val loss 1.2846\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.75it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 223/3000  (7.43%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.31it/s]\n","iter 850: train loss 1.2755, val loss 1.2862\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.11it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 240/3000  (8.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.64it/s]\n","iter 900: train loss 1.2651, val loss 1.2753\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.27it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 232/3000  (7.73%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.93it/s]\n","iter 950: train loss 1.2640, val loss 1.2812\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.94it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 237/3000  (7.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.14it/s]\n","iter 1000: train loss 1.2641, val loss 1.2771\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.69it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 255/3000  (8.50%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.42it/s]\n","iter 1050: train loss 1.2082, val loss 1.2129\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 38.19it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 643/3000  (21.43%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.74it/s]\n","iter 1100: train loss 1.1560, val loss 1.1588\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.42it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 1135/3000  (37.83%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.22it/s]\n","iter 1150: train loss 1.1340, val loss 1.1448\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.83it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 1459/3000  (48.63%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.26it/s]\n","iter 1200: train loss 1.1124, val loss 1.1136\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.24it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 1860/3000  (62.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.07it/s]\n","iter 1250: train loss 1.0996, val loss 1.1015\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.00it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2172/3000  (72.40%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.51it/s]\n","iter 1300: train loss 1.0746, val loss 1.0845\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.05it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2336/3000  (77.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.03it/s]\n","iter 1350: train loss 1.0685, val loss 1.0757\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.47it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2404/3000  (80.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.92it/s]\n","iter 1400: train loss 1.0556, val loss 1.0648\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.24it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2546/3000  (84.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.23it/s]\n","iter 1450: train loss 1.0461, val loss 1.0600\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.40it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2575/3000  (85.83%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.47it/s]\n","iter 1500: train loss 1.0429, val loss 1.0567\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.57it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2663/3000  (88.77%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.79it/s]\n","iter 1550: train loss 1.0372, val loss 1.0538\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.79it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2697/3000  (89.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.66it/s]\n","iter 1600: train loss 1.0399, val loss 1.0530\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.45it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2699/3000  (89.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.14it/s]\n","iter 1650: train loss 1.0523, val loss 1.0538\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.26it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2708/3000  (90.27%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.76it/s]\n","iter 1700: train loss 1.0422, val loss 1.0524\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.58it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2764/3000  (92.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.01it/s]\n","iter 1750: train loss 1.0381, val loss 1.0516\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.91it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2758/3000  (91.93%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.58it/s]\n","iter 1800: train loss 1.0321, val loss 1.0503\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.27it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2748/3000  (91.60%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.35it/s]\n","iter 1850: train loss 1.0370, val loss 1.0528\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.01it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2775/3000  (92.50%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 34.99it/s]\n","iter 1900: train loss 1.0347, val loss 1.0482\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.05it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2815/3000  (93.83%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.91it/s]\n","iter 1950: train loss 1.0226, val loss 1.0535\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.07it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2802/3000  (93.40%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.25it/s]\n","iter 2000: train loss 1.0207, val loss 1.0487\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.90it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2799/3000  (93.30%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.79it/s]\n","iter 2050: train loss 1.0312, val loss 1.0502\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.30it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2819/3000  (93.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.77it/s]\n","iter 2100: train loss 1.0271, val loss 1.0495\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.09it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2829/3000  (94.30%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.52it/s]\n","iter 2150: train loss 1.0272, val loss 1.0531\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.28it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2820/3000  (94.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.57it/s]\n","iter 2200: train loss 1.0141, val loss 1.0523\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.12it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2822/3000  (94.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.92it/s]\n","iter 2250: train loss 1.0218, val loss 1.0549\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.11it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2831/3000  (94.37%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.00it/s]\n","iter 2300: train loss 1.0188, val loss 1.0517\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.23it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2841/3000  (94.70%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.90it/s]\n","iter 2350: train loss 1.0175, val loss 1.0503\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.71it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2856/3000  (95.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.94it/s]\n","iter 2400: train loss 1.0181, val loss 1.0512\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.39it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2851/3000  (95.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.70it/s]\n","iter 2450: train loss 1.0212, val loss 1.0541\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.27it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2836/3000  (94.53%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.99it/s]\n","iter 2500: train loss 1.0158, val loss 1.0552\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.12it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2864/3000  (95.47%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.57it/s]\n","iter 2550: train loss 1.0132, val loss 1.0546\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.33it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2883/3000  (96.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.21it/s]\n","iter 2600: train loss 1.0037, val loss 1.0554\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.47it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2879/3000  (95.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.31it/s]\n","iter 2650: train loss 1.0049, val loss 1.0569\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.58it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2877/3000  (95.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.73it/s]\n","iter 2700: train loss 1.0097, val loss 1.0552\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.43it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2868/3000  (95.60%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.57it/s]\n","iter 2750: train loss 1.0069, val loss 1.0582\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.58it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2873/3000  (95.77%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.23it/s]\n","iter 2800: train loss 1.0063, val loss 1.0597\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.09it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2884/3000  (96.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.13it/s]\n","iter 2850: train loss 1.0010, val loss 1.0645\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.90it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2869/3000  (95.63%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.03it/s]\n","iter 2900: train loss 0.9983, val loss 1.0639\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.43it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2882/3000  (96.07%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.41it/s]\n","iter 2950: train loss 0.9904, val loss 1.0718\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.71it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2871/3000  (95.70%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.84it/s]\n","iter 3000: train loss 0.9933, val loss 1.0707\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.59it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2889/3000  (96.30%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.59it/s]\n","iter 3050: train loss 0.9824, val loss 1.0734\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.14it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2892/3000  (96.40%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.79it/s]\n","iter 3100: train loss 0.9856, val loss 1.0691\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.02it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2899/3000  (96.63%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.63it/s]\n","iter 3150: train loss 0.9810, val loss 1.0788\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.59it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2898/3000  (96.60%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.67it/s]\n","iter 3200: train loss 0.9722, val loss 1.0786\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.16it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2907/3000  (96.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.90it/s]\n","iter 3250: train loss 0.9719, val loss 1.0849\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.68it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2901/3000  (96.70%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.99it/s]\n","iter 3300: train loss 0.9676, val loss 1.0869\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 38.03it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2892/3000  (96.40%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.41it/s]\n","iter 3350: train loss 0.9700, val loss 1.0955\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.03it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2900/3000  (96.67%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.26it/s]\n","iter 3400: train loss 0.9650, val loss 1.0953\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.83it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2901/3000  (96.70%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.03it/s]\n","iter 3450: train loss 0.9559, val loss 1.1012\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.36it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2908/3000  (96.93%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.14it/s]\n","iter 3500: train loss 0.9530, val loss 1.1038\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.08it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2904/3000  (96.80%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.07it/s]\n","iter 3550: train loss 0.9448, val loss 1.1056\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.73it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2907/3000  (96.90%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.59it/s]\n","iter 3600: train loss 0.9474, val loss 1.1124\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.94it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2911/3000  (97.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.15it/s]\n","iter 3650: train loss 0.9332, val loss 1.1125\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.66it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2900/3000  (96.67%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.41it/s]\n","iter 3700: train loss 0.9305, val loss 1.1173\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.32it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2918/3000  (97.27%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.22it/s]\n","iter 3750: train loss 0.9337, val loss 1.1219\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.31it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2906/3000  (96.87%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.95it/s]\n","iter 3800: train loss 0.9165, val loss 1.1197\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.14it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2900/3000  (96.67%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.19it/s]\n","iter 3850: train loss 0.9174, val loss 1.1340\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.12it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2914/3000  (97.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.88it/s]\n","iter 3900: train loss 0.9225, val loss 1.1313\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.02it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2904/3000  (96.80%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.56it/s]\n","iter 3950: train loss 0.9075, val loss 1.1434\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.78it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2913/3000  (97.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.26it/s]\n","iter 4000: train loss 0.9050, val loss 1.1464\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.69it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2909/3000  (96.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.83it/s]\n","iter 4050: train loss 0.8924, val loss 1.1511\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.66it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2911/3000  (97.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.18it/s]\n","iter 4100: train loss 0.8894, val loss 1.1566\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.65it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2914/3000  (97.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.36it/s]\n","iter 4150: train loss 0.8821, val loss 1.1667\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.48it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2911/3000  (97.03%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.45it/s]\n","iter 4200: train loss 0.8727, val loss 1.1647\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.15it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2909/3000  (96.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.84it/s]\n","iter 4250: train loss 0.8729, val loss 1.1783\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.18it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2914/3000  (97.13%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.96it/s]\n","iter 4300: train loss 0.8745, val loss 1.1777\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.99it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2913/3000  (97.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.67it/s]\n","iter 4350: train loss 0.8666, val loss 1.1909\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.17it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2916/3000  (97.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.03it/s]\n","iter 4400: train loss 0.8627, val loss 1.1906\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.25it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2913/3000  (97.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.29it/s]\n","iter 4450: train loss 0.8598, val loss 1.1915\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.30it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2921/3000  (97.37%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.06it/s]\n","iter 4500: train loss 0.8506, val loss 1.1999\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.59it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2921/3000  (97.37%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.79it/s]\n","iter 4550: train loss 0.8510, val loss 1.2034\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.60it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2920/3000  (97.33%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.65it/s]\n","iter 4600: train loss 0.8357, val loss 1.2089\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.38it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2909/3000  (96.97%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.25it/s]\n","iter 4650: train loss 0.8468, val loss 1.2123\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.35it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2910/3000  (97.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.45it/s]\n","iter 4700: train loss 0.8436, val loss 1.2159\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 38.03it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2916/3000  (97.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.03it/s]\n","iter 4750: train loss 0.8316, val loss 1.2310\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.20it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2910/3000  (97.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.03it/s]\n","iter 4800: train loss 0.8259, val loss 1.2367\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 35.97it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2913/3000  (97.10%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.01it/s]\n","iter 4850: train loss 0.8171, val loss 1.2394\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.84it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2910/3000  (97.00%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.31it/s]\n","iter 4900: train loss 0.8148, val loss 1.2403\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.31it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2918/3000  (97.27%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.70it/s]\n","iter 4950: train loss 0.8184, val loss 1.2429\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.36it/s]\n","\n","Test Results:\n","test.txt, 3000 examples: 2916/3000  (97.20%)\n","\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.01it/s]\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 37.04it/s]\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.69it/s]\n","Using precomputed batches\n","100% 25/25 [00:00\u003c00:00, 36.96it/s]\n","\n","Final Test Results:\n","test.txt, 3000 examples: 2912/3000  (97.07%)\n","\n","Running result analysis on results/2_operands_0_to_999_uniform/plain_out/2_operands_0_to_999_uniform_plain_1/test_results.csv\n","Saved digit error plot to results/2_operands_0_to_999_uniform/plain_out/2_operands_0_to_999_uniform_plain_1/digitwise_errors.png\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33m2_operands_0_to_999_uniform_plain\u001b[0m at: \u001b[34m\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mresults/2_operands_0_to_999_uniform/plain_out/wandb/run-20260105_044125-ciscy9ie/logs\u001b[0m\n"]}],"source":["!python train.py 2_operands_addition_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"JLXLrm_chIgo"},"source":["## 4 Operands"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"8S_LDDfvhGLM","outputId":"ee390f98-d079-48bd-f4dc-5ff231017e70"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/4_operands_addition_reversed.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 2000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_0_to_999_uniform_reverse'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","batch_size = 512\n","block_size = 32 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000 # for four operands addition, recommend \u003e= 200000\n","lr_decay_iters = 800000 # make equal to max_iters\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/reverse_out_complete_MI'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","# to edit: Can be 'units', 'tens', 'hundreds', 'thousands', or None\n","randomize = None\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","all the unique characters: \\n, $, +, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \u003cpad\u003e, =\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.63M\n","/content/drive/MyDrive/addition/train.py:587: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W\u0026B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W\u0026B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/reverse_out_complete_MI/wandb/run-20260107_024031-uguvc4kg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m4_operands_0_to_999_uniform_reverse\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/uguvc4kg\u001b[0m\n","WARNING: results directory results/4_operands_0_to_999_uniform/reverse_out_complete_MI/4_operands_0_to_999_uniform_reverse already exists, overwriting...\n","max_new_tokens: 5\n","W0107 02:41:05.207000 1926 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","iter 0: train loss 2.7106, val loss 2.7093\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:02\u003c00:00, 28.24it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02\u003c00:00, 36.42it/s]\n","iter 2000: train loss 1.6504, val loss 1.6621\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.26it/s]\n","iter 4000: train loss 1.5626, val loss 1.5629\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 77/10000  (0.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.95it/s]\n","iter 6000: train loss 1.5441, val loss 1.5488\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 99/10000  (0.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.90it/s]\n","iter 8000: train loss 1.5423, val loss 1.5450\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 91/10000  (0.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.67it/s]\n","iter 10000: train loss 1.5385, val loss 1.5451\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 110/10000  (1.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.37it/s]\n","iter 12000: train loss 1.5440, val loss 1.5437\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 115/10000  (1.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.95it/s]\n","iter 14000: train loss 1.5399, val loss 1.5437\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 96/10000  (0.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.18it/s]\n","iter 16000: train loss 1.5380, val loss 1.5428\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 111/10000  (1.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.93it/s]\n","iter 18000: train loss 1.5390, val loss 1.5439\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 88/10000  (0.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.52it/s]\n","iter 20000: train loss 1.5390, val loss 1.5438\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 102/10000  (1.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.54it/s]\n","iter 22000: train loss 1.5348, val loss 1.5427\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 96/10000  (0.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.58it/s]\n","iter 24000: train loss 1.5391, val loss 1.5432\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 110/10000  (1.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.88it/s]\n","iter 26000: train loss 1.5387, val loss 1.5434\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 74/10000  (0.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.94it/s]\n","iter 28000: train loss 1.5419, val loss 1.5432\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 90/10000  (0.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.43it/s]\n","iter 30000: train loss 1.5377, val loss 1.5432\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 99/10000  (0.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.57it/s]\n","iter 32000: train loss 1.5401, val loss 1.5436\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 105/10000  (1.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.38it/s]\n","iter 34000: train loss 1.5402, val loss 1.5427\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 100/10000  (1.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.46it/s]\n","iter 36000: train loss 1.5328, val loss 1.5424\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 96/10000  (0.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.01it/s]\n","iter 38000: train loss 1.5366, val loss 1.5427\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 100/10000  (1.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.16it/s]\n","iter 40000: train loss 1.5367, val loss 1.5425\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 107/10000  (1.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.27it/s]\n","iter 42000: train loss 1.5458, val loss 1.5419\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 79/10000  (0.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.56it/s]\n","iter 44000: train loss 1.5427, val loss 1.5418\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 91/10000  (0.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.67it/s]\n","iter 46000: train loss 1.5399, val loss 1.5424\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 91/10000  (0.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.41it/s]\n","iter 48000: train loss 1.4561, val loss 1.4563\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 587/10000  (5.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.55it/s]\n","iter 50000: train loss 1.4324, val loss 1.4319\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 898/10000  (8.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.50it/s]\n","iter 52000: train loss 1.4237, val loss 1.4307\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 919/10000  (9.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.74it/s]\n","iter 54000: train loss 1.4210, val loss 1.4248\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 981/10000  (9.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.90it/s]\n","iter 56000: train loss 1.4213, val loss 1.4253\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 991/10000  (9.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.78it/s]\n","iter 58000: train loss 1.4174, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1005/10000  (10.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.55it/s]\n","iter 60000: train loss 1.4220, val loss 1.4246\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 984/10000  (9.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.05it/s]\n","iter 62000: train loss 1.4230, val loss 1.4258\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 996/10000  (9.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.70it/s]\n","iter 64000: train loss 1.4209, val loss 1.4265\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1017/10000  (10.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.66it/s]\n","iter 66000: train loss 1.4197, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1008/10000  (10.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.53it/s]\n","iter 68000: train loss 1.4192, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 962/10000  (9.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.95it/s]\n","iter 70000: train loss 1.4249, val loss 1.4292\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 916/10000  (9.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.16it/s]\n","iter 72000: train loss 1.4189, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1007/10000  (10.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.38it/s]\n","iter 74000: train loss 1.4171, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 950/10000  (9.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.20it/s]\n","iter 76000: train loss 1.4231, val loss 1.4246\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1010/10000  (10.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.63it/s]\n","iter 78000: train loss 1.4261, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1024/10000  (10.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.99it/s]\n","iter 80000: train loss 1.4194, val loss 1.4237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 985/10000  (9.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.59it/s]\n","iter 82000: train loss 1.4200, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1013/10000  (10.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.25it/s]\n","iter 84000: train loss 1.4188, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 951/10000  (9.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.86it/s]\n","iter 86000: train loss 1.4208, val loss 1.4241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 953/10000  (9.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.54it/s]\n","iter 88000: train loss 1.4206, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1008/10000  (10.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.52it/s]\n","iter 90000: train loss 1.4161, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 924/10000  (9.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.97it/s]\n","iter 92000: train loss 1.4233, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 988/10000  (9.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.17it/s]\n","iter 94000: train loss 1.4203, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 956/10000  (9.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.57it/s]\n","iter 96000: train loss 1.4222, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 957/10000  (9.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.18it/s]\n","iter 98000: train loss 1.4198, val loss 1.4244\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1029/10000  (10.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.76it/s]\n","iter 100000: train loss 1.4177, val loss 1.4239\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 967/10000  (9.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.30it/s]\n","iter 102000: train loss 1.4235, val loss 1.4238\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1019/10000  (10.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.07it/s]\n","iter 104000: train loss 1.4246, val loss 1.4240\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1012/10000  (10.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.97it/s]\n","iter 106000: train loss 1.4207, val loss 1.4243\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 990/10000  (9.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.65it/s]\n","iter 108000: train loss 1.3252, val loss 1.3229\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7985/10000  (79.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.50it/s]\n","iter 110000: train loss 1.3031, val loss 1.3072\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9876/10000  (98.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.13it/s]\n","iter 112000: train loss 1.3022, val loss 1.3071\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9864/10000  (98.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.08it/s]\n","iter 114000: train loss 1.3042, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9973/10000  (99.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.45it/s]\n","iter 116000: train loss 1.3000, val loss 1.3063\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9955/10000  (99.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.16it/s]\n","iter 118000: train loss 1.3010, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9968/10000  (99.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.87it/s]\n","iter 120000: train loss 1.3074, val loss 1.3075\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9818/10000  (98.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.78it/s]\n","iter 122000: train loss 1.3065, val loss 1.3063\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9958/10000  (99.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.66it/s]\n","iter 124000: train loss 1.3015, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.15it/s]\n","iter 126000: train loss 1.3032, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9968/10000  (99.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.96it/s]\n","iter 128000: train loss 1.3006, val loss 1.3062\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9972/10000  (99.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.97it/s]\n","iter 130000: train loss 1.3057, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9982/10000  (99.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.51it/s]\n","iter 132000: train loss 1.3037, val loss 1.3067\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9983/10000  (99.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.66it/s]\n","iter 134000: train loss 1.3019, val loss 1.3067\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9961/10000  (99.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.93it/s]\n","iter 136000: train loss 1.3073, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9935/10000  (99.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.66it/s]\n","iter 138000: train loss 1.3055, val loss 1.3064\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9987/10000  (99.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.15it/s]\n","iter 140000: train loss 1.3021, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.44it/s]\n","iter 142000: train loss 1.3027, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9987/10000  (99.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.79it/s]\n","iter 144000: train loss 1.3088, val loss 1.3064\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9901/10000  (99.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.61it/s]\n","iter 146000: train loss 1.3032, val loss 1.3062\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9978/10000  (99.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.60it/s]\n","iter 148000: train loss 1.3011, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.65it/s]\n","iter 150000: train loss 1.2976, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9987/10000  (99.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.00it/s]\n","iter 152000: train loss 1.3011, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9985/10000  (99.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.03it/s]\n","iter 154000: train loss 1.3019, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9986/10000  (99.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.26it/s]\n","iter 156000: train loss 1.3019, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.74it/s]\n","iter 158000: train loss 1.3014, val loss 1.3065\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9985/10000  (99.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.45it/s]\n","iter 160000: train loss 1.3061, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9961/10000  (99.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.90it/s]\n","iter 162000: train loss 1.3009, val loss 1.3052\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9989/10000  (99.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.45it/s]\n","iter 164000: train loss 1.3027, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9977/10000  (99.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.67it/s]\n","iter 166000: train loss 1.3034, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9987/10000  (99.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.77it/s]\n","iter 168000: train loss 1.3038, val loss 1.3065\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9966/10000  (99.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.64it/s]\n","iter 170000: train loss 1.3036, val loss 1.3067\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9993/10000  (99.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.50it/s]\n","iter 172000: train loss 1.2988, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.00it/s]\n","iter 174000: train loss 1.3032, val loss 1.3067\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9982/10000  (99.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.63it/s]\n","iter 176000: train loss 1.3022, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9978/10000  (99.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.12it/s]\n","iter 178000: train loss 1.3023, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9979/10000  (99.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.11it/s]\n","iter 180000: train loss 1.3009, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.40it/s]\n","iter 182000: train loss 1.2988, val loss 1.3056\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9983/10000  (99.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.56it/s]\n","iter 184000: train loss 1.3026, val loss 1.3061\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9989/10000  (99.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.83it/s]\n","iter 186000: train loss 1.3015, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9985/10000  (99.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.60it/s]\n","iter 188000: train loss 1.3028, val loss 1.3062\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.32it/s]\n","iter 190000: train loss 1.2974, val loss 1.3062\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9977/10000  (99.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.97it/s]\n","iter 192000: train loss 1.3050, val loss 1.3062\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9986/10000  (99.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.20it/s]\n","iter 194000: train loss 1.3040, val loss 1.3062\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9983/10000  (99.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.70it/s]\n","iter 196000: train loss 1.3034, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.24it/s]\n","iter 198000: train loss 1.3051, val loss 1.3054\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 37.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9988/10000  (99.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.55it/s]\n","iter 200000: train loss 1.3048, val loss 1.3059\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9989/10000  (99.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.19it/s]\n","iter 202000: train loss 1.3021, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9976/10000  (99.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.52it/s]\n","iter 204000: train loss 1.2995, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9979/10000  (99.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.17it/s]\n","iter 206000: train loss 1.3076, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9989/10000  (99.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.37it/s]\n","iter 208000: train loss 1.3013, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.64it/s]\n","iter 210000: train loss 1.3001, val loss 1.3058\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9993/10000  (99.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.44it/s]\n","iter 212000: train loss 1.3028, val loss 1.3064\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9986/10000  (99.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.39it/s]\n","iter 214000: train loss 1.3001, val loss 1.3057\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9975/10000  (99.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.84it/s]\n","iter 216000: train loss 1.3006, val loss 1.3063\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9987/10000  (99.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 37.14it/s]\n","iter 218000: train loss 1.3015, val loss 1.3060\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9977/10000  (99.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.22it/s]\n","iter 220000: train loss 1.3021, val loss 1.3055\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.33it/s]\n"]}],"source":["!python train.py 4_operands_addition_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0RjN0ldhMGz"},"outputs":[],"source":["!python train.py 4_operands_addition_plain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lbSWGEKuPrVZ"},"outputs":[],"source":["!python train.py 4_operands_addition_perm_3412.txt"]},{"cell_type":"markdown","metadata":{"id":"QDxgYE9zhL02"},"source":["## Multiplication"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNcrd8bDha9j"},"outputs":[],"source":["!python train.py 2_operands_mul_reversed.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nn2GYDQhX7b"},"outputs":[],"source":["!python train.py 2_operands_mul_plain.txt"]},{"cell_type":"markdown","metadata":{"id":"asYxDysPKOzI"},"source":["# Other Commands (May not be fully working)"]},{"cell_type":"markdown","metadata":{"id":"L0ttXoH9PGRV"},"source":["## 4 Operand Addition Scratchpad"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3F4qYRSCQZhE"},"outputs":[],"source":["%cat configuration_files/4_operands_addition_scratchpad.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ZEiUegj1UDm-"},"outputs":[],"source":["%cat evaluation.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"hGSNQOkyPFQF"},"outputs":[],"source":["!python train.py 4_operands_addition_scratchpad.txt"]},{"cell_type":"markdown","metadata":{"id":"dfLn10UAHrM2"},"source":["## 4 Operand Max"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"PFYzFx4vK6Px"},"outputs":[],"source":["%cat configuration_files/4_operands_max.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"CEGW4g1dLgnQ"},"outputs":[],"source":["%cat train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"SSG2giBcf0u9"},"outputs":[],"source":["%cat model.py"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"X2hV4DOEHtrh"},"outputs":[],"source":["!python train.py 4_operands_max.txt"]},{"cell_type":"markdown","metadata":{"id":"7UHO3zBc_jY3"},"source":["## 4 Operand Sorting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXiwk0EUyudt"},"outputs":[],"source":["%cat configuration_files/4_operands_sorting.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"elF2BaDD_izl"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: per_example\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/4_operands_sorting_doubly_bal.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 50\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = 'addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = '4_operands_sorting_doubly_balanced_conflicting_same_correction'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='sorting'\n","operator=''\n","batch_size = 512\n","block_size = 64 # context of up to 256 previous characters\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_iter_19000_acc.pt'\n","\n","# to edit: num of digits in each operand\n","num_digit = 4\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 32\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 10000\n","lr_decay_iters = 10000 # make equal to max_iters\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_sorting_doubly_balanced/'\n","\n","# to edit: training data\n","train_data_name = 'train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# If it's a directory, \"main_test_name\" is the name of the test file (without extension) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test'\n","main_test_name = \"test\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","all the unique characters: \\n, $, ,, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \u003cpad\u003e, =\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/1_3_same_2_4_agreeing.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/1_3_same_2_4_conflicting.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/b1_eq_b3diff.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/b1c1_b3c3_bothdiff.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/b3_eq_b1diff.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_hundred.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_random.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_ten.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_thousand.txt\n","/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/test.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.65M\n","/content/drive/MyDrive/addition/train.py:587: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W\u0026B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W\u0026B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_sorting_doubly_balanced/conflicting_same_control_exp_correction/wandb/run-20260109_214436-04qbr05r\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m4_operands_sorting_doubly_balanced_conflicting_same_correction\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/addition/runs/04qbr05r\u001b[0m\n","max_new_tokens: 32\n","W0109 21:45:06.775000 607 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","iter 0: train loss 2.7141, val loss 2.7098\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/1_3_same_2_4_agreeing.txt\n","Preparing batches for 1000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/1_3_same_2_4_agreeing.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 8 batches\n","100% 8/8 [00:03\u003c00:00,  2.17it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/1_3_same_2_4_conflicting.txt\n","Preparing batches for 1000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/1_3_same_2_4_conflicting.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 8 batches\n","100% 8/8 [00:03\u003c00:00,  2.52it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/b1_eq_b3diff.txt\n","Preparing batches for 1000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/b1_eq_b3diff.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 8 batches\n","100% 8/8 [00:03\u003c00:00,  2.52it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/b1c1_b3c3_bothdiff.txt\n","Preparing batches for 1000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/b1c1_b3c3_bothdiff.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 8 batches\n","100% 8/8 [00:03\u003c00:00,  2.25it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/b3_eq_b1diff.txt\n","Preparing batches for 1000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/b3_eq_b1diff.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 8 batches\n","100% 8/8 [00:03\u003c00:00,  2.52it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_hundred.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_hundred.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:09\u003c00:00,  2.48it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_random.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_random.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:09\u003c00:00,  2.48it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_ten.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_ten.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:09\u003c00:00,  2.48it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_thousand.txt\n","Preparing batches for 3000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/digitwise_thousand.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 24 batches\n","100% 24/24 [00:09\u003c00:00,  2.47it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/test.txt\n","Preparing batches for 5000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/test/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 41 batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test.txt, 5000 examples: 0/5000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/train_eval.txt\n","Preparing batches for 5000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_sorting_doubly_balanced/train_eval.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 41 batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 50: train loss 1.5264, val loss 1.4983\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 0/5000  (0.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 100: train loss 1.1533, val loss 1.1541\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4/5000  (0.08%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 150: train loss 1.0302, val loss 1.0151\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 79/5000  (1.58%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 200: train loss 0.8266, val loss 0.8254\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 1706/5000  (34.12%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.77it/s]\n","iter 250: train loss 0.8129, val loss 0.7875\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 2996/5000  (59.92%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.77it/s]\n","iter 300: train loss 0.7921, val loss 0.7820\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3539/5000  (70.78%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 350: train loss 0.7780, val loss 0.7737\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 3965/5000  (79.30%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.73it/s]\n","iter 400: train loss 0.7878, val loss 0.7737\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4018/5000  (80.36%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 450: train loss 0.7764, val loss 0.7698\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4190/5000  (83.80%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 500: train loss 0.7774, val loss 0.7688\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4212/5000  (84.24%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 550: train loss 0.7741, val loss 0.7682\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4331/5000  (86.62%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 600: train loss 0.7940, val loss 0.7667\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4378/5000  (87.56%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 650: train loss 0.7636, val loss 0.7654\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4432/5000  (88.64%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 700: train loss 0.7729, val loss 0.7629\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4556/5000  (91.12%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 750: train loss 0.7644, val loss 0.7628\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4568/5000  (91.36%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 800: train loss 0.7798, val loss 0.7635\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4540/5000  (90.80%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 850: train loss 0.7656, val loss 0.7627\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4611/5000  (92.22%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 900: train loss 0.7801, val loss 0.7618\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4630/5000  (92.60%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 950: train loss 0.7917, val loss 0.7618\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4646/5000  (92.92%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 1000: train loss 0.7813, val loss 0.7613\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4637/5000  (92.74%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 1050: train loss 0.7739, val loss 0.7626\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4683/5000  (93.66%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 1100: train loss 0.7689, val loss 0.7611\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4734/5000  (94.68%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 1150: train loss 0.7730, val loss 0.7605\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4677/5000  (93.54%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 1200: train loss 0.7582, val loss 0.7607\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4708/5000  (94.16%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 1250: train loss 0.7792, val loss 0.7600\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4739/5000  (94.78%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 1300: train loss 0.7729, val loss 0.7607\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4719/5000  (94.38%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 1350: train loss 0.7815, val loss 0.7606\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4705/5000  (94.10%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 1400: train loss 0.7607, val loss 0.7618\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4697/5000  (93.94%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 1450: train loss 0.7772, val loss 0.7617\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4749/5000  (94.98%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 1500: train loss 0.7758, val loss 0.7602\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4757/5000  (95.14%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 1550: train loss 0.7668, val loss 0.7599\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4721/5000  (94.42%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 1600: train loss 0.7773, val loss 0.7603\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4786/5000  (95.72%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 1650: train loss 0.7738, val loss 0.7611\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4813/5000  (96.26%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 1700: train loss 0.7729, val loss 0.7604\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4794/5000  (95.88%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 1750: train loss 0.7737, val loss 0.7601\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4766/5000  (95.32%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 1800: train loss 0.7651, val loss 0.7590\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4780/5000  (95.60%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 1850: train loss 0.7601, val loss 0.7596\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4805/5000  (96.10%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 1900: train loss 0.7731, val loss 0.7593\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4808/5000  (96.16%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 1950: train loss 0.7690, val loss 0.7597\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4804/5000  (96.08%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 2000: train loss 0.7669, val loss 0.7602\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4841/5000  (96.82%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 2050: train loss 0.7713, val loss 0.7602\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4820/5000  (96.40%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 2100: train loss 0.7688, val loss 0.7594\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4833/5000  (96.66%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 2150: train loss 0.7662, val loss 0.7590\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4806/5000  (96.12%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.73it/s]\n","iter 2200: train loss 0.7709, val loss 0.7592\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:15\u003c00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4839/5000  (96.78%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 2250: train loss 0.7799, val loss 0.7593\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4819/5000  (96.38%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 2300: train loss 0.7643, val loss 0.7594\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4821/5000  (96.42%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 2350: train loss 0.7737, val loss 0.7601\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:15\u003c00:00,  2.72it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4817/5000  (96.34%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 2400: train loss 0.7823, val loss 0.7588\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4859/5000  (97.18%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15\u003c00:00,  2.72it/s]\n","iter 2450: train loss 0.7645, val loss 0.7594\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 41/41 [00:15\u003c00:00,  2.72it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4858/5000  (97.16%)\n","\n","Using precomputed batches\n","100% 41/41 [00:15\u003c00:00,  2.72it/s]\n","iter 2500: train loss 0.7639, val loss 0.7590\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4847/5000  (96.94%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 2550: train loss 0.7692, val loss 0.7595\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4810/5000  (96.20%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 2600: train loss 0.7555, val loss 0.7596\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4849/5000  (96.98%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 2650: train loss 0.7755, val loss 0.7590\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4852/5000  (97.04%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 2700: train loss 0.7653, val loss 0.7580\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.73it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4872/5000  (97.44%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 2750: train loss 0.7785, val loss 0.7595\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4833/5000  (96.66%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 2800: train loss 0.7783, val loss 0.7589\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4877/5000  (97.54%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 2850: train loss 0.7625, val loss 0.7596\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4858/5000  (97.16%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 2900: train loss 0.7683, val loss 0.7613\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4875/5000  (97.50%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 2950: train loss 0.7729, val loss 0.7589\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4870/5000  (97.40%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 3000: train loss 0.7728, val loss 0.7593\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4875/5000  (97.50%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 3050: train loss 0.7578, val loss 0.7586\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4855/5000  (97.10%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 3100: train loss 0.7688, val loss 0.7588\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4850/5000  (97.00%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 3150: train loss 0.7648, val loss 0.7586\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4872/5000  (97.44%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 3200: train loss 0.7766, val loss 0.7593\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4870/5000  (97.40%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 3250: train loss 0.7805, val loss 0.7586\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4872/5000  (97.44%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 3300: train loss 0.7706, val loss 0.7581\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4879/5000  (97.58%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 3350: train loss 0.7685, val loss 0.7586\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4887/5000  (97.74%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 3400: train loss 0.7711, val loss 0.7585\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4894/5000  (97.88%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 3450: train loss 0.7674, val loss 0.7594\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4884/5000  (97.68%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 3500: train loss 0.7559, val loss 0.7590\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4885/5000  (97.70%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 3550: train loss 0.7692, val loss 0.7587\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4888/5000  (97.76%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 3600: train loss 0.7731, val loss 0.7581\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4878/5000  (97.56%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 3650: train loss 0.7691, val loss 0.7585\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4884/5000  (97.68%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.77it/s]\n","iter 3700: train loss 0.7619, val loss 0.7588\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4889/5000  (97.78%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 3750: train loss 0.7799, val loss 0.7583\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4911/5000  (98.22%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 3800: train loss 0.7760, val loss 0.7589\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4891/5000  (97.82%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 3850: train loss 0.7673, val loss 0.7586\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4887/5000  (97.74%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.77it/s]\n","iter 3900: train loss 0.7722, val loss 0.7593\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4902/5000  (98.04%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 3950: train loss 0.7664, val loss 0.7591\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4895/5000  (97.90%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 4000: train loss 0.7633, val loss 0.7589\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4901/5000  (98.02%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4050: train loss 0.7590, val loss 0.7587\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4915/5000  (98.30%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4100: train loss 0.7636, val loss 0.7589\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4899/5000  (97.98%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4150: train loss 0.7645, val loss 0.7581\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4916/5000  (98.32%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.77it/s]\n","iter 4200: train loss 0.7655, val loss 0.7590\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4929/5000  (98.58%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 4250: train loss 0.7664, val loss 0.7588\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4920/5000  (98.40%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 4300: train loss 0.7660, val loss 0.7584\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4925/5000  (98.50%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 4350: train loss 0.7838, val loss 0.7590\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4928/5000  (98.56%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4400: train loss 0.7697, val loss 0.7584\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4930/5000  (98.60%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4450: train loss 0.7721, val loss 0.7582\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4932/5000  (98.64%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 4500: train loss 0.7758, val loss 0.7584\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4951/5000  (99.02%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4550: train loss 0.7811, val loss 0.7590\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4941/5000  (98.82%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4600: train loss 0.7591, val loss 0.7578\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4949/5000  (98.98%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4650: train loss 0.7659, val loss 0.7580\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.43it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4941/5000  (98.82%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4700: train loss 0.7626, val loss 0.7581\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4936/5000  (98.72%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4750: train loss 0.7784, val loss 0.7583\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4942/5000  (98.84%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 4800: train loss 0.7578, val loss 0.7587\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4942/5000  (98.84%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4850: train loss 0.7694, val loss 0.7584\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4943/5000  (98.86%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4900: train loss 0.7598, val loss 0.7578\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4947/5000  (98.94%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 4950: train loss 0.7716, val loss 0.7585\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4948/5000  (98.96%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 5000: train loss 0.7714, val loss 0.7580\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4951/5000  (99.02%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 5050: train loss 0.7609, val loss 0.7582\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4959/5000  (99.18%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 5100: train loss 0.7609, val loss 0.7577\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4948/5000  (98.96%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 5150: train loss 0.7767, val loss 0.7585\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.77it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4965/5000  (99.30%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 5200: train loss 0.7735, val loss 0.7582\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4947/5000  (98.94%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 5250: train loss 0.7605, val loss 0.7583\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4948/5000  (98.96%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 5300: train loss 0.7656, val loss 0.7578\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.44it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.77it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4961/5000  (99.22%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 5350: train loss 0.7677, val loss 0.7584\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4942/5000  (98.84%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 5400: train loss 0.7732, val loss 0.7586\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4952/5000  (99.04%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.74it/s]\n","iter 5450: train loss 0.7749, val loss 0.7582\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4955/5000  (99.10%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 5500: train loss 0.7639, val loss 0.7589\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4964/5000  (99.28%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 5550: train loss 0.7674, val loss 0.7587\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4967/5000  (99.34%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","iter 5600: train loss 0.7694, val loss 0.7583\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4960/5000  (99.20%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 5650: train loss 0.7676, val loss 0.7582\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.47it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.75it/s]\n","\n","Test Results:\n","test.txt, 5000 examples: 4961/5000  (99.22%)\n","\n","Using precomputed batches\n","100% 41/41 [00:14\u003c00:00,  2.76it/s]\n","iter 5700: train loss 0.7665, val loss 0.7588\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.49it/s]\n","Using precomputed batches\n","100% 8/8 [00:03\u003c00:00,  2.48it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.45it/s]\n","Using precomputed batches\n","100% 24/24 [00:09\u003c00:00,  2.46it/s]\n","Using precomputed batches\n"," 88% 36/41 [00:13\u003c00:01,  2.75it/s]"]}],"source":["!python train.py 4_operands_sorting_doubly_bal.txt"]},{"cell_type":"markdown","metadata":{"id":"hdumgWyAWlt6"},"source":["## Slicing -- Addition"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"b-jVt7X7Llry"},"outputs":[],"source":["!python train.py slicing_addition_2_operand.txt --batch slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"pdPiOKoWKw0q","outputId":"b03642c5-8328-466c-96ba-2aef5d654f06"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: slicing\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/slicing_addition_4_operand_plain.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'slicing_4_operand_plain_test'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='plain'\n","operator='+'\n","batch_size = 256\n","block_size = 512\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000\n","lr_decay_iters = 800000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/slicing_plain_out'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test.txt'\n","main_test_name = \"test\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 97\n","Using vocabulary size: 97\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test.txt\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.82M\n","/content/drive/MyDrive/addition/train.py:509: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W\u0026B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W\u0026B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/slicing_plain_out/wandb/run-20251226_204127-omorak6d\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mslicing_4_operand_plain_test\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/omorak6d\u001b[0m\n","max_new_tokens: 5\n","iter 0: train loss 4.3121, val loss 4.3124\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:03\u003c00:00, 26.54it/s]\n","/content/drive/MyDrive/addition/evaluation.py:401: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval.txt\n","/content/drive/MyDrive/addition/evaluation.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02\u003c00:00, 33.68it/s]\n","iter 1000: train loss 1.5218, val loss 1.5212\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.95it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.87it/s]\n","iter 2000: train loss 1.4745, val loss 1.4765\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.55it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 54/10000  (0.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.18it/s]\n","iter 3000: train loss 1.4484, val loss 1.4496\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.39it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 73/10000  (0.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.35it/s]\n","iter 4000: train loss 1.4414, val loss 1.4445\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.67it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 55/10000  (0.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.81it/s]\n","iter 5000: train loss 1.4132, val loss 1.4144\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.87it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 179/10000  (1.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.32it/s]\n","iter 6000: train loss 1.4097, val loss 1.4116\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.28it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 230/10000  (2.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.96it/s]\n","iter 7000: train loss 1.4081, val loss 1.4095\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.71it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 238/10000  (2.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.04it/s]\n","iter 8000: train loss 1.3964, val loss 1.3960\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.56it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 298/10000  (2.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.16it/s]\n","iter 9000: train loss 1.3834, val loss 1.3856\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 453/10000  (4.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.46it/s]\n","iter 10000: train loss 1.3645, val loss 1.3648\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.37it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 675/10000  (6.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.75it/s]\n","iter 11000: train loss 1.3553, val loss 1.3549\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.15it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 797/10000  (7.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.44it/s]\n","iter 12000: train loss 1.3249, val loss 1.3230\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.47it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 1656/10000  (16.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.96it/s]\n","iter 13000: train loss 1.3163, val loss 1.3153\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.43it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2115/10000  (21.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.87it/s]\n","iter 14000: train loss 1.3157, val loss 1.3169\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.84it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2140/10000  (21.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.38it/s]\n","iter 15000: train loss 1.3090, val loss 1.3094\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.37it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2526/10000  (25.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.13it/s]\n","iter 16000: train loss 1.3011, val loss 1.3008\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.77it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 2943/10000  (29.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.48it/s]\n","iter 17000: train loss 1.2950, val loss 1.2962\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.78it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 3584/10000  (35.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.29it/s]\n","iter 18000: train loss 1.2575, val loss 1.2575\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.14it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 7937/10000  (79.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.28it/s]\n","iter 19000: train loss 1.2444, val loss 1.2459\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.95it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9627/10000  (96.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.76it/s]\n","iter 20000: train loss 1.2429, val loss 1.2438\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.94it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9821/10000  (98.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.15it/s]\n","iter 21000: train loss 1.2419, val loss 1.2417\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.73it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9968/10000  (99.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.24it/s]\n","iter 22000: train loss 1.2425, val loss 1.2420\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.08it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9940/10000  (99.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.44it/s]\n","iter 23000: train loss 1.2412, val loss 1.2416\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.87it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9983/10000  (99.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.75it/s]\n","iter 24000: train loss 1.2416, val loss 1.2420\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.59it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9988/10000  (99.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.64it/s]\n","iter 25000: train loss 1.2407, val loss 1.2404\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.23it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9993/10000  (99.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.71it/s]\n","iter 26000: train loss 1.2413, val loss 1.2421\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.96it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9980/10000  (99.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.93it/s]\n","iter 27000: train loss 1.2407, val loss 1.2419\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.59it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9987/10000  (99.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.76it/s]\n","iter 28000: train loss 1.2407, val loss 1.2415\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.78it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.56it/s]\n","iter 29000: train loss 1.2403, val loss 1.2420\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.09it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9976/10000  (99.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.80it/s]\n","iter 30000: train loss 1.2401, val loss 1.2435\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.21it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.80it/s]\n","iter 31000: train loss 1.2389, val loss 1.2418\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.11it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9990/10000  (99.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.74it/s]\n","iter 32000: train loss 1.2382, val loss 1.2421\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.71it/s]\n","iter 33000: train loss 1.2373, val loss 1.2441\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.56it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9992/10000  (99.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.28it/s]\n","iter 34000: train loss 1.2357, val loss 1.2449\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.87it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9997/10000  (99.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.24it/s]\n","iter 35000: train loss 1.2366, val loss 1.2447\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.80it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.60it/s]\n","iter 36000: train loss 1.2340, val loss 1.2471\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.06it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9995/10000  (99.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.25it/s]\n","iter 37000: train loss 1.2317, val loss 1.2495\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.21it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9986/10000  (99.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.45it/s]\n","iter 38000: train loss 1.2309, val loss 1.2504\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.02it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.07it/s]\n","iter 39000: train loss 1.2283, val loss 1.2515\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.09it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9994/10000  (99.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.17it/s]\n","iter 40000: train loss 1.2251, val loss 1.2539\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.62it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9993/10000  (99.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.05it/s]\n","iter 41000: train loss 1.2221, val loss 1.2556\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.04it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9991/10000  (99.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.35it/s]\n","iter 42000: train loss 1.2195, val loss 1.2587\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.95it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9998/10000  (99.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.87it/s]\n","iter 43000: train loss 1.2174, val loss 1.2566\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.12it/s]\n","\n","Test Results:\n","test.txt, 10000 examples: 9996/10000  (99.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.90it/s]\n","iter 44000: train loss 1.2142, val loss 1.2593\n","Using precomputed batches\n"," 20% 16/80 [00:00\u003c00:01, 34.81it/s]"]}],"source":["!python train.py slicing_addition_4_operand_plain.txt --batch slicing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"zqlR08suC-Na","outputId":"16bb6284-36be-42ce-8a55-2c626410f15b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using batch preparation method: slicing\n","Loading config file: /content/drive/MyDrive/addition/configuration_files/slicing_addition_4_operand_reverse.txt\n","# to edit: do an evaluation every {eval_interval} iterations\n","eval_interval = 1000\n","eval_iters = 1\n","always_save_checkpoint = True\n","\n","wandb_log = True # override via command line if you like\n","wandb_project = '4_op_addition'\n","\n","# to edit: wandb run name\n","wandb_run_name = 'slicing_4_operand_reversed_debugging'\n","\n","# baby GPT model :)\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","\n","# to edit: 'plain' or 'reverse' or 'scratchpad' or 'max' or 'sorting'\n","data_format='reverse'\n","operator='+'\n","batch_size = 256\n","block_size = 512\n","\n","# to edit: 'scratch' or 'resume', whether train from scratch or resume from a saved checkpoint\n","init_from = 'scratch'\n","iter_num = 0\n","# to edit: the checkpoint from which to resume training\n","ckpt_path_name = 'ckpt_2_operands_0_to_999_their_training_data_reverse.pt'\n","\n","# to edit: max number of digits in each operand\n","num_digit = 3\n","\n","# to edit: maximum number of output tokens (including '$')\n","max_new_tokens = 5\n","\n","drop_leading_digit = False\n","\n","# ===== Learning Rate Policy ===== #\n","learning_rate = 1e-3\n","\n","# to edit: the number of training iterations\n","max_iters = 800000\n","lr_decay_iters = 800000 # make equal to max_iters usually\n","\n","beta1 = 0.9\n","beta2 = 0.98\n","warmup_iters = 100\n","\n","# ===== Device ===== #\n","device='cuda:0'\n","\n","# to edit: the output directory; Path is relative to the project's main directory. \n","# For example, the project's main directory might be \"/content/drive/MyDrive/addition/\"\n","out_dir = 'results/4_operands_0_to_999_uniform/slicing_4_operand_reversed_debugging'\n","\n","# to edit: data directory; Path is relative to the project's main directory. \n","data_dir = 'data/4_operands_0_to_999_uniform/'\n","\n","# to edit: training data\n","train_data_name = 'train_reverse.txt'\n","\n","# to edit: whether do evaluation on training data to see if the model is simply memorizing\n","eval_addition_train = True\n","train_data_test_name = \"train_eval_reverse.txt\"\n","\n","# to edit: validation data\n","val_data_name = 'val_reverse.txt'\n","\n","# to edit: test data; It can be either without gold (thus gold by computing) or with gold (thus gold by reading)\n","# \"test_file_name\" can be either a single file, or the name of the directory that contains multiple test files \n","# \"main_test_name\" is the name, without extension, of the test file (in case there are multiple test files) that's to be displayed in wandb\n","eval_addition = True\n","test_file_name = 'test_reverse.txt'\n","main_test_name = \"test_reverse\"\n","\n","# to edit: \"compute_gold\" or \"read_gold_as_str\"; read or compute groundtruth in evaluation data\n","mode = \"read_gold_as_str\"\n","/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  self.setter(val)\n","vocab size: 15\n","all the unique characters: \\n,  , $, +, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, =\n","Using vocabulary size: 15\n","Collected test files:\n","/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Using positional encoding: absolute (module: model.py)\n","Initializing a new model from scratch\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","Using Flash attention\n","number of parameters: 10.82M\n","/content/drive/MyDrive/addition/train.py:590: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","using fused:True\n","compiling the model... (takes a ~minute)\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W\u0026B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W\u0026B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W\u0026B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxzhao468\u001b[0m (\u001b[33mdsharma59-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/addition/results/4_operands_0_to_999_uniform/slicing_4_operand_reversed_debugging/wandb/run-20260104_024133-b5ylcp94\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mslicing_4_operand_reversed_debugging\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dsharma59-university-of-wisconsin-madison/4_op_addition/runs/b5ylcp94\u001b[0m\n","WARNING: results directory results/4_operands_0_to_999_uniform/slicing_4_operand_reversed_debugging/slicing_4_operand_reversed_debugging already exists, overwriting...\n","max_new_tokens: 5\n","W0104 02:42:02.580000 2535 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","iter 0: train loss 2.6918, val loss 2.6956\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/test_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 80 batches\n","100% 80/80 [00:02\u003c00:00, 28.20it/s]\n","/content/drive/MyDrive/addition/evaluation.py:540: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  acc_df = pd.concat([acc_df, new_row], ignore_index=True)\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 0/10000  (0.00%)\n","\n","Creating new batches\n","Preparing batches from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","Preparing batches for 10000 examples from: FILE:/content/drive/MyDrive/addition/data/4_operands_0_to_999_uniform/train_eval_reverse.txt\n","/content/drive/MyDrive/addition/evaluation.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n","Created 81 batches\n","100% 81/81 [00:02\u003c00:00, 35.06it/s]\n","iter 1000: train loss 1.5407, val loss 1.5391\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.61it/s]\n","iter 2000: train loss 1.5019, val loss 1.5021\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 23/10000  (0.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.71it/s]\n","iter 3000: train loss 1.4639, val loss 1.4642\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 42/10000  (0.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.59it/s]\n","iter 4000: train loss 1.4496, val loss 1.4493\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 56/10000  (0.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.13it/s]\n","iter 5000: train loss 1.4425, val loss 1.4410\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 84/10000  (0.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.89it/s]\n","iter 6000: train loss 1.4404, val loss 1.4410\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 97/10000  (0.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.27it/s]\n","iter 7000: train loss 1.4410, val loss 1.4406\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.39it/s]\n","iter 8000: train loss 1.4411, val loss 1.4395\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 106/10000  (1.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.31it/s]\n","iter 9000: train loss 1.4420, val loss 1.4400\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 106/10000  (1.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.69it/s]\n","iter 10000: train loss 1.4426, val loss 1.4441\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 102/10000  (1.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.56it/s]\n","iter 11000: train loss 1.4460, val loss 1.4475\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 107/10000  (1.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.74it/s]\n","iter 12000: train loss 1.4541, val loss 1.4553\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 99/10000  (0.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.09it/s]\n","iter 13000: train loss 1.4515, val loss 1.4522\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 95/10000  (0.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.19it/s]\n","iter 14000: train loss 1.4918, val loss 1.4910\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 89/10000  (0.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.28it/s]\n","iter 15000: train loss 1.4902, val loss 1.4907\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 100/10000  (1.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.65it/s]\n","iter 16000: train loss 1.5102, val loss 1.5104\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 102/10000  (1.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.68it/s]\n","iter 17000: train loss 1.5173, val loss 1.5181\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 98/10000  (0.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.63it/s]\n","iter 18000: train loss 1.4816, val loss 1.4827\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 89/10000  (0.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.43it/s]\n","iter 19000: train loss 1.5019, val loss 1.5064\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 101/10000  (1.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.11it/s]\n","iter 20000: train loss 1.7082, val loss 1.7043\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 72/10000  (0.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.67it/s]\n","iter 21000: train loss 1.4708, val loss 1.4709\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 86/10000  (0.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.92it/s]\n","iter 22000: train loss 1.4568, val loss 1.4580\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 108/10000  (1.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.64it/s]\n","iter 23000: train loss 1.4635, val loss 1.4658\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 115/10000  (1.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.51it/s]\n","iter 24000: train loss 1.9357, val loss 1.9328\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 99/10000  (0.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.28it/s]\n","iter 25000: train loss 1.5152, val loss 1.5185\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 102/10000  (1.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.47it/s]\n","iter 26000: train loss 1.4771, val loss 1.4767\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 69/10000  (0.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.87it/s]\n","iter 27000: train loss 4.2519, val loss 4.2521\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 99/10000  (0.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.86it/s]\n","iter 28000: train loss 1.7972, val loss 1.8079\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.22it/s]\n","iter 29000: train loss 2.6684, val loss 2.6596\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 13/10000  (0.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.01it/s]\n","iter 30000: train loss 2.4105, val loss 2.3816\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.76it/s]\n","iter 31000: train loss 1.6203, val loss 1.6188\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.42it/s]\n","iter 32000: train loss 1.7031, val loss 1.6934\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.25it/s]\n","iter 33000: train loss 2.1830, val loss 2.1976\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.74it/s]\n","iter 34000: train loss 1.6095, val loss 1.6180\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 14/10000  (0.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.29it/s]\n","iter 35000: train loss 4.7672, val loss 4.7469\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.97it/s]\n","iter 36000: train loss 2.2512, val loss 2.2257\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 4/10000  (0.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.69it/s]\n","iter 37000: train loss 2.7318, val loss 2.7113\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 5/10000  (0.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.42it/s]\n","iter 38000: train loss 1.6980, val loss 1.7019\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 5/10000  (0.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.16it/s]\n","iter 39000: train loss 1.6002, val loss 1.5981\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 13/10000  (0.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.82it/s]\n","iter 40000: train loss 5.8642, val loss 5.7381\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.92it/s]\n","iter 41000: train loss 1.7379, val loss 1.7296\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 16/10000  (0.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.46it/s]\n","iter 42000: train loss 3.1965, val loss 3.2533\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 5/10000  (0.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.87it/s]\n","iter 43000: train loss 10.3939, val loss 10.4891\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.16it/s]\n","iter 44000: train loss 1.9121, val loss 1.8985\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.07it/s]\n","iter 45000: train loss 53.3369, val loss 53.2971\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.98it/s]\n","iter 46000: train loss 4.4228, val loss 4.3888\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 13/10000  (0.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.92it/s]\n","iter 47000: train loss 1.7394, val loss 1.7416\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 17/10000  (0.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.18it/s]\n","iter 48000: train loss 12.2844, val loss 11.9358\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.72it/s]\n","iter 49000: train loss 24.2238, val loss 24.3381\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.37it/s]\n","iter 50000: train loss 1.9491, val loss 1.9627\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 3/10000  (0.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.67it/s]\n","iter 51000: train loss 1.8058, val loss 1.7951\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.29it/s]\n","iter 52000: train loss 1.9174, val loss 1.9392\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1/10000  (0.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.74it/s]\n","iter 53000: train loss 18.1640, val loss 18.0815\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.52it/s]\n","iter 54000: train loss 1.9621, val loss 1.9287\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.05it/s]\n","iter 55000: train loss 2.0110, val loss 1.9744\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.05it/s]\n","iter 56000: train loss 11.5790, val loss 11.6237\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.71it/s]\n","iter 57000: train loss 9.4991, val loss 9.4997\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.13it/s]\n","iter 58000: train loss 2.2658, val loss 2.2790\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.02it/s]\n","iter 59000: train loss 2.1413, val loss 2.2014\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 14/10000  (0.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.24it/s]\n","iter 60000: train loss 9.5259, val loss 9.4311\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 2/10000  (0.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.90it/s]\n","iter 61000: train loss 4.9433, val loss 4.8994\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.83it/s]\n","iter 62000: train loss 2.6514, val loss 2.6215\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.93it/s]\n","iter 63000: train loss 15.8749, val loss 15.8809\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 15/10000  (0.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.13it/s]\n","iter 64000: train loss 1.8158, val loss 1.8357\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 1/10000  (0.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.37it/s]\n","iter 65000: train loss 10.6230, val loss 10.6212\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 15/10000  (0.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.02it/s]\n","iter 66000: train loss 1.9804, val loss 1.9503\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.92it/s]\n","iter 67000: train loss 11.8050, val loss 11.9251\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 4/10000  (0.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.48it/s]\n","iter 68000: train loss 3.0503, val loss 2.9694\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 2/10000  (0.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.30it/s]\n","iter 69000: train loss 5.2822, val loss 5.1259\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.72it/s]\n","iter 70000: train loss 4.2575, val loss 4.2125\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.59it/s]\n","iter 71000: train loss 2.7984, val loss 2.8292\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.62it/s]\n","iter 72000: train loss 2.8375, val loss 3.0798\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.01it/s]\n","iter 73000: train loss 27.0033, val loss 26.7539\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.50it/s]\n","iter 74000: train loss 2.9947, val loss 3.1106\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 4/10000  (0.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.73it/s]\n","iter 75000: train loss 3.0464, val loss 3.0967\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.06it/s]\n","iter 76000: train loss 2.8617, val loss 2.8768\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.46it/s]\n","iter 77000: train loss 2.9212, val loss 2.8436\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.42it/s]\n","iter 78000: train loss 7.9902, val loss 8.1756\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 3/10000  (0.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.84it/s]\n","iter 79000: train loss 8.7554, val loss 8.9175\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 2/10000  (0.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.33it/s]\n","iter 80000: train loss 41.4864, val loss 42.1651\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.18it/s]\n","iter 81000: train loss 34.9251, val loss 35.2725\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.27it/s]\n","iter 82000: train loss 3.9256, val loss 4.1376\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.70it/s]\n","iter 83000: train loss 41.7062, val loss 41.2735\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.84it/s]\n","iter 84000: train loss 34.6754, val loss 34.9752\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 10/10000  (0.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.31it/s]\n","iter 85000: train loss 8.7134, val loss 8.2566\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7/10000  (0.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.58it/s]\n","iter 86000: train loss 156.6946, val loss 156.6878\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.77it/s]\n","iter 87000: train loss 116.4044, val loss 115.1710\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 15/10000  (0.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.77it/s]\n","iter 88000: train loss 61.8833, val loss 59.9917\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 36.07it/s]\n","iter 89000: train loss 96.0434, val loss 96.1241\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 11/10000  (0.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.84it/s]\n","iter 90000: train loss 151.7155, val loss 153.1028\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 5/10000  (0.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.38it/s]\n","iter 91000: train loss 4.1221, val loss 4.4190\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 13/10000  (0.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.66it/s]\n","iter 92000: train loss 85.3958, val loss 84.7342\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6/10000  (0.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.65it/s]\n","iter 93000: train loss 4.2477, val loss 3.9717\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 34.88it/s]\n","iter 94000: train loss 4.2102, val loss 4.3607\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.96it/s]\n","iter 95000: train loss 4.5333, val loss 4.6869\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.20it/s]\n","iter 96000: train loss 4.0889, val loss 4.0410\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 12/10000  (0.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.35it/s]\n","iter 97000: train loss 4.9853, val loss 5.1369\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 34.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 3/10000  (0.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.81it/s]\n","iter 98000: train loss 2.8816, val loss 2.5970\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.58it/s]\n","iter 99000: train loss 18.4323, val loss 20.1786\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 36.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 8/10000  (0.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.77it/s]\n","iter 100000: train loss 33.9621, val loss 34.0119\n","Using precomputed batches\n","100% 80/80 [00:02\u003c00:00, 35.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 9/10000  (0.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:02\u003c00:00, 35.57it/s]\n"]}],"source":["!python train.py slicing_addition_4_operand_reverse.txt --batch slicing"]},{"cell_type":"markdown","metadata":{"id":"30Zi4RZFvtkw"},"source":["## Positional Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"F9dDD_DmvtPz","outputId":"e5dfdb78-6fcc-4888-a853-81d92bb15348"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","test_reverse.txt, 10000 examples: 7217/10000  (72.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.35it/s]\n","iter 974000: train loss 1.4246, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7311/10000  (73.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.99it/s]\n","iter 976000: train loss 1.4283, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7315/10000  (73.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.87it/s]\n","iter 978000: train loss 1.4204, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7257/10000  (72.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 980000: train loss 1.4227, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7337/10000  (73.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.58it/s]\n","iter 982000: train loss 1.4214, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7087/10000  (70.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.27it/s]\n","iter 984000: train loss 1.4245, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7191/10000  (71.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 20.82it/s]\n","iter 986000: train loss 1.4272, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7206/10000  (72.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.57it/s]\n","iter 988000: train loss 1.4250, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7479/10000  (74.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.63it/s]\n","iter 990000: train loss 1.4234, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7213/10000  (72.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.27it/s]\n","iter 992000: train loss 1.4256, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7454/10000  (74.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.13it/s]\n","iter 994000: train loss 1.4245, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7324/10000  (73.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.38it/s]\n","iter 996000: train loss 1.4216, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7386/10000  (73.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.85it/s]\n","iter 998000: train loss 1.4310, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7263/10000  (72.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.44it/s]\n","iter 1000000: train loss 1.4252, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7353/10000  (73.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1002000: train loss 1.4229, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.58it/s]\n","iter 1004000: train loss 1.4226, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7328/10000  (73.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.56it/s]\n","iter 1006000: train loss 1.4204, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7363/10000  (73.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.31it/s]\n","iter 1008000: train loss 1.4253, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7399/10000  (73.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.48it/s]\n","iter 1010000: train loss 1.4256, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7171/10000  (71.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.50it/s]\n","iter 1012000: train loss 1.4305, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7373/10000  (73.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.46it/s]\n","iter 1014000: train loss 1.4202, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7246/10000  (72.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.08it/s]\n","iter 1016000: train loss 1.4264, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7215/10000  (72.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.67it/s]\n","iter 1018000: train loss 1.4187, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7304/10000  (73.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1020000: train loss 1.4228, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7212/10000  (72.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.59it/s]\n","iter 1022000: train loss 1.4175, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7169/10000  (71.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.10it/s]\n","iter 1024000: train loss 1.4254, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7505/10000  (75.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.31it/s]\n","iter 1026000: train loss 1.4283, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7066/10000  (70.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.57it/s]\n","iter 1028000: train loss 1.4240, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7261/10000  (72.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.48it/s]\n","iter 1030000: train loss 1.4227, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7188/10000  (71.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.57it/s]\n","iter 1032000: train loss 1.4234, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7321/10000  (73.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.77it/s]\n","iter 1034000: train loss 1.4233, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7232/10000  (72.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.03it/s]\n","iter 1036000: train loss 1.4247, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7269/10000  (72.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.51it/s]\n","iter 1038000: train loss 1.4195, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7285/10000  (72.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.06it/s]\n","iter 1040000: train loss 1.4248, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7342/10000  (73.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.19it/s]\n","iter 1042000: train loss 1.4210, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.31it/s]\n","iter 1044000: train loss 1.4221, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.97it/s]\n","iter 1046000: train loss 1.4223, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7095/10000  (70.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.87it/s]\n","iter 1048000: train loss 1.4179, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 20.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7339/10000  (73.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.43it/s]\n","iter 1050000: train loss 1.4206, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7257/10000  (72.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.00it/s]\n","iter 1052000: train loss 1.4203, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7351/10000  (73.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1054000: train loss 1.4275, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7407/10000  (74.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.40it/s]\n","iter 1056000: train loss 1.4243, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7320/10000  (73.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.83it/s]\n","iter 1058000: train loss 1.4206, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7270/10000  (72.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.44it/s]\n","iter 1060000: train loss 1.4243, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7362/10000  (73.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.47it/s]\n","iter 1062000: train loss 1.4250, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7437/10000  (74.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.25it/s]\n","iter 1064000: train loss 1.4240, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.96it/s]\n","iter 1066000: train loss 1.4263, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7252/10000  (72.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.83it/s]\n","iter 1068000: train loss 1.4282, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7318/10000  (73.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.99it/s]\n","iter 1070000: train loss 1.4212, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7215/10000  (72.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.52it/s]\n","iter 1072000: train loss 1.4260, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7255/10000  (72.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.67it/s]\n","iter 1074000: train loss 1.4225, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7199/10000  (71.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.89it/s]\n","iter 1076000: train loss 1.4267, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7361/10000  (73.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.48it/s]\n","iter 1078000: train loss 1.4231, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7253/10000  (72.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.95it/s]\n","iter 1080000: train loss 1.4176, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7227/10000  (72.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.32it/s]\n","iter 1082000: train loss 1.4217, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7282/10000  (72.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.95it/s]\n","iter 1084000: train loss 1.4245, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7162/10000  (71.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.25it/s]\n","iter 1086000: train loss 1.4268, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7270/10000  (72.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.82it/s]\n","iter 1088000: train loss 1.4229, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7439/10000  (74.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.27it/s]\n","iter 1090000: train loss 1.4241, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7151/10000  (71.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.75it/s]\n","iter 1092000: train loss 1.4229, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7313/10000  (73.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.32it/s]\n","iter 1094000: train loss 1.4252, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7226/10000  (72.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.45it/s]\n","iter 1096000: train loss 1.4241, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7345/10000  (73.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.99it/s]\n","iter 1098000: train loss 1.4219, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7418/10000  (74.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.31it/s]\n","iter 1100000: train loss 1.4233, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7246/10000  (72.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.52it/s]\n","iter 1102000: train loss 1.4183, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7423/10000  (74.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.45it/s]\n","iter 1104000: train loss 1.4284, val loss 1.4289\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7072/10000  (70.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.68it/s]\n","iter 1106000: train loss 1.4221, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7339/10000  (73.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.39it/s]\n","iter 1108000: train loss 1.4277, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7352/10000  (73.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.54it/s]\n","iter 1110000: train loss 1.4260, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7170/10000  (71.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1112000: train loss 1.4212, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7409/10000  (74.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.82it/s]\n","iter 1114000: train loss 1.4301, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.86it/s]\n","iter 1116000: train loss 1.4288, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7359/10000  (73.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.66it/s]\n","iter 1118000: train loss 1.4256, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7444/10000  (74.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.41it/s]\n","iter 1120000: train loss 1.4237, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7267/10000  (72.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.49it/s]\n","iter 1122000: train loss 1.4238, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7216/10000  (72.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.80it/s]\n","iter 1124000: train loss 1.4274, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7258/10000  (72.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.23it/s]\n","iter 1126000: train loss 1.4237, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7340/10000  (73.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.16it/s]\n","iter 1128000: train loss 1.4228, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7345/10000  (73.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.62it/s]\n","iter 1130000: train loss 1.4188, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7236/10000  (72.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.95it/s]\n","iter 1132000: train loss 1.4288, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7266/10000  (72.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.18it/s]\n","iter 1134000: train loss 1.4221, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7261/10000  (72.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.58it/s]\n","iter 1136000: train loss 1.4282, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7192/10000  (71.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.74it/s]\n","iter 1138000: train loss 1.4234, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7447/10000  (74.47%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.34it/s]\n","iter 1140000: train loss 1.4279, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.22it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 6982/10000  (69.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.84it/s]\n","iter 1142000: train loss 1.4224, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7225/10000  (72.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.48it/s]\n","iter 1144000: train loss 1.4207, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7345/10000  (73.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.75it/s]\n","iter 1146000: train loss 1.4254, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7223/10000  (72.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.07it/s]\n","iter 1148000: train loss 1.4288, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7523/10000  (75.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.96it/s]\n","iter 1150000: train loss 1.4196, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7345/10000  (73.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.18it/s]\n","iter 1152000: train loss 1.4229, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7452/10000  (74.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.52it/s]\n","iter 1154000: train loss 1.4217, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7259/10000  (72.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.80it/s]\n","iter 1156000: train loss 1.4250, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7512/10000  (75.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.54it/s]\n","iter 1158000: train loss 1.4235, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7356/10000  (73.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.16it/s]\n","iter 1160000: train loss 1.4253, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7334/10000  (73.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.75it/s]\n","iter 1162000: train loss 1.4223, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7159/10000  (71.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.98it/s]\n","iter 1164000: train loss 1.4223, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7221/10000  (72.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1166000: train loss 1.4268, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.67it/s]\n","iter 1168000: train loss 1.4245, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7301/10000  (73.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.89it/s]\n","iter 1170000: train loss 1.4186, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7400/10000  (74.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.61it/s]\n","iter 1172000: train loss 1.4229, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7203/10000  (72.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.56it/s]\n","iter 1174000: train loss 1.4221, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7159/10000  (71.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.63it/s]\n","iter 1176000: train loss 1.4211, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7383/10000  (73.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.74it/s]\n","iter 1178000: train loss 1.4218, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7206/10000  (72.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.51it/s]\n","iter 1180000: train loss 1.4261, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7356/10000  (73.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.70it/s]\n","iter 1182000: train loss 1.4215, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7290/10000  (72.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.68it/s]\n","iter 1184000: train loss 1.4213, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7422/10000  (74.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.37it/s]\n","iter 1186000: train loss 1.4253, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.69it/s]\n","iter 1188000: train loss 1.4263, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.91it/s]\n","iter 1190000: train loss 1.4200, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.08it/s]\n","iter 1192000: train loss 1.4235, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.05it/s]\n","iter 1194000: train loss 1.4231, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7171/10000  (71.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.17it/s]\n","iter 1196000: train loss 1.4252, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7339/10000  (73.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.17it/s]\n","iter 1198000: train loss 1.4248, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7127/10000  (71.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.88it/s]\n","iter 1200000: train loss 1.4258, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7196/10000  (71.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.75it/s]\n","iter 1202000: train loss 1.4230, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7248/10000  (72.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.03it/s]\n","iter 1204000: train loss 1.4211, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.27it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7257/10000  (72.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.68it/s]\n","iter 1206000: train loss 1.4263, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7199/10000  (71.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.96it/s]\n","iter 1208000: train loss 1.4225, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7273/10000  (72.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.48it/s]\n","iter 1210000: train loss 1.4238, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7368/10000  (73.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.44it/s]\n","iter 1212000: train loss 1.4230, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7391/10000  (73.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.76it/s]\n","iter 1214000: train loss 1.4264, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7363/10000  (73.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.71it/s]\n","iter 1216000: train loss 1.4173, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7545/10000  (75.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.74it/s]\n","iter 1218000: train loss 1.4216, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7473/10000  (74.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.73it/s]\n","iter 1220000: train loss 1.4218, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7212/10000  (72.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.74it/s]\n","iter 1222000: train loss 1.4284, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7433/10000  (74.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.06it/s]\n","iter 1224000: train loss 1.4231, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7360/10000  (73.60%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.80it/s]\n","iter 1226000: train loss 1.4236, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7473/10000  (74.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.30it/s]\n","iter 1228000: train loss 1.4255, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7336/10000  (73.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.39it/s]\n","iter 1230000: train loss 1.4258, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7386/10000  (73.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.77it/s]\n","iter 1232000: train loss 1.4249, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7454/10000  (74.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.51it/s]\n","iter 1234000: train loss 1.4205, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7162/10000  (71.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.12it/s]\n","iter 1236000: train loss 1.4203, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.71it/s]\n","iter 1238000: train loss 1.4294, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7227/10000  (72.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.90it/s]\n","iter 1240000: train loss 1.4243, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.77it/s]\n","iter 1242000: train loss 1.4229, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7318/10000  (73.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.22it/s]\n","iter 1244000: train loss 1.4233, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.15it/s]\n","iter 1246000: train loss 1.4283, val loss 1.4282\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7261/10000  (72.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.20it/s]\n","iter 1248000: train loss 1.4249, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.35it/s]\n","iter 1250000: train loss 1.4224, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7322/10000  (73.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.48it/s]\n","iter 1252000: train loss 1.4249, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.58it/s]\n","iter 1254000: train loss 1.4223, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.60it/s]\n","iter 1256000: train loss 1.4207, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7218/10000  (72.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.74it/s]\n","iter 1258000: train loss 1.4168, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7502/10000  (75.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.38it/s]\n","iter 1260000: train loss 1.4246, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7174/10000  (71.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.48it/s]\n","iter 1262000: train loss 1.4237, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7421/10000  (74.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.79it/s]\n","iter 1264000: train loss 1.4228, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7495/10000  (74.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.42it/s]\n","iter 1266000: train loss 1.4215, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7353/10000  (73.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.28it/s]\n","iter 1268000: train loss 1.4255, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7250/10000  (72.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.06it/s]\n","iter 1270000: train loss 1.4177, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.79it/s]\n","iter 1272000: train loss 1.4238, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7222/10000  (72.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.71it/s]\n","iter 1274000: train loss 1.4283, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7504/10000  (75.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.02it/s]\n","iter 1276000: train loss 1.4223, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7469/10000  (74.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.40it/s]\n","iter 1278000: train loss 1.4220, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7530/10000  (75.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.68it/s]\n","iter 1280000: train loss 1.4257, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7444/10000  (74.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.13it/s]\n","iter 1282000: train loss 1.4270, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7481/10000  (74.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.68it/s]\n","iter 1284000: train loss 1.4263, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7332/10000  (73.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.22it/s]\n","iter 1286000: train loss 1.4176, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7368/10000  (73.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.86it/s]\n","iter 1288000: train loss 1.4243, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7081/10000  (70.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.11it/s]\n","iter 1290000: train loss 1.4237, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7147/10000  (71.47%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.58it/s]\n","iter 1292000: train loss 1.4215, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7392/10000  (73.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.22it/s]\n","iter 1294000: train loss 1.4224, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7238/10000  (72.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.60it/s]\n","iter 1296000: train loss 1.4228, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7477/10000  (74.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.38it/s]\n","iter 1298000: train loss 1.4226, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7310/10000  (73.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.40it/s]\n","iter 1300000: train loss 1.4181, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7349/10000  (73.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1302000: train loss 1.4217, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7308/10000  (73.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.55it/s]\n","iter 1304000: train loss 1.4304, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.35it/s]\n","iter 1306000: train loss 1.4219, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7214/10000  (72.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.31it/s]\n","iter 1308000: train loss 1.4238, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7276/10000  (72.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.05it/s]\n","iter 1310000: train loss 1.4261, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7334/10000  (73.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.97it/s]\n","iter 1312000: train loss 1.4272, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7300/10000  (73.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.51it/s]\n","iter 1314000: train loss 1.4268, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7427/10000  (74.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.72it/s]\n","iter 1316000: train loss 1.4284, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7322/10000  (73.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.92it/s]\n","iter 1318000: train loss 1.4225, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7250/10000  (72.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.65it/s]\n","iter 1320000: train loss 1.4226, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7434/10000  (74.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.75it/s]\n","iter 1322000: train loss 1.4278, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7481/10000  (74.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.04it/s]\n","iter 1324000: train loss 1.4244, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7356/10000  (73.56%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.93it/s]\n","iter 1326000: train loss 1.4241, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7453/10000  (74.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1328000: train loss 1.4234, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7218/10000  (72.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.70it/s]\n","iter 1330000: train loss 1.4278, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7259/10000  (72.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.09it/s]\n","iter 1332000: train loss 1.4268, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7324/10000  (73.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.41it/s]\n","iter 1334000: train loss 1.4186, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7212/10000  (72.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.36it/s]\n","iter 1336000: train loss 1.4257, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7320/10000  (73.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.32it/s]\n","iter 1338000: train loss 1.4229, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7290/10000  (72.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.12it/s]\n","iter 1340000: train loss 1.4222, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7193/10000  (71.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.68it/s]\n","iter 1342000: train loss 1.4232, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7430/10000  (74.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.00it/s]\n","iter 1344000: train loss 1.4180, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7410/10000  (74.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.62it/s]\n","iter 1346000: train loss 1.4236, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7364/10000  (73.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.25it/s]\n","iter 1348000: train loss 1.4231, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7406/10000  (74.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.81it/s]\n","iter 1350000: train loss 1.4295, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7316/10000  (73.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.12it/s]\n","iter 1352000: train loss 1.4285, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7393/10000  (73.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.03it/s]\n","iter 1354000: train loss 1.4221, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7230/10000  (72.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.76it/s]\n","iter 1356000: train loss 1.4269, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.63it/s]\n","iter 1358000: train loss 1.4235, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.11it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7367/10000  (73.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.96it/s]\n","iter 1360000: train loss 1.4274, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.42it/s]\n","iter 1362000: train loss 1.4191, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 20.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7257/10000  (72.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.14it/s]\n","iter 1364000: train loss 1.4229, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7250/10000  (72.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.93it/s]\n","iter 1366000: train loss 1.4243, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7389/10000  (73.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.72it/s]\n","iter 1368000: train loss 1.4252, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7264/10000  (72.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.91it/s]\n","iter 1370000: train loss 1.4197, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7344/10000  (73.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.79it/s]\n","iter 1372000: train loss 1.4211, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7425/10000  (74.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.88it/s]\n","iter 1374000: train loss 1.4239, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7144/10000  (71.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.96it/s]\n","iter 1376000: train loss 1.4232, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7302/10000  (73.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.44it/s]\n","iter 1378000: train loss 1.4275, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.44it/s]\n","iter 1380000: train loss 1.4245, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7390/10000  (73.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.20it/s]\n","iter 1382000: train loss 1.4229, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.73it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7288/10000  (72.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.61it/s]\n","iter 1384000: train loss 1.4216, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7274/10000  (72.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.82it/s]\n","iter 1386000: train loss 1.4251, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.25it/s]\n","iter 1388000: train loss 1.4293, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7435/10000  (74.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.54it/s]\n","iter 1390000: train loss 1.4228, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7519/10000  (75.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.92it/s]\n","iter 1392000: train loss 1.4235, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 20.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7318/10000  (73.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.39it/s]\n","iter 1394000: train loss 1.4238, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7237/10000  (72.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.74it/s]\n","iter 1396000: train loss 1.4214, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7360/10000  (73.60%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.63it/s]\n","iter 1398000: train loss 1.4238, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7411/10000  (74.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.17it/s]\n","iter 1400000: train loss 1.4222, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7265/10000  (72.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.41it/s]\n","iter 1402000: train loss 1.4226, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7311/10000  (73.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.66it/s]\n","iter 1404000: train loss 1.4231, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7273/10000  (72.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.40it/s]\n","iter 1406000: train loss 1.4214, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7423/10000  (74.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.37it/s]\n","iter 1408000: train loss 1.4285, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.44it/s]\n","iter 1410000: train loss 1.4202, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7205/10000  (72.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.81it/s]\n","iter 1412000: train loss 1.4243, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7194/10000  (71.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.68it/s]\n","iter 1414000: train loss 1.4260, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7036/10000  (70.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.16it/s]\n","iter 1416000: train loss 1.4243, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7243/10000  (72.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.55it/s]\n","iter 1418000: train loss 1.4265, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.09it/s]\n","iter 1420000: train loss 1.4253, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7279/10000  (72.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.44it/s]\n","iter 1422000: train loss 1.4244, val loss 1.4282\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7283/10000  (72.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.18it/s]\n","iter 1424000: train loss 1.4200, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.04it/s]\n","iter 1426000: train loss 1.4239, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7397/10000  (73.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.77it/s]\n","iter 1428000: train loss 1.4262, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7283/10000  (72.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.72it/s]\n","iter 1430000: train loss 1.4212, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7228/10000  (72.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.10it/s]\n","iter 1432000: train loss 1.4242, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7235/10000  (72.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.89it/s]\n","iter 1434000: train loss 1.4315, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 20.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7258/10000  (72.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.64it/s]\n","iter 1436000: train loss 1.4199, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7275/10000  (72.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.59it/s]\n","iter 1438000: train loss 1.4268, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7391/10000  (73.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.10it/s]\n","iter 1440000: train loss 1.4243, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7301/10000  (73.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.96it/s]\n","iter 1442000: train loss 1.4224, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7276/10000  (72.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.44it/s]\n","iter 1444000: train loss 1.4249, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7476/10000  (74.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.43it/s]\n","iter 1446000: train loss 1.4288, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7190/10000  (71.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.50it/s]\n","iter 1448000: train loss 1.4297, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7150/10000  (71.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.32it/s]\n","iter 1450000: train loss 1.4211, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7042/10000  (70.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.49it/s]\n","iter 1452000: train loss 1.4253, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7362/10000  (73.62%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.63it/s]\n","iter 1454000: train loss 1.4273, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7301/10000  (73.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.35it/s]\n","iter 1456000: train loss 1.4224, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7291/10000  (72.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.12it/s]\n","iter 1458000: train loss 1.4203, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7405/10000  (74.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.63it/s]\n","iter 1460000: train loss 1.4242, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7066/10000  (70.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.41it/s]\n","iter 1462000: train loss 1.4249, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7390/10000  (73.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.49it/s]\n","iter 1464000: train loss 1.4250, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7324/10000  (73.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.57it/s]\n","iter 1466000: train loss 1.4212, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7439/10000  (74.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.96it/s]\n","iter 1468000: train loss 1.4243, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7355/10000  (73.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.94it/s]\n","iter 1470000: train loss 1.4235, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7354/10000  (73.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.61it/s]\n","iter 1472000: train loss 1.4198, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.78it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.88it/s]\n","iter 1474000: train loss 1.4240, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7293/10000  (72.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.07it/s]\n","iter 1476000: train loss 1.4222, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.83it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7414/10000  (74.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.65it/s]\n","iter 1478000: train loss 1.4235, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7236/10000  (72.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.87it/s]\n","iter 1480000: train loss 1.4213, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7308/10000  (73.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.14it/s]\n","iter 1482000: train loss 1.4263, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7316/10000  (73.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.16it/s]\n","iter 1484000: train loss 1.4254, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7237/10000  (72.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.89it/s]\n","iter 1486000: train loss 1.4279, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7417/10000  (74.17%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.41it/s]\n","iter 1488000: train loss 1.4247, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7383/10000  (73.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.62it/s]\n","iter 1490000: train loss 1.4240, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.74it/s]\n","iter 1492000: train loss 1.4270, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7275/10000  (72.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.58it/s]\n","iter 1494000: train loss 1.4278, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7236/10000  (72.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.68it/s]\n","iter 1496000: train loss 1.4268, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7206/10000  (72.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.41it/s]\n","iter 1498000: train loss 1.4246, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7235/10000  (72.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.66it/s]\n","iter 1500000: train loss 1.4243, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7329/10000  (73.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.04it/s]\n","iter 1502000: train loss 1.4183, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7408/10000  (74.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.73it/s]\n","iter 1504000: train loss 1.4203, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7213/10000  (72.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.84it/s]\n","iter 1506000: train loss 1.4274, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7266/10000  (72.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.89it/s]\n","iter 1508000: train loss 1.4280, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7267/10000  (72.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.86it/s]\n","iter 1510000: train loss 1.4261, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7397/10000  (73.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.12it/s]\n","iter 1512000: train loss 1.4215, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.74it/s]\n","iter 1514000: train loss 1.4267, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7379/10000  (73.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.61it/s]\n","iter 1516000: train loss 1.4216, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7273/10000  (72.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.74it/s]\n","iter 1518000: train loss 1.4267, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7404/10000  (74.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.74it/s]\n","iter 1520000: train loss 1.4209, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7352/10000  (73.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.08it/s]\n","iter 1522000: train loss 1.4227, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7258/10000  (72.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.72it/s]\n","iter 1524000: train loss 1.4236, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7366/10000  (73.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.16it/s]\n","iter 1526000: train loss 1.4266, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7194/10000  (71.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.47it/s]\n","iter 1528000: train loss 1.4237, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7382/10000  (73.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.85it/s]\n","iter 1530000: train loss 1.4246, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7229/10000  (72.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.83it/s]\n","iter 1532000: train loss 1.4264, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7450/10000  (74.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.97it/s]\n","iter 1534000: train loss 1.4217, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7252/10000  (72.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.64it/s]\n","iter 1536000: train loss 1.4297, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.52it/s]\n","iter 1538000: train loss 1.4266, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7551/10000  (75.51%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.14it/s]\n","iter 1540000: train loss 1.4203, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7436/10000  (74.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.15it/s]\n","iter 1542000: train loss 1.4226, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7461/10000  (74.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.91it/s]\n","iter 1544000: train loss 1.4198, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7130/10000  (71.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.60it/s]\n","iter 1546000: train loss 1.4218, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7389/10000  (73.89%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.00it/s]\n","iter 1548000: train loss 1.4246, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7400/10000  (74.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.53it/s]\n","iter 1550000: train loss 1.4220, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.08it/s]\n","iter 1552000: train loss 1.4218, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7268/10000  (72.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.82it/s]\n","iter 1554000: train loss 1.4271, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7159/10000  (71.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.44it/s]\n","iter 1556000: train loss 1.4249, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7467/10000  (74.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.95it/s]\n","iter 1558000: train loss 1.4272, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7185/10000  (71.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.75it/s]\n","iter 1560000: train loss 1.4250, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7169/10000  (71.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.85it/s]\n","iter 1562000: train loss 1.4255, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7206/10000  (72.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.37it/s]\n","iter 1564000: train loss 1.4227, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.66it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7429/10000  (74.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.33it/s]\n","iter 1566000: train loss 1.4204, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7146/10000  (71.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1568000: train loss 1.4240, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.63it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7601/10000  (76.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.75it/s]\n","iter 1570000: train loss 1.4221, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7338/10000  (73.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.77it/s]\n","iter 1572000: train loss 1.4242, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7259/10000  (72.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.28it/s]\n","iter 1574000: train loss 1.4213, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7359/10000  (73.59%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.66it/s]\n","iter 1576000: train loss 1.4220, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7320/10000  (73.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.49it/s]\n","iter 1578000: train loss 1.4213, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7182/10000  (71.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.01it/s]\n","iter 1580000: train loss 1.4239, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7293/10000  (72.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.83it/s]\n","iter 1582000: train loss 1.4303, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7263/10000  (72.63%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.69it/s]\n","iter 1584000: train loss 1.4247, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7383/10000  (73.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.19it/s]\n","iter 1586000: train loss 1.4266, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.33it/s]\n","iter 1588000: train loss 1.4194, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7329/10000  (73.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.29it/s]\n","iter 1590000: train loss 1.4266, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7486/10000  (74.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.97it/s]\n","iter 1592000: train loss 1.4200, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7339/10000  (73.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.37it/s]\n","iter 1594000: train loss 1.4223, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7466/10000  (74.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.66it/s]\n","iter 1596000: train loss 1.4213, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7369/10000  (73.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.09it/s]\n","iter 1598000: train loss 1.4248, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.07it/s]\n","iter 1600000: train loss 1.4276, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7395/10000  (73.95%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.59it/s]\n","iter 1602000: train loss 1.4243, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7252/10000  (72.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.44it/s]\n","iter 1604000: train loss 1.4257, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7279/10000  (72.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.56it/s]\n","iter 1606000: train loss 1.4200, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7414/10000  (74.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.47it/s]\n","iter 1608000: train loss 1.4246, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7214/10000  (72.14%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.16it/s]\n","iter 1610000: train loss 1.4212, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.30it/s]\n","iter 1612000: train loss 1.4229, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7246/10000  (72.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.25it/s]\n","iter 1614000: train loss 1.4240, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.91it/s]\n","iter 1616000: train loss 1.4244, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7425/10000  (74.25%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.05it/s]\n","iter 1618000: train loss 1.4256, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7409/10000  (74.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.08it/s]\n","iter 1620000: train loss 1.4229, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7355/10000  (73.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.26it/s]\n","iter 1622000: train loss 1.4265, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7284/10000  (72.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.29it/s]\n","iter 1624000: train loss 1.4249, val loss 1.4266\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.96it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7393/10000  (73.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.49it/s]\n","iter 1626000: train loss 1.4206, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7281/10000  (72.81%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.70it/s]\n","iter 1628000: train loss 1.4214, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.21it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7234/10000  (72.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.33it/s]\n","iter 1630000: train loss 1.4282, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7480/10000  (74.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.06it/s]\n","iter 1632000: train loss 1.4253, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7470/10000  (74.70%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.60it/s]\n","iter 1634000: train loss 1.4197, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.28it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7455/10000  (74.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.45it/s]\n","iter 1636000: train loss 1.4271, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7315/10000  (73.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.68it/s]\n","iter 1638000: train loss 1.4229, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7303/10000  (73.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.63it/s]\n","iter 1640000: train loss 1.4215, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.61it/s]\n","iter 1642000: train loss 1.4255, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7241/10000  (72.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.48it/s]\n","iter 1644000: train loss 1.4191, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7194/10000  (71.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.95it/s]\n","iter 1646000: train loss 1.4282, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.56it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7093/10000  (70.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.60it/s]\n","iter 1648000: train loss 1.4240, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7268/10000  (72.68%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.96it/s]\n","iter 1650000: train loss 1.4244, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7396/10000  (73.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.64it/s]\n","iter 1652000: train loss 1.4216, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7249/10000  (72.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.07it/s]\n","iter 1654000: train loss 1.4239, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7286/10000  (72.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.61it/s]\n","iter 1656000: train loss 1.4244, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7393/10000  (73.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.83it/s]\n","iter 1658000: train loss 1.4193, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.79it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7296/10000  (72.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.16it/s]\n","iter 1660000: train loss 1.4252, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7258/10000  (72.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1662000: train loss 1.4258, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7202/10000  (72.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.55it/s]\n","iter 1664000: train loss 1.4262, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7201/10000  (72.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.10it/s]\n","iter 1666000: train loss 1.4222, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7311/10000  (73.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.78it/s]\n","iter 1668000: train loss 1.4234, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.74it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7285/10000  (72.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.24it/s]\n","iter 1670000: train loss 1.4174, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7187/10000  (71.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.84it/s]\n","iter 1672000: train loss 1.4254, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.02it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7242/10000  (72.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.29it/s]\n","iter 1674000: train loss 1.4258, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7331/10000  (73.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.59it/s]\n","iter 1676000: train loss 1.4221, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7366/10000  (73.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.32it/s]\n","iter 1678000: train loss 1.4250, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7555/10000  (75.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.57it/s]\n","iter 1680000: train loss 1.4216, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.82it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7315/10000  (73.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.06it/s]\n","iter 1682000: train loss 1.4264, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7261/10000  (72.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1684000: train loss 1.4211, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7267/10000  (72.67%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.26it/s]\n","iter 1686000: train loss 1.4172, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7328/10000  (73.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.50it/s]\n","iter 1688000: train loss 1.4219, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1690000: train loss 1.4192, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7280/10000  (72.80%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.67it/s]\n","iter 1692000: train loss 1.4199, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7390/10000  (73.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.75it/s]\n","iter 1694000: train loss 1.4192, val loss 1.4268\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7294/10000  (72.94%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1696000: train loss 1.4215, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7454/10000  (74.54%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.54it/s]\n","iter 1698000: train loss 1.4245, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7457/10000  (74.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.96it/s]\n","iter 1700000: train loss 1.4236, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.67it/s]\n","iter 1702000: train loss 1.4232, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7384/10000  (73.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.88it/s]\n","iter 1704000: train loss 1.4220, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7416/10000  (74.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.54it/s]\n","iter 1706000: train loss 1.4213, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.36it/s]\n","iter 1708000: train loss 1.4226, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7338/10000  (73.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.96it/s]\n","iter 1710000: train loss 1.4267, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7253/10000  (72.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.49it/s]\n","iter 1712000: train loss 1.4239, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7157/10000  (71.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.16it/s]\n","iter 1714000: train loss 1.4241, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7419/10000  (74.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.83it/s]\n","iter 1716000: train loss 1.4255, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7313/10000  (73.13%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.21it/s]\n","iter 1718000: train loss 1.4230, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7330/10000  (73.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.33it/s]\n","iter 1720000: train loss 1.4239, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7248/10000  (72.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.91it/s]\n","iter 1722000: train loss 1.4255, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7329/10000  (73.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.46it/s]\n","iter 1724000: train loss 1.4264, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7229/10000  (72.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.17it/s]\n","iter 1726000: train loss 1.4276, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.18it/s]\n","iter 1728000: train loss 1.4247, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7234/10000  (72.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.62it/s]\n","iter 1730000: train loss 1.4228, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7300/10000  (73.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.79it/s]\n","iter 1732000: train loss 1.4223, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7505/10000  (75.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.43it/s]\n","iter 1734000: train loss 1.4203, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7319/10000  (73.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.19it/s]\n","iter 1736000: train loss 1.4259, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7307/10000  (73.07%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.12it/s]\n","iter 1738000: train loss 1.4274, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.67it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.99it/s]\n","iter 1740000: train loss 1.4256, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7402/10000  (74.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.64it/s]\n","iter 1742000: train loss 1.4208, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.30it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7440/10000  (74.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.51it/s]\n","iter 1744000: train loss 1.4218, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7315/10000  (73.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.83it/s]\n","iter 1746000: train loss 1.4238, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.14it/s]\n","iter 1748000: train loss 1.4243, val loss 1.4267\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.08it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7373/10000  (73.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.16it/s]\n","iter 1750000: train loss 1.4272, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7411/10000  (74.11%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.66it/s]\n","iter 1752000: train loss 1.4264, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7343/10000  (73.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.90it/s]\n","iter 1754000: train loss 1.4240, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.93it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7249/10000  (72.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 1756000: train loss 1.4281, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7437/10000  (74.37%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.63it/s]\n","iter 1758000: train loss 1.4220, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.70it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7428/10000  (74.28%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.51it/s]\n","iter 1760000: train loss 1.4228, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7392/10000  (73.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.72it/s]\n","iter 1762000: train loss 1.4233, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7218/10000  (72.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.96it/s]\n","iter 1764000: train loss 1.4246, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7186/10000  (71.86%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.60it/s]\n","iter 1766000: train loss 1.4251, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7319/10000  (73.19%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.05it/s]\n","iter 1768000: train loss 1.4268, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7235/10000  (72.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.34it/s]\n","iter 1770000: train loss 1.4223, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7164/10000  (71.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.82it/s]\n","iter 1772000: train loss 1.4239, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7372/10000  (73.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.39it/s]\n","iter 1774000: train loss 1.4272, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7388/10000  (73.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.59it/s]\n","iter 1776000: train loss 1.4291, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7145/10000  (71.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.07it/s]\n","iter 1778000: train loss 1.4284, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.98it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7353/10000  (73.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.76it/s]\n","iter 1780000: train loss 1.4269, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7424/10000  (74.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.95it/s]\n","iter 1782000: train loss 1.4205, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7264/10000  (72.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.70it/s]\n","iter 1784000: train loss 1.4232, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7297/10000  (72.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.77it/s]\n","iter 1786000: train loss 1.4234, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.42it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7477/10000  (74.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.54it/s]\n","iter 1788000: train loss 1.4318, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7153/10000  (71.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.33it/s]\n","iter 1790000: train loss 1.4278, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.60it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7353/10000  (73.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.33it/s]\n","iter 1792000: train loss 1.4221, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7435/10000  (74.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.44it/s]\n","iter 1794000: train loss 1.4216, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.48it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7385/10000  (73.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.70it/s]\n","iter 1796000: train loss 1.4274, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.90it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7164/10000  (71.64%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.80it/s]\n","iter 1798000: train loss 1.4213, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7365/10000  (73.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.03it/s]\n","iter 1800000: train loss 1.4192, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7365/10000  (73.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.30it/s]\n","iter 1802000: train loss 1.4237, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7290/10000  (72.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.16it/s]\n","iter 1804000: train loss 1.4268, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7269/10000  (72.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.72it/s]\n","iter 1806000: train loss 1.4215, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7188/10000  (71.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.37it/s]\n","iter 1808000: train loss 1.4306, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.91it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7312/10000  (73.12%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.54it/s]\n","iter 1810000: train loss 1.4235, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7220/10000  (72.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.67it/s]\n","iter 1812000: train loss 1.4274, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.51it/s]\n","iter 1814000: train loss 1.4226, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7500/10000  (75.00%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.24it/s]\n","iter 1816000: train loss 1.4239, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7320/10000  (73.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.27it/s]\n","iter 1818000: train loss 1.4246, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.39it/s]\n","iter 1820000: train loss 1.4206, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.54it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7330/10000  (73.30%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.32it/s]\n","iter 1822000: train loss 1.4238, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7241/10000  (72.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.42it/s]\n","iter 1824000: train loss 1.4265, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7275/10000  (72.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.67it/s]\n","iter 1826000: train loss 1.4227, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.24it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7403/10000  (74.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.44it/s]\n","iter 1828000: train loss 1.4208, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7348/10000  (73.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.41it/s]\n","iter 1830000: train loss 1.4273, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 20.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7283/10000  (72.83%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.56it/s]\n","iter 1832000: train loss 1.4211, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7269/10000  (72.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 20.28it/s]\n","iter 1834000: train loss 1.4198, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.40it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7357/10000  (73.57%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.88it/s]\n","iter 1836000: train loss 1.4204, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7358/10000  (73.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.53it/s]\n","iter 1838000: train loss 1.4211, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.64it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7258/10000  (72.58%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.48it/s]\n","iter 1840000: train loss 1.4258, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7371/10000  (73.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.79it/s]\n","iter 1842000: train loss 1.4259, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7371/10000  (73.71%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.37it/s]\n","iter 1844000: train loss 1.4203, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.77it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7398/10000  (73.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.21it/s]\n","iter 1846000: train loss 1.4208, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7353/10000  (73.53%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.60it/s]\n","iter 1848000: train loss 1.4275, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7327/10000  (73.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.36it/s]\n","iter 1850000: train loss 1.4201, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7436/10000  (74.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.46it/s]\n","iter 1852000: train loss 1.4247, val loss 1.4283\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7435/10000  (74.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.34it/s]\n","iter 1854000: train loss 1.4237, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7318/10000  (73.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.56it/s]\n","iter 1856000: train loss 1.4235, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7479/10000  (74.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.65it/s]\n","iter 1858000: train loss 1.4212, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7388/10000  (73.88%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.50it/s]\n","iter 1860000: train loss 1.4218, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7485/10000  (74.85%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.76it/s]\n","iter 1862000: train loss 1.4194, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7347/10000  (73.47%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.97it/s]\n","iter 1864000: train loss 1.4266, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.58it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7379/10000  (73.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.56it/s]\n","iter 1866000: train loss 1.4225, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.55it/s]\n","iter 1868000: train loss 1.4260, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.76it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7396/10000  (73.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.48it/s]\n","iter 1870000: train loss 1.4249, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.36it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7222/10000  (72.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.52it/s]\n","iter 1872000: train loss 1.4313, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7397/10000  (73.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.57it/s]\n","iter 1874000: train loss 1.4241, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7348/10000  (73.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.51it/s]\n","iter 1876000: train loss 1.4255, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.50it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7216/10000  (72.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.12it/s]\n","iter 1878000: train loss 1.4290, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.99it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7339/10000  (73.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.26it/s]\n","iter 1880000: train loss 1.4263, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 20.95it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7302/10000  (73.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.94it/s]\n","iter 1882000: train loss 1.4244, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7150/10000  (71.50%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.22it/s]\n","iter 1884000: train loss 1.4196, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.93it/s]\n","iter 1886000: train loss 1.4232, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7287/10000  (72.87%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.49it/s]\n","iter 1888000: train loss 1.4243, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7438/10000  (74.38%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.47it/s]\n","iter 1890000: train loss 1.4241, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7261/10000  (72.61%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.51it/s]\n","iter 1892000: train loss 1.4196, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7115/10000  (71.15%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.44it/s]\n","iter 1894000: train loss 1.4239, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7416/10000  (74.16%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.73it/s]\n","iter 1896000: train loss 1.4213, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7240/10000  (72.40%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.83it/s]\n","iter 1898000: train loss 1.4258, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.61it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7418/10000  (74.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.88it/s]\n","iter 1900000: train loss 1.4185, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7405/10000  (74.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 20.85it/s]\n","iter 1902000: train loss 1.4164, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7431/10000  (74.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.11it/s]\n","iter 1904000: train loss 1.4261, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7229/10000  (72.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.30it/s]\n","iter 1906000: train loss 1.4242, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.25it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7452/10000  (74.52%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.38it/s]\n","iter 1908000: train loss 1.4232, val loss 1.4266\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7246/10000  (72.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.68it/s]\n","iter 1910000: train loss 1.4282, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.19it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7382/10000  (73.82%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.57it/s]\n","iter 1912000: train loss 1.4207, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.62it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7421/10000  (74.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.62it/s]\n","iter 1914000: train loss 1.4257, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.16it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7323/10000  (73.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.00it/s]\n","iter 1916000: train loss 1.4219, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.68it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7391/10000  (73.91%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.08it/s]\n","iter 1918000: train loss 1.4253, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7248/10000  (72.48%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.34it/s]\n","iter 1920000: train loss 1.4238, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.88it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7242/10000  (72.42%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.12it/s]\n","iter 1922000: train loss 1.4238, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.44it/s]\n","iter 1924000: train loss 1.4242, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.04it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7278/10000  (72.78%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.08it/s]\n","iter 1926000: train loss 1.4251, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.65it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7175/10000  (71.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.62it/s]\n","iter 1928000: train loss 1.4277, val loss 1.4282\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.89it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7272/10000  (72.72%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.73it/s]\n","iter 1930000: train loss 1.4271, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7369/10000  (73.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.05it/s]\n","iter 1932000: train loss 1.4237, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7279/10000  (72.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.03it/s]\n","iter 1934000: train loss 1.4257, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7275/10000  (72.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.48it/s]\n","iter 1936000: train loss 1.4219, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.07it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7196/10000  (71.96%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.14it/s]\n","iter 1938000: train loss 1.4250, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7203/10000  (72.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.86it/s]\n","iter 1940000: train loss 1.4199, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.75it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7265/10000  (72.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.65it/s]\n","iter 1942000: train loss 1.4278, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.71it/s]\n","iter 1944000: train loss 1.4197, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.49it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7365/10000  (73.65%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.20it/s]\n","iter 1946000: train loss 1.4218, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.14it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7160/10000  (71.60%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.73it/s]\n","iter 1948000: train loss 1.4166, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7269/10000  (72.69%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.45it/s]\n","iter 1950000: train loss 1.4270, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.41it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7426/10000  (74.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.06it/s]\n","iter 1952000: train loss 1.4249, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.10it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7526/10000  (75.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.72it/s]\n","iter 1954000: train loss 1.4247, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.20it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7397/10000  (73.97%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.35it/s]\n","iter 1956000: train loss 1.4215, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7335/10000  (73.35%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.42it/s]\n","iter 1958000: train loss 1.4278, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.72it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7079/10000  (70.79%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.81it/s]\n","iter 1960000: train loss 1.4210, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.23it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.53it/s]\n","iter 1962000: train loss 1.4198, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.81it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7303/10000  (73.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.59it/s]\n","iter 1964000: train loss 1.4314, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.38it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7393/10000  (73.93%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.38it/s]\n","iter 1966000: train loss 1.4254, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7420/10000  (74.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.72it/s]\n","iter 1968000: train loss 1.4201, val loss 1.4281\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7226/10000  (72.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.62it/s]\n","iter 1970000: train loss 1.4183, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.45it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7526/10000  (75.26%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.26it/s]\n","iter 1972000: train loss 1.4208, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.05it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7332/10000  (73.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.54it/s]\n","iter 1974000: train loss 1.4226, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.70it/s]\n","iter 1976000: train loss 1.4276, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.32it/s]\n","iter 1978000: train loss 1.4253, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.35it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7218/10000  (72.18%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.30it/s]\n","iter 1980000: train loss 1.4242, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.26it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7299/10000  (72.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.42it/s]\n","iter 1982000: train loss 1.4238, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.15it/s]\n","iter 1984000: train loss 1.4229, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.59it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7298/10000  (72.98%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.89it/s]\n","iter 1986000: train loss 1.4285, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7303/10000  (73.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.84it/s]\n","iter 1988000: train loss 1.4255, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.18it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7221/10000  (72.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.88it/s]\n","iter 1990000: train loss 1.4240, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.15it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7374/10000  (73.74%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.41it/s]\n","iter 1992000: train loss 1.4229, val loss 1.4276\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.31it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7202/10000  (72.02%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.57it/s]\n","iter 1994000: train loss 1.4284, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7232/10000  (72.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.53it/s]\n","iter 1996000: train loss 1.4261, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 20.46it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7309/10000  (73.09%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.60it/s]\n","iter 1998000: train loss 1.4220, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7336/10000  (73.36%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.07it/s]\n","iter 2000000: train loss 1.4254, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7423/10000  (74.23%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.90it/s]\n","iter 2002000: train loss 1.4262, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.55it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7375/10000  (73.75%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.53it/s]\n","iter 2004000: train loss 1.4249, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.51it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7345/10000  (73.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.73it/s]\n","iter 2006000: train loss 1.4246, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7201/10000  (72.01%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.96it/s]\n","iter 2008000: train loss 1.4295, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.43it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7476/10000  (74.76%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.97it/s]\n","iter 2010000: train loss 1.4195, val loss 1.4280\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7306/10000  (73.06%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.99it/s]\n","iter 2012000: train loss 1.4246, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.47it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7410/10000  (74.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.86it/s]\n","iter 2014000: train loss 1.4223, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.00it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7373/10000  (73.73%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.90it/s]\n","iter 2016000: train loss 1.4200, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.84it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7310/10000  (73.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.08it/s]\n","iter 2018000: train loss 1.4218, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7245/10000  (72.45%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.57it/s]\n","iter 2020000: train loss 1.4248, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.92it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7108/10000  (71.08%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.91it/s]\n","iter 2022000: train loss 1.4251, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.17it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7341/10000  (73.41%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.34it/s]\n","iter 2024000: train loss 1.4232, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7292/10000  (72.92%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.84it/s]\n","iter 2026000: train loss 1.4250, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.37it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7344/10000  (73.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.74it/s]\n","iter 2028000: train loss 1.4233, val loss 1.4269\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.12it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7266/10000  (72.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.05it/s]\n","iter 2030000: train loss 1.4276, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7431/10000  (74.31%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.90it/s]\n","iter 2032000: train loss 1.4238, val loss 1.4273\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.44it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7203/10000  (72.03%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.43it/s]\n","iter 2034000: train loss 1.4248, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.53it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7190/10000  (71.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.04it/s]\n","iter 2036000: train loss 1.4275, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.32it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7243/10000  (72.43%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.23it/s]\n","iter 2038000: train loss 1.4266, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.03it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7334/10000  (73.34%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.75it/s]\n","iter 2040000: train loss 1.4243, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.80it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7227/10000  (72.27%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.39it/s]\n","iter 2042000: train loss 1.4249, val loss 1.4271\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.13it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7329/10000  (73.29%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.86it/s]\n","iter 2044000: train loss 1.4191, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7239/10000  (72.39%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.91it/s]\n","iter 2046000: train loss 1.4223, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7377/10000  (73.77%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.19it/s]\n","iter 2048000: train loss 1.4322, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.85it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7421/10000  (74.21%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.94it/s]\n","iter 2050000: train loss 1.4239, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.71it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7305/10000  (73.05%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.76it/s]\n","iter 2052000: train loss 1.4231, val loss 1.4279\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.09it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7204/10000  (72.04%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.94it/s]\n","iter 2054000: train loss 1.4190, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.94it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7299/10000  (72.99%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 21.65it/s]\n","iter 2056000: train loss 1.4258, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.69it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7346/10000  (73.46%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.94it/s]\n","iter 2058000: train loss 1.4236, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 22.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7244/10000  (72.44%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.62it/s]\n","iter 2060000: train loss 1.4224, val loss 1.4274\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 21.87it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7324/10000  (73.24%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 22.05it/s]\n","iter 2062000: train loss 1.4238, val loss 1.4278\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.57it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7332/10000  (73.32%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.79it/s]\n","iter 2064000: train loss 1.4201, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.01it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7433/10000  (74.33%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.41it/s]\n","iter 2066000: train loss 1.4208, val loss 1.4277\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7190/10000  (71.90%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.53it/s]\n","iter 2068000: train loss 1.4205, val loss 1.4270\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.34it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7220/10000  (72.20%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.39it/s]\n","iter 2070000: train loss 1.4274, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.39it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7455/10000  (74.55%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.93it/s]\n","iter 2072000: train loss 1.4330, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.52it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7366/10000  (73.66%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.99it/s]\n","iter 2074000: train loss 1.4271, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.86it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7084/10000  (70.84%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.54it/s]\n","iter 2076000: train loss 1.4217, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.33it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7210/10000  (72.10%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.03it/s]\n","iter 2078000: train loss 1.4280, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 23.97it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7222/10000  (72.22%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.56it/s]\n","iter 2080000: train loss 1.4182, val loss 1.4272\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.29it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7249/10000  (72.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 23.69it/s]\n","iter 2082000: train loss 1.4297, val loss 1.4275\n","Using precomputed batches\n","100% 80/80 [00:03\u003c00:00, 24.06it/s]\n","\n","Test Results:\n","test_reverse.txt, 10000 examples: 7349/10000  (73.49%)\n","\n","Using precomputed batches\n","100% 81/81 [00:03\u003c00:00, 24.60it/s]\n"]}],"source":["!python train.py 4_operands_addition_reversed.txt --PE RoPE"]},{"cell_type":"markdown","metadata":{"id":"ZxrvGK6Eoebe"},"source":["## Long Multiplication"]},{"cell_type":"markdown","metadata":{"id":"cPspwmWgoh5S"},"source":["### reversed"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1767392816701,"user":{"displayName":"LLMFun Experiment","userId":"06729372927322080600"},"user_tz":360},"id":"RH9iNUA1nF70"},"outputs":[],"source":["! python train.py 40_1_digits_mul_reversed.txt"]},{"cell_type":"markdown","metadata":{"id":"PxLUxyr_u8m-"},"source":["### Greedy Decoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FG_n3Y_YvDr2"},"outputs":[],"source":["!python train.py 2_operands_addition_reversed.txt --greedy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_ThupuUvIyH"},"outputs":[],"source":["!python train.py 2_operands_addition_plain.txt --greedy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UE5ttFeNvBfb"},"outputs":[],"source":["!python train.py 4_operands_addition_reversed.txt --greedy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4yS-i2zvMLT"},"outputs":[],"source":["!python train.py 4_operands_addition_plain.txt --greedy"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}